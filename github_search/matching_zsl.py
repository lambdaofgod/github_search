# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Matching_ZSL.ipynb (unless otherwise specified).

__all__ = ['RepoTaskData', 'get_first_vocab_entry', 'PairedKeyedVectors', 'RetrieverLearner', 'filter_smaller_tasks',
           'prepare_paperswithcode_with_features_df', 'get_outgoing_edges', 'get_repo_functions',
           'prepare_task_train_test_split', 'prepare_graph_repo_task_data', 'maybe_get_ndarray_elem',
           'get_retrieval_results', 'get_retrieval_metrics', 'get_retrieval_accuracy', 'run_learner_experiment',
           'LambdaTransformer', 'PyGGraphModelTransformer', 'get_vertex_embeddings', 'make_pyggraph_retriever_learner',
           'save_pyggraph_retriever_learner', 'get_query_level_results', 'get_idx_or_inf', 'get_areas',
           'analyze_query_level_results', 'get_analyzed_query_level_results', 'erroneous_area_tasks']

# Cell
import os
import ast
import tqdm
import json
import attr
from operator import itemgetter
from scipy.stats import hmean
import logging

import concurrent.futures

import itertools


import pandas as pd
import numpy as np
from sklearn import feature_extraction, metrics, model_selection

import matplotlib.pyplot as plt
import gensim


from functools import partial

from mlutil.feature_extraction import embeddings
import mlutil
from scarce_learn import zero_shot
from scarce_learn.zero_shot import devise_jax, devise_torch
from github_search import paperswithcode_tasks, github_readmes, python_call_graph, data_utils

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Cell

@attr.s
class RepoTaskData:

    tasks = attr.ib()
    repos = attr.ib()
    X = attr.ib()
    all_tasks = attr.ib()
    y = attr.ib()

    def split_tasks(area_grouped_tasks, test_size=0.2):
        tasks_train, tasks_test = model_selection.train_test_split(area_grouped_tasks['task'], stratify=area_grouped_tasks['area'], test_size=test_size, random_state=0)
        return tasks_train, tasks_test

    def create_split(tasks_test, all_tasks, paperswithcode_with_features_df, X_repr, y_col='least_common_task'):
        train_indicator = paperswithcode_with_features_df['tasks'].apply(lambda ts: not(any([t in list(tasks_test) for t in ts])))
        repos_train = paperswithcode_with_features_df['repo'][train_indicator]
        repos_test = paperswithcode_with_features_df['repo'][~train_indicator]
        X_repr = X_repr.apply(lambda x: " ".join(x))
        X_train = X_repr[train_indicator]
        X_test = X_repr[~train_indicator]
        all_tasks_train = all_tasks[train_indicator]
        all_tasks_test = all_tasks[~train_indicator]
        y_train = paperswithcode_with_features_df[train_indicator][y_col].str.lower().apply(clean_task_name)
        y_test = paperswithcode_with_features_df[~train_indicator][y_col].str.lower().apply(clean_task_name)

        return (
            RepoTaskData(tasks_train, repos_train, X_train, all_tasks_train, y_train),
            RepoTaskData(tasks_test, repos_test, X_test, all_tasks_test, y_test)
        )

# Cell


def get_first_vocab_entry(vocab):
    return list(itertools.islice(vocab.items(), 1))[0][0]


class PairedKeyedVectors:

    @attr.s
    class wv:
        vocab = attr.ib()

    def __init__(self, kv1, kv2):
        self.kv1 = kv1
        self.kv2 = kv2
        self.vocab = {**kv1.vocab, **kv2.vocab}
        self.dim1 = len(kv1[get_first_vocab_entry(kv1.vocab)])
        self.dim2 = len(kv2[get_first_vocab_entry(kv2.vocab)])
        self.wv= PairedKeyedVectors.wv(self.vocab)

    def __getitem__(self, item):
        if not item in self.kv1.vocab.keys():
            return np.concatenate([np.zeros(self.dim1), self.kv2[item]])
        elif not item in self.kv2.vocab.keys():
            return np.concatenate([self.kv1[item], np.zeros(self.dim2)])
        else:
            return np.concatenate([self.kv1[item], self.kv2[item]])


@attr.s
class RetrieverLearner:

    zs_learner: zero_shot.ZeroShotClassifier = attr.ib()
    input_embedder: embeddings.EmbeddingVectorizer = attr.ib()
    y_embedder: embeddings.EmbeddingVectorizer = attr.ib()
    input_embedder_kwargs = attr.ib(default=dict())

    @staticmethod
    def create(
        zs_learner: zero_shot.ZeroShotClassifier,
        input_embeddings: gensim.models.KeyedVectors,
        target_embeddings: gensim.models.KeyedVectors,
        input_embedding_method: embeddings.EmbeddingVectorizer,
        y_embedding_method: embeddings.EmbeddingVectorizer,
        input_embedder_kwargs=dict()
    ):
        input_embedder = input_embedding_method(input_embeddings, **input_embedder_kwargs)
        y_embedder = y_embedding_method(target_embeddings)
        return RetrieverLearner(zs_learner, input_embedder, y_embedder)

    def get_target_embeddings(self, y):
        unique_y = pd.Series(y.unique())
        y_embeddings = self.y_embedder.transform(unique_y)
        return unique_y, y_embeddings

    def fit_learner(self, data, **kwargs):
        self.input_embedder.fit(data.X)
        X_embeddings = self.input_embedder.transform(data.X)
        self.y_embedder.fit(data.y)
        unique_y, y_embeddings = self.get_target_embeddings(data.y)
        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])
        self.zs_learner.fit(np.array(X_embeddings), np.array(input_y_idxs), np.array(y_embeddings), **kwargs)

    def predict_idxs(self, X, y_embeddings):
        X_embeddings = self.input_embedder.transform(X)
        return self.zs_learner.predict(X_embeddings, y_embeddings)

    def predict_topk(self, X, y_embeddings, target_names, k=5, similarity=metrics.pairwise.cosine_similarity):
        X_embeddings = self.input_embedder.transform(X)
        predictions = self.zs_learner.predict_raw(X_embeddings)
        target_similarities = similarity(predictions, y_embeddings)
        targets = [target_names[row[:k]] for row in (-target_similarities).argsort(axis=1)]
        return targets

    def evaluate(self, data, metric):
        unique_y, y_embeddings = self.get_target_embeddings(data.y)
        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])
        predicted_idxs = self.predict_idxs(data.X, y_embeddings)
        return metric(input_y_idxs, predicted_idxs)

# Cell


def filter_smaller_tasks(paperswithcode_with_tasks_df, min_task_count=10):
    task_counts = paperswithcode_with_tasks_df['least_common_task'].value_counts()
    return paperswithcode_with_tasks_df[paperswithcode_with_tasks_df['least_common_task'].isin(task_counts[task_counts >= min_task_count].index)]


def prepare_paperswithcode_with_features_df(paperswithcode_with_tasks_df, dependency_records_df, min_task_count):
    paperswithcode_with_features_df = paperswithcode_with_tasks_df[
        paperswithcode_with_tasks_df['repo'].isin(graph.get_vertex_dataframe()['name']) |
        paperswithcode_with_tasks_df['repo'].apply(lambda s: s.split("/")[1]).isin(graph.get_vertex_dataframe()['name'])
    ]
    paperswithcode_with_features_df = paperswithcode_with_features_df.dropna(subset=['readme', 'abstract'])
    tasks = paperswithcode_with_features_df['least_common_task'].str.lower()

    per_repo_dependency_records = data_utils.get_repo_records(paperswithcode_with_features_df['repo'], dependency_records_df)
    per_repo_dependency_records= per_repo_dependency_records.reset_index()
    per_repo_dependency_records.columns = ['source', 'dependency_records']
    paperswithcode_with_features_df = paperswithcode_with_features_df.merge(per_repo_dependency_records.reset_index(), left_on='repo', right_on='source')
    all_tasks = paperswithcode_with_features_df['tasks']
    is_valid_record = (all_tasks.apply(len) > 0)
    paperswithcode_with_features_df = filter_smaller_tasks(paperswithcode_with_features_df[is_valid_record], min_task_count)
    all_tasks = paperswithcode_with_features_df['tasks']
    all_task_counts = all_tasks.explode().value_counts()
    valid_tasks = all_task_counts[all_task_counts >= min_task_count].index
    paperswithcode_with_features_df['tasks'] = paperswithcode_with_features_df['tasks'].apply(lambda ts: [t for t in ts if t in valid_tasks] )
    return paperswithcode_with_features_df

# Cell


def get_outgoing_edges(graph, node):
    #idx = pd.Index(graph.names).get_loc(node)
    #outgoing_edges_idx = np.where(graph.mat[idx].todense())[1]
    return graph.get_vertex_dataframe().iloc[graph.successors(node)]['name']
    #return graph.names[outgoing_edges_idx]


def get_repo_functions(graph_records, repo):
    return ' '.join(set(get_outgoing_edges(graph, repo).values)).replace(repo + ":", "")

# Cell


def prepare_task_train_test_split(upstream, product):
    area_grouped_tasks = pd.read_csv(str(upstream['prepare_area_grouped_tasks']))
    tasks_train, tasks_test = RepoTaskData.split_tasks(area_grouped_tasks)
    tasks_train.to_csv(product['train'], index=None)
    tasks_test.to_csv(product['test'], index=None)


def prepare_graph_repo_task_data(upstream, product):
    graph_data_train, graph_data_test = RepoTaskData.create_split(tasks_train, all_tasks, paperswithcode_with_features_df, paperswithcode_with_imports_df['imports'])
    graph_data_train.X = graph_data_train.repos.apply(lambda x: get_repo_functions(graph, x))
    graph_data_test.X = graph_data_test.repos.apply(lambda x: get_repo_functions(graph, x))
    pickle.dump((graph_data_train, graph_data_test), open(str(product), "wb"))

# Cell
def maybe_get_ndarray_elem(arr, idx, default=-1):
    if len(arr) <= idx:
        return default
    else:
        return arr[idx]


def get_retrieval_results(learner, data, queried_tasks, k=10, similarity=metrics.pairwise.cosine_similarity):
    if queried_tasks == 'all':
        tasks = data.all_tasks.explode().drop_duplicates()
    elif queried_tasks == 'target':
        tasks = data.y.drop_duplicates()
    else:
        tasks = queried_tasks
    y_names, __ = learner.get_target_embeddings(tasks)
    input_embeddings = learner.input_embedder.transform(data.X)
    y_embeddings = learner.y_embedder.transform(y_names)
    predictions = learner.zs_learner.predict_raw(input_embeddings)
    input_target_similarities = similarity(predictions, y_embeddings)

    X_recalled = [
        np.argsort(-input_target_similarities[:,y_idx])[:k]
        for (y_idx, __) in enumerate(y_names)
    ]
    return y_names, X_recalled


def get_retrieval_metrics(learner, data, k=10, similarity=metrics.pairwise.cosine_similarity, queried_tasks='all'):
    y_names, retrieved_X = get_retrieval_results(learner, data, k=k, similarity=similarity, queried_tasks=queried_tasks)
    retrieved_X_actual_labels = [data.all_tasks.iloc[idxs_recalled].explode().values for idxs_recalled in retrieved_X]
    retrieved_idxs = [
        np.where(retrieved_X_actual_labels[y_idx] == y_name)[0]
        for (y_idx, y_name) in enumerate(y_names)
    ]
    num_recalled = [len(r) for r in retrieved_idxs]
    pos_recalled = [maybe_get_ndarray_elem(r, 0) for r in retrieved_idxs]
    accurately_recalled = [r > -1 for r in pos_recalled]
    return pd.DataFrame({"retrieved_labels": retrieved_X_actual_labels, "num_recalled": num_recalled, "recalled": accurately_recalled, "position": pos_recalled}, index=y_names)


def get_retrieval_accuracy(learner, data, k=10, similarity=metrics.pairwise.cosine_similarity, queried_tasks=None):
    return np.mean(get_retrieval_metrics(learner, data, k, similarity, queried_tasks)['recalled'])

# Cell


def run_learner_experiment(
    retriever_learner,
    data_train, data_test,
    queried_tasks='all',
    fit_learner=True
):
    if fit_learner:
        retriever_learner.fit_learner(data_train)

    accuracy_train = retriever_learner.evaluate(data_train, metrics.accuracy_score)
    accuracy_test = retriever_learner.evaluate(data_test, metrics.accuracy_score)
    top10_accuracy_train = get_retrieval_accuracy(retriever_learner, data_train, queried_tasks=queried_tasks, k=10)
    top10_accuracy_test = get_retrieval_accuracy(retriever_learner, data_test, queried_tasks=queried_tasks, k=10)

    return dict(
        accuracy_train=accuracy_train,
        accuracy_test=accuracy_test,
        top10_accuracy_train=top10_accuracy_train,
        top10_accuracy_test=top10_accuracy_test
    )

# Cell


class LambdaTransformer:

    def __init__(self, transform_fn):
        self.transform = transform_fn

    def fit(self, X, **kwargs):
        return self


class PyGGraphModelTransformer:

    def __init__(self, model, dependency_graph_wrapper):
        self.model = model
        self.dependency_graph_wrapper = dependency_graph_wrapper

    def transform(self, x):
        return self.dependency_graph_wrapper.get_vertex_embeddings(x, self.model)

    def fit(self, X, **kwargs):
        return self

# Cell


def get_vertex_embeddings(wrapper, vertex_subset, model):
    features = (
        model.full_forward(
            wrapper.dataset.x, wrapper.dataset.edge_index
        )
        .cpu()
        .detach()
        .numpy()
    )
    return features[wrapper.vertex_mapping.loc[vertex_subset]]

# Cell
import pathlib



def make_pyggraph_retriever_learner(zs_learner, dependency_graph_wrapper, model, y_embedder):

    lambda_transformer = PyGGraphModelTransformer(model, dependency_graph_wrapper)
    return RetrieverLearner(
        zs_learner,
        lambda_transformer,
        y_embedder
    )


def save_pyggraph_retriever_learner(pyggraph_retriever_learner, directory):
    p = pathlib.Path(directory)
    p.mkdir(exist_ok=True)

# Cell


def get_query_level_results(
    retriever_learner,
    data_test,
    k=10
):

    accuracy_test = retriever_learner.evaluate(data_test, metrics.accuracy_score)
    results = get_retrieval_metrics(retriever_learner, data_test, k=k)
    results['position'] = results['position'].replace(-1, np.inf)

    return accuracy_test, results

# Cell
def get_idx_or_inf(xs, a):
    idxs = np.where(xs == a)[0].astype(int)
    if len(idxs) == 0:
        return np.inf
    else:
        return idxs[0]


def get_areas(area_grouped_tasks, tasks):
    return tasks.apply(
        lambda ts:
        area_grouped_tasks['area'][
            area_grouped_tasks['task'].isin(ts)
        ].unique()
    )

erroneous_area_tasks = []


def analyze_query_level_results(query_level_results, area_grouped_tasks, erroneous_area_tasks=erroneous_area_tasks):
    retrieval_results_with_area_test = area_grouped_tasks.merge(query_level_results, left_on='task', right_index=True)
    for tasks in retrieval_results_with_area_test['retrieved_labels'].values:
        for task in tasks:
            try:
                partial(get_areas, area_grouped_tasks)(pd.Series([[task]]))
            except:
                erroneous_area_tasks.append(task)
    retrieved_areas = get_areas(area_grouped_tasks, retrieval_results_with_area_test['retrieved_labels'])#apply(partial(get_areas, area_grouped_tasks))
    retrieval_results_with_area_test['retrieved_areas'] = retrieved_areas
    is_area_retrieved = retrieval_results_with_area_test.apply(lambda row: row['area'] in row['retrieved_areas'][:10], axis=1)
    num_area_retrieved = retrieval_results_with_area_test.apply(lambda row: len(np.where(row['area'] == np.array(row['retrieved_areas'])[:10])[0]), axis=1)
    area_idx = retrieval_results_with_area_test.apply(lambda row: get_idx_or_inf(np.array(row['retrieved_areas']), row['area']), axis=1)
    retrieval_results_with_area_test['area_recalled'] = is_area_retrieved
    retrieval_results_with_area_test['area_recalled_position'] = area_idx
    retrieval_results_with_area_test['num_area_recalled'] = num_area_retrieved
    query_level_results = retrieval_results_with_area_test.groupby('area').agg(
        {
            "recalled": "mean",
            "num_recalled": "mean",
            #"position": ["median", "mean"],
            "area_recalled": "mean",
            "num_area_recalled": "mean",
            "area_recalled_position": ["median"],
        }
    )
    query_level_results['count'] = retrieval_results_with_area_test['area'].value_counts()#groupby('area').agg('count')
    return query_level_results

def get_analyzed_query_level_results(retriever_learner, data_test, area_grouped_tasks):
    detailed_results_all = get_query_level_results(retriever_learner, data_test)
    retrieval_results_with_area_test = analyze_query_level_results(query_level_results, area_grouped_tasks)
    return retrieval_results_with_area_test
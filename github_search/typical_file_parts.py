# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Code_Prototypes.ipynb (unless otherwise specified).

__all__ = [
    "select_class_names",
    "select_function_names",
    "select_lines",
    "tokenize_snakecase",
    "tokenize_camelcase",
    "tokenize_python",
    "get_file_variable_token_set",
    "maybe_get_file_variable_token_string",
    "PYTHON_KEYWORDS",
    "get_selected_lines_and_repos",
]

import numpy as np

# Cell
import pandas as pd


# Cell


def select_class_names(lines):
    return [
        line.strip()
        for line in lines
        if line.lstrip().startswith("class ") and line.rstrip().endswith(":")
    ]


def select_function_names(lines):
    return [line.strip() for line in lines if line.lstrip().startswith("def ")]


def select_lines(text, use_function_names=True, use_class_names=False):
    lines = text.split("\n")
    selected_lines = []
    if use_class_names:
        selected_lines = selected_lines + select_class_names(lines)
    if use_function_names:
        selected_lines = selected_lines + select_function_names(lines)

    return selected_lines


# Cell
import io
import keyword
import re
import tokenize

PYTHON_KEYWORDS = set(keyword.kwlist)


def tokenize_snakecase(identifier):
    return identifier.split("_")


def tokenize_camelcase(identifier):
    matches = re.finditer(
        ".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)", identifier
    )
    return [m.group(0) for m in matches]


def tokenize_python(identifier, lowercase=False):
    if "_" in identifier:
        tokens = tokenize_snakecase(identifier)
    else:
        tokens = tokenize_camelcase(identifier)
    return [t.lower() for t in tokens]


def get_file_variable_token_set(file_text, min_token_length=2, lowercase=True):
    token_infos = list(tokenize.generate_tokens(io.StringIO(file_text).readline))
    raw_tokens = [t.string for t in token_infos if t.type == 1]
    all_tokens = (tokenize_python(t, lowercase) for t in raw_tokens)
    all_tokens = [
        token
        for tokens in all_tokens
        for token in tokens
        if len(token) > min_token_length and not token in PYTHON_KEYWORDS
    ]
    return set(all_tokens)


def maybe_get_file_variable_token_string(file_text, min_token_length=2):
    try:
        tokens = get_file_variable_token_set(file_text)
    except:
        return None
    return " ".join(tokens)


# Cell


def get_selected_lines_and_repos(repos, file_contents):

    selected_lines_by_repo = {
        repo: np.unique(select_lines(" ".join(file_contents[repos == repo].dropna())))
        for repo in repos.unique()
    }
    selected_lines_by_repo = {
        repo: lines
        for (repo, lines) in selected_lines_by_repo.items()
        if len(lines) > 0
    }
    line_repos = [
        k for k in selected_lines_by_repo.keys() for __ in selected_lines_by_repo[k]
    ]
    all_selected_lines = [
        line for lines in selected_lines_by_repo.values() for line in lines
    ]
    return pd.DataFrame({"repo": line_repos, "line": all_selected_lines})

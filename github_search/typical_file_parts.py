# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Code_Prototypes.ipynb (unless otherwise specified).

__all__ = ['select_class_names', 'select_function_names', 'select_lines', 'tokenize_snakecase', 'tokenize_camelcase',
           'tokenize_python', 'get_file_variable_token_set', 'maybe_get_file_variable_token_string', 'PYTHON_KEYWORDS',
           'get_selected_lines_and_repos']

# Cell
import pandas as pd
import numpy as np
from github_search import python_tokens
from mlutil.feature_extraction import embeddings
from mlutil import prototype_selection

# Cell


def select_class_names(lines):
    return [line.strip() for line in lines if line.lstrip().startswith('class ') and line.rstrip().endswith(':')]


def select_function_names(lines):
    return [line.strip() for line in lines if line.lstrip().startswith('def ')]


def select_lines(text):
    lines = text.split('\n')
    return select_function_names(lines) + select_class_names(lines)

# Cell
import io
import tokenize
import keyword
import re


PYTHON_KEYWORDS = set(keyword.kwlist)


def tokenize_snakecase(identifier):
    return identifier.split('_')


def tokenize_camelcase(identifier):
    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)
    return [m.group(0) for m in matches]


def tokenize_python(identifier, lowercase=False):
    if '_' in identifier:
        tokens = tokenize_snakecase(identifier)
    else:
        tokens = tokenize_camelcase(identifier)
    return [
        t.lower()
        for t in tokens
    ]


def get_file_variable_token_set(file_text, min_token_length=2, lowercase=True):
    token_infos = list(tokenize.generate_tokens(io.StringIO(file_text).readline))
    raw_tokens = [t.string for t in token_infos if t.type == 1]
    all_tokens = (
        tokenize_python(t, lowercase) for t in raw_tokens
    )
    all_tokens = [
        token
        for tokens in all_tokens
        for token in tokens
        if len(token) > min_token_length and not token in PYTHON_KEYWORDS
    ]
    return set(all_tokens)


def maybe_get_file_variable_token_string(file_text, min_token_length=2):
    try:
        tokens = get_file_variable_token_set(file_text)
    except:
        return None
    return ' '.join(tokens)

# Cell

def get_selected_lines_and_repos(repos, file_contents):

    selected_lines_by_repo = {
        repo: np.unique(select_lines(' '.join(file_contents[repos == repo].dropna())))
        for repo in repos.unique()
    }
    selected_lines_by_repo = {
        repo: lines
        for (repo, lines) in selected_lines_by_repo.items()
        if len(lines) > 0
    }
    line_repos = [k for k in selected_lines_by_repo.keys() for __ in selected_lines_by_repo[k]]
    all_selected_lines = [line for lines in selected_lines_by_repo.values() for line in lines]
    return pd.DataFrame({'repo': line_repos, 'line': all_selected_lines})
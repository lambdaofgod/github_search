# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Python_Tokens.ipynb (unless otherwise specified).

from collections import namedtuple

__all__ = [
    "tokenize_snakecase",
    "tokenize_camelcase",
    "tokenize_python",
    "get_file_variable_token_set",
    "maybe_get_file_variable_token_string",
    "PYTHON_KEYWORDS",
    "Token",
]

import io
import keyword

# Cell
import re
import tokenize

# Cell

PYTHON_KEYWORDS = set(keyword.kwlist)


def tokenize_snakecase(identifier):
    return identifier.split("_")


def tokenize_camelcase(identifier):
    matches = re.finditer(
        ".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)", identifier
    )
    return [m.group(0) for m in matches]


def tokenize_python(identifier, lowercase=False):
    if "_" in identifier:
        tokens = tokenize_snakecase(identifier)
    else:
        tokens = tokenize_camelcase(identifier)
    return [t.lower() for t in tokens]


def get_file_variable_token_set(file_text, min_token_length=2, lowercase=True):
    token_infos = list(tokenize.generate_tokens(io.StringIO(file_text).readline))
    raw_tokens = [t.string for t in token_infos if t.type == 1]
    all_tokens = (tokenize_python(t, lowercase) for t in raw_tokens)
    all_tokens = [
        token
        for tokens in all_tokens
        for token in tokens
        if len(token) > min_token_length and not token in PYTHON_KEYWORDS
    ]
    return set(all_tokens)


def maybe_get_file_variable_token_string(file_text, min_token_length=2):
    try:
        tokens = get_file_variable_token_set(file_text)
    except:
        return None
    return " ".join(tokens)


# Cell

Token = namedtuple("Token", ["name", "type"])


def _get_imports(file_content):
    root = ast.parse(file_content)

    for node in ast.iter_child_nodes(root):
        if isinstance(node, ast.Import):
            module = []
        elif isinstance(node, ast.ImportFrom) and not node.module is None:
            module = node.module.split(".")
        else:
            continue

        for n in node.names:
            yield Import(module, n.name.split("."), n.asname)


def tokenize_python_code(code_text):
    """tokenize each word in code_text as python token"""
    toks = code_text.split()
    return [tok for raw_tok in code_text.split() for tok in tokenize_python(raw_tok)]

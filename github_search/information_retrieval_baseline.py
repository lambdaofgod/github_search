# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/IR_Title_Baseline.ipynb (unless otherwise specified).

__all__ = ['get_task_grouped_rows', 'RetrievalDataset', 'get_information_retrieval_metrics',
           'write_results_to_markdown', 'get_query_results_df', 'get_most_similar_results']

# Cell
import os
import pprint
import numpy as np
import pandas as pd
import requests
import sentence_transformers

from github_search import token2vec_zsl, paperswithcode_tasks
import pandas as pd
import ast

# Cell


def get_task_grouped_rows(papers_with_repo_df, col="title"):
    papers_df = papers_with_repo_df[[col, "tasks"]]
    papers_df = papers_df.explode("tasks")
    papers_df["tasks"] = papers_df["tasks"].str.replace("3D", "").str.strip()
    papers_df["tasks"] = papers_df["tasks"].str.replace("2D", "").str.strip()
    papers_df["tasks"] = papers_df["tasks"].str.replace("4D", "").str.strip()
    papers_df["tasks"] = papers_df["tasks"].str.replace("6D", "").str.strip()
    papers_df["tasks"] = papers_df["tasks"].str.lower().str.replace(" ", "-")
    papers_df.columns = ["title", "task"]
    task_grouped_papers = papers_df.groupby("task")["title"].agg(lambda res: list(set(res)))
    return task_grouped_papers

# Cell


class RetrievalDataset:

    def __init__(self, query_results_dict):
        self.query_results_dict = query_results_dict
        self.queries = list(query_results_dict.keys())
        self.results_counter = {k: len(v) for (k, v) in query_results_dict.items()}

    def truncate_small_results_queries(self, n_smallest_allowed_results):
        filtered_query_results_dict = {
            k: v
            for (k, v) in self.query_results_dict.items()
            if len(v) >= n_smallest_allowed_results
        }
        return RetrievalDataset(filtered_query_results_dict)

    def sample_queries(self, size=0.2, query_grouper=None, query_hash_fn=lambda s: s[1] + s[-1]):
        if not query_grouper is None:
            queries = []
            for k, group in query_grouper.items():
                group_queries = [q for q in self.queries if q in group]
                n_samples = int(np.ceil(len(group_queries) * size))
                sampled_group_queries = sorted(group_queries, key=query_hash_fn)[:n_samples]
                print(k, group_queries)
                print(k, sampled_group_queries)
                queries = queries + sampled_group_queries
        else:
            n_samples = int(len(self.query_results_dict) * size)
            queries = sorted(self.queries, key=query_hash_fn)[:n_samples]

        print(queries)
        print(len(queries))
        query_results_dict = {q:v for (q, v) in self.query_results_dict.items() if q in queries}
        return RetrievalDataset(query_results_dict)

    def get_relevance_dict(self):
        return {
            query: {res: 1 for res in result}
            for (query, result) in self.query_results_dict.items()
        }

    def filter_queries(self, filter_fn):
        return RetrievalDataset({q: res for (q, res) in self.query_results_dict.items() if filter_fn(q)})

# Cell


def get_information_retrieval_metrics(evaluator, models, model_names):
    for m in models:
        res = evaluator(m, output_path='.')
    results_df = pd.read_csv('Information-Retrieval_evaluation_results.csv')
    os.remove('Information-Retrieval_evaluation_results.csv')
    cos_sim_results_df = results_df[[col for col in results_df.columns if 'cos_sim' in col]]
    cos_sim_results_df.columns = [col.replace('cos_sim-', '') for col in cos_sim_results_df.columns]
    cos_sim_results_df = cos_sim_results_df.round(3)
    cos_sim_results_df.index = model_names
    return cos_sim_results_df

# Cell


def write_results_to_markdown(metrics_df, file_name):
    metrics = ["Accuracy", "Precision", "MRR", "NDCG", "MAP"]
    with open(file_name, 'w') as f:
        for metric in metrics:
            metrics_df[[col for col in metrics_df.columns if metric in col]].to_markdown(f)
            f.write('\n\n')

# Cell


def get_query_results_df(evaluator, model, queries):
    query_results = evaluator.get_queries_result_list(model, queries)['cos_sim']
    return pd.DataFrame({
        q: pd.DataFrame.from_records(query_results[i])['corpus_id']
        for (i, q) in enumerate(queries)
    })


# Cell


def get_most_similar_results(model, query, texts, encodings, n_results=10):
    query_encoding = model.encode(query)
    scores = metrics.pairwise.cosine_distances(encodings, query_encoding.reshape(1,-1)).ravel()
    best_score_indices = scores.argsort()[:n_results]
    return pd.DataFrame({"text": texts[best_score_indices], "score": scores[best_score_indices]})
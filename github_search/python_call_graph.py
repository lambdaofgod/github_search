# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Python_Function_Call_Graph.ipynb (unless otherwise specified).

__all__ = ['get_calls_from_expr_or_assign', 'zip_dicts', 'get_ast_function_calls', 'get_function_calls',
           'get_sample_files_df', 'clean_task_names', 'get_repo_task_edges', 'try_run', 'get_upper_level_edges',
           'make_records', 'encode_bytes', 'RepoDependencyGraphFetcher', 'get_node_degrees',
           'get_aggregated_edge_type_df', 'get_descriptions', 'get_description_records_df',
           'get_records_df_with_labeled_files']

# Cell

import csrgraph as cg
import pandas as pd
import numpy as np
from github_search import python_tokens, paperswithcode_tasks
from mlutil.feature_extraction import embeddings
from mlutil import prototype_selection
import mlutil
from mlutil.feature_extraction import embeddings

import ast
import astunparse
import csrgraph
import nodevectors

import os
import itertools
import igraph
from sklearn import metrics
import gensim

from operator import itemgetter
import pickle

# Cell

def get_calls_from_expr_or_assign(expr):
    try:
        if type(expr) is ast.Name:
            return [expr.id]
        if type(expr) is ast.Call:
            return get_calls_from_expr_or_assign(expr.func) + get_calls_from_expr_or_assign(expr.args)
        elif type(expr) is ast.Attribute:
            return [expr.attr]
        elif type(expr) is ast.BinOp:
            return get_calls_from_expr_or_assign(expr.left) + get_calls_from_expr_or_assign(expr.right)
        elif type(expr) is ast.Expr:
            return get_calls_from_expr_or_assign(expr.value)
        elif type(expr) is ast.Assign:
            return get_calls_from_expr_or_assign(expr.value)
        elif type(expr) is ast.While or type(expr) is ast.If:
            return (
                get_calls_from_expr_or_assign(expr.test) +
                get_calls_from_expr_or_assign(expr.orelse) +
                get_calls_from_expr_or_assign(expr.body)
            )
        elif type(expr) is ast.For:
            return (
                get_calls_from_expr_or_assign(expr.target) +
                get_calls_from_expr_or_assign(expr.body) +
                get_calls_from_expr_or_assign(expr.iter)
            )
        elif type(expr) is ast.Try:
            return get_calls_from_expr_or_assign(expr.body)
        elif type(expr) is list:
            return [res for subexpr in expr for res in get_calls_from_expr_or_assign(subexpr)]
        else:
            return []
    except Exception as e:
        print(str(astunparse.unparse(expr)), ' ', str(e))

# Cell


def zip_dicts(dicts):
    res = {}
    for d in dicts:
        res = {**res, **d}
    return res


def get_ast_function_calls(code_ast, calls={}):
    if type(code_ast) is ast.FunctionDef:
        return {code_ast.name: frozenset(
            sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])
        )}
    elif type(code_ast) is ast.ClassDef:
        return {code_ast.name: frozenset(
            sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])
        )}
    elif hasattr(code_ast, 'body'):
        return zip_dicts([get_ast_function_calls(item) for item in code_ast.body])
    else: return {}


def get_function_calls(code):
    code_ast = ast.parse(code)
    return get_ast_function_calls(code_ast)

# Cell


def get_sample_files_df(python_files_df, n_files=100, repo_col='repo_name'):
    "sample n_files from each repo"
    if n_files ==None:
        n_files = python_files_df.shape[0]
    return python_files_df.dropna().groupby(repo_col).apply(lambda df: df.iloc[:min(df.shape[0], n_files)]).reset_index(drop=True)

# Cell


def clean_task_names(tasks):
    return tasks.str.replace("2D ", "").str.replace("3D ", "").str.replace("4D ", "").str.replace("6D ", "")


def get_repo_task_edges(python_files_df):
    task_exploded_df = python_files_df[['repo_name', 'tasks']].explode('tasks')
    task_exploded_df['tasks'] = clean_task_names(task_exploded_df['tasks'])
    return task_exploded_df.groupby("tasks").apply(lambda df: {df['tasks'].iloc[0]: frozenset(df['repo_name'].values)}).tolist()

# Cell


def try_run(f):
    def _maybe_failed_f(args):
        try:
            return f(args)
        except:
            return None
    return _maybe_failed_f

# Cell
import tqdm


def get_upper_level_edges(upper_level_names, lower_level_edges):
    return [
        {
            file_name:
            frozenset(
                [
                    encode_bytes(function)
                    for function in lower_level_edges.keys()
                ])
        }
        for (file_name, lower_level_edges) in zip(upper_level_names, lower_level_edges)
        if type(lower_level_edges) is dict
        and len(lower_level_edges.keys()) > 0
    ]


def make_records(function_edges):
    return [{
        'calling_function': calling_fn,
        'called_function': called_fn
    }
        for calling_fn in function_edges.keys()
        for called_fn in function_edges[calling_fn]
    ]

# Cell

def encode_bytes(s):
    return bytes(s, "UTF-8")


class RepoDependencyGraphFetcher:

    def clean_content(self, python_files_df, cleaned_column_name='clean_content'):
        python_files_df[cleaned_column_name] = python_files_df['content'].str.replace("429: Too Many Requests", "")
        python_files_df[cleaned_column_name] = python_files_df['clean_content'].apply(lambda s: np.nan if s == "" else s)
        return python_files_df

    def get_repo_and_root_edges(self, python_files_df, path_col='path', add_repo_label=False):
        filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')

        if add_repo_label:
            filenames = python_files_df['repo_name'] + ":" + filenames
        filenames = filenames.apply(encode_bytes)
        repo_edges_dict = filenames.groupby(python_files_df['repo_name']).agg(lambda args: frozenset(args)).to_dict()
        repo_edges = [{k: repo_edges_dict[k]} for k in repo_edges_dict]
        root_edges = [{
                '<ROOT>': frozenset(repo for edges in repo_edges for repo in repo_edges_dict.keys())
            }
        ]
        return root_edges, repo_edges

    def get_filename_and_function_edges(self, python_files_df, clean_content, add_repo_label=False, path_col='path'):
        if clean_content:
            content_column = 'clean_content'
            python_files_df = self.clean_content(python_files_df, content_column)
        else:
            content_column = 'content'

        filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')
        if add_repo_label:
            filenames = python_files_df['repo_name'] + ":" + filenames
        filenames = filenames.apply(encode_bytes)
        files = python_files_df[content_column]
        function_edges = files.apply(try_run(get_function_calls)).dropna()
        function_edges = function_edges.to_list()
        filename_edges = get_upper_level_edges(filenames, function_edges)
        return filename_edges, function_edges

    def get_records(self, edges):
        return [record
            for edge_group in edges
            for record in make_records(edge_group)
        ]

    def make_dependency_df(self, records, edge_type):
        dependency_records_df = pd.DataFrame.from_records(records)
        dependency_records_df.columns = ['source', 'destination']
        dependency_records_df['edge_type'] = edge_type
        dependency_records_df = dependency_records_df[
            (dependency_records_df['source'].apply(len) > 0) &
            (dependency_records_df['source'] != 'null') &
            (dependency_records_df['destination'].apply(len) > 0) &
            (dependency_records_df['source'] != dependency_records_df['destination'])
        ]
        return dependency_records_df.drop_duplicates()

    def get_dependency_df(self, python_files_df, dep_type="repo", add_filename_repo_label=False, clean_content=True):
        print("constructing edges")
        if dep_type == "repo":
            root_edges, repo_edges = self.get_repo_and_root_edges(python_files_df, add_repo_label=add_filename_repo_label)
            edge_groups = [root_edges, repo_edges]
            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]
            edge_types = ['root-repo', 'repo-file']
        elif dep_type == "function":
            filename_edges, function_edges = self.get_filename_and_function_edges(python_files_df, clean_content, add_repo_label=add_filename_repo_label)
            edge_groups = [filename_edges, function_edges]
            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]
            edge_types = ['file-function', 'function-function']
        else:
            raise NotImplementedError("unsupported dep_type: {}".format(dep_type))
        print("creating dependency dataframe")
        return pd.concat([
            self.make_dependency_df(records, edge_type)
            for edge_type, records in zip(edge_types, record_groups)
        ]).drop_duplicates(subset=['source', 'destination'], keep='first')

    def prepare_dependency_records(self, python_files_df, add_filename_repo_label=False):
        repo_records_df = self.get_dependency_df(
            python_files_df, "repo", clean_content=True, add_filename_repo_label=add_filename_repo_label
        )
        function_records_df = self.get_dependency_df(
            python_files_df, "function", clean_content=True, add_filename_repo_label=add_filename_repo_label
        )
        dependency_records_df = pd.concat([repo_records_df, function_records_df])
        dependency_records_df["source"] = dependency_records_df["source"].apply(
            lambda s: s if type(s) is str else s.decode("utf-8")
        )
        dependency_records_df["destination"] = dependency_records_df["destination"].apply(
            lambda s: s if type(s) is str else s.decode("utf-8")
        )
        return dependency_records_df

# Cell

def get_node_degrees(records_df):
    outdegree = records_df['source'].value_counts()
    indegree = records_df['destination'].value_counts()
    degree = outdegree.add(indegree, fill_value=0)
    return outdegree, indegree, degree

# Cell


def get_aggregated_edge_type_df(records_df, edge_type):
    sample_df = records_df[records_df['edge_type'] == edge_type]
    repos = sample_df.groupby("source").groups.keys()
    descriptions = sample_df.groupby("source").apply(
        lambda df:
        " ".join(
            set(df['destination'].sample(min(len(df), 1000)))))
    raw_descriptions_df = descriptions.reset_index()
    descriptions_df = raw_descriptions_df.apply(lambda item: item["source"] + " " + item[0].replace(item["source"]+":", ""), axis=1)
    descriptions_df.index = raw_descriptions_df['source']
    return descriptions_df

def get_descriptions(records_df):
    repo_descriptions = get_aggregated_edge_type_df(records_df, "repo-file")
    file_descriptions = get_aggregated_edge_type_df(records_df, "file-function")
    file_descriptions.name = "file_description"
    repo_descriptions.name = 'repo_description'
    return repo_descriptions, file_descriptions


def get_description_records_df(records_df):
    repo_descriptions, file_descriptions = get_descriptions(records_df)
    return (
        records_df.merge(
            repo_descriptions,
            left_on='source',
            right_index=True
        ).merge(
            file_descriptions,
            left_on='destination',
            right_index=True
        )
    )

# Cell

def get_records_df_with_labeled_files(records_df):
    outdegree, indegree, degree = get_node_degrees(records_df)
    labeled_files_dependency_records_df = records_df.copy()
    labeled_files = labeled_files_dependency_records_df[labeled_files_dependency_records_df['edge_type'] == 'repo-file']['destination']
    labeled_files = labeled_files_dependency_records_df[labeled_files_dependency_records_df['edge_type'] == 'repo-file']['source'] + ":" + labeled_files
    labeled_files_dependency_records_df['destination'][labeled_files_dependency_records_df['edge_type'] == 'repo-file'] = labeled_files
    return labeled_files_dependency_records_df
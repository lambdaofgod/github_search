# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/Python_Function_Call_Graph.ipynb (unless otherwise specified).

__all__ = [
    "get_calls_from_expr_or_assign",
    "zip_dicts",
    "get_ast_function_calls",
    "get_function_calls",
    "get_sample_files_df",
    "clean_task_names",
    "get_repo_task_edges",
    "try_run",
    "get_upper_level_edges",
    "make_records",
    "encode_bytes",
    "RepoDependencyFetcher",
    "get_node_degrees",
    "get_aggregated_edge_type_df",
    "get_descriptions",
    "get_description_records_df",
    "get_records_df_with_labeled_files",
]


import ast
import itertools
import os
import pickle
from operator import itemgetter

import astunparse
import csrgraph
import csrgraph as cg
import gensim
import igraph
import mlutil
import nodevectors
import numpy as np
import pandas as pd
from mlutil import prototype_selection
from mlutil.feature_extraction import embeddings
from sklearn import metrics

from github_search import paperswithcode_tasks, python_tokens, utils
import logging


def get_calls_from_expr_or_assign(expr):
    try:
        if type(expr) is ast.Name:
            return []
        if type(expr) is ast.Call:
            # function call
            if type(expr.func) is ast.Name:
                name = [expr.func.id]
            # method
            elif type(expr.func) is ast.Attribute:
                name = [expr.func.attr]
            else:
                name = []
            return name + get_calls_from_expr_or_assign(expr.args)
        elif type(expr) is ast.Attribute:
            return [expr.attr]
        elif type(expr) is ast.BinOp:
            return get_calls_from_expr_or_assign(
                expr.left
            ) + get_calls_from_expr_or_assign(expr.right)
        elif type(expr) is ast.Expr:
            return get_calls_from_expr_or_assign(expr.value)
        elif type(expr) is ast.Assign:
            return get_calls_from_expr_or_assign(expr.value)
        elif type(expr) is ast.While or type(expr) is ast.If:
            return (
                get_calls_from_expr_or_assign(expr.test)
                + get_calls_from_expr_or_assign(expr.orelse)
                + get_calls_from_expr_or_assign(expr.body)
            )
        elif type(expr) is ast.For:
            return (
                get_calls_from_expr_or_assign(expr.target)
                + get_calls_from_expr_or_assign(expr.body)
                + get_calls_from_expr_or_assign(expr.iter)
            )
        elif type(expr) is ast.Try:
            return get_calls_from_expr_or_assign(expr.body)
        elif type(expr) is ast.If:
            return get_calls_from_expr_or_assign(
                expr.body
            ) + get_calls_from_expr_or_assign(expr.orelse)
        elif type(expr) is list:
            return [
                res
                for subexpr in expr
                for res in get_calls_from_expr_or_assign(subexpr)
            ]
        else:
            return []
    except Exception as e:
        logging.info(str(astunparse.unparse(expr)), " ", str(e))
        return []


def zip_dicts(dicts):
    res = {}
    for d in dicts:
        res = {**res, **d}
    return res


def get_ast_function_calls(code_ast, calls={}):
    if type(code_ast) is ast.FunctionDef:
        return {
            code_ast.name: frozenset(
                sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])
            )
        }
    elif type(code_ast) is ast.ClassDef:
        return {
            code_ast.name: frozenset(
                sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])
            )
        }
    elif hasattr(code_ast, "body"):
        return zip_dicts([get_ast_function_calls(item) for item in code_ast.body])
    else:
        return {}


def get_function_calls(code):
    code_ast = ast.parse(code)
    return get_ast_function_calls(code_ast)


def get_sample_files_df(python_files_df, n_files=100, repo_col="repo_name"):
    "sample n_files from each repo"
    if n_files == None:
        n_files = python_files_df.shape[0]
    return (
        python_files_df.dropna()
        .groupby(repo_col)
        .apply(lambda df: df.iloc[: min(df.shape[0], n_files)])
        .reset_index(drop=True)
    )


def clean_task_names(tasks):
    return (
        tasks.str.replace("2D ", "")
        .str.replace("3D ", "")
        .str.replace("4D ", "")
        .str.replace("6D ", "")
    )


def get_repo_task_edges(python_files_df):
    task_exploded_df = python_files_df[["repo_name", "tasks"]].explode("tasks")
    task_exploded_df["tasks"] = clean_task_names(task_exploded_df["tasks"])
    return (
        task_exploded_df.groupby("tasks")
        .apply(lambda df: {df["tasks"].iloc[0]: frozenset(df["repo_name"].values)})
        .tolist()
    )


def try_run(f):
    def _maybe_failed_f(args):
        try:
            return f(args)
        except:
            return None

    return _maybe_failed_f


import tqdm


def get_upper_level_edges(upper_level_names, lower_level_edges):
    return [
        {
            file_name: frozenset(
                [encode_bytes(function) for function in lower_level_edges.keys()]
            )
        }
        for (file_name, lower_level_edges) in zip(upper_level_names, lower_level_edges)
        if type(lower_level_edges) is dict and len(lower_level_edges.keys()) > 0
    ]


def make_records(function_edges):
    return [
        {"calling_function": calling_fn, "called_function": called_fn}
        for calling_fn in function_edges.keys()
        for called_fn in function_edges[calling_fn]
    ]


def encode_bytes(s):
    return bytes(s, "UTF-8")


class RepoDependencyFetcher:
    def clean_content(self, python_files_df, cleaned_column_name="clean_content"):
        python_files_df[cleaned_column_name] = python_files_df["content"].str.replace(
            "429: Too Many Requests", ""
        )
        python_files_df[cleaned_column_name] = python_files_df[
            cleaned_column_name
        ].apply(lambda s: np.nan if s == "" else s)
        return python_files_df

    def get_repo_and_root_edges(self, python_files_df, use_basename, path_col="path"):
        filenames = python_files_df[path_col]
        if use_basename:
            filenames = filenames.apply(os.path.basename).str.replace(".py", "")

        filenames = filenames.apply(encode_bytes)
        repo_edges_dict = (
            filenames.groupby(python_files_df["repo_name"])
            .agg(lambda args: frozenset(args))
            .to_dict()
        )
        repo_edges = [{k: repo_edges_dict[k]} for k in repo_edges_dict]
        root_edges = [
            {
                "<ROOT>": frozenset(
                    repo for edges in repo_edges for repo in repo_edges_dict.keys()
                )
            }
        ]
        return root_edges, repo_edges

    def get_filename_and_function_edges(
        self,
        python_files_df,
        clean_content,
        add_repo_col=False,
        use_basename=False,
        path_col="path",
    ):
        if clean_content:
            content_column = "clean_content"
            python_files_df = self.clean_content(python_files_df, content_column)
        else:
            content_column = "content"

        if use_basename:
            filenames = (
                python_files_df[path_col].apply(os.path.basename).str.replace(".py", "")
            )
        else:
            filenames = python_files_df[path_col]
        if add_repo_col:
            filenames = python_files_df["repo_name"] + ":" + filenames
        filenames = filenames.apply(encode_bytes)
        files = python_files_df[content_column]
        function_edges = files.apply(try_run(get_function_calls)).dropna()
        function_edges = function_edges.to_list()
        filename_edges = get_upper_level_edges(filenames, function_edges)
        return filename_edges, function_edges

    def get_records(self, edges):
        return [record for edge_group in edges for record in make_records(edge_group)]

    def make_dependency_df(self, records, edge_type):
        if len(records) == 0:
            return pd.DataFrame({})
        dependency_records_df = pd.DataFrame.from_records(records)
        dependency_records_df.columns = ["source", "destination"]
        dependency_records_df["edge_type"] = edge_type
        dependency_records_df = dependency_records_df[
            (dependency_records_df["source"].apply(len) > 0)
            & (dependency_records_df["source"] != "null")
            & (dependency_records_df["destination"].apply(len) > 0)
            & (dependency_records_df["source"] != dependency_records_df["destination"])
        ]
        return dependency_records_df.drop_duplicates()

    def get_dependency_df(
        self,
        python_files_df,
        dep_type="repo",
        use_basename=False,
        clean_content=True,
    ):
        if dep_type == "repo":
            root_edges, repo_edges = self.get_repo_and_root_edges(
                python_files_df, use_basename=use_basename
            )
            edge_groups = [root_edges, repo_edges]
            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]
            edge_types = ["root-repo", "repo-file"]
        elif dep_type == "function":
            filename_edges, function_edges = self.get_filename_and_function_edges(
                python_files_df,
                clean_content,
                use_basename=use_basename,
            )
            edge_groups = [filename_edges, function_edges]
            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]
            edge_types = ["file-function", "function-function"]
        else:
            raise NotImplementedError("unsupported dep_type: {}".format(dep_type))
        return pd.concat(
            [
                self.make_dependency_df(records, edge_type)
                for edge_type, records in zip(edge_types, record_groups)
            ]
        ).drop_duplicates(subset=["source", "destination"], keep="first")

    def prepare_dependency_records_df(
        self,
        python_files_df,
        repo_col="repo_name",
        add_repo_col=False,
        use_basename=False,
    ):
        repos = python_files_df[repo_col].unique()
        dfs = []
        progbar = tqdm.auto.tqdm(repos)
        for repo in progbar:
            progbar.set_description(f"memory in GB: {utils.get_current_memory_usage()}")
            df = self.prepare_repo_dependency_records(
                repo, python_files_df, repo_col, add_repo_col, use_basename
            )
            if len(df) > 0:
                dfs.append(df)
        return pd.concat(dfs)

    def prepare_repo_dependency_records(
        self, repo, python_files_df, repo_col, add_repo_col, use_basename
    ):
        python_files_df = python_files_df[python_files_df[repo_col] == repo].copy()
        repo_records_df = self.get_dependency_df(
            python_files_df, "repo", clean_content=True, use_basename=use_basename
        )
        function_records_df = self.get_dependency_df(
            python_files_df, "function", clean_content=True, use_basename=use_basename
        )
        dependency_records_df = pd.concat([repo_records_df, function_records_df])
        dependency_records_df["source"] = dependency_records_df["source"].apply(
            lambda s: s if type(s) is str else s.decode("utf-8")
        )
        dependency_records_df["destination"] = dependency_records_df[
            "destination"
        ].apply(lambda s: s if type(s) is str else s.decode("utf-8"))
        if add_repo_col:
            dependency_records_df["repo"] = repo
        return dependency_records_df


def get_node_degrees(records_df):
    outdegree = records_df["source"].value_counts()
    indegree = records_df["destination"].value_counts()
    degree = outdegree.add(indegree, fill_value=0)
    return outdegree, indegree, degree


def get_aggregated_edge_type_df(records_df, edge_type):
    sample_df = records_df[records_df["edge_type"] == edge_type]
    repos = sample_df.groupby("source").groups.keys()
    descriptions = sample_df.groupby("source").apply(
        lambda df: " ".join(set(df["destination"].sample(min(len(df), 1000))))
    )
    raw_descriptions_df = descriptions.reset_index()
    descriptions_df = raw_descriptions_df.apply(
        lambda item: item["source"] + " " + item[0].replace(item["source"] + ":", ""),
        axis=1,
    )
    descriptions_df.index = raw_descriptions_df["source"]
    return descriptions_df


def get_descriptions(records_df):
    repo_descriptions = get_aggregated_edge_type_df(records_df, "repo-file")
    file_descriptions = get_aggregated_edge_type_df(records_df, "file-function")
    file_descriptions.name = "file_description"
    repo_descriptions.name = "repo_description"
    return repo_descriptions, file_descriptions


def get_description_records_df(records_df):
    repo_descriptions, file_descriptions = get_descriptions(records_df)
    return records_df.merge(
        repo_descriptions, left_on="source", right_index=True
    ).merge(file_descriptions, left_on="destination", right_index=True)


def get_records_df_with_labeled_files(records_df):
    outdegree, indegree, degree = get_node_degrees(records_df)
    labeled_files_dependency_records_df = records_df.copy()
    labeled_files = labeled_files_dependency_records_df[
        labeled_files_dependency_records_df["edge_type"] == "repo-file"
    ]["destination"]
    labeled_files = (
        labeled_files_dependency_records_df[
            labeled_files_dependency_records_df["edge_type"] == "repo-file"
        ]["source"]
        + ":"
        + labeled_files
    )
    labeled_files_dependency_records_df["destination"][
        labeled_files_dependency_records_df["edge_type"] == "repo-file"
    ] = labeled_files
    return labeled_files_dependency_records_df


def get_dependency_records_df(
    files_df,
    sample_files_per_repo,
    add_repo_col,
    use_basename,
):
    repo_dependency_fetcher = RepoDependencyFetcher()
    sample_files_df = get_sample_files_df(files_df, n_files=sample_files_per_repo)
    dependency_records_df = repo_dependency_fetcher.prepare_dependency_records_df(
        sample_files_df, add_repo_col=add_repo_col, use_basename=use_basename
    )
    return dependency_records_df

from dagster import (
    DataVersion,
    asset,
    multi_asset,
    Definitions,
    Output,
    AssetOut,
    AssetIn,
    ConfigurableResource,
    AssetExecutionContext,
    MetadataValue,
)
import dspy
import logging
import pandas as pd
from github_search.pipelines.steps import Code2DocSteps
from github_search.pipelines.dagster.resources import (
    Code2DocDataConfig,
    Code2DocModelConfig,
    PhoenixTracker,
)
from tqdm.contrib.logging import tqdm_logging_redirect
from github_search.lms.code2documentation import (
    run_code2doc_on_df,
    run_code2doc_on_files_df,
)

logging.basicConfig(level=logging.INFO)


@asset(
    ins={"repos_df": AssetIn(), "python_code_df": AssetIn()},
)
def sampled_repos(
    context: AssetExecutionContext,
    config: Code2DocDataConfig,
    repos_df: pd.DataFrame,
    python_code_df: pd.DataFrame,
) -> pd.DataFrame:
    """
    Repos selected with a procedure described in 2.3 'Selecting repository subset'
    """
    sampled_repos_df = Code2DocSteps.create_repos_sample_df(
        repos_df,
        python_code_df,
        config.n_repos_per_task,
        config.min_task_size,
        config.max_task_size,
        config.max_random_baseline_score,
    )
    repomaps_df = (
        pd.read_json(config.repomaps_path, lines=True).T.iloc[:, 0].rename("repomap")
    )
    sampled_repos_df = sampled_repos_df.merge(
        repomaps_df, left_on="repo", right_index=True
    )
    context.add_output_metadata(
        {
            "len": len(sampled_repos_df),
            "head": MetadataValue.md(sampled_repos_df.head().to_markdown()),
        }
    )
    sampled_repos_df.to_csv("/tmp/sampled_repos.csv", index=False, encoding="utf-8")
    return sampled_repos_df


@asset(
    ins={"repos_df": AssetIn(), "python_code_df": AssetIn()},
)
def sampled_repos_small(
    context: AssetExecutionContext,
    config: Code2DocDataConfig,
    repos_df: pd.DataFrame,
    python_code_df: pd.DataFrame,
) -> pd.DataFrame:
    """
    Repos selected with a procedure described in 2.3 'Selecting repository subset'
    """
    sampled_repos_df = Code2DocSteps.create_repos_sample_df(
        repos_df,
        python_code_df,
        config.n_repos_per_task // 2,
        config.min_task_size * 2,
        config.max_task_size // 2,
        config.max_random_baseline_score,
    )
    print(f"procedure selected {len(sampled_repos_df)} repositories")
    context.add_output_metadata(
        {
            "len": len(sampled_repos_df),
            "head": MetadataValue.md(sampled_repos_df.head().to_markdown()),
        }
    )
    return sampled_repos_df


@multi_asset(
    outs={"generated_readmes": AssetOut()},
)
def code2doc_readmes(
    context: AssetExecutionContext,
    config: Code2DocModelConfig,
    python_code_df: pd.DataFrame,
    sampled_repos: pd.DataFrame,
    phoenix: PhoenixTracker,
) -> pd.DataFrame:
    """
    README files generated by Code2Doc 5.3 'Code2Doc - README generation'
    """
    code2doc_config = config
    logging.info(
        f"Generating readmes with code2doc using {code2doc_config.lm_model_name}, "
        f"using maximum of {code2doc_config.files_per_repo} files per repo"
    )
    python_code_df = python_code_df[
        python_code_df["repo_name"].isin(sampled_repos["repo"])
    ]
    n_code_files = len(python_code_df)
    python_code_df = python_code_df.dropna(subset=["selected_code"])
    logging.info(f"Generating readmes for {len(sampled_repos)} repos.")
    logging.info(f"Dropped {n_code_files - len(python_code_df)} empty code files.")
    assert len(python_code_df["repo_name"].unique()) == len(
        sampled_repos["repo"]
    ), "Some repos are missing code files"
    logging.info(f"Using {len(python_code_df)} code files")
    avg_files_per_repo = len(python_code_df) / len(sampled_repos)
    logging.info(f"{round(avg_files_per_repo, 2)} files per repo on average")
    logging.info(f"Using {config.files_per_repo} files per repo")
    lm = dspy.LM(
        f"ollama_chat/{config.lm_model_name}", api_base=config.lm_base_url, api_key=""
    )
    dspy.configure(lm=lm)

    if config.is_debug_run:
        python_code_df = python_code_df.head(10)

    with tqdm_logging_redirect():
        generated_readme_df = run_code2doc_on_files_df(
            python_code_df,
            config.files_per_repo,
            "selected_code",
        )
    context.add_output_metadata(
        {
            "len": len(generated_readme_df),
            "head": MetadataValue.md(generated_readme_df.head().to_markdown()),
        },
        output_name="generated_readmes",
    )
    yield Output(
        generated_readme_df,
        output_name="generated_readmes",
        data_version=DataVersion(code2doc_config.lm_model_name),
    )


@multi_asset(
    outs={
        "repomap_generated_readmes": AssetOut(),
    },
)
def code2doc_readmes_from_repomaps(
    context: AssetExecutionContext,
    config: Code2DocModelConfig,
    python_code_df: pd.DataFrame,
    sampled_repos: pd.DataFrame,
    phoenix: PhoenixTracker,
) -> pd.DataFrame:
    """
    README files generated by Code2Doc 5.3 'Code2Doc - README generation'
    """
    code2doc_config = config
    logging.info(
        f"Generating readmes with code2doc using {code2doc_config.lm_model_name}, "
        f"using maximum of {code2doc_config.files_per_repo} files per repo"
    )
    lm = dspy.LM(
        model="ollama_chat/" + config.lm_model_name, base_url=config.lm_base_url
    )
    dspy.configure(lm=lm)

    if config.is_debug_run:
        sampled_repos = sampled_repos.head()

    with tqdm_logging_redirect():
        generated_readme_df = run_code2doc_on_df(
            sampled_repos, code_col="repomap", name_col="repo"
        )
    context.add_output_metadata(
        {
            "len": len(generated_readme_df),
            "head": MetadataValue.md(generated_readme_df.head().to_markdown()),
        },
        output_name="repomap_generated_readmes",
    )
    yield Output(
        generated_readme_df,
        output_name="repomap_generated_readmes",
        data_version=DataVersion(code2doc_config.lm_model_name),
    )


@multi_asset(
    outs={"flat_generated_readmes": AssetOut()},
)
def code2doc_flat_readmes(
    context: AssetExecutionContext,
    config: Code2DocModelConfig,
    python_code_df: pd.DataFrame,
    sampled_repos: pd.DataFrame,
    phoenix: PhoenixTracker,
) -> pd.DataFrame:
    """
    README files generated by Code2Doc Flat - single-step direct code-to-documentation generation
    """
    code2doc_config = config
    logging.info(
        f"Generating readmes with code2doc FLAT using {code2doc_config.lm_model_name}, "
        f"using maximum of {code2doc_config.files_per_repo} files per repo"
    )
    python_code_df = python_code_df[
        python_code_df["repo_name"].isin(sampled_repos["repo"])
    ]
    n_code_files = len(python_code_df)
    python_code_df = python_code_df.dropna(subset=["selected_code"])
    logging.info(f"Generating readmes for {len(sampled_repos)} repos.")
    logging.info(f"Dropped {n_code_files - len(python_code_df)} empty code files.")
    assert len(python_code_df["repo_name"].unique()) == len(
        sampled_repos["repo"]
    ), "Some repos are missing code files"
    logging.info(f"Using {len(python_code_df)} code files")
    avg_files_per_repo = len(python_code_df) / len(sampled_repos)
    logging.info(f"{round(avg_files_per_repo, 2)} files per repo on average")
    logging.info(f"Using {config.files_per_repo} files per repo")
    lm = dspy.LM(
        f"ollama_chat/{config.lm_model_name}", api_base=config.lm_base_url, api_key=""
    )
    dspy.configure(lm=lm)

    if config.is_debug_run:
        python_code_df = python_code_df.head(10)

    with tqdm_logging_redirect():
        generated_readme_df = run_code2doc_on_files_df(
            python_code_df,
            config.files_per_repo,
            "selected_code",
            use_flat=True,
        )
    context.add_output_metadata(
        {
            "len": len(generated_readme_df),
            "head": MetadataValue.md(generated_readme_df.head().to_markdown()),
        },
        output_name="flat_generated_readmes",
    )
    yield Output(
        generated_readme_df,
        output_name="flat_generated_readmes",
        data_version=DataVersion(f"flat_{code2doc_config.lm_model_name}"),
    )


@multi_asset(
    outs={
        "flat_repomap_generated_readmes": AssetOut(),
    },
)
def code2doc_flat_readmes_from_repomaps(
    context: AssetExecutionContext,
    config: Code2DocModelConfig,
    python_code_df: pd.DataFrame,
    sampled_repos: pd.DataFrame,
    phoenix: PhoenixTracker,
) -> pd.DataFrame:
    """
    README files generated by Code2Doc Flat from repomaps - single-step direct repomap-to-documentation generation
    """
    code2doc_config = config
    logging.info(
        f"Generating readmes with code2doc FLAT from repomaps using {code2doc_config.lm_model_name}, "
        f"using maximum of {code2doc_config.files_per_repo} files per repo"
    )
    lm = dspy.LM(
        model="ollama_chat/" + config.lm_model_name, base_url=config.lm_base_url
    )
    dspy.configure(lm=lm)

    if config.is_debug_run:
        sampled_repos = sampled_repos.head()

    with tqdm_logging_redirect():
        generated_readme_df = run_code2doc_on_df(
            sampled_repos, code_col="repomap", name_col="repo", use_flat=True
        )
    context.add_output_metadata(
        {
            "len": len(generated_readme_df),
            "head": MetadataValue.md(generated_readme_df.head().to_markdown()),
        },
        output_name="flat_repomap_generated_readmes",
    )
    yield Output(
        generated_readme_df,
        output_name="flat_repomap_generated_readmes",
        data_version=DataVersion(f"flat_{code2doc_config.lm_model_name}"),
    )

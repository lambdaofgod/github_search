#+title: Pipeline_steps
#+PROPERTY: header-args :tangle generate_texts.py

Conda env github_search

* Running pipeline step by step

#+BEGIN_SRC python :session generate_texts.org  :exports both
import pandas as pd
from github_search.pipelines.steps import sample_data_step, expand_documents_step, evaluate_generated_texts_step, evaluate_generated_texts
from tgutil.configs import PipelineConfig, ConfigPaths, APIConfig
from tgutil.prompting_runner import sample_data, expand_documents
import logging
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both
logging.basicConfig(level="INFO")
cfg_path = "conf/text_generation/config.yaml"
path_cfg = ConfigPaths.load(cfg_path)
cfg = PipelineConfig.load(cfg_path)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
model_type = path_cfg.generation
model_type = "rwkv-4-raven-7b"
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
cfg
#+END_SRC

#+RESULTS:
: sampling_config=SamplingConfig(pq_data_path='../../output/nbow_data_test.parquet', out_data_path='data/prompt_infos.jsonl', n_samples=100, dset_kwargs={'dataset_project': 'tgutil_llms', 'dataset_name': 'prompt_info_sample'}) prompt_config=PromptConfig(prompt_template_name='md_prompt.jinja', templates_path='prompt_templates', data_path='data/prompt_infos.jsonl') generation_config=TextGenerationConfig(out_dir='output') name='sampled document expansion pipeline' project='github_search/document_expansion'

#+BEGIN_SRC python :session generate_texts.org  :exports both
url = "http://localhost:8765/generate"

sampling_config = cfg.sampling_config
generation_config = APIConfig(endpoint_url=url, out_dir=cfg.generation_config.out_dir, model_name="rwvk-4-raven-7b")
prompt_config = cfg.prompt_config
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session generate_texts.org  :exports both :async
cfg
#+END_SRC

#+RESULTS:
: sampling_config=SamplingConfig(pq_data_path='../../output/nbow_data_test.parquet', out_data_path='data/prompt_infos.jsonl', n_samples=100, dset_kwargs={'dataset_project': 'tgutil_llms', 'dataset_name': 'prompt_info_sample'}) prompt_config=PromptConfig(prompt_template_name='md_prompt.jinja', templates_path='prompt_templates', data_path='data/prompt_infos.jsonl') generation_config=TextGenerationConfig(out_dir='output') name='sampled document expansion pipeline' project='github_search/document_expansion'

** Sample
#+BEGIN_SRC python :session generate_texts.org  :exports both
prompt_infos = sample_data(sampling_config)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both
prompt_infos[:1]
#+END_SRC

#+RESULTS:
| PromptInfo | (repo_records= ((dependencies : comic_layout/tests.py frontend/apps.py keyframes/admin.py settings/urls.py keyframes/keyframes.py get_yt_comix_media_urls.py style_transfer/apps.py settings/wsgi.py popularity/models.py comic_layout/comic_layout.py tasks : ['style transfer'] repo : maciej3031/comixify) (dependencies : contagiograms/consts.py setup.py contagiograms/contagiograms.py contagiograms/__init__.py contagiograms/cli.py contagiograms/utils.py valid_windowsize parse_args valid_date SortedMenu tasks : ['time series'] repo : compstorylab/contagiograms)) predicted_repo_record= (dependencies : table/__init__.py main.py table/IO.py join_dicts TableDataset __setstate__ __getstate__ merge_vocabs OrderedIterator read_anno_json tasks : ['semantic parsing'] repo : inyukwo1/Coarse2fine_boilerplate) true_tasks= (semantic parsing) repo_text_field= dependencies) |

** Expand documents (generate texts)
#+BEGIN_SRC python :session generate_texts.org  :exports both :async
generated_records = expand_documents(generation_config, prompt_config, prompt_infos)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
generated_records
#+END_SRC

#+RESULTS:
#+begin_example
                                         repo_records  ...                                     generated_text
0   [{'dependencies': 'validation_mimic3_dec.py mi...  ...  \n['generative adversarial']\n\n## repository\...
1   [{'dependencies': 'pytorch_pretrained_bert/tok...  ...  \n['relation detection']\n\n## repository\nAff...
2   [{'dependencies': 'tests/test_benchmark.py tes...  ...  \nconcept learning visualisation natural langu...
3   [{'dependencies': 'datasets_handler/cifar10_in...  ...  \n['C++ simulation']\n\n## repository\nmattis/...
4   [{'dependencies': 'func/sampling/vsum.py func/...  ...  \n['future prediction', 'grounding based' 'vis...
..                                                ...  ...                                                ...
95  [{'dependencies': 'dmn_pytorch/models/dpn/adap...  ...  \n['object detection', 'cnn', 'deep learning',...
96  [{'dependencies': 'src/pycls/core/benchmark.py...  ...  \ntext-to-speech (MT),\n## files\nhcls/train.p...
97  [{'dependencies': 'annotation_site/serve.py co...  ...  \n['nms', 'corner detection', 'corner detectio...
98  [{'dependencies': 'workflow/BayesOpt_SOAP.py w...  ...  \n['cnn_checkpoint','loss']\n\n## repository\n...
99  [{'dependencies': 'Task02_Heart/train.py Task0...  ...  \n['deform_conv', 'joint_transforms', 'transfo...

[100 rows x 6 columns]
#+end_example

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
generated_records.to_json(f"output/{model_type}_generated_records.json", orient="records", lines=True)
#+END_SRC

** Evaluate text generation
#+RESULTS:
: None

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
generated_records = pd.read_json(f"output/{model_type}_generated_records.json", orient="records", lines=True)
#+END_SRC

#+RESULTS:

#+RESULTS:
: <class 'numpy.recarray'>

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
generated_records["repo"] = generated_records["predicted_repo_record"].apply(lambda rec: rec["repo"])
generated_records["tasks"] = generated_records["true_tasks"]
generated_records.columns
#+END_SRC

#+RESULTS:
: Index(['repo_records', 'predicted_repo_record', 'true_tasks',
:        'repo_text_field', 'input_text', 'generated_text', 'repo', 'tasks'],
:       dtype='object')

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
evaluated_df = evaluate_generated_texts(generated_records[["repo", "generated_text", "tasks"]], "../../data/paperswithcode_with_tasks.csv")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session generate_texts.org  :exports both :async
evaluated_df.to_json(f"output/{model_type}_evaluated_records.json", orient="records", lines=True)
#+END_SRC

#+RESULTS:
: None

*** Results
#+BEGIN_SRC python :session generate_texts.org  :exports both :async
evaluated_df.describe()
#+END_SRC

#+RESULTS:
#+begin_example
        edit_word  jaccard_lst  HuggingfaceMetricName.bleurt  ...   rougeLsum         wmd  sentence_transformer_similarity
count  100.000000   100.000000                    100.000000  ...  100.000000  100.000000                       100.000000
mean     0.984133     0.009345                     -1.532699  ...    0.031916    0.251381                         0.199314
std      0.052829     0.069030                      0.242009  ...    0.071915    0.060800                         0.112203
min      0.666667     0.000000                     -2.070768  ...    0.000000    0.117869                        -0.000019
25%      1.000000     0.000000                     -1.678775  ...    0.000000    0.213104                         0.123313
50%      1.000000     0.000000                     -1.514000  ...    0.000000    0.247335                         0.180249
75%      1.000000     0.000000                     -1.371900  ...    0.000000    0.286132                         0.265422
max      1.000000     0.666667                     -0.910514  ...    0.400000    0.490718                         0.541195

[8 rows x 9 columns]
#+end_example

** Evaluate information retrieval

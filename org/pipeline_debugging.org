#+title: Pipeline_debugging


* Accessing ZenML runs

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
runs = Client().get_pipeline("generation_pipeline").runs
completed_runs = [r for r in runs if r.status == "completed"]
[r.id for r in completed_runs]
#+END_SRC

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
completed_runs[1].steps["sample_data_step"].config.parameters["sampling_config"]
#+END_SRC

* Generated texts

** Without list of tags

#+BEGIN_QUOTE
{% for repo_record in context_prompt_infos %}
## repository
{{repo_record["name"]}}
## files
{{repo_record["content"]}}
## tags
{{repo_record["true_text"]}}
{% endfor %}
## repository
{{prompt_info["name"]}}
## files
{{prompt_info["content"]}}
## tags
#+END_QUOTE

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
from zenml.client import Client

artifact = Client().get_artifact('d1b7a3bb-a1ba-48d6-8bc2-2603e60fbc27')
loaded_artifact = artifact.load()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both :results output
for t in loaded_artifact["generated_text"]:
    print(t.split("##")[0])
#+END_SRC

#+RESULTS:
#+begin_example

denoising, resunet, jdnet, GAN, o

bert foolbox poisoning



graph neural network



large text classfiers



data augmentation



taxonomic tree, taxonomy



question answering



remote sensing






relation network
#+end_example

** With list of tags

{% for repo_record in context_prompt_infos %}
## repository
{{repo_record["name"]}}
## files
{{repo_record["content"]}}
## tags
[{{repo_record["true_text"]}}]
{% endfor %}
## repository
{{prompt_info["name"]}}
## files
{{prompt_info["content"]}}
##  tags
[


#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
from zenml.client import Client

artifact = Client().get_artifact('9249ef62-691e-4375-ab13-fc1e7719f559')
loaded_artifact = artifact.load()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both :results output
for t in loaded_artifact["generated_text"]:
    print(t.split("##")[0])

#+END_SRC

#+RESULTS:
#+begin_example
image dehazing]


deepfakes]


graphical models, variational autoencoder]


hashtag detection]

reconstruction, image reconstruction, density estimation]


graph classification]


information retrieval]


neural style transfer]


medical, ai]


Graph Neural Networks]
#+end_example


** With list of tags and specified number

#+BEGIN_QUOTE


{% for repo_record in context_prompt_infos %}
## repository
{{repo_record["name"]}}
## files
{{repo_record["content"]}}
## tags
[{{repo_record["true_text"]}}]
{% endfor %}
## repository
{{prompt_info["name"]}}
## files
{{prompt_info["content"]}}
## 3 tags
[
#+END_QUOTE

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both :results output

from zenml.client import Client

artifact = Client().get_artifact('9dabeb80-0c42-4289-acae-e796690db620')
loaded_artifact = artifact.load()
for t in loaded_artifact["generated_text"]:
    print(t.split("##")[0])


#+END_SRC

#+RESULTS:
#+begin_example
image inpainting], [supervised learning], [denoising], [neural networks
network optimization] [optimization] [online generalization]


graph generation, graph reasoning]


hashtag] [neural ranking],


segmentation]


machine learning, similarity, deep learning]


negative sampling], [nmt], [Huggingface]


vision]
[open-source-toolkit]


medicine, recommender system, OCR]


graph learning]
#+end_example

** With cleaned tasks

*** Clean stuff

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
import re

s = """
image inpainting], [supervised learning], [denoising], [neural networks
network optimization] [optimization] [online generalization]
"""

s_without_enclosing_parens = re.sub(r'\]\s*\[', ', ', s)
s_without_enclosing_parens = re.sub(r'\],\s*\[', ', ', s_without_enclosing_parens)
s_cleaned_commas = re.sub(r',\s*,', ', ', s_without_enclosing_parens)
s_cleaned_commas
#+END_SRC

#+RESULTS:
:
: image inpainting, supervised learning, denoising, neural networks
: network optimization, optimization, online generalization]

#+BEGIN_AI
Write a Python function that replaces "] [" pattern, possibly with more whitespaces than one, with an empty string
Sure, you can accomplish that using Python's built-in `re` module to handle regular expressions. Here's a function that does that:

```python
import re

def replace_pattern(string):
    return re.sub(r'\]\s*\[', '', string)
```

In this function:
- `re.sub(r'\]\s*\[', '', string)` replaces every occurrence of the "] [" pattern with an empty string. The `\s*` part means "match 0 or more whitespace characters".
- The `r` before the pattern string marks it as a raw string, which means backslashes are treated as literal backslashes. It's generally a good idea to use raw strings for regex patterns in Python to avoid backslash escaping confusion.

Here's an example of how you might use this function:

```python
s = 'Hello] [world'
print(replace_pattern(s))  # 'Helloworld'
```

[ME]: #+END_AI

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
s = "image segmentation, image restoration]"

s.replace("]", "")
#+END_SRC

#+RESULTS:
: image segmentation, image restoration

**** Cleaning code
[[/home/kuba/Projects/github_search/github_search/pipelines/postprocessing.py::10][postprocessing.py::10 (in /home/kuba/Projects/github_search/github_search/pipelines/postprocessing.py)]]

** Generated_texts_df in metrics pipeline

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
from zenml.client import Client

artifact = Client().get_artifact('a157a2b6-3b67-48e6-bd2c-3d9573f048b0')
raw_generated_texts_df = artifact.load()
raw_generated_texts_df["prompt_info"].iloc[0]
#+END_SRC

#+RESULTS:
| content | : | code/ablation/r1/config/cal_ssim.py, code/ablation/r1/config/dataset.py, code/ablation/r1/config/eval.py, code/ablation/r1/config/model.py, code/ablation/r1/config/show.py, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, save_checkpoints_net, load_checkpoints_net, inf_batch_test, inf_batch, inf_batch | id | : | 3a9d53917dc37d935451857e2e7d5319 | name | : | Ohraincu/JDNet | true_text | : | rain removal, single image deraining |

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
from zenml.client import Client

artifact = Client().get_artifact('67c39bd1-b4f8-4b87-b1d8-399d7b070b01')
generated_texts_df = artifact.load()
generated_texts_df["tasks"].iloc[]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
generated_texts_df.columns
#+END_SRC

#+RESULTS:
: Index(['repo', 'tasks', 'true_tasks', 'generated_text', 'prompt_info',
:        'generation', 'input_text'],
:       dtype='object')

** DONE Bug in text cleaning
#+RESULTS:

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
from github_search.pipelines.postprocessing import GenerationPostprocessor

s = "neural networks], [deep learning], [cifar]\n\n## repository\nliyu2005/DeepSpeech\n## files\nREADME.md, data/librispeech_lm_2017_00_1"

GenerationPostprocessor._sanitize_generated_text(s, "")
#+END_SRC

#+RESULTS:
: neural networks, deep learning, cifar,

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
#:results output

raw_strings = [
    ['deep learning]\n# [video prediction]\n# [denoising]\n\n## repository\nausten-whitney/neural-style-tf\n## files\nneural_style, util, data_provider\n## 4'],
    ["numpy]\n[machine learning]\n\n## repository\ncassiehenson/billiards_py\n## files\nREADME.md, 106_color_full.py, model.ipynb, 41_"]]

GenerationPostprocessor._sanitize_generated_text(raw_strings[0], "")
#+END_SRC

#+RESULTS:
: deep learning
: # [video prediction
: # [denoising


#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
import re

def clean_parens(s):
    s_without_enclosing_parens = re.sub(r"\]", ", ", s)
    s_semi_cleaned = re.sub(r",\s*,", ", ", s_without_enclosing_parens).replace("\n", " ")
    return re.sub(",\W+", ", ", s_semi_cleaned)
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
generated_texts_df["tasks"].apply(lambda l: l[0].split("##")[0]).apply(clean_parens)
#+END_SRC

#+RESULTS:
#+begin_example
0                            stochastic optimization,
1                            numpy, machine learning,
2    graph neural networks, link prediction, graph ...
3       image tagging, tag cloud, sentence embedding,
4    GAN, debias + adversarially learn synthetic, u...
5                             learning from examples,
6         deep learning, video prediction, denoising,
7            photo-style transfer, pytorch, research,
8                                 news, stock market,
9                                  3D reconstruction,
0            model, weather forecasting, neural net ,
1                   cifar10, ml-agents, segmentation,
2                            geometric deep learning,
3              social networking, hashtag extraction,
4     medical imaging, reconstruction, deep learning,
5                   denoising, machine learning, GAN,
6    question answering, qn-matching, sentence embe...
7     stylized image processing, decoupled attention,
8        data mining, stock market, machine learning,
9                              human pose estimation,
0             deep learning, stochastic optimization,
1                                       unsupervised,
2    graph embedding, graph neural network, graph c...
3    text generation, coreference resolution, machi...
4    visual object navigation, mask generation, 3d ...
5                                davis, panoptic, 3D,
6                           QA, dialog system, SQuAD,
7                            pytorch, opencv, python,
8    medicine, data extraction, forecast pattern, d...
9                                generative modeling,
Name: tasks, dtype: object
#+end_example

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
re.semi_cleaned
#+END_SRC

* Results

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both :results output
from zenml.client import Client

artifact = Client().get_artifact('bdaf8cbf-900f-44f4-97f5-b79bfe818a38')
ir_experiments_results = artifact.load()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session pipeline_debugging.org  :exports both
ir_experiments_results[1]
#+END_SRC

#+RESULTS:
: ir_config=InformationRetrievalEvaluatorConfig(embedder_config=EmbedderPairConfig(query_embedder_path='sentence-transformers/all-mpnet-base-v2', document_embedder_path='sentence-transformers/all-mpnet-base-v2', doc_max_length=None, query_max_length=None), column_config=InformationRetrievalColumnConfig(document_cols=('dependencies',), query_col='true_tasks', list_cols=())) per_query_metrics={0: [{'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.022, 'query': 'rain removal'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.5, 'MRR@10': 0.1, 'ndcg@10': 0.177, 'AveP@50': 0.07, 'query': 'single image deraining'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.637, 'AveP@50': 0.567, 'query': 'adversarial attack'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'link prediction'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.5, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.613, 'AveP@50': 0.562, 'query': 'graph generation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.5, 'ndcg@10': 0.631, 'AveP@50': 0.5, 'query': 'graph clustering'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.5, 'MRR@10': 0.5, 'ndcg@10': 0.385, 'AveP@50': 0.308, 'query': 'sentiment analysis'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.637, 'AveP@50': 0.565, 'query': 'depth estimation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.024, 'query': 'product recommendation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.6, 'precisions@10': 0.4, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.75, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 0.918, 'AveP@50': 0.793, 'query': 'response generation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'image stylization'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.4, 'precisions@10': 0.3, 'recall@1': 0.0, 'recall@3': 0.091, 'recall@5': 0.182, 'recall@10': 0.273, 'MRR@10': 0.333, 'ndcg@10': 0.283, 'AveP@50': 0.163, 'query': 'semantic segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.2, 'ndcg@10': 0.387, 'AveP@50': 0.2, 'query': 'argument mining'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.667, 'precisions@5': 0.6, 'precisions@10': 0.3, 'recall@1': 0.0, 'recall@3': 0.286, 'recall@5': 0.429, 'recall@10': 0.429, 'MRR@10': 0.5, 'ndcg@10': 0.429, 'AveP@50': 0.367, 'query': 'style transfer'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.25, 'MRR@10': 0.5, 'ndcg@10': 0.246, 'AveP@50': 0.202, 'query': 'time series'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.5, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.613, 'AveP@50': 0.537, 'query': 'gaussian processes'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.091, 'query': 'dialogue evaluation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'anomaly detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.017, 'query': 'scene parsing'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'sparse learning'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'demosaicking'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.036, 'query': 'denoising'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.038, 'query': 'template matching'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.333, 'ndcg@10': 0.307, 'AveP@50': 0.167, 'query': 'instance segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.062, 'query': 'information retrieval'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'table detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.125, 'ndcg@10': 0.315, 'AveP@50': 0.125, 'query': 'human object interaction detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.25, 'ndcg@10': 0.264, 'AveP@50': 0.125, 'query': 'text classification'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'data compression'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'speech enhancement'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.083, 'query': 'text generation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.2, 'ndcg@10': 0.387, 'AveP@50': 0.2, 'query': 'aspect based sentiment analysis'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.25, 'ndcg@10': 0.431, 'AveP@50': 0.25, 'query': 'face swapping'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.25, 'ndcg@10': 0.431, 'AveP@50': 0.25, 'query': 'one shot learning'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.333, 'ndcg@10': 0.307, 'AveP@50': 0.167, 'query': 'word embeddings'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'outlier detection'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.333, 'recall@3': 0.333, 'recall@5': 0.333, 'recall@10': 0.333, 'MRR@10': 1.0, 'ndcg@10': 0.469, 'AveP@50': 0.414, 'query': 'imitation learning'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.333, 'recall@5': 0.667, 'recall@10': 0.667, 'MRR@10': 0.5, 'ndcg@10': 0.498, 'AveP@50': 0.361, 'query': 'relation extraction'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.25, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.25, 'MRR@10': 1.0, 'ndcg@10': 0.39, 'AveP@50': 0.301, 'query': 'data augmentation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.5, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'person re identification'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'video based person re identification'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.143, 'ndcg@10': 0.333, 'AveP@50': 0.143, 'query': 'out of distribution detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'scene text recognition'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.077, 'query': 'chunking'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.143, 'ndcg@10': 0.333, 'AveP@50': 0.143, 'query': 'word alignment'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.05, 'query': 'machine translation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.5, 'ndcg@10': 0.387, 'AveP@50': 0.327, 'query': 'density estimation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.104, 'query': 'scene text detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.5, 'MRR@10': 0.125, 'ndcg@10': 0.193, 'AveP@50': 0.083, 'query': 'object localization'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'human detection'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.5, 'ndcg@10': 0.631, 'AveP@50': 0.5, 'query': 'pedestrian detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.022, 'query': 'unity'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'face detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'bilingual lexicon induction'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.333, 'MRR@10': 0.143, 'ndcg@10': 0.156, 'AveP@50': 0.066, 'query': 'autonomous driving'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.25, 'ndcg@10': 0.264, 'AveP@50': 0.125, 'query': 'few shot learning'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'monocular depth estimation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'stochastic optimization'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.034, 'query': 'program induction'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'language acquisition'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'one class classifier'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'learning to execute'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.667, 'recall@5': 0.667, 'recall@10': 0.667, 'MRR@10': 0.5, 'ndcg@10': 0.531, 'AveP@50': 0.431, 'query': 'adversarial defense'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'automl'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'boundary detection'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'chinese word segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.023, 'query': 'molecular property prediction'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'generalization bounds'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.043, 'query': 'depth and camera motion'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.067, 'query': 'image augmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.056, 'query': 'image cropping'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'optical character recognition'}]} aggregate_metrics={0: [{'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.022, 'query': 'rain removal'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.5, 'MRR@10': 0.1, 'ndcg@10': 0.177, 'AveP@50': 0.07, 'query': 'single image deraining'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.637, 'AveP@50': 0.567, 'query': 'adversarial attack'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'link prediction'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.5, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.613, 'AveP@50': 0.562, 'query': 'graph generation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.5, 'ndcg@10': 0.631, 'AveP@50': 0.5, 'query': 'graph clustering'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.5, 'MRR@10': 0.5, 'ndcg@10': 0.385, 'AveP@50': 0.308, 'query': 'sentiment analysis'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.637, 'AveP@50': 0.565, 'query': 'depth estimation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.024, 'query': 'product recommendation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.6, 'precisions@10': 0.4, 'recall@1': 0.25, 'recall@3': 0.5, 'recall@5': 0.75, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 0.918, 'AveP@50': 0.793, 'query': 'response generation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'image stylization'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.4, 'precisions@10': 0.3, 'recall@1': 0.0, 'recall@3': 0.091, 'recall@5': 0.182, 'recall@10': 0.273, 'MRR@10': 0.333, 'ndcg@10': 0.283, 'AveP@50': 0.163, 'query': 'semantic segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.2, 'ndcg@10': 0.387, 'AveP@50': 0.2, 'query': 'argument mining'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.667, 'precisions@5': 0.6, 'precisions@10': 0.3, 'recall@1': 0.0, 'recall@3': 0.286, 'recall@5': 0.429, 'recall@10': 0.429, 'MRR@10': 0.5, 'ndcg@10': 0.429, 'AveP@50': 0.367, 'query': 'style transfer'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.25, 'MRR@10': 0.5, 'ndcg@10': 0.246, 'AveP@50': 0.202, 'query': 'time series'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.5, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 1.0, 'ndcg@10': 0.613, 'AveP@50': 0.537, 'query': 'gaussian processes'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.091, 'query': 'dialogue evaluation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'anomaly detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.017, 'query': 'scene parsing'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'sparse learning'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'demosaicking'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.036, 'query': 'denoising'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.038, 'query': 'template matching'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.333, 'ndcg@10': 0.307, 'AveP@50': 0.167, 'query': 'instance segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.062, 'query': 'information retrieval'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'table detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.125, 'ndcg@10': 0.315, 'AveP@50': 0.125, 'query': 'human object interaction detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.25, 'ndcg@10': 0.264, 'AveP@50': 0.125, 'query': 'text classification'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'data compression'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'speech enhancement'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.083, 'query': 'text generation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.2, 'ndcg@10': 0.387, 'AveP@50': 0.2, 'query': 'aspect based sentiment analysis'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.25, 'ndcg@10': 0.431, 'AveP@50': 0.25, 'query': 'face swapping'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.25, 'ndcg@10': 0.431, 'AveP@50': 0.25, 'query': 'one shot learning'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.333, 'ndcg@10': 0.307, 'AveP@50': 0.167, 'query': 'word embeddings'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'outlier detection'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.333, 'recall@3': 0.333, 'recall@5': 0.333, 'recall@10': 0.333, 'MRR@10': 1.0, 'ndcg@10': 0.469, 'AveP@50': 0.414, 'query': 'imitation learning'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.333, 'recall@5': 0.667, 'recall@10': 0.667, 'MRR@10': 0.5, 'ndcg@10': 0.498, 'AveP@50': 0.361, 'query': 'relation extraction'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.25, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.25, 'MRR@10': 1.0, 'ndcg@10': 0.39, 'AveP@50': 0.301, 'query': 'data augmentation'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.5, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'person re identification'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'video based person re identification'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.143, 'ndcg@10': 0.333, 'AveP@50': 0.143, 'query': 'out of distribution detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.02, 'query': 'scene text recognition'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.077, 'query': 'chunking'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 1.0, 'MRR@10': 0.143, 'ndcg@10': 0.333, 'AveP@50': 0.143, 'query': 'word alignment'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.05, 'query': 'machine translation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.5, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.5, 'ndcg@10': 0.387, 'AveP@50': 0.327, 'query': 'density estimation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.104, 'query': 'scene text detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.5, 'MRR@10': 0.125, 'ndcg@10': 0.193, 'AveP@50': 0.083, 'query': 'object localization'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'human detection'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.5, 'ndcg@10': 0.631, 'AveP@50': 0.5, 'query': 'pedestrian detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.022, 'query': 'unity'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'face detection'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'bilingual lexicon induction'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.333, 'MRR@10': 0.143, 'ndcg@10': 0.156, 'AveP@50': 0.066, 'query': 'autonomous driving'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.5, 'recall@10': 0.5, 'MRR@10': 0.25, 'ndcg@10': 0.264, 'AveP@50': 0.125, 'query': 'few shot learning'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'monocular depth estimation'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'stochastic optimization'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.034, 'query': 'program induction'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'language acquisition'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'one class classifier'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'learning to execute'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.667, 'precisions@5': 0.4, 'precisions@10': 0.2, 'recall@1': 0.0, 'recall@3': 0.667, 'recall@5': 0.667, 'recall@10': 0.667, 'MRR@10': 0.5, 'ndcg@10': 0.531, 'AveP@50': 0.431, 'query': 'adversarial defense'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'automl'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'boundary detection'}, {'hit@1': 1, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 1.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 1.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 1.0, 'ndcg@10': 1.0, 'AveP@50': 1.0, 'query': 'chinese word segmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.023, 'query': 'molecular property prediction'}, {'hit@1': 0, 'hit@3': 1, 'hit@5': 1, 'hit@10': 1, 'precisions@1': 0.0, 'precisions@3': 0.333, 'precisions@5': 0.2, 'precisions@10': 0.1, 'recall@1': 0.0, 'recall@3': 1.0, 'recall@5': 1.0, 'recall@10': 1.0, 'MRR@10': 0.333, 'ndcg@10': 0.5, 'AveP@50': 0.333, 'query': 'generalization bounds'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.043, 'query': 'depth and camera motion'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.067, 'query': 'image augmentation'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.056, 'query': 'image cropping'}, {'hit@1': 0, 'hit@3': 0, 'hit@5': 0, 'hit@10': 0, 'precisions@1': 0.0, 'precisions@3': 0.0, 'precisions@5': 0.0, 'precisions@10': 0.0, 'recall@1': 0.0, 'recall@3': 0.0, 'recall@5': 0.0, 'recall@10': 0.0, 'MRR@10': 0.0, 'ndcg@10': 0.0, 'AveP@50': 0.0, 'query': 'optical character recognition'}]} generation_metrics={0: [{'repo': 'nikolamilosevic86/TabInOut', 'tasks_x': 'semantic information retrieval, databases, person name, ', 'true_tasks': ['information retrieval', 'table detection'], 'generated_text': 'camera calibration information retrieval', 'prompt_info': {'content': 'AnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC', 'generated_text': '\n## repository\nhahnec/plenopticam\n## files\nplenopticam/bin/cli_script.py, plenopticam/cfg/cfg.py, plenopticam/gui/top_level.py, plenopticam/gui/widget_about.py, plenopticam/gui/widget_cmnd.py, rmdir_p, w2_metric, idx_str_sort, isbool, create_gauss_kernel, cond_lfp_align, load_cal_data, lfp_img, vp_img_linear, cond_auto_find\n## tags\n[information retrieval, camera calibration]\n\n## repository\ndongyp13/Non-Targeted-Adversarial-Attacks\n## files\nattack_iter.py, nets/mobilenet_v1_test.py, nets/alexnet_test.py, nets/overfeat_test.py, nets/inception_resnet_v2_test.py, resnet_v2_block, MobilenetV1Test, resnet_v2_200, VGG19Test, resnet_v2_152, prediction_fn, max_epsilon, max_epsilon, num_iter, num_iter\n## tags\n[adversarial attack]\n\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## 3 tags\n[semantic information retrieval, databases, person name]\n\n## repository\nificador/', 'id': '1f4b742456894eaf9d5310b8ac1faec9', 'name': 'nikolamilosevic86/TabInOut', 'tasks': 'semantic information retrieval, databases, person name, ', 'true_text': 'information retrieval, table detection'}, 'generation': 0, 'input_text': '\n## repository\nhahnec/plenopticam\n## files\nplenopticam/bin/cli_script.py, plenopticam/cfg/cfg.py, plenopticam/gui/top_level.py, plenopticam/gui/widget_about.py, plenopticam/gui/widget_cmnd.py, rmdir_p, w2_metric, idx_str_sort, isbool, create_gauss_kernel, cond_lfp_align, load_cal_data, lfp_img, vp_img_linear, cond_auto_find\n## tags\n[information retrieval, camera calibration]\n\n## repository\ndongyp13/Non-Targeted-Adversarial-Attacks\n## files\nattack_iter.py, nets/mobilenet_v1_test.py, nets/alexnet_test.py, nets/overfeat_test.py, nets/inception_resnet_v2_test.py, resnet_v2_block, MobilenetV1Test, resnet_v2_200, VGG19Test, resnet_v2_152, prediction_fn, max_epsilon, max_epsilon, num_iter, num_iter\n## tags\n[adversarial attack]\n\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/marvin-semantic-annotation-using-multiple', 'https://paperswithcode.com/paper/a-framework-for-information-extraction-from']", 'paper_titles': "['Marvin: Semantic annotation using multiple knowledge sources', 'A framework for information extraction from tables in biomedical literature']", 'arxiv_ids': "['1602.00515', '1902.10031']", 'authors': '["[\'Nikola Milosevic\']", "[\'Nikola Milosevic\', \'Cassie Gregson\', \'Robert Hernandez\', \'Goran Nenadic\']"]', 'raw_generated_text': '\n## repository\nhahnec/plenopticam\n## files\nplenopticam/bin/cli_script.py, plenopticam/cfg/cfg.py, plenopticam/gui/top_level.py, plenopticam/gui/widget_about.py, plenopticam/gui/widget_cmnd.py, rmdir_p, w2_metric, idx_str_sort, isbool, create_gauss_kernel, cond_lfp_align, load_cal_data, lfp_img, vp_img_linear, cond_auto_find\n## tags\n[information retrieval, camera calibration]\n\n## repository\ndongyp13/Non-Targeted-Adversarial-Attacks\n## files\nattack_iter.py, nets/mobilenet_v1_test.py, nets/alexnet_test.py, nets/overfeat_test.py, nets/inception_resnet_v2_test.py, resnet_v2_block, MobilenetV1Test, resnet_v2_200, VGG19Test, resnet_v2_152, prediction_fn, max_epsilon, max_epsilon, num_iter, num_iter\n## tags\n[adversarial attack]\n\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## 3 tags\n[semantic information retrieval, databases, person name]\n\n## repository\nificador/', 'raw_reference_text': 'information retrieval, table detection', 'reference_text': 'information retrieval table detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8737976551055908, 'bertscore_recall': 0.8918699026107788, 'bertscore_f1': 0.8827412724494934, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.5, 'rouge2': 0.3333333333333333, 'rougeL': 0.5, 'rougeLsum': 0.5, 'sentence_transformer_similarity': 0.4526696801185608}, {'repo': 'JLrumberger/SpatialEmbeddingLoss', 'tasks_x': 'semantic segmentation, untrusted roberry, tifffile, ', 'true_tasks': ['semantic segmentation', 'autonomous driving', 'instance segmentation'], 'generated_text': 'lesion segmentation semantic segmentation medical image segmentation', 'prompt_info': {'content': 'embedding_loss_complete.py, joint_loss_single, seed_loss_single, map_lovasz, constr_phi_2D, lovasz_hinge, constr_phi_2D, lovasz_hinge, unique_with_counts, lovasz_hinge_flat, phi', 'generated_text': '\n## repository\nneuropoly/ivado-medical-imaging\n## files\ndev/class_balance.py, dev/data_aug_dilation.py, dev/filtering_lesion.py, dev/metadata_config.py, dev/plot_cluster_metadata.py, apply_preprocessing_transforms, tio_transform, RandomReverse, ImedTransform, two_dim_compatible, ofolder, suffixUnc, bids, ofolders, CROP_PARAMS\n## tags\n[medical image segmentation, semantic segmentation, lesion segmentation]\n\n## repository\njeffheaton/t81_558_deep_learning\n## files\npy/image_server_1.py, py/image_web_server_1.py, py/mpg_server_1.py, send_root, calc_mpg, send_index, upload_image, upload_image, decode_predictions, secure_filename, img_to_array, ANTIALIAS, preprocess_input\n## tags\n[time series]\n\n## repository\nJLrumberger/SpatialEmbeddingLoss\n## files\nembedding_loss_complete.py, joint_loss_single, seed_loss_single, map_lovasz, constr_phi_2D, lovasz_hinge, constr_phi_2D, lovasz_hinge, unique_with_counts, lovasz_hinge_flat, phi\n## 3 tags\n[semantic segmentation, untrusted roberry, tifffile]\n\n##', 'id': '00b78c3bf0f3ea2ab27795f6e0953ce9', 'name': 'JLrumberger/SpatialEmbeddingLoss', 'tasks': 'semantic segmentation, untrusted roberry, tifffile, ', 'true_text': 'semantic segmentation, autonomous driving, instance segmentation'}, 'generation': 0, 'input_text': '\n## repository\nneuropoly/ivado-medical-imaging\n## files\ndev/class_balance.py, dev/data_aug_dilation.py, dev/filtering_lesion.py, dev/metadata_config.py, dev/plot_cluster_metadata.py, apply_preprocessing_transforms, tio_transform, RandomReverse, ImedTransform, two_dim_compatible, ofolder, suffixUnc, bids, ofolders, CROP_PARAMS\n## tags\n[medical image segmentation, semantic segmentation, lesion segmentation]\n\n## repository\njeffheaton/t81_558_deep_learning\n## files\npy/image_server_1.py, py/image_web_server_1.py, py/mpg_server_1.py, send_root, calc_mpg, send_index, upload_image, upload_image, decode_predictions, secure_filename, img_to_array, ANTIALIAS, preprocess_input\n## tags\n[time series]\n\n## repository\nJLrumberger/SpatialEmbeddingLoss\n## files\nembedding_loss_complete.py, joint_loss_single, seed_loss_single, map_lovasz, constr_phi_2D, lovasz_hinge, constr_phi_2D, lovasz_hinge, unique_with_counts, lovasz_hinge_flat, phi\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/instance-segmentation-by-jointly-optimizing-1']", 'paper_titles': "['Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth']", 'arxiv_ids': "['1906.11109']", 'authors': '["[\'Davy Neven\', \'Bert de Brabandere\', \'Marc Proesmans\', \'Luc van Gool\']"]', 'raw_generated_text': '\n## repository\nneuropoly/ivado-medical-imaging\n## files\ndev/class_balance.py, dev/data_aug_dilation.py, dev/filtering_lesion.py, dev/metadata_config.py, dev/plot_cluster_metadata.py, apply_preprocessing_transforms, tio_transform, RandomReverse, ImedTransform, two_dim_compatible, ofolder, suffixUnc, bids, ofolders, CROP_PARAMS\n## tags\n[medical image segmentation, semantic segmentation, lesion segmentation]\n\n## repository\njeffheaton/t81_558_deep_learning\n## files\npy/image_server_1.py, py/image_web_server_1.py, py/mpg_server_1.py, send_root, calc_mpg, send_index, upload_image, upload_image, decode_predictions, secure_filename, img_to_array, ANTIALIAS, preprocess_input\n## tags\n[time series]\n\n## repository\nJLrumberger/SpatialEmbeddingLoss\n## files\nembedding_loss_complete.py, joint_loss_single, seed_loss_single, map_lovasz, constr_phi_2D, lovasz_hinge, constr_phi_2D, lovasz_hinge, unique_with_counts, lovasz_hinge_flat, phi\n## 3 tags\n[semantic segmentation, untrusted roberry, tifffile]\n\n##', 'raw_reference_text': 'semantic segmentation, autonomous driving, instance segmentation', 'reference_text': 'instance segmentation semantic segmentation autonomous driving', 'edit_word': 0.5714285714285714, 'jaccard_lst': 0.0, 'bertscore_precision': 0.9091979265213013, 'bertscore_recall': 0.9382361173629761, 'bertscore_f1': 0.9234887957572937, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.4615384615384615, 'rouge2': 0.3636363636363636, 'rougeL': 0.4615384615384615, 'rougeLsum': 0.4615384615384615, 'sentence_transformer_similarity': 0.6099397540092468}, {'repo': 'braunefe/BWEeval', 'tasks_x': 'clfs, clustering, ', 'true_tasks': ['word embeddings', 'bilingual lexicon induction', 'machine translation'], 'generated_text': 'word embeddings', 'prompt_info': {'content': 'scripts/average_for_eval.py, scripts/checkIfInSeed.py, scripts/countMatches.py, scripts/maxmarg/computeMostSimilar.py, scripts/ridge/computeMostSimilar.py, average_n_distances, load_valid, get_most_similar, get_most_similar, print_eval, my_dists, load_valid, edit, ratio, OptionParser', 'generated_text': '\n## repository\nvinid/cade\n## files\ncade/cade.py, cade/cli.py, cade/metrics/comparative.py, cade/metrics/stability.py, tests/test_cade.py, initialize_avgs, lncs2_setted, lncs2, get_mean_if_missing, moving_lncs2, shared_voc, shared_voc, get_neighbors_set, get_neighbors_set, get_neighbors_set\n## tags\n[word embeddings]\n\n## repository\ngudovskiy/al-fk-self-supervision\n## files\nimagenet/gen_descr.py, imagenet/main_descr.py, imagenet/unsup.py, imagenet/custom_datasets/dali.py, imagenet/custom_datasets/folder.py, NetMC, NetMSA, get_miss_and_cm, imagenet_train_transformer, imagenet_test_transformer, subtype_method, ensemble_index, noisy_ratio, descriptor_length, descriptor_length\n## tags\n[active learning]\n\n## repository\nbraunefe/BWEeval\n## files\nscripts/average_for_eval.py, scripts/checkIfInSeed.py, scripts/countMatches.py, scripts/maxmarg/computeMostSimilar.py, scripts/ridge/computeMostSimilar.py, average_n_distances, load_valid, get_most_similar, get_most_similar, print_eval, my_dists, load_valid, edit, ratio, OptionParser\n## 3 tags\n[clfs] [clustering]\n\n## repository\nekapope/room', 'id': '0416360c3e4216f841dd84ee4a2cdfa2', 'name': 'braunefe/BWEeval', 'tasks': 'clfs, clustering, ', 'true_text': 'word embeddings, bilingual lexicon induction, machine translation'}, 'generation': 0, 'input_text': '\n## repository\nvinid/cade\n## files\ncade/cade.py, cade/cli.py, cade/metrics/comparative.py, cade/metrics/stability.py, tests/test_cade.py, initialize_avgs, lncs2_setted, lncs2, get_mean_if_missing, moving_lncs2, shared_voc, shared_voc, get_neighbors_set, get_neighbors_set, get_neighbors_set\n## tags\n[word embeddings]\n\n## repository\ngudovskiy/al-fk-self-supervision\n## files\nimagenet/gen_descr.py, imagenet/main_descr.py, imagenet/unsup.py, imagenet/custom_datasets/dali.py, imagenet/custom_datasets/folder.py, NetMC, NetMSA, get_miss_and_cm, imagenet_train_transformer, imagenet_test_transformer, subtype_method, ensemble_index, noisy_ratio, descriptor_length, descriptor_length\n## tags\n[active learning]\n\n## repository\nbraunefe/BWEeval\n## files\nscripts/average_for_eval.py, scripts/checkIfInSeed.py, scripts/countMatches.py, scripts/maxmarg/computeMostSimilar.py, scripts/ridge/computeMostSimilar.py, average_n_distances, load_valid, get_most_similar, get_most_similar, print_eval, my_dists, load_valid, edit, ratio, OptionParser\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/evaluating-bilingual-word-embeddings-on-the']", 'paper_titles': "['Evaluating bilingual word embeddings on the long tail']", 'arxiv_ids': '[nan]', 'authors': '["[\'Fabienne Braune\', \'Viktor Hangya\', \'Tobias Eder\', \'Alex Fraser\', \'er\']"]', 'raw_generated_text': '\n## repository\nvinid/cade\n## files\ncade/cade.py, cade/cli.py, cade/metrics/comparative.py, cade/metrics/stability.py, tests/test_cade.py, initialize_avgs, lncs2_setted, lncs2, get_mean_if_missing, moving_lncs2, shared_voc, shared_voc, get_neighbors_set, get_neighbors_set, get_neighbors_set\n## tags\n[word embeddings]\n\n## repository\ngudovskiy/al-fk-self-supervision\n## files\nimagenet/gen_descr.py, imagenet/main_descr.py, imagenet/unsup.py, imagenet/custom_datasets/dali.py, imagenet/custom_datasets/folder.py, NetMC, NetMSA, get_miss_and_cm, imagenet_train_transformer, imagenet_test_transformer, subtype_method, ensemble_index, noisy_ratio, descriptor_length, descriptor_length\n## tags\n[active learning]\n\n## repository\nbraunefe/BWEeval\n## files\nscripts/average_for_eval.py, scripts/checkIfInSeed.py, scripts/countMatches.py, scripts/maxmarg/computeMostSimilar.py, scripts/ridge/computeMostSimilar.py, average_n_distances, load_valid, get_most_similar, get_most_similar, print_eval, my_dists, load_valid, edit, ratio, OptionParser\n## 3 tags\n[clfs] [clustering]\n\n## repository\nekapope/room', 'raw_reference_text': 'word embeddings, bilingual lexicon induction, machine translation', 'reference_text': 'bilingual lexicon induction word embeddings machine translation', 'edit_word': 0.7142857142857143, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8952443599700928, 'bertscore_recall': 0.8452011942863464, 'bertscore_f1': 0.8695033192634583, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.4444444444444445, 'rouge2': 0.2857142857142857, 'rougeL': 0.4444444444444445, 'rougeLsum': 0.4444444444444445, 'sentence_transformer_similarity': 0.49965786933898926}, {'repo': 'sunguanxiong/my_dwt', 'tasks_x': 'PointCloud, Denoising, Eye-nose-pair, ', 'true_tasks': ['semantic segmentation', 'template matching', 'instance segmentation'], 'generated_text': 'semantic segmentation edge detection', 'prompt_info': {'content': 'DN/direction_model.py, DN/ioUtils.py, DN/lossFunction.py, DN/train_direction.py, E2E/forward.py, ssProcess, angularErrorLoss, exceedingAngleThreshold, watershed_cut, angularErrorTotal, angularErrorTotal, depthCELoss2, countTotal, countTotalWeighted, remove_small_holes', 'generated_text': '\n## repository\nxjock/deeplebv3plus-hedplus\n## files\ndeeplabv3plus_hedplus_model.py, loss_al.py, inference.py, train.py, hed_model_fn, side_layer, deeplabv3_plus_model_fn, deeplab_v3_plus_generator, attention_loss, infer_data_list, clean_model_dir, mean_image_addition, epochs_per_eval, compute_mean_iou\n## tags\n[semantic segmentation, edge detection]\n\n## repository\noellop/Style_Transfer\n## files\nget_picture.py, VGG16_AvgPool_CutOff, get_loss_and_grads_wrapper, create_style_model, unpreprocess, VGG16_AvgPool, VGG16_AvgPool, get_loss_grads, AveragePooling2D, permute_dimensions, batch_flatten\n## tags\n[style transfer]\n\n## repository\nsunguanxiong/my_dwt\n## files\nDN/direction_model.py, DN/ioUtils.py, DN/lossFunction.py, DN/train_direction.py, E2E/forward.py, ssProcess, angularErrorLoss, exceedingAngleThreshold, watershed_cut, angularErrorTotal, angularErrorTotal, depthCELoss2, countTotal, countTotalWeighted, remove_small_holes\n## 3 tags\n[PointCloud, Denoising, Eye-nose-pair]\n\n##', 'id': '27ef4b13b093e0f6674f43c22e0fc860', 'name': 'sunguanxiong/my_dwt', 'tasks': 'PointCloud, Denoising, Eye-nose-pair, ', 'true_text': 'semantic segmentation, template matching, instance segmentation'}, 'generation': 0, 'input_text': '\n## repository\nxjock/deeplebv3plus-hedplus\n## files\ndeeplabv3plus_hedplus_model.py, loss_al.py, inference.py, train.py, hed_model_fn, side_layer, deeplabv3_plus_model_fn, deeplab_v3_plus_generator, attention_loss, infer_data_list, clean_model_dir, mean_image_addition, epochs_per_eval, compute_mean_iou\n## tags\n[semantic segmentation, edge detection]\n\n## repository\noellop/Style_Transfer\n## files\nget_picture.py, VGG16_AvgPool_CutOff, get_loss_and_grads_wrapper, create_style_model, unpreprocess, VGG16_AvgPool, VGG16_AvgPool, get_loss_grads, AveragePooling2D, permute_dimensions, batch_flatten\n## tags\n[style transfer]\n\n## repository\nsunguanxiong/my_dwt\n## files\nDN/direction_model.py, DN/ioUtils.py, DN/lossFunction.py, DN/train_direction.py, E2E/forward.py, ssProcess, angularErrorLoss, exceedingAngleThreshold, watershed_cut, angularErrorTotal, angularErrorTotal, depthCELoss2, countTotal, countTotalWeighted, remove_small_holes\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/deep-watershed-transform-for-instance']", 'paper_titles': "['Deep Watershed Transform for Instance Segmentation']", 'arxiv_ids': "['1611.08303']", 'authors': '["[\'Min Bai\', \'Raquel Urtasun\']"]', 'raw_generated_text': '\n## repository\nxjock/deeplebv3plus-hedplus\n## files\ndeeplabv3plus_hedplus_model.py, loss_al.py, inference.py, train.py, hed_model_fn, side_layer, deeplabv3_plus_model_fn, deeplab_v3_plus_generator, attention_loss, infer_data_list, clean_model_dir, mean_image_addition, epochs_per_eval, compute_mean_iou\n## tags\n[semantic segmentation, edge detection]\n\n## repository\noellop/Style_Transfer\n## files\nget_picture.py, VGG16_AvgPool_CutOff, get_loss_and_grads_wrapper, create_style_model, unpreprocess, VGG16_AvgPool, VGG16_AvgPool, get_loss_grads, AveragePooling2D, permute_dimensions, batch_flatten\n## tags\n[style transfer]\n\n## repository\nsunguanxiong/my_dwt\n## files\nDN/direction_model.py, DN/ioUtils.py, DN/lossFunction.py, DN/train_direction.py, E2E/forward.py, ssProcess, angularErrorLoss, exceedingAngleThreshold, watershed_cut, angularErrorTotal, angularErrorTotal, depthCELoss2, countTotal, countTotalWeighted, remove_small_holes\n## 3 tags\n[PointCloud, Denoising, Eye-nose-pair]\n\n##', 'raw_reference_text': 'semantic segmentation, template matching, instance segmentation', 'reference_text': 'instance segmentation semantic segmentation template matching', 'edit_word': 0.6666666666666666, 'jaccard_lst': 0.0, 'bertscore_precision': 0.9289980530738831, 'bertscore_recall': 0.9158260226249695, 'bertscore_f1': 0.9223650097846985, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.4, 'rouge2': 0.25, 'rougeL': 0.4, 'rougeLsum': 0.4, 'sentence_transformer_similarity': 0.530237078666687}, {'repo': 'jeasung-pf/MORAN_v2', 'tasks_x': 'multi-aspect sentiment, multi-aspect classification, ', 'true_tasks': ['scene text recognition'], 'generated_text': 'extractive summarization text summarization', 'prompt_info': {'content': 'models/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version', 'generated_text': '\n## repository\nvagisha-nidhi/TextSummarizer\n## files\nentity2.py, numofSen.py, para_reader.py, saveCre.py, plotLines.py, executeForAFile, cuePhraseFeature, posTagger, tfIsf, similarityScores, gibbs_vhv, posTagger, tfIsf, similarityScores, properNounScores\n## tags\n[text summarization, extractive summarization]\n\n## repository\nshenwzh3/RGAT-ABSA\n## files\ndata_preprocess_semeval.py, data_preprocess_twitter.py, model_gcn.py, tree.py, model_utils.py, DotprodAttention, RelationAttention, DepparseMultiHeadAttention, reshape_dependency_tree, my_collate_elmo, gat_bert, gat_our, load_datasets_and_vocabs, Pure_Bert, Aspect_Text_GAT_only\n## tags\n[sentiment analysis, aspect based sentiment analysis, graph attention]\n\n## repository\njeasung-pf/MORAN_v2\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## 3 tags\n[multi-aspect sentiment,     multi-aspect classification]\n\n## repository\n', 'id': 'f9a2dc515b91d08ddea6992ad0bfa290', 'name': 'jeasung-pf/MORAN_v2', 'tasks': 'multi-aspect sentiment, multi-aspect classification, ', 'true_text': 'scene text recognition'}, 'generation': 0, 'input_text': '\n## repository\nvagisha-nidhi/TextSummarizer\n## files\nentity2.py, numofSen.py, para_reader.py, saveCre.py, plotLines.py, executeForAFile, cuePhraseFeature, posTagger, tfIsf, similarityScores, gibbs_vhv, posTagger, tfIsf, similarityScores, properNounScores\n## tags\n[text summarization, extractive summarization]\n\n## repository\nshenwzh3/RGAT-ABSA\n## files\ndata_preprocess_semeval.py, data_preprocess_twitter.py, model_gcn.py, tree.py, model_utils.py, DotprodAttention, RelationAttention, DepparseMultiHeadAttention, reshape_dependency_tree, my_collate_elmo, gat_bert, gat_our, load_datasets_and_vocabs, Pure_Bert, Aspect_Text_GAT_only\n## tags\n[sentiment analysis, aspect based sentiment analysis, graph attention]\n\n## repository\njeasung-pf/MORAN_v2\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-multi-object-rectified-attention-network']", 'paper_titles': "['A Multi-Object Rectified Attention Network for Scene Text Recognition']", 'arxiv_ids': "['1901.03003']", 'authors': '["[\'Canjie Luo\', \'Lianwen Jin\', \'Zenghui Sun\']"]', 'raw_generated_text': '\n## repository\nvagisha-nidhi/TextSummarizer\n## files\nentity2.py, numofSen.py, para_reader.py, saveCre.py, plotLines.py, executeForAFile, cuePhraseFeature, posTagger, tfIsf, similarityScores, gibbs_vhv, posTagger, tfIsf, similarityScores, properNounScores\n## tags\n[text summarization, extractive summarization]\n\n## repository\nshenwzh3/RGAT-ABSA\n## files\ndata_preprocess_semeval.py, data_preprocess_twitter.py, model_gcn.py, tree.py, model_utils.py, DotprodAttention, RelationAttention, DepparseMultiHeadAttention, reshape_dependency_tree, my_collate_elmo, gat_bert, gat_our, load_datasets_and_vocabs, Pure_Bert, Aspect_Text_GAT_only\n## tags\n[sentiment analysis, aspect based sentiment analysis, graph attention]\n\n## repository\njeasung-pf/MORAN_v2\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## 3 tags\n[multi-aspect sentiment,     multi-aspect classification]\n\n## repository\n', 'raw_reference_text': 'scene text recognition', 'reference_text': 'scene text recognition', 'edit_word': 0.75, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7987899780273438, 'bertscore_recall': 0.8435332179069519, 'bertscore_f1': 0.8205521106719971, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.28571428571428575, 'rouge2': 0.0, 'rougeL': 0.28571428571428575, 'rougeLsum': 0.28571428571428575, 'sentence_transformer_similarity': 0.42711806297302246}, {'repo': 'ezhan94/multiagent-programmatic-supervision', 'tasks_x': 'planning, game, ', 'true_tasks': ['imitation learning'], 'generated_text': 'facial expression recognition', 'prompt_info': {'content': 'datasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN', 'generated_text': '\n## repository\ndonydchen/FMPN-FER\n## files\ndata/affectnet.py, data/ckplus_res.py, data/mmi_res.py, model/res_baseline.py, model/res_cls.py, define_ClassifierNet, define_ResFaceGenNet, define_FusionNet, PseudoNorm, ResFaceGenNet, ResFaceGenNet, ResGenModel, ResClsModel, ResFaceClsSolver, ResFaceSolver\n## tags\n[facial expression recognition]\n\n## repository\nasoroa/ukb\n## files\ncontrib/python-server/example.py, contrib/python-server/ukbprotocol.py, start_ukb_server, start_shutdown_server, UkbSession, test, UkbSession, recv, send, call, call\n## tags\n[entity disambiguation]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## 3 tags\n[planning] [game]\n\n## repository\naditya-vora', 'id': '315ccb062062c44a156c3977f8f37c29', 'name': 'ezhan94/multiagent-programmatic-supervision', 'tasks': 'planning, game, ', 'true_text': 'imitation learning'}, 'generation': 0, 'input_text': '\n## repository\ndonydchen/FMPN-FER\n## files\ndata/affectnet.py, data/ckplus_res.py, data/mmi_res.py, model/res_baseline.py, model/res_cls.py, define_ClassifierNet, define_ResFaceGenNet, define_FusionNet, PseudoNorm, ResFaceGenNet, ResFaceGenNet, ResGenModel, ResClsModel, ResFaceClsSolver, ResFaceSolver\n## tags\n[facial expression recognition]\n\n## repository\nasoroa/ukb\n## files\ncontrib/python-server/example.py, contrib/python-server/ukbprotocol.py, start_ukb_server, start_shutdown_server, UkbSession, test, UkbSession, recv, send, call, call\n## tags\n[entity disambiguation]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/generating-multi-agent-trajectories-using']", 'paper_titles': "['Generating Multi-Agent Trajectories using Programmatic Weak Supervision']", 'arxiv_ids': "['1803.07612']", 'authors': '["[\'Eric Zhan\', \'Stephan Zheng\', \'Yisong Yue\', \'Long Sha\', \'Patrick Lucey\']"]', 'raw_generated_text': '\n## repository\ndonydchen/FMPN-FER\n## files\ndata/affectnet.py, data/ckplus_res.py, data/mmi_res.py, model/res_baseline.py, model/res_cls.py, define_ClassifierNet, define_ResFaceGenNet, define_FusionNet, PseudoNorm, ResFaceGenNet, ResFaceGenNet, ResGenModel, ResClsModel, ResFaceClsSolver, ResFaceSolver\n## tags\n[facial expression recognition]\n\n## repository\nasoroa/ukb\n## files\ncontrib/python-server/example.py, contrib/python-server/ukbprotocol.py, start_ukb_server, start_shutdown_server, UkbSession, test, UkbSession, recv, send, call, call\n## tags\n[entity disambiguation]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## 3 tags\n[planning] [game]\n\n## repository\naditya-vora', 'raw_reference_text': 'imitation learning', 'reference_text': 'imitation learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8605663180351257, 'bertscore_recall': 0.9074534177780151, 'bertscore_f1': 0.8833881616592407, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23837250471115112}, {'repo': 'StanfordVL/arxivbot', 'tasks_x': 'programming language, github bot, ', 'true_tasks': ['program induction', 'few shot learning'], 'generated_text': 'autonomous driving', 'prompt_info': {'content': 'forever.py, bot.py, format_arxiv, parse_bot_commands, parse_direct_mention, handle_command, parse_arxiv, parse_direct_mention, PlaintextParser, parse_arxiv, api_call, summarizer', 'generated_text': '\n## repository\nthouyang/ccd-dsa\n## files\ndif_dsa.py, train_model.py, run.py, utils.py, cal_cla_matrix, cal_dsa0, cal_dsa1, cal_dsa2, cal_dsa3, find_closest_at, find_closest_at, find_closest_at, compute_roc, predict_classes\n## tags\n[autonomous driving]\n\n## repository\nmkduer/semantic-fluency-nn\n## files\nexperiments/irt_comparisons.py, experiments/tSNE_prototype.py, src/algorithms/hill_climbing.py, src/algorithms/irt.py, src/algorithms/random_walk.py, NetworkxPlot, SemanticGraphTests, TestTextWrangler, TsnePlot, HillClimber, C_COMPONENTS, DIM3_ITER, DIM3_COMPONENTS, DIM3_PERPLEXITY, C_ETA\n## tags\n[language acquisition]\n\n## repository\nStanfordVL/arxivbot\n## files\nforever.py, bot.py, format_arxiv, parse_bot_commands, parse_direct_mention, handle_command, parse_arxiv, parse_direct_mention, PlaintextParser, parse_arxiv, api_call, summarizer\n## 3 tags\n[programming language] [github bot]\n\n## repository\ntext-equival', 'id': '99f8cd89f5fe5493a73fad72ff883f8a', 'name': 'StanfordVL/arxivbot', 'tasks': 'programming language, github bot, ', 'true_text': 'program induction, few shot learning'}, 'generation': 0, 'input_text': '\n## repository\nthouyang/ccd-dsa\n## files\ndif_dsa.py, train_model.py, run.py, utils.py, cal_cla_matrix, cal_dsa0, cal_dsa1, cal_dsa2, cal_dsa3, find_closest_at, find_closest_at, find_closest_at, compute_roc, predict_classes\n## tags\n[autonomous driving]\n\n## repository\nmkduer/semantic-fluency-nn\n## files\nexperiments/irt_comparisons.py, experiments/tSNE_prototype.py, src/algorithms/hill_climbing.py, src/algorithms/irt.py, src/algorithms/random_walk.py, NetworkxPlot, SemanticGraphTests, TestTextWrangler, TsnePlot, HillClimber, C_COMPONENTS, DIM3_ITER, DIM3_COMPONENTS, DIM3_PERPLEXITY, C_ETA\n## tags\n[language acquisition]\n\n## repository\nStanfordVL/arxivbot\n## files\nforever.py, bot.py, format_arxiv, parse_bot_commands, parse_direct_mention, handle_command, parse_arxiv, parse_direct_mention, PlaintextParser, parse_arxiv, api_call, summarizer\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/neural-task-programming-learning-to']", 'paper_titles': "['Neural Task Programming: Learning to Generalize Across Hierarchical Tasks']", 'arxiv_ids': "['1710.01813']", 'authors': '["[\'Danfei Xu\', \'Suraj Nair\', \'Yuke Zhu\', \'Julian Gao\', \'Animesh Garg\', \'Li Fei-Fei\', \'Silvio Savarese\']"]', 'raw_generated_text': '\n## repository\nthouyang/ccd-dsa\n## files\ndif_dsa.py, train_model.py, run.py, utils.py, cal_cla_matrix, cal_dsa0, cal_dsa1, cal_dsa2, cal_dsa3, find_closest_at, find_closest_at, find_closest_at, compute_roc, predict_classes\n## tags\n[autonomous driving]\n\n## repository\nmkduer/semantic-fluency-nn\n## files\nexperiments/irt_comparisons.py, experiments/tSNE_prototype.py, src/algorithms/hill_climbing.py, src/algorithms/irt.py, src/algorithms/random_walk.py, NetworkxPlot, SemanticGraphTests, TestTextWrangler, TsnePlot, HillClimber, C_COMPONENTS, DIM3_ITER, DIM3_COMPONENTS, DIM3_PERPLEXITY, C_ETA\n## tags\n[language acquisition]\n\n## repository\nStanfordVL/arxivbot\n## files\nforever.py, bot.py, format_arxiv, parse_bot_commands, parse_direct_mention, handle_command, parse_arxiv, parse_direct_mention, PlaintextParser, parse_arxiv, api_call, summarizer\n## 3 tags\n[programming language] [github bot]\n\n## repository\ntext-equival', 'raw_reference_text': 'program induction, few shot learning', 'reference_text': 'few shot learning program induction', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8888254761695862, 'bertscore_recall': 0.8443263173103333, 'bertscore_f1': 0.8660045862197876, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.09811987727880478}, {'repo': 'tensorflow/kfac', 'tasks_x': 'neural network, optimizer, sampling, ', 'true_tasks': ['stochastic optimization'], 'generated_text': 'scene text recognition optical character recognition', 'prompt_info': {'content': 'kfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss', 'generated_text': '\n## repository\nshivaverma/Score-Time-Detection\n## files\napp.py, main.py, create_result, strLabelConverter, resizeNormalize, hello_world, CRNN, strLabelConverter, resizeNormalize, CRNN, IntTensor, transformer\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\niancovert/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## tags\n[time series, sparse learning]\n\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## 3 tags\n[neural network, optimizer, sampling]\n\n## repository\ntensorflow/benchmarks\n', 'id': 'e59ca2ac0776a545d41cad522bb81167', 'name': 'tensorflow/kfac', 'tasks': 'neural network, optimizer, sampling, ', 'true_text': 'stochastic optimization'}, 'generation': 0, 'input_text': '\n## repository\nshivaverma/Score-Time-Detection\n## files\napp.py, main.py, create_result, strLabelConverter, resizeNormalize, hello_world, CRNN, strLabelConverter, resizeNormalize, CRNN, IntTensor, transformer\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\niancovert/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## tags\n[time series, sparse learning]\n\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/optimizing-neural-networks-with-kronecker']", 'paper_titles': "['Optimizing Neural Networks with Kronecker-factored Approximate Curvature']", 'arxiv_ids': "['1503.05671']", 'authors': '["[\'James Martens\', \'Roger Grosse\']"]', 'raw_generated_text': '\n## repository\nshivaverma/Score-Time-Detection\n## files\napp.py, main.py, create_result, strLabelConverter, resizeNormalize, hello_world, CRNN, strLabelConverter, resizeNormalize, CRNN, IntTensor, transformer\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\niancovert/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## tags\n[time series, sparse learning]\n\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## 3 tags\n[neural network, optimizer, sampling]\n\n## repository\ntensorflow/benchmarks\n', 'raw_reference_text': 'stochastic optimization', 'reference_text': 'stochastic optimization', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8334263563156128, 'bertscore_recall': 0.7748105525970459, 'bertscore_f1': 0.8030502796173096, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23253268003463745}, {'repo': 'aimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop', 'tasks_x': 'deep learning, modules, ', 'true_tasks': ['semantic segmentation', 'monocular depth estimation', 'scene parsing', 'depth estimation'], 'generated_text': 'speech recognition', 'prompt_info': {'content': 'matconvnet/utils/proto/caffe_fastrcnn_pb2.py, matconvnet/doc/matdoc.py, matconvnet/doc/matdocparser.py, matconvnet/utils/proto/caffe_b590f1d_pb2.py, matconvnet/utils/import-caffe.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH', 'generated_text': '\n## repository\nmailong25/wav2letter\n## files\nbindings/python/examples/decoder_example.py, bindings/python/examples/feature_example.py, bindings/python/wav2letter/criterion_torch.py, bindings/python/wav2letter/feature.py, bindings/python/wav2letter/common.py, get_cuda_stream_as_bytes, FACFunction, FCCFunction, ASGLoss, get_data_ptr_as_bytes, BeginSentenceWrite, cpu_impl, run_get_workspace_size, transform_asg_back, run_direction\n## tags\n[speech recognition]\n\n## repository\ntaimurhassan/cancer\n## files\nsalient.py, codebase/train2.py, codebase/models/drn_encoder.py, codebase/models/net2.py, codebase/models/net_models.py, drn_segnet, drn_net, net_50, net_101, get_drn_encoder, ImageSegmentationGen, train_action, evaluate_model_action, verify_dataset_action, visualize_dataset_action\n## tags\n[whole slide images, semantic segmentation, scene parsing]\n\n## repository\naimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop\n## files\nmatconvnet/utils/proto/caffe_fastrcnn_pb2.py, matconvnet/doc/matdoc.py, matconvnet/doc/matdocparser.py, matconvnet/utils/proto/caffe_b590f1d_pb2.py, matconvnet/utils/import-caffe.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## 3 tags\n[deep learning, modules]\n\n## repository\nxuwei-k/c', 'id': '41165bbbbbed7ea3ec56455361ef6153', 'name': 'aimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop', 'tasks': 'deep learning, modules, ', 'true_text': 'semantic segmentation, monocular depth estimation, scene parsing, depth estimation'}, 'generation': 0, 'input_text': '\n## repository\nmailong25/wav2letter\n## files\nbindings/python/examples/decoder_example.py, bindings/python/examples/feature_example.py, bindings/python/wav2letter/criterion_torch.py, bindings/python/wav2letter/feature.py, bindings/python/wav2letter/common.py, get_cuda_stream_as_bytes, FACFunction, FCCFunction, ASGLoss, get_data_ptr_as_bytes, BeginSentenceWrite, cpu_impl, run_get_workspace_size, transform_asg_back, run_direction\n## tags\n[speech recognition]\n\n## repository\ntaimurhassan/cancer\n## files\nsalient.py, codebase/train2.py, codebase/models/drn_encoder.py, codebase/models/net2.py, codebase/models/net_models.py, drn_segnet, drn_net, net_50, net_101, get_drn_encoder, ImageSegmentationGen, train_action, evaluate_model_action, verify_dataset_action, visualize_dataset_action\n## tags\n[whole slide images, semantic segmentation, scene parsing]\n\n## repository\naimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop\n## files\nmatconvnet/utils/proto/caffe_fastrcnn_pb2.py, matconvnet/doc/matdoc.py, matconvnet/doc/matdocparser.py, matconvnet/utils/proto/caffe_b590f1d_pb2.py, matconvnet/utils/import-caffe.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/recurrent-scene-parsing-with-perspective']", 'paper_titles': "['Recurrent Scene Parsing with Perspective Understanding in the Loop']", 'arxiv_ids': "['1705.07238']", 'authors': '["[\'Shu Kong\', \'Charless Fowlkes\']"]', 'raw_generated_text': '\n## repository\nmailong25/wav2letter\n## files\nbindings/python/examples/decoder_example.py, bindings/python/examples/feature_example.py, bindings/python/wav2letter/criterion_torch.py, bindings/python/wav2letter/feature.py, bindings/python/wav2letter/common.py, get_cuda_stream_as_bytes, FACFunction, FCCFunction, ASGLoss, get_data_ptr_as_bytes, BeginSentenceWrite, cpu_impl, run_get_workspace_size, transform_asg_back, run_direction\n## tags\n[speech recognition]\n\n## repository\ntaimurhassan/cancer\n## files\nsalient.py, codebase/train2.py, codebase/models/drn_encoder.py, codebase/models/net2.py, codebase/models/net_models.py, drn_segnet, drn_net, net_50, net_101, get_drn_encoder, ImageSegmentationGen, train_action, evaluate_model_action, verify_dataset_action, visualize_dataset_action\n## tags\n[whole slide images, semantic segmentation, scene parsing]\n\n## repository\naimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop\n## files\nmatconvnet/utils/proto/caffe_fastrcnn_pb2.py, matconvnet/doc/matdoc.py, matconvnet/doc/matdocparser.py, matconvnet/utils/proto/caffe_b590f1d_pb2.py, matconvnet/utils/import-caffe.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## 3 tags\n[deep learning, modules]\n\n## repository\nxuwei-k/c', 'raw_reference_text': 'semantic segmentation, monocular depth estimation, scene parsing, depth estimation', 'reference_text': 'depth estimation semantic segmentation scene parsing monocular depth estimation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8807287216186523, 'bertscore_recall': 0.8242811560630798, 'bertscore_f1': 0.8515704870223999, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2662668228149414}, {'repo': 'anguyen8/generative-attribution-methods', 'tasks_x': 'adversarial, theory, adversarial, theory, ', 'true_tasks': ['object localization'], 'generated_text': 'text generation', 'prompt_info': {'content': 'formal_LIME_single_image.py, formal_MP_single_image.py, formal_SP_single_image.py, formal_utils.py, lime_image.py, load_class_label, load_model_places365, load_orig_places365_model, load_imagenet_loader, unnormalize_imagenet_img, perturb_binary, DISCOUNTED_MASK, SPATIAL_DISCOUNTING_GAMMA, imageA, imageB', 'generated_text': '\n## repository\nAWLyrics/SeqGAN_Poem\n## files\nbleu_calc.py, lyric_preprocessing.py, mydis.py, mygen.py, myG_beta.py, Input_Data_loader, generate_samples_v2, pre_train_epoch_v2, rhyme1, rhyme2, g_update, Input_Data_loader, pretrain_step_v2, generate_samples_v2, pre_train_epoch_v2\n## tags\n[text generation]\n\n## repository\nyanggeng1995/vae_tacotron\n## files\ndatasets/blizzard2013.py, datasets/datafeeder.py, tests/cmudict_test.py, tests/numbers_test.py, tests/text_test.py, vae_weight, preprocess_blizzard2013, inv_spectrogram_tensorflow, _stft_tensorflow, _istft_tensorflow, reference_audio, preprocess_blizzard2013, preprocess_blizzard, get_output_base_path, preprocess_ljspeech\n## tags\n[style transfer, speech synthesis]\n\n## repository\nanguyen8/generative-attribution-methods\n## files\nformal_LIME_single_image.py, formal_MP_single_image.py, formal_SP_single_image.py, formal_utils.py, lime_image.py, load_class_label, load_model_places365, load_orig_places365_model, load_imagenet_loader, unnormalize_imagenet_img, perturb_binary, DISCOUNTED_MASK, SPATIAL_DISCOUNTING_GAMMA, imageA, imageB\n## 3 tags\n[adversarial, theory]\n[adversarial, theory]\n\n', 'id': '699aefcd5b5c3042f24ab14cf651a233', 'name': 'anguyen8/generative-attribution-methods', 'tasks': 'adversarial, theory, adversarial, theory, ', 'true_text': 'object localization'}, 'generation': 0, 'input_text': '\n## repository\nAWLyrics/SeqGAN_Poem\n## files\nbleu_calc.py, lyric_preprocessing.py, mydis.py, mygen.py, myG_beta.py, Input_Data_loader, generate_samples_v2, pre_train_epoch_v2, rhyme1, rhyme2, g_update, Input_Data_loader, pretrain_step_v2, generate_samples_v2, pre_train_epoch_v2\n## tags\n[text generation]\n\n## repository\nyanggeng1995/vae_tacotron\n## files\ndatasets/blizzard2013.py, datasets/datafeeder.py, tests/cmudict_test.py, tests/numbers_test.py, tests/text_test.py, vae_weight, preprocess_blizzard2013, inv_spectrogram_tensorflow, _stft_tensorflow, _istft_tensorflow, reference_audio, preprocess_blizzard2013, preprocess_blizzard, get_output_base_path, preprocess_ljspeech\n## tags\n[style transfer, speech synthesis]\n\n## repository\nanguyen8/generative-attribution-methods\n## files\nformal_LIME_single_image.py, formal_MP_single_image.py, formal_SP_single_image.py, formal_utils.py, lime_image.py, load_class_label, load_model_places365, load_orig_places365_model, load_imagenet_loader, unnormalize_imagenet_img, perturb_binary, DISCOUNTED_MASK, SPATIAL_DISCOUNTING_GAMMA, imageA, imageB\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/removing-input-features-via-a-generative-1']", 'paper_titles': "['Explaining image classifiers by removing input features using generative models']", 'arxiv_ids': "['1910.04256']", 'authors': '["[\'Chirag Agarwal\', \'Anh Nguyen\']"]', 'raw_generated_text': '\n## repository\nAWLyrics/SeqGAN_Poem\n## files\nbleu_calc.py, lyric_preprocessing.py, mydis.py, mygen.py, myG_beta.py, Input_Data_loader, generate_samples_v2, pre_train_epoch_v2, rhyme1, rhyme2, g_update, Input_Data_loader, pretrain_step_v2, generate_samples_v2, pre_train_epoch_v2\n## tags\n[text generation]\n\n## repository\nyanggeng1995/vae_tacotron\n## files\ndatasets/blizzard2013.py, datasets/datafeeder.py, tests/cmudict_test.py, tests/numbers_test.py, tests/text_test.py, vae_weight, preprocess_blizzard2013, inv_spectrogram_tensorflow, _stft_tensorflow, _istft_tensorflow, reference_audio, preprocess_blizzard2013, preprocess_blizzard, get_output_base_path, preprocess_ljspeech\n## tags\n[style transfer, speech synthesis]\n\n## repository\nanguyen8/generative-attribution-methods\n## files\nformal_LIME_single_image.py, formal_MP_single_image.py, formal_SP_single_image.py, formal_utils.py, lime_image.py, load_class_label, load_model_places365, load_orig_places365_model, load_imagenet_loader, unnormalize_imagenet_img, perturb_binary, DISCOUNTED_MASK, SPATIAL_DISCOUNTING_GAMMA, imageA, imageB\n## 3 tags\n[adversarial, theory]\n[adversarial, theory]\n\n', 'raw_reference_text': 'object localization', 'reference_text': 'object localization', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.9035288095474243, 'bertscore_recall': 0.9035288095474243, 'bertscore_f1': 0.9035288095474243, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.386760950088501}, {'repo': 'renmengye/oc-fewshot-public', 'tasks_x': 'segmentation, bbox extraction, bbox segmentation, ', 'true_tasks': ['few shot learning'], 'generated_text': 'combinatorial optimization', 'prompt_info': {'content': 'fewshot/data/metadata.py, fewshot/data/registry.py, fewshot/data/datasets/dataset.py, fewshot/data/datasets/episode.py, fewshot/data/datasets/matterport.py, attention2bbox, decode_png_to_np, bbox2centroid, extract_headings, extract_elevations, set_backbone_lr, try_flush, var_to_optimize, try_log, usebest', 'generated_text': '\n## repository\ndgchachlakis/The-exact-solution-to-rank-1-L1-norm-Tucker2-decomposition\n## files\nalgorithms/exact.py, algorithms/exactpoly.py, utils/computeCandidates.py, utils/decimal2binary.py, utils/mysign.py, exactpoly, computeCandidates, ymatrix, mysign, xmatrix, computeCandidates, ymatrix, xmatrix, xmatrix, decimal2binary\n## tags\n[combinatorial optimization]\n\n## repository\n131250208/TPlinker-joint-extraction\n## files\ncommon/components.py, tplinker/tplinker.py, tplinker/train.py, tplinker_plus/tplinker_plus.py, tplinker_plus/train.py, TPLinkerBert, TPLinkerBiLSTM, TPLinkerPlusBiLSTM, TPLinkerPlusBert, bias_loss, rel_extractor, get_rel_cpg, get_cpg, get_sample_accuracy, get_sample_accuracy\n## tags\n[relation extraction]\n\n## repository\nrenmengye/oc-fewshot-public\n## files\nfewshot/data/metadata.py, fewshot/data/registry.py, fewshot/data/datasets/dataset.py, fewshot/data/datasets/episode.py, fewshot/data/datasets/matterport.py, attention2bbox, decode_png_to_np, bbox2centroid, extract_headings, extract_elevations, set_backbone_lr, try_flush, var_to_optimize, try_log, usebest\n## 3 tags\n[segmentation], [bbox extraction], [bbox segmentation]\n\n## repository\njor', 'id': '2a763980175f2665c5c1a1de8c431706', 'name': 'renmengye/oc-fewshot-public', 'tasks': 'segmentation, bbox extraction, bbox segmentation, ', 'true_text': 'few shot learning'}, 'generation': 0, 'input_text': '\n## repository\ndgchachlakis/The-exact-solution-to-rank-1-L1-norm-Tucker2-decomposition\n## files\nalgorithms/exact.py, algorithms/exactpoly.py, utils/computeCandidates.py, utils/decimal2binary.py, utils/mysign.py, exactpoly, computeCandidates, ymatrix, mysign, xmatrix, computeCandidates, ymatrix, xmatrix, xmatrix, decimal2binary\n## tags\n[combinatorial optimization]\n\n## repository\n131250208/TPlinker-joint-extraction\n## files\ncommon/components.py, tplinker/tplinker.py, tplinker/train.py, tplinker_plus/tplinker_plus.py, tplinker_plus/train.py, TPLinkerBert, TPLinkerBiLSTM, TPLinkerPlusBiLSTM, TPLinkerPlusBert, bias_loss, rel_extractor, get_rel_cpg, get_cpg, get_sample_accuracy, get_sample_accuracy\n## tags\n[relation extraction]\n\n## repository\nrenmengye/oc-fewshot-public\n## files\nfewshot/data/metadata.py, fewshot/data/registry.py, fewshot/data/datasets/dataset.py, fewshot/data/datasets/episode.py, fewshot/data/datasets/matterport.py, attention2bbox, decode_png_to_np, bbox2centroid, extract_headings, extract_elevations, set_backbone_lr, try_flush, var_to_optimize, try_log, usebest\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/wandering-within-a-world-online']", 'paper_titles': "['Wandering Within a World: Online Contextualized Few-Shot Learning']", 'arxiv_ids': "['2007.04546']", 'authors': '["[\'Mengye Ren\', \'Michael L. Iuzzolino\', \'Michael C. Mozer\', \'Richard S. Zemel\']"]', 'raw_generated_text': '\n## repository\ndgchachlakis/The-exact-solution-to-rank-1-L1-norm-Tucker2-decomposition\n## files\nalgorithms/exact.py, algorithms/exactpoly.py, utils/computeCandidates.py, utils/decimal2binary.py, utils/mysign.py, exactpoly, computeCandidates, ymatrix, mysign, xmatrix, computeCandidates, ymatrix, xmatrix, xmatrix, decimal2binary\n## tags\n[combinatorial optimization]\n\n## repository\n131250208/TPlinker-joint-extraction\n## files\ncommon/components.py, tplinker/tplinker.py, tplinker/train.py, tplinker_plus/tplinker_plus.py, tplinker_plus/train.py, TPLinkerBert, TPLinkerBiLSTM, TPLinkerPlusBiLSTM, TPLinkerPlusBert, bias_loss, rel_extractor, get_rel_cpg, get_cpg, get_sample_accuracy, get_sample_accuracy\n## tags\n[relation extraction]\n\n## repository\nrenmengye/oc-fewshot-public\n## files\nfewshot/data/metadata.py, fewshot/data/registry.py, fewshot/data/datasets/dataset.py, fewshot/data/datasets/episode.py, fewshot/data/datasets/matterport.py, attention2bbox, decode_png_to_np, bbox2centroid, extract_headings, extract_elevations, set_backbone_lr, try_flush, var_to_optimize, try_log, usebest\n## 3 tags\n[segmentation], [bbox extraction], [bbox segmentation]\n\n## repository\njor', 'raw_reference_text': 'few shot learning', 'reference_text': 'few shot learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.816845178604126, 'bertscore_recall': 0.8471550941467285, 'bertscore_f1': 0.8317241072654724, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.15279245376586914}, {'repo': 'VDIGPKU/DADA', 'tasks_x': 'ml research code, ', 'true_tasks': ['data augmentation'], 'generated_text': 'semantic segmentation', 'prompt_info': {'content': 'fast-autoaugment/archive.py, fast-autoaugment/FastAutoAugment/archive.py, fast-autoaugment/FastAutoAugment/augmentations.py, fast-autoaugment/FastAutoAugment/common.py, fast-autoaugment/FastAutoAugment/data.py, MixedAugment, MixedAugment, DifferentiableAugment, DifferentiableAugment, AugmentDataset, sample_ops_weights_index, sample_probabilities_index, probabilities_b, ops_weights_b, ops_weights', 'generated_text': '\n## repository\nSoumyabrata/nighttime-imaging\n## files\nscripts/undistort_WAHRSIS_imgs.py, scripts/internal_calibration.py, cuda_interpolate, cuda_interpolate3D, undistortCC, cam2world, world2cam, get_texref, get_texref, matrix_to_texref, matrix_to_texref, set_filter_mode\n## tags\n[semantic segmentation]\n\n## repository\nlzx551402/contextdesc\n## files\nevaluations_geodesc.py, evaluations_rootsift.py, datasets/imw2019.py, models/aug_model.py, models/geodesc_model.py, recoverer, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features, loc_model, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features\n## tags\n[geometric matching]\n\n## repository\nVDIGPKU/DADA\n## files\nfast-autoaugment/archive.py, fast-autoaugment/FastAutoAugment/archive.py, fast-autoaugment/FastAutoAugment/augmentations.py, fast-autoaugment/FastAutoAugment/common.py, fast-autoaugment/FastAutoAugment/data.py, MixedAugment, MixedAugment, DifferentiableAugment, DifferentiableAugment, AugmentDataset, sample_ops_weights_index, sample_probabilities_index, probabilities_b, ops_weights_b, ops_weights\n## 3 tags\n[ml research code]\n\n## repository\nlzx55140', 'id': 'f051eee95dbb74ffa572e336cacb033b', 'name': 'VDIGPKU/DADA', 'tasks': 'ml research code, ', 'true_text': 'data augmentation'}, 'generation': 0, 'input_text': '\n## repository\nSoumyabrata/nighttime-imaging\n## files\nscripts/undistort_WAHRSIS_imgs.py, scripts/internal_calibration.py, cuda_interpolate, cuda_interpolate3D, undistortCC, cam2world, world2cam, get_texref, get_texref, matrix_to_texref, matrix_to_texref, set_filter_mode\n## tags\n[semantic segmentation]\n\n## repository\nlzx551402/contextdesc\n## files\nevaluations_geodesc.py, evaluations_rootsift.py, datasets/imw2019.py, models/aug_model.py, models/geodesc_model.py, recoverer, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features, loc_model, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features\n## tags\n[geometric matching]\n\n## repository\nVDIGPKU/DADA\n## files\nfast-autoaugment/archive.py, fast-autoaugment/FastAutoAugment/archive.py, fast-autoaugment/FastAutoAugment/augmentations.py, fast-autoaugment/FastAutoAugment/common.py, fast-autoaugment/FastAutoAugment/data.py, MixedAugment, MixedAugment, DifferentiableAugment, DifferentiableAugment, AugmentDataset, sample_ops_weights_index, sample_probabilities_index, probabilities_b, ops_weights_b, ops_weights\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/dada-differentiable-automatic-data']", 'paper_titles': "['DADA: Differentiable Automatic Data Augmentation']", 'arxiv_ids': "['2003.03780']", 'authors': '["[\'Yonggang Li\', \'Guosheng Hu\', \'Yongtao Wang\', \'Timothy Hospedales\', \'Neil M. Robertson\', \'Yongxin Yang\']"]', 'raw_generated_text': '\n## repository\nSoumyabrata/nighttime-imaging\n## files\nscripts/undistort_WAHRSIS_imgs.py, scripts/internal_calibration.py, cuda_interpolate, cuda_interpolate3D, undistortCC, cam2world, world2cam, get_texref, get_texref, matrix_to_texref, matrix_to_texref, set_filter_mode\n## tags\n[semantic segmentation]\n\n## repository\nlzx551402/contextdesc\n## files\nevaluations_geodesc.py, evaluations_rootsift.py, datasets/imw2019.py, models/aug_model.py, models/geodesc_model.py, recoverer, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features, loc_model, extract_reg_feat, extract_aug_feat, extract_regional_features, extract_augmented_features\n## tags\n[geometric matching]\n\n## repository\nVDIGPKU/DADA\n## files\nfast-autoaugment/archive.py, fast-autoaugment/FastAutoAugment/archive.py, fast-autoaugment/FastAutoAugment/augmentations.py, fast-autoaugment/FastAutoAugment/common.py, fast-autoaugment/FastAutoAugment/data.py, MixedAugment, MixedAugment, DifferentiableAugment, DifferentiableAugment, AugmentDataset, sample_ops_weights_index, sample_probabilities_index, probabilities_b, ops_weights_b, ops_weights\n## 3 tags\n[ml research code]\n\n## repository\nlzx55140', 'raw_reference_text': 'data augmentation', 'reference_text': 'data augmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8813839554786682, 'bertscore_recall': 0.8968353271484375, 'bertscore_f1': 0.8890425562858582, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23017701506614685}, {'repo': 'julian-risch/KONVENS2019_and_LREC2020', 'tasks_x': "EL MODIFICATIONN'     - EL MODIFICATIONN'n mr", 'true_tasks': ['text classification'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'farm/convert_tf_checkpoint_to_pytorch.py, farm/metrics.py, test/test_doc_classification.py, examples/doc_classification.py, examples/doc_regression.py, test_qa, read_ner_file, read_squad_file, read_docs_from_txt, create_samples_sentence_pairs, getArgValue, add_cls_sep, mask_random_words, _SQUAD_improve_answer_span, _SQUAD_check_is_max_context', 'generated_text': "\n## repository\nsumuzhao/CycleGAN-Music-Style-Transfer\n## files\nconvert_clean.py, style_classifier.py, Testfile.py, write_midi.py, module.py, conv2d_musegan, deconv2d_musegan, load_midi_data, get_rand_samples, get_now_datetime, midi_filter, estimate_beat_start, get_midi_info, from_pretty_midi, get_merged_pianoroll\n## tags\n[style transfer]\n\n## repository\nElementAI/LCFCN\n## files\nlcfcn/lcfcn.py, lcfcn/lcfcn_loss.py, lcfcn/networks.py, scripts/test_on_image.py, src/datasets/shanghai.py, shrink2roi, FCN8_ResNet, Shanghai, ToLong, LCFCN, delete_and_backup_experiment, Shanghai, train_on_loader, val_on_loader, LCFCN\n## tags\n[object counting]\n\n## repository\njulian-risch/KONVENS2019_and_LREC2020\n## files\nfarm/convert_tf_checkpoint_to_pytorch.py, farm/metrics.py, test/test_doc_classification.py, examples/doc_classification.py, examples/doc_regression.py, test_qa, read_ner_file, read_squad_file, read_docs_from_txt, create_samples_sentence_pairs, getArgValue, add_cls_sep, mask_random_words, _SQUAD_improve_answer_span, _SQUAD_check_is_max_context\n## 3 tags\n[EL MODIFICATIONN'\n    - EL MODIFICATIONN'n mr", 'id': '1adacf3d6db6712dcdda4267410f5797', 'name': 'julian-risch/KONVENS2019_and_LREC2020', 'tasks': "EL MODIFICATIONN'     - EL MODIFICATIONN'n mr", 'true_text': 'text classification'}, 'generation': 0, 'input_text': '\n## repository\nsumuzhao/CycleGAN-Music-Style-Transfer\n## files\nconvert_clean.py, style_classifier.py, Testfile.py, write_midi.py, module.py, conv2d_musegan, deconv2d_musegan, load_midi_data, get_rand_samples, get_now_datetime, midi_filter, estimate_beat_start, get_midi_info, from_pretty_midi, get_merged_pianoroll\n## tags\n[style transfer]\n\n## repository\nElementAI/LCFCN\n## files\nlcfcn/lcfcn.py, lcfcn/lcfcn_loss.py, lcfcn/networks.py, scripts/test_on_image.py, src/datasets/shanghai.py, shrink2roi, FCN8_ResNet, Shanghai, ToLong, LCFCN, delete_and_backup_experiment, Shanghai, train_on_loader, val_on_loader, LCFCN\n## tags\n[object counting]\n\n## repository\njulian-risch/KONVENS2019_and_LREC2020\n## files\nfarm/convert_tf_checkpoint_to_pytorch.py, farm/metrics.py, test/test_doc_classification.py, examples/doc_classification.py, examples/doc_regression.py, test_qa, read_ner_file, read_squad_file, read_docs_from_txt, create_samples_sentence_pairs, getArgValue, add_cls_sep, mask_random_words, _SQUAD_improve_answer_span, _SQUAD_check_is_max_context\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/bagging-bert-models-for-robust-aggression']", 'paper_titles': "['Bagging BERT Models for Robust Aggression Identification']", 'arxiv_ids': '[nan]', 'authors': '["[\'Julian Risch\', \'Ralf Krestel\']"]', 'raw_generated_text': "\n## repository\nsumuzhao/CycleGAN-Music-Style-Transfer\n## files\nconvert_clean.py, style_classifier.py, Testfile.py, write_midi.py, module.py, conv2d_musegan, deconv2d_musegan, load_midi_data, get_rand_samples, get_now_datetime, midi_filter, estimate_beat_start, get_midi_info, from_pretty_midi, get_merged_pianoroll\n## tags\n[style transfer]\n\n## repository\nElementAI/LCFCN\n## files\nlcfcn/lcfcn.py, lcfcn/lcfcn_loss.py, lcfcn/networks.py, scripts/test_on_image.py, src/datasets/shanghai.py, shrink2roi, FCN8_ResNet, Shanghai, ToLong, LCFCN, delete_and_backup_experiment, Shanghai, train_on_loader, val_on_loader, LCFCN\n## tags\n[object counting]\n\n## repository\njulian-risch/KONVENS2019_and_LREC2020\n## files\nfarm/convert_tf_checkpoint_to_pytorch.py, farm/metrics.py, test/test_doc_classification.py, examples/doc_classification.py, examples/doc_regression.py, test_qa, read_ner_file, read_squad_file, read_docs_from_txt, create_samples_sentence_pairs, getArgValue, add_cls_sep, mask_random_words, _SQUAD_improve_answer_span, _SQUAD_check_is_max_context\n## 3 tags\n[EL MODIFICATIONN'\n    - EL MODIFICATIONN'n mr", 'raw_reference_text': 'text classification', 'reference_text': 'text classification', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8910351991653442, 'bertscore_recall': 0.8897929191589355, 'bertscore_f1': 0.8904135823249817, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.28095871210098267}, {'repo': 'clovaai/EXTD_Pytorch', 'tasks_x': 'anchor match, face detection, widerface dataset evaluation, ', 'true_tasks': ['face detection'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'EXTD_32.py, EXTD_48.py, EXTD_64.py, mobileFacenet_32_PReLU.py, mobileFacenet_48_PReLU.py, plot_anchor_match, eval_wider, eval_wider, HandDetection, dyna_anchor, EPOCHES, OVERLAP_THRESH, WIDER_DIR, fnames, pull_image', 'generated_text': '\n## repository\nnightldj/behance_release\n## files\nmake_loss.py, make_opt.py, my_autoencoder.py, my_discriminator.py, train_mask.py, s_make_dataset, SelectImageFolder, get_autoencoder_args, get_dise_cfg, get_base_dep, style_data, content_data, lr_freq, s_is_image_file, PredAdam\n## tags\n[style transfer]\n\n## repository\nmike-n-7/ADEM\n## files\nvhred/src/adam.py, vhred/src/eval_model_hred.py, vhred/src/eval_model_hred_fair.py, vhred/src/hred_encoder.py, vhred/src/model.py, demo_config, demo_config, standard_config, standard_config, AMT_DataLoader, NormalInit, batched_dot, sharedX, sharedX, sharedX\n## tags\n[dialogue evaluation, response generation]\n\n## repository\nclovaai/EXTD_Pytorch\n## files\nEXTD_32.py, EXTD_48.py, EXTD_64.py, mobileFacenet_32_PReLU.py, mobileFacenet_48_PReLU.py, plot_anchor_match, eval_wider, eval_wider, HandDetection, dyna_anchor, EPOCHES, OVERLAP_THRESH, WIDER_DIR, fnames, pull_image\n## 3 tags\n[anchor match, face detection, widerface dataset evaluation]\n\n## repository', 'id': 'ecce58c05de3d40c05b14ab8c355e021', 'name': 'clovaai/EXTD_Pytorch', 'tasks': 'anchor match, face detection, widerface dataset evaluation, ', 'true_text': 'face detection'}, 'generation': 0, 'input_text': '\n## repository\nnightldj/behance_release\n## files\nmake_loss.py, make_opt.py, my_autoencoder.py, my_discriminator.py, train_mask.py, s_make_dataset, SelectImageFolder, get_autoencoder_args, get_dise_cfg, get_base_dep, style_data, content_data, lr_freq, s_is_image_file, PredAdam\n## tags\n[style transfer]\n\n## repository\nmike-n-7/ADEM\n## files\nvhred/src/adam.py, vhred/src/eval_model_hred.py, vhred/src/eval_model_hred_fair.py, vhred/src/hred_encoder.py, vhred/src/model.py, demo_config, demo_config, standard_config, standard_config, AMT_DataLoader, NormalInit, batched_dot, sharedX, sharedX, sharedX\n## tags\n[dialogue evaluation, response generation]\n\n## repository\nclovaai/EXTD_Pytorch\n## files\nEXTD_32.py, EXTD_48.py, EXTD_64.py, mobileFacenet_32_PReLU.py, mobileFacenet_48_PReLU.py, plot_anchor_match, eval_wider, eval_wider, HandDetection, dyna_anchor, EPOCHES, OVERLAP_THRESH, WIDER_DIR, fnames, pull_image\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/extd-extremely-tiny-face-detector-via']", 'paper_titles': "['EXTD: Extremely Tiny Face Detector via Iterative Filter Reuse']", 'arxiv_ids': "['1906.06579']", 'authors': '["[\'YoungJoon Yoo\', \'Dongyoon Han\', \'Sangdoo Yun\']"]', 'raw_generated_text': '\n## repository\nnightldj/behance_release\n## files\nmake_loss.py, make_opt.py, my_autoencoder.py, my_discriminator.py, train_mask.py, s_make_dataset, SelectImageFolder, get_autoencoder_args, get_dise_cfg, get_base_dep, style_data, content_data, lr_freq, s_is_image_file, PredAdam\n## tags\n[style transfer]\n\n## repository\nmike-n-7/ADEM\n## files\nvhred/src/adam.py, vhred/src/eval_model_hred.py, vhred/src/eval_model_hred_fair.py, vhred/src/hred_encoder.py, vhred/src/model.py, demo_config, demo_config, standard_config, standard_config, AMT_DataLoader, NormalInit, batched_dot, sharedX, sharedX, sharedX\n## tags\n[dialogue evaluation, response generation]\n\n## repository\nclovaai/EXTD_Pytorch\n## files\nEXTD_32.py, EXTD_48.py, EXTD_64.py, mobileFacenet_32_PReLU.py, mobileFacenet_48_PReLU.py, plot_anchor_match, eval_wider, eval_wider, HandDetection, dyna_anchor, EPOCHES, OVERLAP_THRESH, WIDER_DIR, fnames, pull_image\n## 3 tags\n[anchor match, face detection, widerface dataset evaluation]\n\n## repository', 'raw_reference_text': 'face detection', 'reference_text': 'face detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8350099325180054, 'bertscore_recall': 0.8308817148208618, 'bertscore_f1': 0.8329406976699829, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1712818741798401}, {'repo': 'lminvielle/mom-kde', 'tasks_x': 'robustness, gPromat Tag 7/2/7', 'true_tasks': ['density estimation'], 'generated_text': 'gaussian processes', 'prompt_info': {'content': 'libs/exp_lib.py, libs/kde_lib.py, compare_kde_methods_synthetic.py, compare_kde_methods.py, libs/metrics.py, set_metricname, set_algoname, load_data_outlier, balance_outlier, true_density, area_density, area_MC_mom, set_io_class, set_datasetname, irls', 'generated_text': '\n## repository\ncsjtx1021/DGBO\n## files\nAdaptiveBasis.py, base_prior.py, ChooseNext.py, DeepSurrogateModel.py, DGBO.py, BO_process, CombDGCNWithDNN, Pooling_sum_paramsfree, Pooling_sum_normal_params2, RelationGraphConvolution_noBasisRegularization, phi_matrix, w_m0, hyp_samples, eval_jump, forcejump\n## tags\n[gaussian processes]\n\n## repository\nYuanXinCherry/Person-reID-Evaluation\n## files\nGOM.py, test.py, norm_distance, evaluate_closed, evaluate_open, print_GOM, ProgressBar, intersect1d, intersect1d, trapz, ProgressBar, ProgressBar\n## tags\n[image retrieval]\n\n## repository\nlminvielle/mom-kde\n## files\nlibs/exp_lib.py, libs/kde_lib.py, compare_kde_methods_synthetic.py, compare_kde_methods.py, libs/metrics.py, set_metricname, set_algoname, load_data_outlier, balance_outlier, true_density, area_density, area_MC_mom, set_io_class, set_datasetname, irls\n## 3 tags\n[robustness]\n\ngPromat Tag 7/2/7', 'id': '8ac13b47b1f81351b7e7ca468a5b8ce2', 'name': 'lminvielle/mom-kde', 'tasks': 'robustness, gPromat Tag 7/2/7', 'true_text': 'density estimation'}, 'generation': 0, 'input_text': '\n## repository\ncsjtx1021/DGBO\n## files\nAdaptiveBasis.py, base_prior.py, ChooseNext.py, DeepSurrogateModel.py, DGBO.py, BO_process, CombDGCNWithDNN, Pooling_sum_paramsfree, Pooling_sum_normal_params2, RelationGraphConvolution_noBasisRegularization, phi_matrix, w_m0, hyp_samples, eval_jump, forcejump\n## tags\n[gaussian processes]\n\n## repository\nYuanXinCherry/Person-reID-Evaluation\n## files\nGOM.py, test.py, norm_distance, evaluate_closed, evaluate_open, print_GOM, ProgressBar, intersect1d, intersect1d, trapz, ProgressBar, ProgressBar\n## tags\n[image retrieval]\n\n## repository\nlminvielle/mom-kde\n## files\nlibs/exp_lib.py, libs/kde_lib.py, compare_kde_methods_synthetic.py, compare_kde_methods.py, libs/metrics.py, set_metricname, set_algoname, load_data_outlier, balance_outlier, true_density, area_density, area_MC_mom, set_io_class, set_datasetname, irls\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/robust-kernel-density-estimation-with-median']", 'paper_titles': "['Robust Kernel Density Estimation with Median-of-Means principle']", 'arxiv_ids': "['2006.16590']", 'authors': '["[\'Pierre Humbert\', \'Batiste Le Bars\', \'Ludovic Minvielle\', \'Nicolas Vayatis\']"]', 'raw_generated_text': '\n## repository\ncsjtx1021/DGBO\n## files\nAdaptiveBasis.py, base_prior.py, ChooseNext.py, DeepSurrogateModel.py, DGBO.py, BO_process, CombDGCNWithDNN, Pooling_sum_paramsfree, Pooling_sum_normal_params2, RelationGraphConvolution_noBasisRegularization, phi_matrix, w_m0, hyp_samples, eval_jump, forcejump\n## tags\n[gaussian processes]\n\n## repository\nYuanXinCherry/Person-reID-Evaluation\n## files\nGOM.py, test.py, norm_distance, evaluate_closed, evaluate_open, print_GOM, ProgressBar, intersect1d, intersect1d, trapz, ProgressBar, ProgressBar\n## tags\n[image retrieval]\n\n## repository\nlminvielle/mom-kde\n## files\nlibs/exp_lib.py, libs/kde_lib.py, compare_kde_methods_synthetic.py, compare_kde_methods.py, libs/metrics.py, set_metricname, set_algoname, load_data_outlier, balance_outlier, true_density, area_density, area_MC_mom, set_io_class, set_datasetname, irls\n## 3 tags\n[robustness]\n\ngPromat Tag 7/2/7', 'raw_reference_text': 'density estimation', 'reference_text': 'density estimation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8089927434921265, 'bertscore_recall': 0.864582896232605, 'bertscore_f1': 0.8358646035194397, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14043453335762024}, {'repo': 'carlotes247/IGGI19_Imitation_Learning_Workshop', 'tasks_x': 'dqn, drl, reinfo, bug, ', 'true_tasks': ['unity'], 'generated_text': 'word embeddings sentiment analysis', 'prompt_info': {'content': 'ml-agents/mlagents/trainers/tests/test_environments/test_simple.py, ml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, test_gail_dc, test_gail_visual, test_bc_trainer, test_initialize_trainer_parameters_uses_defaults, test_start_learning_updates_meta_curriculum_lesson_number, layer_ranks, model_tensors, map_ignored_layer_to_its_input, previous_all_brain_info, current_all_brain_info', 'generated_text': '\n## repository\nsaleiro/Financial-Sentiment-Analysis\n## files\nCode/sentiment_analysis.py, Code/preprocess.py, process_crawled, news_crawled, create_sentences, loadmpqas, loadl, loadmpqas, loadl, normalize_news, loadmpqa, getLexicon\n## tags\n[word embeddings, sentiment analysis]\n\n## repository\nmorduspordus/SingleClassRL\n## files\nget_losses.py, get_standard_arguments.py, other_utils.py, oxford_iii_pet.py, param_functions.py, transform_to_image, get_oxford_pet_labels, process_visualize_image, compute_edge_mask, compute_edge_mask_diag, get_oxford_pet_labels, process_visualize_image, create_train_epoch_runner, create_valid_epoch_runner, BorderLoss\n## tags\n[semantic segmentation]\n\n## repository\ncarlotes247/IGGI19_Imitation_Learning_Workshop\n## files\nml-agents/mlagents/trainers/tests/test_environments/test_simple.py, ml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, test_gail_dc, test_gail_visual, test_bc_trainer, test_initialize_trainer_parameters_uses_defaults, test_start_learning_updates_meta_curriculum_lesson_number, layer_ranks, model_tensors, map_ignored_layer_to_its_input, previous_all_brain_info, current_all_brain_info\n## 3 tags\n[dqn, drl, reinfo]\n[bug]\n\n\n\n##', 'id': '9e90d10a5faf7e63e92a0eb1168ec1f9', 'name': 'carlotes247/IGGI19_Imitation_Learning_Workshop', 'tasks': 'dqn, drl, reinfo, bug, ', 'true_text': 'unity'}, 'generation': 0, 'input_text': '\n## repository\nsaleiro/Financial-Sentiment-Analysis\n## files\nCode/sentiment_analysis.py, Code/preprocess.py, process_crawled, news_crawled, create_sentences, loadmpqas, loadl, loadmpqas, loadl, normalize_news, loadmpqa, getLexicon\n## tags\n[word embeddings, sentiment analysis]\n\n## repository\nmorduspordus/SingleClassRL\n## files\nget_losses.py, get_standard_arguments.py, other_utils.py, oxford_iii_pet.py, param_functions.py, transform_to_image, get_oxford_pet_labels, process_visualize_image, compute_edge_mask, compute_edge_mask_diag, get_oxford_pet_labels, process_visualize_image, create_train_epoch_runner, create_valid_epoch_runner, BorderLoss\n## tags\n[semantic segmentation]\n\n## repository\ncarlotes247/IGGI19_Imitation_Learning_Workshop\n## files\nml-agents/mlagents/trainers/tests/test_environments/test_simple.py, ml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, test_gail_dc, test_gail_visual, test_bc_trainer, test_initialize_trainer_parameters_uses_defaults, test_start_learning_updates_meta_curriculum_lesson_number, layer_ranks, model_tensors, map_ignored_layer_to_its_input, previous_all_brain_info, current_all_brain_info\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/unity-a-general-platform-for-intelligent']", 'paper_titles': "['Unity: A General Platform for Intelligent Agents']", 'arxiv_ids': "['1809.02627']", 'authors': '["[\'Arthur Juliani\', \'Vincent-Pierre Berges\', \'Ervin Teng\', \'Andrew Cohen\', \'Jonathan Harper\', \'Chris Elion\', \'Chris Goy\', \'Yuan Gao\', \'Hunter Henry\', \'Marwan Mattar\', \'Danny Lange\']"]', 'raw_generated_text': '\n## repository\nsaleiro/Financial-Sentiment-Analysis\n## files\nCode/sentiment_analysis.py, Code/preprocess.py, process_crawled, news_crawled, create_sentences, loadmpqas, loadl, loadmpqas, loadl, normalize_news, loadmpqa, getLexicon\n## tags\n[word embeddings, sentiment analysis]\n\n## repository\nmorduspordus/SingleClassRL\n## files\nget_losses.py, get_standard_arguments.py, other_utils.py, oxford_iii_pet.py, param_functions.py, transform_to_image, get_oxford_pet_labels, process_visualize_image, compute_edge_mask, compute_edge_mask_diag, get_oxford_pet_labels, process_visualize_image, create_train_epoch_runner, create_valid_epoch_runner, BorderLoss\n## tags\n[semantic segmentation]\n\n## repository\ncarlotes247/IGGI19_Imitation_Learning_Workshop\n## files\nml-agents/mlagents/trainers/tests/test_environments/test_simple.py, ml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, test_gail_dc, test_gail_visual, test_bc_trainer, test_initialize_trainer_parameters_uses_defaults, test_start_learning_updates_meta_curriculum_lesson_number, layer_ranks, model_tensors, map_ignored_layer_to_its_input, previous_all_brain_info, current_all_brain_info\n## 3 tags\n[dqn, drl, reinfo]\n[bug]\n\n\n\n##', 'raw_reference_text': 'unity', 'reference_text': 'unity', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7363160848617554, 'bertscore_recall': 0.8656187057495117, 'bertscore_f1': 0.7957489490509033, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.18852493166923523}, {'repo': 'hou-yz/MVDet', 'tasks_x': 'detection, video, ', 'true_tasks': ['human detection', 'pedestrian detection'], 'generated_text': 'word embeddings', 'prompt_info': {'content': 'video_visualize.py, multiview_detector/trainer.py, multiview_detector/datasets/frameDataset.py, multiview_detector/datasets/MultiviewX.py, multiview_detector/datasets/Wildtrack.py, BBOXTrainer, PerspectiveTrainer, add_heatmap_to_image, get_worldcoord_from_imagecoord, get_imagecoord_from_worldcoord, cls_thres, read_pom, get_pos_from_worldgrid, reducedgrid_shape, map_kernel', 'generated_text': '\n## repository\nbakarov/cross-lang-embeddings\n## files\ncurrent/translate.py, make_translated_dataset, set_translator, Lemmatizer, translate, set_from_lang, set_to_lang, Translater, set_key, set_text\n## tags\n[word embeddings]\n\n## repository\nsilversparro/wav2letter.pytorch\n## files\ndata/merge_manifests.py, data/an4.py, data/common_voice.py, data/ted.py, data/voxforge.py, _get_stft_kernels, PCEN, Cov1dBlock, WaveToLetter, werCalc, top_paths, cut_utterance, get_utterances_from_stm, _format_files, max_duration\n## tags\n[speech recognition]\n\n## repository\nhou-yz/MVDet\n## files\nvideo_visualize.py, multiview_detector/trainer.py, multiview_detector/datasets/frameDataset.py, multiview_detector/datasets/MultiviewX.py, multiview_detector/datasets/Wildtrack.py, BBOXTrainer, PerspectiveTrainer, add_heatmap_to_image, get_worldcoord_from_imagecoord, get_imagecoord_from_worldcoord, cls_thres, read_pom, get_pos_from_worldgrid, reducedgrid_shape, map_kernel\n## 3 tags\n[detection] [video]\n\n## repository\npglass/overlapped', 'id': '1e11169de3dd6eb2085fef5a4d47e991', 'name': 'hou-yz/MVDet', 'tasks': 'detection, video, ', 'true_text': 'human detection, pedestrian detection'}, 'generation': 0, 'input_text': '\n## repository\nbakarov/cross-lang-embeddings\n## files\ncurrent/translate.py, make_translated_dataset, set_translator, Lemmatizer, translate, set_from_lang, set_to_lang, Translater, set_key, set_text\n## tags\n[word embeddings]\n\n## repository\nsilversparro/wav2letter.pytorch\n## files\ndata/merge_manifests.py, data/an4.py, data/common_voice.py, data/ted.py, data/voxforge.py, _get_stft_kernels, PCEN, Cov1dBlock, WaveToLetter, werCalc, top_paths, cut_utterance, get_utterances_from_stm, _format_files, max_duration\n## tags\n[speech recognition]\n\n## repository\nhou-yz/MVDet\n## files\nvideo_visualize.py, multiview_detector/trainer.py, multiview_detector/datasets/frameDataset.py, multiview_detector/datasets/MultiviewX.py, multiview_detector/datasets/Wildtrack.py, BBOXTrainer, PerspectiveTrainer, add_heatmap_to_image, get_worldcoord_from_imagecoord, get_imagecoord_from_worldcoord, cls_thres, read_pom, get_pos_from_worldgrid, reducedgrid_shape, map_kernel\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/multiview-detection-with-feature-perspective']", 'paper_titles': "['Multiview Detection with Feature Perspective Transformation']", 'arxiv_ids': "['2007.07247']", 'authors': '["[\'Yunzhong Hou\', \'Liang Zheng\', \'Stephen Gould\']"]', 'raw_generated_text': '\n## repository\nbakarov/cross-lang-embeddings\n## files\ncurrent/translate.py, make_translated_dataset, set_translator, Lemmatizer, translate, set_from_lang, set_to_lang, Translater, set_key, set_text\n## tags\n[word embeddings]\n\n## repository\nsilversparro/wav2letter.pytorch\n## files\ndata/merge_manifests.py, data/an4.py, data/common_voice.py, data/ted.py, data/voxforge.py, _get_stft_kernels, PCEN, Cov1dBlock, WaveToLetter, werCalc, top_paths, cut_utterance, get_utterances_from_stm, _format_files, max_duration\n## tags\n[speech recognition]\n\n## repository\nhou-yz/MVDet\n## files\nvideo_visualize.py, multiview_detector/trainer.py, multiview_detector/datasets/frameDataset.py, multiview_detector/datasets/MultiviewX.py, multiview_detector/datasets/Wildtrack.py, BBOXTrainer, PerspectiveTrainer, add_heatmap_to_image, get_worldcoord_from_imagecoord, get_imagecoord_from_worldcoord, cls_thres, read_pom, get_pos_from_worldgrid, reducedgrid_shape, map_kernel\n## 3 tags\n[detection] [video]\n\n## repository\npglass/overlapped', 'raw_reference_text': 'human detection, pedestrian detection', 'reference_text': 'pedestrian detection human detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7915815114974976, 'bertscore_recall': 0.8291522860527039, 'bertscore_f1': 0.8099314570426941, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.04342927038669586}, {'repo': 'vincentzhang/roi-fcn', 'tasks_x': 'instance segmentation, semantic segmentation, Segmentation, ', 'true_tasks': ['object localization'], 'generated_text': 'visual tracking', 'prompt_info': {'content': 'data/scripts/google_drive.py, lib/autoseg/autoseg.py, lib/datasets/acce.py, lib/datasets/hip.py, lib/datasets/ins.py, _get_image_label_blob, test_net_seg, do_seg_tests_on_slices, get_minibatch_with_img_labels, compute_hist_imdb, compute_hist_imdb, _get_image_blob_and_label, label_path_at, label_path_at, compute_metrics_flat', 'generated_text': '\n## repository\nlifeng9472/IBCCF\n## files\nexternal_libs/matconvnet/doc/matdoc.py, external_libs/matconvnet/doc/matdocparser.py, external_libs/matconvnet/utils/import-caffe.py, external_libs/matconvnet/utils/layers.py, external_libs/matconvnet/utils/proto/caffe_0115_pb2.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## tags\n[visual tracking]\n\n## repository\nSrijha09/Making-Images-Artsy-Neural-Style-Transfer\n## files\nstyle_transfer.py, app.py, run.py, utils.py, is_file_allowed, serve_image, add_time, get_style_model_and_losses, get_input_optimizer, is_file_allowed, add_time, get_style_model_and_losses, get_input_optimizer, run_style_transfer\n## tags\n[style transfer]\n\n## repository\nvincentzhang/roi-fcn\n## files\ndata/scripts/google_drive.py, lib/autoseg/autoseg.py, lib/datasets/acce.py, lib/datasets/hip.py, lib/datasets/ins.py, _get_image_label_blob, test_net_seg, do_seg_tests_on_slices, get_minibatch_with_img_labels, compute_hist_imdb, compute_hist_imdb, _get_image_blob_and_label, label_path_at, label_path_at, compute_metrics_flat\n## 3 tags\n[instance segmentation], [semantic segmentation], [Segmentation]\n\n## repository\nhen', 'id': 'b30efbd0da54431d0806c8902f7e0a4b', 'name': 'vincentzhang/roi-fcn', 'tasks': 'instance segmentation, semantic segmentation, Segmentation, ', 'true_text': 'object localization'}, 'generation': 0, 'input_text': '\n## repository\nlifeng9472/IBCCF\n## files\nexternal_libs/matconvnet/doc/matdoc.py, external_libs/matconvnet/doc/matdocparser.py, external_libs/matconvnet/utils/import-caffe.py, external_libs/matconvnet/utils/layers.py, external_libs/matconvnet/utils/proto/caffe_0115_pb2.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## tags\n[visual tracking]\n\n## repository\nSrijha09/Making-Images-Artsy-Neural-Style-Transfer\n## files\nstyle_transfer.py, app.py, run.py, utils.py, is_file_allowed, serve_image, add_time, get_style_model_and_losses, get_input_optimizer, is_file_allowed, add_time, get_style_model_and_losses, get_input_optimizer, run_style_transfer\n## tags\n[style transfer]\n\n## repository\nvincentzhang/roi-fcn\n## files\ndata/scripts/google_drive.py, lib/autoseg/autoseg.py, lib/datasets/acce.py, lib/datasets/hip.py, lib/datasets/ins.py, _get_image_label_blob, test_net_seg, do_seg_tests_on_slices, get_minibatch_with_img_labels, compute_hist_imdb, compute_hist_imdb, _get_image_blob_and_label, label_path_at, label_path_at, compute_metrics_flat\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/end-to-end-detection-segmentation-network']", 'paper_titles': "['End-to-end detection-segmentation network with ROI convolution']", 'arxiv_ids': "['1801.02722']", 'authors': '["[\'Zichen Zhang\', \'Min Tang\', \'Dana Cobzas\', \'Dornoosh Zonoobi\', \'Martin Jagersand\', \'Jacob L. Jaremko\']"]', 'raw_generated_text': '\n## repository\nlifeng9472/IBCCF\n## files\nexternal_libs/matconvnet/doc/matdoc.py, external_libs/matconvnet/doc/matdocparser.py, external_libs/matconvnet/utils/import-caffe.py, external_libs/matconvnet/utils/layers.py, external_libs/matconvnet/utils/proto/caffe_0115_pb2.py, CaffeROIPooling, ROIPoolingParameter, CaffeBatchNorm, CaffeBlob, CaffeScale, render_L_from_indent, isa, isa, render_B, render_DH\n## tags\n[visual tracking]\n\n## repository\nSrijha09/Making-Images-Artsy-Neural-Style-Transfer\n## files\nstyle_transfer.py, app.py, run.py, utils.py, is_file_allowed, serve_image, add_time, get_style_model_and_losses, get_input_optimizer, is_file_allowed, add_time, get_style_model_and_losses, get_input_optimizer, run_style_transfer\n## tags\n[style transfer]\n\n## repository\nvincentzhang/roi-fcn\n## files\ndata/scripts/google_drive.py, lib/autoseg/autoseg.py, lib/datasets/acce.py, lib/datasets/hip.py, lib/datasets/ins.py, _get_image_label_blob, test_net_seg, do_seg_tests_on_slices, get_minibatch_with_img_labels, compute_hist_imdb, compute_hist_imdb, _get_image_blob_and_label, label_path_at, label_path_at, compute_metrics_flat\n## 3 tags\n[instance segmentation], [semantic segmentation], [Segmentation]\n\n## repository\nhen', 'raw_reference_text': 'object localization', 'reference_text': 'object localization', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8968978524208069, 'bertscore_recall': 0.8968978524208069, 'bertscore_f1': 0.8968978524208069, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.3237037658691406}, {'repo': 'liuch37/pan-tensorflow', 'tasks_x': 'hand keypoint detection, ', 'true_tasks': ['scene text detection'], 'generated_text': 'link prediction graph generation graph clustering', 'prompt_info': {'content': 'dataset/ctw1500.py, dataset/testdataset.py, loss/emb_loss_v1.py, loss/iou.py, loss/ohem.py, loss_tensor, PAN_CTW, write_result_ic15, PA_Head, loss_keras, text_loss, emb_loss, kernel_loss, EmbLoss_v1, PolyArea', 'generated_text': '\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## tags\n[link prediction, graph generation, graph clustering]\n\n## repository\nfaustomilletari/CFCM-2D\n## files\ncfcm/cli.py, cfcm/data/loading.py, cfcm/data/managers.py, cfcm/data/transforms.py, cfcm/layers/layers.py, block_layer_compressing, block_layer_expanding, normalize_between_zero_and_one, add_trailing_singleton_dimension_transform, specularity, run_epoch_train, run_epoch_valid, LSTMResNetMontgomeryXray, LSTMResNetEndovis, ResNetMontgomeryXray\n## tags\n[semantic segmentation]\n\n## repository\nliuch37/pan-tensorflow\n## files\ndataset/ctw1500.py, dataset/testdataset.py, loss/emb_loss_v1.py, loss/iou.py, loss/ohem.py, loss_tensor, PAN_CTW, write_result_ic15, PA_Head, loss_keras, text_loss, emb_loss, kernel_loss, EmbLoss_v1, PolyArea\n## 3 tags\n[hand keypoint detection]\n\n## folding-link\nhttp://archive', 'id': '8116bb02d8a26fb2a3519048a8318611', 'name': 'liuch37/pan-tensorflow', 'tasks': 'hand keypoint detection, ', 'true_text': 'scene text detection'}, 'generation': 0, 'input_text': '\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## tags\n[link prediction, graph generation, graph clustering]\n\n## repository\nfaustomilletari/CFCM-2D\n## files\ncfcm/cli.py, cfcm/data/loading.py, cfcm/data/managers.py, cfcm/data/transforms.py, cfcm/layers/layers.py, block_layer_compressing, block_layer_expanding, normalize_between_zero_and_one, add_trailing_singleton_dimension_transform, specularity, run_epoch_train, run_epoch_valid, LSTMResNetMontgomeryXray, LSTMResNetEndovis, ResNetMontgomeryXray\n## tags\n[semantic segmentation]\n\n## repository\nliuch37/pan-tensorflow\n## files\ndataset/ctw1500.py, dataset/testdataset.py, loss/emb_loss_v1.py, loss/iou.py, loss/ohem.py, loss_tensor, PAN_CTW, write_result_ic15, PA_Head, loss_keras, text_loss, emb_loss, kernel_loss, EmbLoss_v1, PolyArea\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/efficient-and-accurate-arbitrary-shaped-text']", 'paper_titles': "['Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network']", 'arxiv_ids': "['1908.05900']", 'authors': '["[\'Wenhai Wang\', \'Enze Xie\', \'Xiaoge Song\', \'Yuhang Zang\', \'Wenjia Wang\', \'Tong Lu\', \'Gang Yu\', \'Chunhua Shen\']"]', 'raw_generated_text': '\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## tags\n[link prediction, graph generation, graph clustering]\n\n## repository\nfaustomilletari/CFCM-2D\n## files\ncfcm/cli.py, cfcm/data/loading.py, cfcm/data/managers.py, cfcm/data/transforms.py, cfcm/layers/layers.py, block_layer_compressing, block_layer_expanding, normalize_between_zero_and_one, add_trailing_singleton_dimension_transform, specularity, run_epoch_train, run_epoch_valid, LSTMResNetMontgomeryXray, LSTMResNetEndovis, ResNetMontgomeryXray\n## tags\n[semantic segmentation]\n\n## repository\nliuch37/pan-tensorflow\n## files\ndataset/ctw1500.py, dataset/testdataset.py, loss/emb_loss_v1.py, loss/iou.py, loss/ohem.py, loss_tensor, PAN_CTW, write_result_ic15, PA_Head, loss_keras, text_loss, emb_loss, kernel_loss, EmbLoss_v1, PolyArea\n## 3 tags\n[hand keypoint detection]\n\n## folding-link\nhttp://archive', 'raw_reference_text': 'scene text detection', 'reference_text': 'scene text detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8555101156234741, 'bertscore_recall': 0.8762882351875305, 'bertscore_f1': 0.8657744526863098, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.32373347878456116}, {'repo': 'zhanghang1989/MXNet-Gluon-Style-Transfer', 'tasks_x': 'computer vision, computer vision, neural networks, ', 'true_tasks': ['style transfer'], 'generated_text': 'autonomous driving', 'prompt_info': {'content': 'models/download_model.py, option.py, net.py, data.py, main.py, ReflectancePadding, UpBottleneck, subtract_imagenet_mean_preprocess_batch, init_vgg_params, add_imagenet_mean_batch, style_folder, style_size, style_weight, subtract_imagenet_mean_preprocess_batch, subtract_imagenet_mean_preprocess_batch', 'generated_text': '\n## repository\npascalxia/driver_attention_prediction\n## files\nBatchDatasetReader.py, data_point_collector.py, data_point_collector_tests.py, make_feature_maps.py, my_squeezenet.py, xception_encoder, squeeze_encoder, readout_big, readout_bn, readout_net_BDD, epochs_before_validation, big_conv_lstm_readout_net, n_future_steps, gazemap_size, blackwhite\n## tags\n[autonomous driving]\n\n## repository\nDanieleAlessandro/KENN2\n## files\nsrc/KENN2/parsers.py, src/KENN2/layers/Kenn.py, src/KENN2/layers/RangeConstraint.py, src/KENN2/layers/RelationalKENN.py, src/KENN2/layers/relational/GroupBy.py, unary_parser, unary_parser_ke, relational_parser, Kenn, RangeConstraint, split, split, append\n## tags\n[multi label classification]\n\n## repository\nzhanghang1989/MXNet-Gluon-Style-Transfer\n## files\nmodels/download_model.py, option.py, net.py, data.py, main.py, ReflectancePadding, UpBottleneck, subtract_imagenet_mean_preprocess_batch, init_vgg_params, add_imagenet_mean_batch, style_folder, style_size, style_weight, subtract_imagenet_mean_preprocess_batch, subtract_imagenet_mean_preprocess_batch\n## 3 tags\n[computer vision]\n[computer vision, neural networks]\n\n## repository\n', 'id': 'd71456a2c2e76133404a22ce91c4d790', 'name': 'zhanghang1989/MXNet-Gluon-Style-Transfer', 'tasks': 'computer vision, computer vision, neural networks, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\npascalxia/driver_attention_prediction\n## files\nBatchDatasetReader.py, data_point_collector.py, data_point_collector_tests.py, make_feature_maps.py, my_squeezenet.py, xception_encoder, squeeze_encoder, readout_big, readout_bn, readout_net_BDD, epochs_before_validation, big_conv_lstm_readout_net, n_future_steps, gazemap_size, blackwhite\n## tags\n[autonomous driving]\n\n## repository\nDanieleAlessandro/KENN2\n## files\nsrc/KENN2/parsers.py, src/KENN2/layers/Kenn.py, src/KENN2/layers/RangeConstraint.py, src/KENN2/layers/RelationalKENN.py, src/KENN2/layers/relational/GroupBy.py, unary_parser, unary_parser_ke, relational_parser, Kenn, RangeConstraint, split, split, append\n## tags\n[multi label classification]\n\n## repository\nzhanghang1989/MXNet-Gluon-Style-Transfer\n## files\nmodels/download_model.py, option.py, net.py, data.py, main.py, ReflectancePadding, UpBottleneck, subtract_imagenet_mean_preprocess_batch, init_vgg_params, add_imagenet_mean_batch, style_folder, style_size, style_weight, subtract_imagenet_mean_preprocess_batch, subtract_imagenet_mean_preprocess_batch\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-neural-algorithm-of-artistic-style', 'https://paperswithcode.com/paper/multi-style-generative-network-for-real-time']", 'paper_titles': "['A Neural Algorithm of Artistic Style', 'Multi-style Generative Network for Real-time Transfer']", 'arxiv_ids': "['1508.06576', '1703.06953']", 'authors': '["[\'Leon A. Gatys\', \'Alexander S. Ecker\', \'Matthias Bethge\']", "[\'Hang Zhang\', \'Kristin Dana\']"]', 'raw_generated_text': '\n## repository\npascalxia/driver_attention_prediction\n## files\nBatchDatasetReader.py, data_point_collector.py, data_point_collector_tests.py, make_feature_maps.py, my_squeezenet.py, xception_encoder, squeeze_encoder, readout_big, readout_bn, readout_net_BDD, epochs_before_validation, big_conv_lstm_readout_net, n_future_steps, gazemap_size, blackwhite\n## tags\n[autonomous driving]\n\n## repository\nDanieleAlessandro/KENN2\n## files\nsrc/KENN2/parsers.py, src/KENN2/layers/Kenn.py, src/KENN2/layers/RangeConstraint.py, src/KENN2/layers/RelationalKENN.py, src/KENN2/layers/relational/GroupBy.py, unary_parser, unary_parser_ke, relational_parser, Kenn, RangeConstraint, split, split, append\n## tags\n[multi label classification]\n\n## repository\nzhanghang1989/MXNet-Gluon-Style-Transfer\n## files\nmodels/download_model.py, option.py, net.py, data.py, main.py, ReflectancePadding, UpBottleneck, subtract_imagenet_mean_preprocess_batch, init_vgg_params, add_imagenet_mean_batch, style_folder, style_size, style_weight, subtract_imagenet_mean_preprocess_batch, subtract_imagenet_mean_preprocess_batch\n## 3 tags\n[computer vision]\n[computer vision, neural networks]\n\n## repository\n', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8855528831481934, 'bertscore_recall': 0.8887113332748413, 'bertscore_f1': 0.8871293067932129, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.09467417001724243}, {'repo': 'DevashishPrasad/Smart-Traffic-Junction', 'tasks_x': 'face recognition, HOG, shape classifier, facial detection, dlib, ', 'true_tasks': ['density estimation'], 'generated_text': 'medical diagnosis medical image segmentation', 'prompt_info': {'content': 'image-processing/main.py, save_HOG_LBP.py, saverois.py, predictor.py, classifier.py, distMap, uint8, float32, sqrt', 'generated_text': '\n## repository\nAIEMMU/MRI_Prostate\n## files\napp/contour.py, app/dicom.py, app/displayView.py, app/displayViewModel.py, app/ui.py, enableButtons, saveContour, loadContour, SubScene, get_relative_path_if_possible, Format_RGB888, QPixmap, QImage, setEnabled, QPoint\n## tags\n[medical image segmentation, medical diagnosis]\n\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDevashishPrasad/Smart-Traffic-Junction\n## files\nimage-processing/main.py, save_HOG_LBP.py, saverois.py, predictor.py, classifier.py, distMap, uint8, float32, sqrt\n## 3 tags\n[face recognition, HOG, shape classifier, facial detection, dlib]', 'id': '01efd8c2c60c5eab6b26cde53ab9e23c', 'name': 'DevashishPrasad/Smart-Traffic-Junction', 'tasks': 'face recognition, HOG, shape classifier, facial detection, dlib, ', 'true_text': 'density estimation'}, 'generation': 0, 'input_text': '\n## repository\nAIEMMU/MRI_Prostate\n## files\napp/contour.py, app/dicom.py, app/displayView.py, app/displayViewModel.py, app/ui.py, enableButtons, saveContour, loadContour, SubScene, get_relative_path_if_possible, Format_RGB888, QPixmap, QImage, setEnabled, QPoint\n## tags\n[medical image segmentation, medical diagnosis]\n\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDevashishPrasad/Smart-Traffic-Junction\n## files\nimage-processing/main.py, save_HOG_LBP.py, saverois.py, predictor.py, classifier.py, distMap, uint8, float32, sqrt\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/hog-lbp-and-svm-based-traffic-density']", 'paper_titles': "['HOG, LBP and SVM based Traffic Density Estimation at Intersection']", 'arxiv_ids': "['2005.01770']", 'authors': '["[\'Devashish Prasad\', \'Kshitij Kapadni\', \'Ayan Gadpal\', \'Manish Visave\', \'Kavita Sultanpure\']"]', 'raw_generated_text': '\n## repository\nAIEMMU/MRI_Prostate\n## files\napp/contour.py, app/dicom.py, app/displayView.py, app/displayViewModel.py, app/ui.py, enableButtons, saveContour, loadContour, SubScene, get_relative_path_if_possible, Format_RGB888, QPixmap, QImage, setEnabled, QPoint\n## tags\n[medical image segmentation, medical diagnosis]\n\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDevashishPrasad/Smart-Traffic-Junction\n## files\nimage-processing/main.py, save_HOG_LBP.py, saverois.py, predictor.py, classifier.py, distMap, uint8, float32, sqrt\n## 3 tags\n[face recognition, HOG, shape classifier, facial detection, dlib]', 'raw_reference_text': 'density estimation', 'reference_text': 'density estimation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8429347276687622, 'bertscore_recall': 0.9003310203552246, 'bertscore_f1': 0.8706880211830139, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.318591833114624}, {'repo': 'ssahn3087/pedestrian_detection', 'tasks_x': 'trained model, ', 'true_tasks': ['person re identification'], 'generated_text': 'data augmentation', 'prompt_info': {'content': 'demo_tracking.py, visualize_tsne.py, dataset_utils/coco_user_dataset_generator.py, dataset_utils/dataset_info.py, dataset_utils/extract_annotations.py, load_net_pedestrians, print_weight_grad, train_net_params, get_triplet_rois, id_match_test, id_track, FasterRCNN_RES, FasterRCNN_RES, FasterRCNN_VGG, FasterRCNN_VGG', 'generated_text': '\n## repository\nmoskomule/dda\n## files\ndda/functional.py, dda/kernels.py, dda/operations.py, dda/pil.py, examples/randaugment.py, test_function_with_magnitude, test_function_without_magnitude, new_getitem, OriginalSVHN, ExtraSVHN, faster_auto_augment_policy, CosineAnnealingWithWarmup, get_sharpness_kernel, get_gaussian_3x3kernel, _split_dataset\n## tags\n[data augmentation]\n\n## repository\nwangzh0ng/adversarial_yolo2\n## files\nfrom_notebook.py, batch_rotate.py, craft_adv.py, oude versies/craft_adv_draaien_scalen_plaatsen_werkt.py, oude versies/craft_adv_patch_plaatsen_werkt.py, ReproducePaperObj, Experiment4ClassOnly, Experiment3LowRes, Experiment1Desktop, Experiment2HighRes, patch_configs, region_loss, th_nearest_interp2d, mypad, mypad\n## tags\n[human detection]\n\n## repository\nssahn3087/pedestrian_detection\n## files\ndemo_tracking.py, visualize_tsne.py, dataset_utils/coco_user_dataset_generator.py, dataset_utils/dataset_info.py, dataset_utils/extract_annotations.py, load_net_pedestrians, print_weight_grad, train_net_params, get_triplet_rois, id_match_test, id_track, FasterRCNN_RES, FasterRCNN_RES, FasterRCNN_VGG, FasterRCNN_VGG\n## 3 tags\n[trained model]\n\n## repository\nDamianX/fbpca\n', 'id': 'e431ea1a44aa9c47d91e8e901316c1af', 'name': 'ssahn3087/pedestrian_detection', 'tasks': 'trained model, ', 'true_text': 'person re identification'}, 'generation': 0, 'input_text': '\n## repository\nmoskomule/dda\n## files\ndda/functional.py, dda/kernels.py, dda/operations.py, dda/pil.py, examples/randaugment.py, test_function_with_magnitude, test_function_without_magnitude, new_getitem, OriginalSVHN, ExtraSVHN, faster_auto_augment_policy, CosineAnnealingWithWarmup, get_sharpness_kernel, get_gaussian_3x3kernel, _split_dataset\n## tags\n[data augmentation]\n\n## repository\nwangzh0ng/adversarial_yolo2\n## files\nfrom_notebook.py, batch_rotate.py, craft_adv.py, oude versies/craft_adv_draaien_scalen_plaatsen_werkt.py, oude versies/craft_adv_patch_plaatsen_werkt.py, ReproducePaperObj, Experiment4ClassOnly, Experiment3LowRes, Experiment1Desktop, Experiment2HighRes, patch_configs, region_loss, th_nearest_interp2d, mypad, mypad\n## tags\n[human detection]\n\n## repository\nssahn3087/pedestrian_detection\n## files\ndemo_tracking.py, visualize_tsne.py, dataset_utils/coco_user_dataset_generator.py, dataset_utils/dataset_info.py, dataset_utils/extract_annotations.py, load_net_pedestrians, print_weight_grad, train_net_params, get_triplet_rois, id_match_test, id_track, FasterRCNN_RES, FasterRCNN_RES, FasterRCNN_VGG, FasterRCNN_VGG\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/triplet-based-deep-similarity-learning-for']", 'paper_titles': "['Triplet-based Deep Similarity Learning for Person Re-Identification']", 'arxiv_ids': "['1802.03254']", 'authors': '["[\'Wentong Liao\', \'Michael Ying Yang\', \'Ni Zhan\', \'Bodo Rosenhahn\']"]', 'raw_generated_text': '\n## repository\nmoskomule/dda\n## files\ndda/functional.py, dda/kernels.py, dda/operations.py, dda/pil.py, examples/randaugment.py, test_function_with_magnitude, test_function_without_magnitude, new_getitem, OriginalSVHN, ExtraSVHN, faster_auto_augment_policy, CosineAnnealingWithWarmup, get_sharpness_kernel, get_gaussian_3x3kernel, _split_dataset\n## tags\n[data augmentation]\n\n## repository\nwangzh0ng/adversarial_yolo2\n## files\nfrom_notebook.py, batch_rotate.py, craft_adv.py, oude versies/craft_adv_draaien_scalen_plaatsen_werkt.py, oude versies/craft_adv_patch_plaatsen_werkt.py, ReproducePaperObj, Experiment4ClassOnly, Experiment3LowRes, Experiment1Desktop, Experiment2HighRes, patch_configs, region_loss, th_nearest_interp2d, mypad, mypad\n## tags\n[human detection]\n\n## repository\nssahn3087/pedestrian_detection\n## files\ndemo_tracking.py, visualize_tsne.py, dataset_utils/coco_user_dataset_generator.py, dataset_utils/dataset_info.py, dataset_utils/extract_annotations.py, load_net_pedestrians, print_weight_grad, train_net_params, get_triplet_rois, id_match_test, id_track, FasterRCNN_RES, FasterRCNN_RES, FasterRCNN_VGG, FasterRCNN_VGG\n## 3 tags\n[trained model]\n\n## repository\nDamianX/fbpca\n', 'raw_reference_text': 'person re identification', 'reference_text': 'person re identification', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8544962406158447, 'bertscore_recall': 0.8424843549728394, 'bertscore_f1': 0.8484477996826172, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.11702299863100052}, {'repo': 'nrupatunga/Fast-Image-Filters', 'tasks_x': 'image manipulation, image filtering, image processing, ', 'true_tasks': ['style transfer'], 'generated_text': 'scene text recognition optical character recognition', 'prompt_info': {'content': 'src/core/dataloaders/mit_dataloader.py, src/core/network/basic_blocks.py, src/core/network/custom_nets.py, src/core/network/weights_init.py, src/core/trainers/filter_trainer.py, MitData, FIP, AdaptiveBatchNorm2d, LitModel, apply_filter, load_from_checkpoint, freeze, imread, uint8, clip', 'generated_text': '\n## repository\nbai-shang/crnn_ctc_ocr_tf\n## files\ncrnn_model/model.py, data/create_synth90k_tfrecord.py, tools/create_crnn_ctc_tfrecord.py, tools/train_crnn_ctc.py, tools/eval_crnn_ctc.py, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord, _read_tfrecord, validation_split_fraction, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\njaydeepthik/kaggle-facial-expression-recognition\n## files\nfacial_expression.py, generate_model, generate_dataset, read_csv, values, array\n## tags\n[l2 regularization, facial expression recognition]\n\n## repository\nnrupatunga/Fast-Image-Filters\n## files\nsrc/core/dataloaders/mit_dataloader.py, src/core/network/basic_blocks.py, src/core/network/custom_nets.py, src/core/network/weights_init.py, src/core/trainers/filter_trainer.py, MitData, FIP, AdaptiveBatchNorm2d, LitModel, apply_filter, load_from_checkpoint, freeze, imread, uint8, clip\n## 3 tags\n[image manipulation, image filtering, image processing]\n\n## repository\nchok', 'id': 'c2837f719b18e55cd7f9128e56ede945', 'name': 'nrupatunga/Fast-Image-Filters', 'tasks': 'image manipulation, image filtering, image processing, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\nbai-shang/crnn_ctc_ocr_tf\n## files\ncrnn_model/model.py, data/create_synth90k_tfrecord.py, tools/create_crnn_ctc_tfrecord.py, tools/train_crnn_ctc.py, tools/eval_crnn_ctc.py, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord, _read_tfrecord, validation_split_fraction, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\njaydeepthik/kaggle-facial-expression-recognition\n## files\nfacial_expression.py, generate_model, generate_dataset, read_csv, values, array\n## tags\n[l2 regularization, facial expression recognition]\n\n## repository\nnrupatunga/Fast-Image-Filters\n## files\nsrc/core/dataloaders/mit_dataloader.py, src/core/network/basic_blocks.py, src/core/network/custom_nets.py, src/core/network/weights_init.py, src/core/trainers/filter_trainer.py, MitData, FIP, AdaptiveBatchNorm2d, LitModel, apply_filter, load_from_checkpoint, freeze, imread, uint8, clip\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/fast-image-processing-with-fully']", 'paper_titles': "['Fast Image Processing with Fully-Convolutional Networks']", 'arxiv_ids': "['1709.00643']", 'authors': '["[\'Qifeng Chen\', \'Jia Xu\', \'Vladlen Koltun\']"]', 'raw_generated_text': '\n## repository\nbai-shang/crnn_ctc_ocr_tf\n## files\ncrnn_model/model.py, data/create_synth90k_tfrecord.py, tools/create_crnn_ctc_tfrecord.py, tools/train_crnn_ctc.py, tools/eval_crnn_ctc.py, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord, _read_tfrecord, validation_split_fraction, _train_crnn_ctc, _eval_crnn_ctc, _inference_crnn_ctc, _read_tfrecord\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\njaydeepthik/kaggle-facial-expression-recognition\n## files\nfacial_expression.py, generate_model, generate_dataset, read_csv, values, array\n## tags\n[l2 regularization, facial expression recognition]\n\n## repository\nnrupatunga/Fast-Image-Filters\n## files\nsrc/core/dataloaders/mit_dataloader.py, src/core/network/basic_blocks.py, src/core/network/custom_nets.py, src/core/network/weights_init.py, src/core/trainers/filter_trainer.py, MitData, FIP, AdaptiveBatchNorm2d, LitModel, apply_filter, load_from_checkpoint, freeze, imread, uint8, clip\n## 3 tags\n[image manipulation, image filtering, image processing]\n\n## repository\nchok', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8068004846572876, 'bertscore_recall': 0.8753297924995422, 'bertscore_f1': 0.8396691679954529, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.12924858927726746}, {'repo': 'thunlp/OpenAttack', 'tasks_x': 'Black HC, ML, ', 'true_tasks': ['adversarial attack'], 'generated_text': 'stochastic optimization', 'prompt_info': {'content': 'build-doc.py, demo_deo.py, examples/adversarial_training.py, examples/chinese.py, examples/custom_attacker.py, getDocMembers, make_attacker, make_attack_eval, make_victim, make_data_manager, loadVictim, TimeCalcClsf, url_downloader, ieval, attack_time', 'generated_text': '\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## tags\n[stochastic optimization]\n\n## repository\nBlackHC/BatchBALD\n## files\nlaaos_results/paper/mnist_multibald_bald_k100_b5_734490.py, laaos_results/paper/emnist_multibald_bald_k10_728719.py, laaos_results/paper/mnist_independent_bald_k100_b10_661442.py, laaos_results/paper/cinic10_nispc20_pretrained_multibald_bald_k50_b10_885898.py, laaos_results/paper/mnist_independent_bald_k100_b1_1038804.py, groupby_dict, gather_accuracy, get_samples_values_I, expand_samples_I_values_I, aggregate_values_sample_points_T, experiment_task_id, experiments_laaos, initial_samples_per_class, quickquick, create_file_store\n## tags\n[active learning]\n\n## repository\nthunlp/OpenAttack\n## files\nbuild-doc.py, demo_deo.py, examples/adversarial_training.py, examples/chinese.py, examples/custom_attacker.py, getDocMembers, make_attacker, make_attack_eval, make_victim, make_data_manager, loadVictim, TimeCalcClsf, url_downloader, ieval, attack_time\n## 3 tags\n[Black HC]\n[ML]\n\n## repository\nOpenNMT/', 'id': '36e0a2e370281766bd983f188a6c1316', 'name': 'thunlp/OpenAttack', 'tasks': 'Black HC, ML, ', 'true_text': 'adversarial attack'}, 'generation': 0, 'input_text': '\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## tags\n[stochastic optimization]\n\n## repository\nBlackHC/BatchBALD\n## files\nlaaos_results/paper/mnist_multibald_bald_k100_b5_734490.py, laaos_results/paper/emnist_multibald_bald_k10_728719.py, laaos_results/paper/mnist_independent_bald_k100_b10_661442.py, laaos_results/paper/cinic10_nispc20_pretrained_multibald_bald_k50_b10_885898.py, laaos_results/paper/mnist_independent_bald_k100_b1_1038804.py, groupby_dict, gather_accuracy, get_samples_values_I, expand_samples_I_values_I, aggregate_values_sample_points_T, experiment_task_id, experiments_laaos, initial_samples_per_class, quickquick, create_file_store\n## tags\n[active learning]\n\n## repository\nthunlp/OpenAttack\n## files\nbuild-doc.py, demo_deo.py, examples/adversarial_training.py, examples/chinese.py, examples/custom_attacker.py, getDocMembers, make_attacker, make_attack_eval, make_victim, make_data_manager, loadVictim, TimeCalcClsf, url_downloader, ieval, attack_time\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/openattack-an-open-source-textual-adversarial']", 'paper_titles': "['OpenAttack: An Open-source Textual Adversarial Attack Toolkit']", 'arxiv_ids': "['2009.09191']", 'authors': '["[\'Guoyang Zeng\', \'Fanchao Qi\', \'Qianrui Zhou\', \'Tingji Zhang\', \'Zixian Ma\', \'Bairu Hou\', \'Yuan Zang\', \'Zhiyuan Liu\', \'Maosong Sun\']"]', 'raw_generated_text': '\n## repository\ntensorflow/kfac\n## files\nkfac/examples/autoencoder_mnist.py, kfac/examples/autoencoder_mnist_tpu_estimator.py, kfac/examples/autoencoder_mnist_tpu_strategy.py, kfac/examples/classifier_mnist.py, kfac/examples/classifier_mnist_tpu_estimator.py, smart_assign, preferred_int_dtype, all_average, map_gather, ip_p, use_control_flow_v2, update_damping_immediately, use_custom_patches_op, cached_reader, register_sigmoid_cross_entropy_loss\n## tags\n[stochastic optimization]\n\n## repository\nBlackHC/BatchBALD\n## files\nlaaos_results/paper/mnist_multibald_bald_k100_b5_734490.py, laaos_results/paper/emnist_multibald_bald_k10_728719.py, laaos_results/paper/mnist_independent_bald_k100_b10_661442.py, laaos_results/paper/cinic10_nispc20_pretrained_multibald_bald_k50_b10_885898.py, laaos_results/paper/mnist_independent_bald_k100_b1_1038804.py, groupby_dict, gather_accuracy, get_samples_values_I, expand_samples_I_values_I, aggregate_values_sample_points_T, experiment_task_id, experiments_laaos, initial_samples_per_class, quickquick, create_file_store\n## tags\n[active learning]\n\n## repository\nthunlp/OpenAttack\n## files\nbuild-doc.py, demo_deo.py, examples/adversarial_training.py, examples/chinese.py, examples/custom_attacker.py, getDocMembers, make_attacker, make_attack_eval, make_victim, make_data_manager, loadVictim, TimeCalcClsf, url_downloader, ieval, attack_time\n## 3 tags\n[Black HC]\n[ML]\n\n## repository\nOpenNMT/', 'raw_reference_text': 'adversarial attack', 'reference_text': 'adversarial attack', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7818953990936279, 'bertscore_recall': 0.8216042518615723, 'bertscore_f1': 0.8012582063674927, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1349184364080429}, {'repo': 'jonsafari/clustercat', 'tasks_x': 'testing, genome annotation, clustering, ', 'true_tasks': ['chunking', 'word alignment', 'machine translation'], 'generated_text': 'density estimation', 'prompt_info': {'content': 'python/clustercat.py, bin/ngram_counts.py, tag_stdin, tag_string, cluster, save, load, tag_stdin, tag_string, find_executable, cluster, stdin', 'generated_text': '\n## repository\npawelc/NeuralLikelihoods\n## files\ncode/mutual_info.py, code/my_log.py, code/data/bsds300.py, code/data/gas.py, code/data/hepmass.py, load_model_and_params, build_inverse_quadrature_for_monotonic, calc_MI2, get_all_2_element_combinations, normal_mixture_1d, InMemoryCollector, model_input_shape, Integrated, NoOpProgressMonitor, MixtureNd\n## tags\n[density estimation]\n\n## repository\npdasigi/onto-lstm\n## files\nindex_data.py, model_entailment.py, model_pp_attachment.py, model_pp_relation.py, model_sentences.py, OntoLSTMEncoder, OntoAwareEmbedding, PrepositionModel, LSTMEntailmentModel, NSEEntailmentModel, onto_aware, nse_shared_memory, print_attention_values, load_model_from_epoch, LSTMEntailmentModel\n## tags\n[word embeddings]\n\n## repository\njonsafari/clustercat\n## files\npython/clustercat.py, bin/ngram_counts.py, tag_stdin, tag_string, cluster, save, load, tag_stdin, tag_string, find_executable, cluster, stdin\n## 3 tags\n[testing] [genome annotation] [clustering]\n***\n## repository\nY', 'id': '7413b673ab5a9154643dc1c597d0f73e', 'name': 'jonsafari/clustercat', 'tasks': 'testing, genome annotation, clustering, ', 'true_text': 'chunking, word alignment, machine translation'}, 'generation': 0, 'input_text': '\n## repository\npawelc/NeuralLikelihoods\n## files\ncode/mutual_info.py, code/my_log.py, code/data/bsds300.py, code/data/gas.py, code/data/hepmass.py, load_model_and_params, build_inverse_quadrature_for_monotonic, calc_MI2, get_all_2_element_combinations, normal_mixture_1d, InMemoryCollector, model_input_shape, Integrated, NoOpProgressMonitor, MixtureNd\n## tags\n[density estimation]\n\n## repository\npdasigi/onto-lstm\n## files\nindex_data.py, model_entailment.py, model_pp_attachment.py, model_pp_relation.py, model_sentences.py, OntoLSTMEncoder, OntoAwareEmbedding, PrepositionModel, LSTMEntailmentModel, NSEEntailmentModel, onto_aware, nse_shared_memory, print_attention_values, load_model_from_epoch, LSTMEntailmentModel\n## tags\n[word embeddings]\n\n## repository\njonsafari/clustercat\n## files\npython/clustercat.py, bin/ngram_counts.py, tag_stdin, tag_string, cluster, save, load, tag_stdin, tag_string, find_executable, cluster, stdin\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/bira-improved-predictive-exchange-word']", 'paper_titles': "['BIRA: Improved Predictive Exchange Word Clustering']", 'arxiv_ids': '[nan]', 'authors': '["[\'Jon Dehdari\', \'Liling Tan\', \'Josef van Genabith\']"]', 'raw_generated_text': '\n## repository\npawelc/NeuralLikelihoods\n## files\ncode/mutual_info.py, code/my_log.py, code/data/bsds300.py, code/data/gas.py, code/data/hepmass.py, load_model_and_params, build_inverse_quadrature_for_monotonic, calc_MI2, get_all_2_element_combinations, normal_mixture_1d, InMemoryCollector, model_input_shape, Integrated, NoOpProgressMonitor, MixtureNd\n## tags\n[density estimation]\n\n## repository\npdasigi/onto-lstm\n## files\nindex_data.py, model_entailment.py, model_pp_attachment.py, model_pp_relation.py, model_sentences.py, OntoLSTMEncoder, OntoAwareEmbedding, PrepositionModel, LSTMEntailmentModel, NSEEntailmentModel, onto_aware, nse_shared_memory, print_attention_values, load_model_from_epoch, LSTMEntailmentModel\n## tags\n[word embeddings]\n\n## repository\njonsafari/clustercat\n## files\npython/clustercat.py, bin/ngram_counts.py, tag_stdin, tag_string, cluster, save, load, tag_stdin, tag_string, find_executable, cluster, stdin\n## 3 tags\n[testing] [genome annotation] [clustering]\n***\n## repository\nY', 'raw_reference_text': 'chunking, word alignment, machine translation', 'reference_text': 'word alignment chunking machine translation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8721004724502563, 'bertscore_recall': 0.8357932567596436, 'bertscore_f1': 0.8535609841346741, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.27746689319610596}, {'repo': 'zcc31415926/FFNet', 'tasks_x': 'beekeepers in North America, salaries in US, salaries, ', 'true_tasks': ['autonomous driving'], 'generated_text': 'text generation text summarization', 'prompt_info': {'content': 'data_processing.py, main.py, generate_one_hot_list, find_second_largest_element, organize_train_data, organize_val_data, extract_random_val_batch, result_2d_path, extract_random_val_batch, num_val_data, num_val_data, determine_angle', 'generated_text': '\n## repository\nneulab/REALSumm\n## files\nprocess_data/file2dir.py, process_data/filter_by_rouge.py, process_data/get_alignment.py, process_data/preprocess.py, process_data/process_fast_abs_rl.py, print_score_ranges, print_ktau_matrix, get_doc_y_val, get_system_level_scores, tokenize_strings, get_each_score, run_google_rouge, out_dec, out_ref, emd_with_flow\n## tags\n[text summarization, text generation]\n\n## repository\narne-cl/feng-hirst-rst-parser\n## files\nsrc/parser_wrapper.py, src/prep/preprocesser.py, src/sanity_check.py, src/test_feng.py, src/classifiers/crf_classifier.py, sorted_dict_values_by_key, argsmax, split_mrg_by_sentence, simplified_tag, split_hilda_inputfile_by_sentence, feng_main, copy_subtree, get_word_list_from_span, CRFSUITE_PATH, LOGS_PATH\n## tags\n[discourse segmentation]\n\n## repository\nzcc31415926/FFNet\n## files\ndata_processing.py, main.py, generate_one_hot_list, find_second_largest_element, organize_train_data, organize_val_data, extract_random_val_batch, result_2d_path, extract_random_val_batch, num_val_data, num_val_data, determine_angle\n## 3 tags\n[beekeepers in North America, salaries in US, salaries]', 'id': '1ae7255655fc57b81c0d6fa2ae6e6acf', 'name': 'zcc31415926/FFNet', 'tasks': 'beekeepers in North America, salaries in US, salaries, ', 'true_text': 'autonomous driving'}, 'generation': 0, 'input_text': '\n## repository\nneulab/REALSumm\n## files\nprocess_data/file2dir.py, process_data/filter_by_rouge.py, process_data/get_alignment.py, process_data/preprocess.py, process_data/process_fast_abs_rl.py, print_score_ranges, print_ktau_matrix, get_doc_y_val, get_system_level_scores, tokenize_strings, get_each_score, run_google_rouge, out_dec, out_ref, emd_with_flow\n## tags\n[text summarization, text generation]\n\n## repository\narne-cl/feng-hirst-rst-parser\n## files\nsrc/parser_wrapper.py, src/prep/preprocesser.py, src/sanity_check.py, src/test_feng.py, src/classifiers/crf_classifier.py, sorted_dict_values_by_key, argsmax, split_mrg_by_sentence, simplified_tag, split_hilda_inputfile_by_sentence, feng_main, copy_subtree, get_word_list_from_span, CRFSUITE_PATH, LOGS_PATH\n## tags\n[discourse segmentation]\n\n## repository\nzcc31415926/FFNet\n## files\ndata_processing.py, main.py, generate_one_hot_list, find_second_largest_element, organize_train_data, organize_val_data, extract_random_val_batch, result_2d_path, extract_random_val_batch, num_val_data, num_val_data, determine_angle\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/monocular-pedestrian-orientation-estimation']", 'paper_titles': "['Monocular Pedestrian Orientation Estimation Based on Deep 2D-3D Feedforward']", 'arxiv_ids': "['1909.10970']", 'authors': '["[\'Chenchen Zhao\', \'Yeqiang Qian\', \'Ming Yang\']"]', 'raw_generated_text': '\n## repository\nneulab/REALSumm\n## files\nprocess_data/file2dir.py, process_data/filter_by_rouge.py, process_data/get_alignment.py, process_data/preprocess.py, process_data/process_fast_abs_rl.py, print_score_ranges, print_ktau_matrix, get_doc_y_val, get_system_level_scores, tokenize_strings, get_each_score, run_google_rouge, out_dec, out_ref, emd_with_flow\n## tags\n[text summarization, text generation]\n\n## repository\narne-cl/feng-hirst-rst-parser\n## files\nsrc/parser_wrapper.py, src/prep/preprocesser.py, src/sanity_check.py, src/test_feng.py, src/classifiers/crf_classifier.py, sorted_dict_values_by_key, argsmax, split_mrg_by_sentence, simplified_tag, split_hilda_inputfile_by_sentence, feng_main, copy_subtree, get_word_list_from_span, CRFSUITE_PATH, LOGS_PATH\n## tags\n[discourse segmentation]\n\n## repository\nzcc31415926/FFNet\n## files\ndata_processing.py, main.py, generate_one_hot_list, find_second_largest_element, organize_train_data, organize_val_data, extract_random_val_batch, result_2d_path, extract_random_val_batch, num_val_data, num_val_data, determine_angle\n## 3 tags\n[beekeepers in North America, salaries in US, salaries]', 'raw_reference_text': 'autonomous driving', 'reference_text': 'autonomous driving', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8347885608673096, 'bertscore_recall': 0.8435153961181641, 'bertscore_f1': 0.8391292691230774, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.10028667747974396}, {'repo': 'Ohraincu/JDNet', 'tasks_x': 'pytorch, rain dehaze, ', 'true_tasks': ['rain removal', 'single image deraining'], 'generated_text': 'weather forecasting', 'prompt_info': {'content': 'code/ablation/r1/config/cal_ssim.py, code/ablation/r1/config/dataset.py, code/ablation/r1/config/eval.py, code/ablation/r1/config/model.py, code/ablation/r1/config/show.py, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, save_checkpoints_net, load_checkpoints_net, inf_batch_test, inf_batch, inf_batch', 'generated_text': '\n## repository\nBruceBinBoxing/WF\n## files\nsrc/data/data_load.py, src/data/helper.py, src/data/make_dataset_from_nc2df.py, src/data/make_dataset_missing_fill.py, src/data/make_TestOnlineData_from_nc.py, cal_loss_dataset, evl_fn, cal_miss, intplt_nan_1d, random_sine, read_nc_data, x_temp, y_temp, variance_dense, renorm_for_submit\n## tags\n[weather forecasting]\n\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\nOhraincu/JDNet\n## files\ncode/ablation/r1/config/cal_ssim.py, code/ablation/r1/config/dataset.py, code/ablation/r1/config/eval.py, code/ablation/r1/config/model.py, code/ablation/r1/config/show.py, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, save_checkpoints_net, load_checkpoints_net, inf_batch_test, inf_batch, inf_batch\n## 3 tags\n[pytorch][rain dehaze]\n\n## configs\ncode/dataset.', 'id': '3a9d53917dc37d935451857e2e7d5319', 'name': 'Ohraincu/JDNet', 'tasks': 'pytorch, rain dehaze, ', 'true_text': 'rain removal, single image deraining'}, 'generation': 0, 'input_text': '\n## repository\nBruceBinBoxing/WF\n## files\nsrc/data/data_load.py, src/data/helper.py, src/data/make_dataset_from_nc2df.py, src/data/make_dataset_missing_fill.py, src/data/make_TestOnlineData_from_nc.py, cal_loss_dataset, evl_fn, cal_miss, intplt_nan_1d, random_sine, read_nc_data, x_temp, y_temp, variance_dense, renorm_for_submit\n## tags\n[weather forecasting]\n\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\nOhraincu/JDNet\n## files\ncode/ablation/r1/config/cal_ssim.py, code/ablation/r1/config/dataset.py, code/ablation/r1/config/eval.py, code/ablation/r1/config/model.py, code/ablation/r1/config/show.py, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, save_checkpoints_net, load_checkpoints_net, inf_batch_test, inf_batch, inf_batch\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/joint-self-attention-and-scale-aggregation']", 'paper_titles': "['Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining Network']", 'arxiv_ids': "['2008.02763']", 'authors': '["[\'Cong Wang\', \'Yutong Wu\', \'Zhixun Su\', \'Junyang Chen\']"]', 'raw_generated_text': '\n## repository\nBruceBinBoxing/WF\n## files\nsrc/data/data_load.py, src/data/helper.py, src/data/make_dataset_from_nc2df.py, src/data/make_dataset_missing_fill.py, src/data/make_TestOnlineData_from_nc.py, cal_loss_dataset, evl_fn, cal_miss, intplt_nan_1d, random_sine, read_nc_data, x_temp, y_temp, variance_dense, renorm_for_submit\n## tags\n[weather forecasting]\n\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\nOhraincu/JDNet\n## files\ncode/ablation/r1/config/cal_ssim.py, code/ablation/r1/config/dataset.py, code/ablation/r1/config/eval.py, code/ablation/r1/config/model.py, code/ablation/r1/config/show.py, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, ODE_DerainNet, save_checkpoints_net, load_checkpoints_net, inf_batch_test, inf_batch, inf_batch\n## 3 tags\n[pytorch][rain dehaze]\n\n## configs\ncode/dataset.', 'raw_reference_text': 'rain removal, single image deraining', 'reference_text': 'rain removal single image deraining', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8706282377243042, 'bertscore_recall': 0.8345434665679932, 'bertscore_f1': 0.8522040247917175, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.048090945929288864}, {'repo': 'akakzia/decstr', 'tasks_x': 'text classification, decision tree learning, ', 'true_tasks': ['language acquisition'], 'generated_text': 'stochastic optimization text classification', 'prompt_info': {'content': 'demo_sentence_expr.py, demo_sentence_seq.py, demo_sentence_test.py, goal_sampler.py, env/fetch_manipulate_env.py, CompressPDF, check_length_and_seeds, plot_c_lp_p_sr, plot_lp_av, plot_sr_av, update_LP, n_cycles, _update_normalizer, save_bucket_contents, valid_goals', 'generated_text': '\n## repository\nstephenbeckr/AIMS\n## files\nCVX_tutorial/handout_cvxpy.py, problem4, problem5, problem6, problem7, problem8, get_problem1_dual_value, get_vars, get_vars, get_vars, get_vars\n## tags\n[text classification, stochastic optimization]\n\n## repository\ndavide-belli/toulouse-road-network-dataset\n## files\nexperiments/experiment_graphsize_distribution.py, experiments/experiment_maxprevnode_distribution.py, utils/generate_bins.py, utils/generate_datapoints.py, utils/generate_image_arrays.py, euclidean_distance_points, save_image_by_lines, save_image_by_nodes_edges, save_image_bfs, custom_collate_fn_with_coordinates, ToulouseRoadNetworkDataset, get_nodes_from_lines, get_possible_lines, get_valid_lines, handle_crossings\n## tags\n[graph generation, semantic segmentation]\n\n## repository\nakakzia/decstr\n## files\ndemo_sentence_expr.py, demo_sentence_seq.py, demo_sentence_test.py, goal_sampler.py, env/fetch_manipulate_env.py, CompressPDF, check_length_and_seeds, plot_c_lp_p_sr, plot_lp_av, plot_sr_av, update_LP, n_cycles, _update_normalizer, save_bucket_contents, valid_goals\n## 3 tags\n[text classification, decision tree learning]\n\n## repository\nvickygupta', 'id': '415e94831b13fedd3606668e54901d76', 'name': 'akakzia/decstr', 'tasks': 'text classification, decision tree learning, ', 'true_text': 'language acquisition'}, 'generation': 0, 'input_text': '\n## repository\nstephenbeckr/AIMS\n## files\nCVX_tutorial/handout_cvxpy.py, problem4, problem5, problem6, problem7, problem8, get_problem1_dual_value, get_vars, get_vars, get_vars, get_vars\n## tags\n[text classification, stochastic optimization]\n\n## repository\ndavide-belli/toulouse-road-network-dataset\n## files\nexperiments/experiment_graphsize_distribution.py, experiments/experiment_maxprevnode_distribution.py, utils/generate_bins.py, utils/generate_datapoints.py, utils/generate_image_arrays.py, euclidean_distance_points, save_image_by_lines, save_image_by_nodes_edges, save_image_bfs, custom_collate_fn_with_coordinates, ToulouseRoadNetworkDataset, get_nodes_from_lines, get_possible_lines, get_valid_lines, handle_crossings\n## tags\n[graph generation, semantic segmentation]\n\n## repository\nakakzia/decstr\n## files\ndemo_sentence_expr.py, demo_sentence_seq.py, demo_sentence_test.py, goal_sampler.py, env/fetch_manipulate_env.py, CompressPDF, check_length_and_seeds, plot_c_lp_p_sr, plot_lp_av, plot_sr_av, update_LP, n_cycles, _update_normalizer, save_bucket_contents, valid_goals\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/decstr-learning-goal-directed-abstract']", 'paper_titles': "['Grounding Language to Autonomously-Acquired Skills via Goal Generation']", 'arxiv_ids': "['2006.07185']", 'authors': '["[\'Ahmed Akakzia\', \'Cdric Colas\', \'Pierre-Yves Oudeyer\', \'Mohamed Chetouani\', \'Olivier Sigaud\']"]', 'raw_generated_text': '\n## repository\nstephenbeckr/AIMS\n## files\nCVX_tutorial/handout_cvxpy.py, problem4, problem5, problem6, problem7, problem8, get_problem1_dual_value, get_vars, get_vars, get_vars, get_vars\n## tags\n[text classification, stochastic optimization]\n\n## repository\ndavide-belli/toulouse-road-network-dataset\n## files\nexperiments/experiment_graphsize_distribution.py, experiments/experiment_maxprevnode_distribution.py, utils/generate_bins.py, utils/generate_datapoints.py, utils/generate_image_arrays.py, euclidean_distance_points, save_image_by_lines, save_image_by_nodes_edges, save_image_bfs, custom_collate_fn_with_coordinates, ToulouseRoadNetworkDataset, get_nodes_from_lines, get_possible_lines, get_valid_lines, handle_crossings\n## tags\n[graph generation, semantic segmentation]\n\n## repository\nakakzia/decstr\n## files\ndemo_sentence_expr.py, demo_sentence_seq.py, demo_sentence_test.py, goal_sampler.py, env/fetch_manipulate_env.py, CompressPDF, check_length_and_seeds, plot_c_lp_p_sr, plot_lp_av, plot_sr_av, update_LP, n_cycles, _update_normalizer, save_bucket_contents, valid_goals\n## 3 tags\n[text classification, decision tree learning]\n\n## repository\nvickygupta', 'raw_reference_text': 'language acquisition', 'reference_text': 'language acquisition', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7682785987854004, 'bertscore_recall': 0.8602006435394287, 'bertscore_f1': 0.8116453289985657, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.24829761683940887}, {'repo': 'DFKI-NLP/REval', 'tasks_x': 'srl, ', 'true_tasks': ['relation extraction'], 'generated_text': 'multiple instance learning', 'prompt_info': {'content': 'probing_task_evaluation.py, reval.py, reval/datasets.py, reval/dataset_utils.py, reval/dependency_graph_utils.py, EntityTypeCountBetweenHeadTailEval, PosTagArgPositionEval, EntityExistsBetweenHeadTailEval, TreeDepthEval, REPROBINGEval, probing_task_generator, get_probing_task_generator, absolute_entity_dist_in_bucket, tacred_generate, semeval_generate', 'generated_text': '\n## repository\nYapengTian/AVVP-ECCV20\n## files\nmain_avvp.py, nets/net_audiovisual.py, scripts/extract_3D_feat.py, scripts/extract_frames.py, scripts/extract_rgb_feat.py, LLP_dataset, event_level, segment_level, ids_to_multinomial, MMIL_Net, LLP_dataset, load_image_fn, extract_event, event_wise_metric, label_val\n## tags\n[multiple instance learning]\n\n## repository\nmseitzer/csmri-refinement\n## files\ndata/transform_utils.py, data/transform_wrappers.py, data/reconstruction/io.py, data/reconstruction/rec_seg_transforms.py, data/reconstruction/rec_transforms.py, scale_batch_per_example, normalize_batch_per_example, magnitude_image, get_same_padding_layer, need_bias, epoch_beginning, runner_type, early_stoppers, files_or_dirs, out_checkpoint\n## tags\n[mri reconstruction]\n\n## repository\nDFKI-NLP/REval\n## files\nprobing_task_evaluation.py, reval.py, reval/datasets.py, reval/dataset_utils.py, reval/dependency_graph_utils.py, EntityTypeCountBetweenHeadTailEval, PosTagArgPositionEval, EntityExistsBetweenHeadTailEval, TreeDepthEval, REPROBINGEval, probing_task_generator, get_probing_task_generator, absolute_entity_dist_in_bucket, tacred_generate, semeval_generate\n## 3 tags\n[srl]\n\n## repository\nasyml/texar-pytorch\n##', 'id': '25dedac81a5666c1a607660ebd0c3811', 'name': 'DFKI-NLP/REval', 'tasks': 'srl, ', 'true_text': 'relation extraction'}, 'generation': 0, 'input_text': '\n## repository\nYapengTian/AVVP-ECCV20\n## files\nmain_avvp.py, nets/net_audiovisual.py, scripts/extract_3D_feat.py, scripts/extract_frames.py, scripts/extract_rgb_feat.py, LLP_dataset, event_level, segment_level, ids_to_multinomial, MMIL_Net, LLP_dataset, load_image_fn, extract_event, event_wise_metric, label_val\n## tags\n[multiple instance learning]\n\n## repository\nmseitzer/csmri-refinement\n## files\ndata/transform_utils.py, data/transform_wrappers.py, data/reconstruction/io.py, data/reconstruction/rec_seg_transforms.py, data/reconstruction/rec_transforms.py, scale_batch_per_example, normalize_batch_per_example, magnitude_image, get_same_padding_layer, need_bias, epoch_beginning, runner_type, early_stoppers, files_or_dirs, out_checkpoint\n## tags\n[mri reconstruction]\n\n## repository\nDFKI-NLP/REval\n## files\nprobing_task_evaluation.py, reval.py, reval/datasets.py, reval/dataset_utils.py, reval/dependency_graph_utils.py, EntityTypeCountBetweenHeadTailEval, PosTagArgPositionEval, EntityExistsBetweenHeadTailEval, TreeDepthEval, REPROBINGEval, probing_task_generator, get_probing_task_generator, absolute_entity_dist_in_bucket, tacred_generate, semeval_generate\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/probing-linguistic-features-of-sentence-level']", 'paper_titles': "['Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction']", 'arxiv_ids': "['2004.08134']", 'authors': '["[\'Christoph Alt\', \'Aleksandra Gabryszak\', \'Leonhard Hennig\']"]', 'raw_generated_text': '\n## repository\nYapengTian/AVVP-ECCV20\n## files\nmain_avvp.py, nets/net_audiovisual.py, scripts/extract_3D_feat.py, scripts/extract_frames.py, scripts/extract_rgb_feat.py, LLP_dataset, event_level, segment_level, ids_to_multinomial, MMIL_Net, LLP_dataset, load_image_fn, extract_event, event_wise_metric, label_val\n## tags\n[multiple instance learning]\n\n## repository\nmseitzer/csmri-refinement\n## files\ndata/transform_utils.py, data/transform_wrappers.py, data/reconstruction/io.py, data/reconstruction/rec_seg_transforms.py, data/reconstruction/rec_transforms.py, scale_batch_per_example, normalize_batch_per_example, magnitude_image, get_same_padding_layer, need_bias, epoch_beginning, runner_type, early_stoppers, files_or_dirs, out_checkpoint\n## tags\n[mri reconstruction]\n\n## repository\nDFKI-NLP/REval\n## files\nprobing_task_evaluation.py, reval.py, reval/datasets.py, reval/dataset_utils.py, reval/dependency_graph_utils.py, EntityTypeCountBetweenHeadTailEval, PosTagArgPositionEval, EntityExistsBetweenHeadTailEval, TreeDepthEval, REPROBINGEval, probing_task_generator, get_probing_task_generator, absolute_entity_dist_in_bucket, tacred_generate, semeval_generate\n## 3 tags\n[srl]\n\n## repository\nasyml/texar-pytorch\n##', 'raw_reference_text': 'relation extraction', 'reference_text': 'relation extraction', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8462950587272644, 'bertscore_recall': 0.8859041333198547, 'bertscore_f1': 0.8656467795372009, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.13132479786872864}, {'repo': 'masoudpz/AVID-Adversarial-Visual-Irregularity-Detection', 'tasks_x': 'adversarial learning, image generation, ', 'true_tasks': ['anomaly detection'], 'generated_text': 'molecular property prediction', 'prompt_info': {'content': 'fake_data_loader.py, unet/unet_parts.py, unet/unet_model.py, unet/__init__.py, solver.py, our_Discriminator, Alex_disc, U_Discriminator, U_Generator, get_fake_loader, get_fake_loader, fake_data_loader, sample_path, ConvTranspose2d, model_path', 'generated_text': '\n## repository\nbenatorc/PA-Graph-Transformer\n## files\ndatasets/mol_dataset.py, models/atom_predictor.py, models/mol_conv_net.py, models/mol_transformer.py, models/prop_predictor.py, map_equiv, dict_to_dstr, dict_to_pstr, convert_to_3D, convert_to_2D, use_paths, batch_splits, n_neg, n_rounds, dict_to_dstr\n## tags\n[molecular property prediction]\n\n## repository\nckbjimmy/clneg\n## files\nsrc/concept_extraction.py, src/create_tokenization.py, src/negex.py, src/syntactic_parsing.py, src/tree_rules.py, print_out_result, extract_cuis, ctakes_concept_extraction, synparse, tregex_tsurgeon, getNegationFlag, getScopes, getNegTaggedSentence, get_cui_spans, sortRules\n## tags\n[text summarization]\n\n## repository\nmasoudpz/AVID-Adversarial-Visual-Irregularity-Detection\n## files\nfake_data_loader.py, unet/unet_parts.py, unet/unet_model.py, unet/__init__.py, solver.py, our_Discriminator, Alex_disc, U_Discriminator, U_Generator, get_fake_loader, get_fake_loader, fake_data_loader, sample_path, ConvTranspose2d, model_path\n## 3 tags\n[adversarial learning]\n[image generation]\n\n## repository\nDis', 'id': 'bfe128385353b15ca2de7fd06c41544a', 'name': 'masoudpz/AVID-Adversarial-Visual-Irregularity-Detection', 'tasks': 'adversarial learning, image generation, ', 'true_text': 'anomaly detection'}, 'generation': 0, 'input_text': '\n## repository\nbenatorc/PA-Graph-Transformer\n## files\ndatasets/mol_dataset.py, models/atom_predictor.py, models/mol_conv_net.py, models/mol_transformer.py, models/prop_predictor.py, map_equiv, dict_to_dstr, dict_to_pstr, convert_to_3D, convert_to_2D, use_paths, batch_splits, n_neg, n_rounds, dict_to_dstr\n## tags\n[molecular property prediction]\n\n## repository\nckbjimmy/clneg\n## files\nsrc/concept_extraction.py, src/create_tokenization.py, src/negex.py, src/syntactic_parsing.py, src/tree_rules.py, print_out_result, extract_cuis, ctakes_concept_extraction, synparse, tregex_tsurgeon, getNegationFlag, getScopes, getNegTaggedSentence, get_cui_spans, sortRules\n## tags\n[text summarization]\n\n## repository\nmasoudpz/AVID-Adversarial-Visual-Irregularity-Detection\n## files\nfake_data_loader.py, unet/unet_parts.py, unet/unet_model.py, unet/__init__.py, solver.py, our_Discriminator, Alex_disc, U_Discriminator, U_Generator, get_fake_loader, get_fake_loader, fake_data_loader, sample_path, ConvTranspose2d, model_path\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/avid-adversarial-visual-irregularity']", 'paper_titles': "['AVID: Adversarial Visual Irregularity Detection']", 'arxiv_ids': "['1805.09521']", 'authors': '["[\'Mohammad Sabokrou\', \'Masoud Pourreza\', \'Mohsen Fayyaz\', \'Rahim Entezari\', \'Mahmood Fathy\', \'Jrgen Gall\', \'Ehsan Adeli\']"]', 'raw_generated_text': '\n## repository\nbenatorc/PA-Graph-Transformer\n## files\ndatasets/mol_dataset.py, models/atom_predictor.py, models/mol_conv_net.py, models/mol_transformer.py, models/prop_predictor.py, map_equiv, dict_to_dstr, dict_to_pstr, convert_to_3D, convert_to_2D, use_paths, batch_splits, n_neg, n_rounds, dict_to_dstr\n## tags\n[molecular property prediction]\n\n## repository\nckbjimmy/clneg\n## files\nsrc/concept_extraction.py, src/create_tokenization.py, src/negex.py, src/syntactic_parsing.py, src/tree_rules.py, print_out_result, extract_cuis, ctakes_concept_extraction, synparse, tregex_tsurgeon, getNegationFlag, getScopes, getNegTaggedSentence, get_cui_spans, sortRules\n## tags\n[text summarization]\n\n## repository\nmasoudpz/AVID-Adversarial-Visual-Irregularity-Detection\n## files\nfake_data_loader.py, unet/unet_parts.py, unet/unet_model.py, unet/__init__.py, solver.py, our_Discriminator, Alex_disc, U_Discriminator, U_Generator, get_fake_loader, get_fake_loader, fake_data_loader, sample_path, ConvTranspose2d, model_path\n## 3 tags\n[adversarial learning]\n[image generation]\n\n## repository\nDis', 'raw_reference_text': 'anomaly detection', 'reference_text': 'anomaly detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8674962520599365, 'bertscore_recall': 0.9050971865653992, 'bertscore_f1': 0.8858978748321533, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.3775424659252167}, {'repo': 'YunseokJANG/l2l-da', 'tasks_x': 'blackbox attack, l2 loneliness technique, machine learning, ', 'true_tasks': ['adversarial defense'], 'generated_text': 'chinese word segmentation', 'prompt_info': {'content': 'evaluation_cifar.py, evaluation_mnist.py, evaluation_tinyimagenet.py, adv_defence/attacks.py, adv_defence/classifier_tester.py, build_delta_fgsm, build_delta_pgd, build_stadv_pgd, build_rot_trans_pgd, build_delta_rot_trans_pgd, need_samples, test_classifier_with_l2lda_att, get_sample_pdf_of_checkpoint, flavor_blackbox, pretty_printer', 'generated_text': '\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## tags\n[chinese word segmentation]\n\n## repository\nLi-Chengyang/MSDS-RCNN\n## files\nlib/layer_utils/fusion_layer.py, lib/layer_utils/proposal_layer_combine.py, lib/datasets/kaist.py, lib/utils/nms.py, lib/model/nms_wrapper.py, pad_rois, im_detect_demo, proposal_layer_combine_bcn, proposal_layer_combine_rpn, fusion_layer, RPN_CONF_THRESH, im_detect_demo, POOL_PAD_RATIO, RPN_NMS_THRESH, RPN_NMS_THRESH\n## tags\n[pedestrian detection, autonomous driving, semantic segmentation]\n\n## repository\nYunseokJANG/l2l-da\n## files\nevaluation_cifar.py, evaluation_mnist.py, evaluation_tinyimagenet.py, adv_defence/attacks.py, adv_defence/classifier_tester.py, build_delta_fgsm, build_delta_pgd, build_stadv_pgd, build_rot_trans_pgd, build_delta_rot_trans_pgd, need_samples, test_classifier_with_l2lda_att, get_sample_pdf_of_checkpoint, flavor_blackbox, pretty_printer\n## 3 tags\n[blackbox attack, l2 loneliness technique, machine learning]\n\n', 'id': 'a526072be1055107252ab26dd1df7750', 'name': 'YunseokJANG/l2l-da', 'tasks': 'blackbox attack, l2 loneliness technique, machine learning, ', 'true_text': 'adversarial defense'}, 'generation': 0, 'input_text': '\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## tags\n[chinese word segmentation]\n\n## repository\nLi-Chengyang/MSDS-RCNN\n## files\nlib/layer_utils/fusion_layer.py, lib/layer_utils/proposal_layer_combine.py, lib/datasets/kaist.py, lib/utils/nms.py, lib/model/nms_wrapper.py, pad_rois, im_detect_demo, proposal_layer_combine_bcn, proposal_layer_combine_rpn, fusion_layer, RPN_CONF_THRESH, im_detect_demo, POOL_PAD_RATIO, RPN_NMS_THRESH, RPN_NMS_THRESH\n## tags\n[pedestrian detection, autonomous driving, semantic segmentation]\n\n## repository\nYunseokJANG/l2l-da\n## files\nevaluation_cifar.py, evaluation_mnist.py, evaluation_tinyimagenet.py, adv_defence/attacks.py, adv_defence/classifier_tester.py, build_delta_fgsm, build_delta_pgd, build_stadv_pgd, build_rot_trans_pgd, build_delta_rot_trans_pgd, need_samples, test_classifier_with_l2lda_att, get_sample_pdf_of_checkpoint, flavor_blackbox, pretty_printer\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/adversarial-defense-via-learning-to-generate']", 'paper_titles': "['Adversarial Defense via Learning to Generate Diverse Attacks']", 'arxiv_ids': '[nan]', 'authors': '["[\'Yunseok Jang\', \' Tianchen Zhao\', \' Seunghoon Hong\', \' Honglak Lee\']"]', 'raw_generated_text': '\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## tags\n[chinese word segmentation]\n\n## repository\nLi-Chengyang/MSDS-RCNN\n## files\nlib/layer_utils/fusion_layer.py, lib/layer_utils/proposal_layer_combine.py, lib/datasets/kaist.py, lib/utils/nms.py, lib/model/nms_wrapper.py, pad_rois, im_detect_demo, proposal_layer_combine_bcn, proposal_layer_combine_rpn, fusion_layer, RPN_CONF_THRESH, im_detect_demo, POOL_PAD_RATIO, RPN_NMS_THRESH, RPN_NMS_THRESH\n## tags\n[pedestrian detection, autonomous driving, semantic segmentation]\n\n## repository\nYunseokJANG/l2l-da\n## files\nevaluation_cifar.py, evaluation_mnist.py, evaluation_tinyimagenet.py, adv_defence/attacks.py, adv_defence/classifier_tester.py, build_delta_fgsm, build_delta_pgd, build_stadv_pgd, build_rot_trans_pgd, build_delta_rot_trans_pgd, need_samples, test_classifier_with_l2lda_att, get_sample_pdf_of_checkpoint, flavor_blackbox, pretty_printer\n## 3 tags\n[blackbox attack, l2 loneliness technique, machine learning]\n\n', 'raw_reference_text': 'adversarial defense', 'reference_text': 'adversarial defense', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8269888162612915, 'bertscore_recall': 0.8047726154327393, 'bertscore_f1': 0.8157294988632202, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1352863758802414}, {'repo': 'nish03/FuseVis', 'tasks_x': 'reinforcement learning, visual learning, machine learning, machine learning, ', 'true_tasks': ['autonomous driving'], 'generated_text': 'automl', 'prompt_info': {'content': 'FuseVis/networks.py, FuseVis/preprocess.py, FuseVis/user_interface.py, FuseVis/_init_.py, FunFuseAn, Weighted_Averaging, ConvLayer_DeepFuse, DeepFuse, DeepPedestrian, model6, mouseover_Callback, start_mouseover, DoubleVar, ButtonObject', 'generated_text': '\n## repository\npfnet-research/autogbt-alt\n## files\nautogbt/average.py, autogbt/classifier.py, autogbt/logging.py, autogbt/objective.py, autogbt/optimizer.py, test_sample_with_series, test_sample_with_ndarray, test_sample_for_regression, test_extract_datetime_features, test_extract_datetime_difference, model_valid_frac, model_train_frac, competitions, AutoGBTRegressor, GBTCVTrainer\n## tags\n[automl]\n\n## repository\nolegl/distort\n## files\ndistort/remap-blur.py, quality/imshowpair.py, quality/show_flow.py, distort/gen_damage.py, quality/jaccard.py, mapFromFlowCBAD, mapFromFlowC, mapFromFlowD, base_map, displacements_scale, mapFromFlow, mapFromFlow, normalvariate, write_map, THRESH_OTSU\n## tags\n[medical image registration]\n\n## repository\nnish03/FuseVis\n## files\nFuseVis/networks.py, FuseVis/preprocess.py, FuseVis/user_interface.py, FuseVis/_init_.py, FunFuseAn, Weighted_Averaging, ConvLayer_DeepFuse, DeepFuse, DeepPedestrian, model6, mouseover_Callback, start_mouseover, DoubleVar, ButtonObject\n## 3 tags\n[reinforcement learning, visual learning, machine learning]\n[machine learning]', 'id': '66ff30546b3e6b6dae00d011499dd83f', 'name': 'nish03/FuseVis', 'tasks': 'reinforcement learning, visual learning, machine learning, machine learning, ', 'true_text': 'autonomous driving'}, 'generation': 0, 'input_text': '\n## repository\npfnet-research/autogbt-alt\n## files\nautogbt/average.py, autogbt/classifier.py, autogbt/logging.py, autogbt/objective.py, autogbt/optimizer.py, test_sample_with_series, test_sample_with_ndarray, test_sample_for_regression, test_extract_datetime_features, test_extract_datetime_difference, model_valid_frac, model_train_frac, competitions, AutoGBTRegressor, GBTCVTrainer\n## tags\n[automl]\n\n## repository\nolegl/distort\n## files\ndistort/remap-blur.py, quality/imshowpair.py, quality/show_flow.py, distort/gen_damage.py, quality/jaccard.py, mapFromFlowCBAD, mapFromFlowC, mapFromFlowD, base_map, displacements_scale, mapFromFlow, mapFromFlow, normalvariate, write_map, THRESH_OTSU\n## tags\n[medical image registration]\n\n## repository\nnish03/FuseVis\n## files\nFuseVis/networks.py, FuseVis/preprocess.py, FuseVis/user_interface.py, FuseVis/_init_.py, FunFuseAn, Weighted_Averaging, ConvLayer_DeepFuse, DeepFuse, DeepPedestrian, model6, mouseover_Callback, start_mouseover, DoubleVar, ButtonObject\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/fusevis-interpreting-neural-networks-for']", 'paper_titles': "['FuseVis: Interpreting neural networks for image fusion using per-pixel saliency visualization']", 'arxiv_ids': "['2012.08932']", 'authors': '["[\'Nishant Kumar\', \'Stefan Gumhold\']"]', 'raw_generated_text': '\n## repository\npfnet-research/autogbt-alt\n## files\nautogbt/average.py, autogbt/classifier.py, autogbt/logging.py, autogbt/objective.py, autogbt/optimizer.py, test_sample_with_series, test_sample_with_ndarray, test_sample_for_regression, test_extract_datetime_features, test_extract_datetime_difference, model_valid_frac, model_train_frac, competitions, AutoGBTRegressor, GBTCVTrainer\n## tags\n[automl]\n\n## repository\nolegl/distort\n## files\ndistort/remap-blur.py, quality/imshowpair.py, quality/show_flow.py, distort/gen_damage.py, quality/jaccard.py, mapFromFlowCBAD, mapFromFlowC, mapFromFlowD, base_map, displacements_scale, mapFromFlow, mapFromFlow, normalvariate, write_map, THRESH_OTSU\n## tags\n[medical image registration]\n\n## repository\nnish03/FuseVis\n## files\nFuseVis/networks.py, FuseVis/preprocess.py, FuseVis/user_interface.py, FuseVis/_init_.py, FunFuseAn, Weighted_Averaging, ConvLayer_DeepFuse, DeepFuse, DeepPedestrian, model6, mouseover_Callback, start_mouseover, DoubleVar, ButtonObject\n## 3 tags\n[reinforcement learning, visual learning, machine learning]\n[machine learning]', 'raw_reference_text': 'autonomous driving', 'reference_text': 'autonomous driving', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8333958983421326, 'bertscore_recall': 0.870456337928772, 'bertscore_f1': 0.8515231013298035, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.4412150979042053}, {'repo': 'jackryo/ricap', 'tasks_x': 'image+vr, modeling, 2019-09-0', 'true_tasks': ['image augmentation', 'data augmentation', 'image cropping'], 'generated_text': 'visual tracking common sense reasoning', 'prompt_info': {'content': 'models/commonlib.py, models/wide_resnet_imagenet.py, models/wide_resnet_cifar.py, trainers.py, models/__init__.py, TrainerRICAP, WideResNetBottleneck, WideResNetDropoutBottleneck, WideBlockBottleneck, WideResNetDropout, adlr, savemodel, make_trainer, Vectorize, Vectorize', 'generated_text': '\n## repository\nmhazoglou/PVM_PyCUDA\n## files\nclient_cv.py, FormattingFiles.py, FractalConstructor.py, panTiltDataCollector.py, PVM_PyCUDA.py, new_block_idx, base_pvm_gpu_forward, base_pvm_gpu_backprop, adam_update_parameters, PVMtracker, adam_train_and_validate, L_full_input, grid_i2h_dot, grid_h2op_dot, stream5\n## tags\n[visual tracking, common sense reasoning]\n\n## repository\noulutan/OP-Bilinear-Model\n## files\n01_Seismic.py, 04_Images_Inception.py, 17_ST_Concat.py, 18_ST_bilinear.py, 19_Bilinear_Without_3d.py, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, resize_bicubic, with_dependencies, UPDATE_OPS, TFRecordReader, run_training\n## tags\n[human detection]\n\n## repository\njackryo/ricap\n## files\nmodels/commonlib.py, models/wide_resnet_imagenet.py, models/wide_resnet_cifar.py, trainers.py, models/__init__.py, TrainerRICAP, WideResNetBottleneck, WideResNetDropoutBottleneck, WideBlockBottleneck, WideResNetDropout, adlr, savemodel, make_trainer, Vectorize, Vectorize\n## 3 tags\n[image+vr, modeling]\n\n\n2019-09-0', 'id': '0c6967f6ae1de7180fe56abad43852e6', 'name': 'jackryo/ricap', 'tasks': 'image+vr, modeling, 2019-09-0', 'true_text': 'image augmentation, data augmentation, image cropping'}, 'generation': 0, 'input_text': '\n## repository\nmhazoglou/PVM_PyCUDA\n## files\nclient_cv.py, FormattingFiles.py, FractalConstructor.py, panTiltDataCollector.py, PVM_PyCUDA.py, new_block_idx, base_pvm_gpu_forward, base_pvm_gpu_backprop, adam_update_parameters, PVMtracker, adam_train_and_validate, L_full_input, grid_i2h_dot, grid_h2op_dot, stream5\n## tags\n[visual tracking, common sense reasoning]\n\n## repository\noulutan/OP-Bilinear-Model\n## files\n01_Seismic.py, 04_Images_Inception.py, 17_ST_Concat.py, 18_ST_bilinear.py, 19_Bilinear_Without_3d.py, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, resize_bicubic, with_dependencies, UPDATE_OPS, TFRecordReader, run_training\n## tags\n[human detection]\n\n## repository\njackryo/ricap\n## files\nmodels/commonlib.py, models/wide_resnet_imagenet.py, models/wide_resnet_cifar.py, trainers.py, models/__init__.py, TrainerRICAP, WideResNetBottleneck, WideResNetDropoutBottleneck, WideBlockBottleneck, WideResNetDropout, adlr, savemodel, make_trainer, Vectorize, Vectorize\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/data-augmentation-using-random-image-cropping']", 'paper_titles': "['Data Augmentation using Random Image Cropping and Patching for Deep CNNs']", 'arxiv_ids': "['1811.09030']", 'authors': '["[\'Ryo Takahashi\', \'Takashi Matsubara\', \'Kuniaki Uehara\']"]', 'raw_generated_text': '\n## repository\nmhazoglou/PVM_PyCUDA\n## files\nclient_cv.py, FormattingFiles.py, FractalConstructor.py, panTiltDataCollector.py, PVM_PyCUDA.py, new_block_idx, base_pvm_gpu_forward, base_pvm_gpu_backprop, adam_update_parameters, PVMtracker, adam_train_and_validate, L_full_input, grid_i2h_dot, grid_h2op_dot, stream5\n## tags\n[visual tracking, common sense reasoning]\n\n## repository\noulutan/OP-Bilinear-Model\n## files\n01_Seismic.py, 04_Images_Inception.py, 17_ST_Concat.py, 18_ST_bilinear.py, 19_Bilinear_Without_3d.py, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, run_eval_testing, resize_bicubic, with_dependencies, UPDATE_OPS, TFRecordReader, run_training\n## tags\n[human detection]\n\n## repository\njackryo/ricap\n## files\nmodels/commonlib.py, models/wide_resnet_imagenet.py, models/wide_resnet_cifar.py, trainers.py, models/__init__.py, TrainerRICAP, WideResNetBottleneck, WideResNetDropoutBottleneck, WideBlockBottleneck, WideResNetDropout, adlr, savemodel, make_trainer, Vectorize, Vectorize\n## 3 tags\n[image+vr, modeling]\n\n\n2019-09-0', 'raw_reference_text': 'image augmentation, data augmentation, image cropping', 'reference_text': 'data augmentation image cropping image augmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8417675495147705, 'bertscore_recall': 0.818745493888855, 'bertscore_f1': 0.8300969004631042, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.04352516680955887}, {'repo': 'Shiaoming/vid2depth', 'tasks_x': 'computer vision, image processing, ', 'true_tasks': ['depth and camera motion', 'depth estimation'], 'generated_text': 'network pruning', 'prompt_info': {'content': 'dataset/dataset_loader.py, dataset/gen_data.py, ops/icp_grad.py, ops/icp_grad_test.py, ops/icp_test.py, _normalize_depth_for_display, get_seq_middle, get_resource_path, _gen_example, np_transform_cloud_xyz, get_example_with_index, pretrained_ckpt, Bike, _stack_image_seq, np_get_transformation_matrix', 'generated_text': '\n## repository\nfriendshipkim/neuron-merging\n## files\nmodels/LeNet_300_100.py, models/WideResNet.py, models/VGG.py, decompose.py, models/ResNet.py, create_scaling_mat_ip_thres_bias, create_scaling_mat_conv_thres_bn, LeNet_300_100, Decompose, save_state, save_state, pairwise_distances, pairwise_distances, lr, criterion\n## tags\n[network pruning]\n\n## repository\nifnspaml/perceptual-weighting-filter-loss\n## files\nGitHub_all_test_mask_dnn_baseline.py, GitHub_all_test_mask_dnn_weight_filter.py, GitHub_mask_dnn_baseline_train.py, GitHub_mask_dnn_weight_filter_train.py, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, min, max, zeros, range\n## tags\n[speech enhancement]\n\n## repository\nShiaoming/vid2depth\n## files\ndataset/dataset_loader.py, dataset/gen_data.py, ops/icp_grad.py, ops/icp_grad_test.py, ops/icp_test.py, _normalize_depth_for_display, get_seq_middle, get_resource_path, _gen_example, np_transform_cloud_xyz, get_example_with_index, pretrained_ckpt, Bike, _stack_image_seq, np_get_transformation_matrix\n## 3 tags\n[computer vision], [image processing]\n\n## repository\nydufeng', 'id': 'b998d201e0716811469833ee9337c770', 'name': 'Shiaoming/vid2depth', 'tasks': 'computer vision, image processing, ', 'true_text': 'depth and camera motion, depth estimation'}, 'generation': 0, 'input_text': '\n## repository\nfriendshipkim/neuron-merging\n## files\nmodels/LeNet_300_100.py, models/WideResNet.py, models/VGG.py, decompose.py, models/ResNet.py, create_scaling_mat_ip_thres_bias, create_scaling_mat_conv_thres_bn, LeNet_300_100, Decompose, save_state, save_state, pairwise_distances, pairwise_distances, lr, criterion\n## tags\n[network pruning]\n\n## repository\nifnspaml/perceptual-weighting-filter-loss\n## files\nGitHub_all_test_mask_dnn_baseline.py, GitHub_all_test_mask_dnn_weight_filter.py, GitHub_mask_dnn_baseline_train.py, GitHub_mask_dnn_weight_filter_train.py, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, min, max, zeros, range\n## tags\n[speech enhancement]\n\n## repository\nShiaoming/vid2depth\n## files\ndataset/dataset_loader.py, dataset/gen_data.py, ops/icp_grad.py, ops/icp_grad_test.py, ops/icp_test.py, _normalize_depth_for_display, get_seq_middle, get_resource_path, _gen_example, np_transform_cloud_xyz, get_example_with_index, pretrained_ckpt, Bike, _stack_image_seq, np_get_transformation_matrix\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-2']", 'paper_titles': "['Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints']", 'arxiv_ids': "['1802.05522']", 'authors': '["[\'Reza Mahjourian\', \'Martin Wicke\', \'Anelia Angelova\']"]', 'raw_generated_text': '\n## repository\nfriendshipkim/neuron-merging\n## files\nmodels/LeNet_300_100.py, models/WideResNet.py, models/VGG.py, decompose.py, models/ResNet.py, create_scaling_mat_ip_thres_bias, create_scaling_mat_conv_thres_bn, LeNet_300_100, Decompose, save_state, save_state, pairwise_distances, pairwise_distances, lr, criterion\n## tags\n[network pruning]\n\n## repository\nifnspaml/perceptual-weighting-filter-loss\n## files\nGitHub_all_test_mask_dnn_baseline.py, GitHub_all_test_mask_dnn_weight_filter.py, GitHub_mask_dnn_baseline_train.py, GitHub_mask_dnn_weight_filter_train.py, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, reshapeDataMatrix, min, max, zeros, range\n## tags\n[speech enhancement]\n\n## repository\nShiaoming/vid2depth\n## files\ndataset/dataset_loader.py, dataset/gen_data.py, ops/icp_grad.py, ops/icp_grad_test.py, ops/icp_test.py, _normalize_depth_for_display, get_seq_middle, get_resource_path, _gen_example, np_transform_cloud_xyz, get_example_with_index, pretrained_ckpt, Bike, _stack_image_seq, np_get_transformation_matrix\n## 3 tags\n[computer vision], [image processing]\n\n## repository\nydufeng', 'raw_reference_text': 'depth and camera motion, depth estimation', 'reference_text': 'depth estimation depth and camera motion', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8800957202911377, 'bertscore_recall': 0.8521224856376648, 'bertscore_f1': 0.8658831715583801, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2047816514968872}, {'repo': 'Hitsaa/Practice', 'tasks_x': 'vector machines, convnets, deployment, ', 'true_tasks': ['data augmentation'], 'generated_text': 'word sense induction word embeddings', 'prompt_info': {'content': 'benchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn', 'generated_text': '\n## repository\nakutuzov/russian_wsi\n## files\nwsi.py, helpers.py, evaluate.py, ari_per_word_weighted, wordweight, gold_predict, fingerprint, visualize, gold_sense_id, twostage, ari_per_word_weighted, wordweight, gold_predict\n## tags\n[word sense induction, word embeddings]\n\n## repository\njakeju/wav2letter\n## files\nrecipes/data/timit/prepare.py, recipes/models/self_training/librispeech/lm/clean_lm_text.py, recipes/models/self_training/librispeech/lm/filter_contractions.py, recipes/models/self_training/librispeech/lm/generate_lm_raw_text.py, recipes/models/self_training/librispeech/lm/sentence_ify.py, TranscriptPrediction, generate_wp_selling, get_am_bookids, remove_am_books_from_lm, load_lm_books, lexicon2, lexicon1, distributed_decoding, listpath, inputhyp\n## tags\n[speech recognition]\n\n## repository\nHitsaa/Practice\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## 3 tags\n[vector machines, convnets, deployment]\n\n## repository\nranker.', 'id': '1e49b963231b2a34bcf1f046fc087482', 'name': 'Hitsaa/Practice', 'tasks': 'vector machines, convnets, deployment, ', 'true_text': 'data augmentation'}, 'generation': 0, 'input_text': '\n## repository\nakutuzov/russian_wsi\n## files\nwsi.py, helpers.py, evaluate.py, ari_per_word_weighted, wordweight, gold_predict, fingerprint, visualize, gold_sense_id, twostage, ari_per_word_weighted, wordweight, gold_predict\n## tags\n[word sense induction, word embeddings]\n\n## repository\njakeju/wav2letter\n## files\nrecipes/data/timit/prepare.py, recipes/models/self_training/librispeech/lm/clean_lm_text.py, recipes/models/self_training/librispeech/lm/filter_contractions.py, recipes/models/self_training/librispeech/lm/generate_lm_raw_text.py, recipes/models/self_training/librispeech/lm/sentence_ify.py, TranscriptPrediction, generate_wp_selling, get_am_bookids, remove_am_books_from_lm, load_lm_books, lexicon2, lexicon1, distributed_decoding, listpath, inputhyp\n## tags\n[speech recognition]\n\n## repository\nHitsaa/Practice\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/denser-deep-evolutionary-network-structured']", 'paper_titles': "['DENSER: Deep Evolutionary Network Structured Representation']", 'arxiv_ids': "['1801.01563']", 'authors': '["[\'Filipe Assuno\', \'Nuno Loureno\', \'Penousal Machado\', \'Bernardete Ribeiro\']"]', 'raw_generated_text': '\n## repository\nakutuzov/russian_wsi\n## files\nwsi.py, helpers.py, evaluate.py, ari_per_word_weighted, wordweight, gold_predict, fingerprint, visualize, gold_sense_id, twostage, ari_per_word_weighted, wordweight, gold_predict\n## tags\n[word sense induction, word embeddings]\n\n## repository\njakeju/wav2letter\n## files\nrecipes/data/timit/prepare.py, recipes/models/self_training/librispeech/lm/clean_lm_text.py, recipes/models/self_training/librispeech/lm/filter_contractions.py, recipes/models/self_training/librispeech/lm/generate_lm_raw_text.py, recipes/models/self_training/librispeech/lm/sentence_ify.py, TranscriptPrediction, generate_wp_selling, get_am_bookids, remove_am_books_from_lm, load_lm_books, lexicon2, lexicon1, distributed_decoding, listpath, inputhyp\n## tags\n[speech recognition]\n\n## repository\nHitsaa/Practice\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## 3 tags\n[vector machines, convnets, deployment]\n\n## repository\nranker.', 'raw_reference_text': 'data augmentation', 'reference_text': 'data augmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.862718939781189, 'bertscore_recall': 0.8845905065536499, 'bertscore_f1': 0.8735178112983704, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2970585823059082}, {'repo': 'wendazhou/nnet-compression-generalization', 'tasks_x': 'network compression, ', 'true_tasks': ['generalization bounds'], 'generated_text': 'action recognition', 'prompt_info': {'content': 'nnet/compression/bounds.py, nnet/compression/coding.py, nnet/compression/mnist.py, nnet/compression/weights_noise.py, nnet/compression/_arithmetic_coding.py, _compress_default, _decompress_default, decompress_variable, compress_checkpoint, _is_pointwise_variable, scale_dense, scale_depthwise, scale_pointwise, scale_prior, scale_posterior', 'generated_text': '\n## repository\nvivoutlaw/tcbp\n## files\nTCBP.py, video_ordering_dataset/Audio_Visual_Place_Temporal.py, video_ordering_dataset/lsmdc_glove_300D_text_feats.py, generate_start_end_index, generic_layer_feat_extractor, load_pretrained_soundnet8, load_audio_librosa, load_audio_all_methods, SoundNet8, load_lua, get_duration, InceptionI3d, register_forward_hook\n## tags\n[action recognition]\n\n## repository\ndatamllab/pyodds\n## files\npyodds/algo/algorithm_utils.py, pyodds/algo/autoencoder.py, pyodds/algo/base.py, pyodds/algo/cblof.py, pyodds/algo/dagmm.py, construct_search_space, test_io_static, test_io_time_serie, deepBase, query_demo_data, visualize_distribution_static, visualize_outlierscore, visualize_distribution_time_serie, IFOREST, luminolDet\n## tags\n[outlier detection]\n\n## repository\nwendazhou/nnet-compression-generalization\n## files\nnnet/compression/bounds.py, nnet/compression/coding.py, nnet/compression/mnist.py, nnet/compression/weights_noise.py, nnet/compression/_arithmetic_coding.py, _compress_default, _decompress_default, decompress_variable, compress_checkpoint, _is_pointwise_variable, scale_dense, scale_depthwise, scale_pointwise, scale_prior, scale_posterior\n## 3 tags\n[network compression]\n\n## repository\nSmilingYan/DeepCosine', 'id': '65e762c6228052b9eb6fdf0163e6b6d8', 'name': 'wendazhou/nnet-compression-generalization', 'tasks': 'network compression, ', 'true_text': 'generalization bounds'}, 'generation': 0, 'input_text': '\n## repository\nvivoutlaw/tcbp\n## files\nTCBP.py, video_ordering_dataset/Audio_Visual_Place_Temporal.py, video_ordering_dataset/lsmdc_glove_300D_text_feats.py, generate_start_end_index, generic_layer_feat_extractor, load_pretrained_soundnet8, load_audio_librosa, load_audio_all_methods, SoundNet8, load_lua, get_duration, InceptionI3d, register_forward_hook\n## tags\n[action recognition]\n\n## repository\ndatamllab/pyodds\n## files\npyodds/algo/algorithm_utils.py, pyodds/algo/autoencoder.py, pyodds/algo/base.py, pyodds/algo/cblof.py, pyodds/algo/dagmm.py, construct_search_space, test_io_static, test_io_time_serie, deepBase, query_demo_data, visualize_distribution_static, visualize_outlierscore, visualize_distribution_time_serie, IFOREST, luminolDet\n## tags\n[outlier detection]\n\n## repository\nwendazhou/nnet-compression-generalization\n## files\nnnet/compression/bounds.py, nnet/compression/coding.py, nnet/compression/mnist.py, nnet/compression/weights_noise.py, nnet/compression/_arithmetic_coding.py, _compress_default, _decompress_default, decompress_variable, compress_checkpoint, _is_pointwise_variable, scale_dense, scale_depthwise, scale_pointwise, scale_prior, scale_posterior\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/non-vacuous-generalization-bounds-at-the']", 'paper_titles': "['Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach']", 'arxiv_ids': "['1804.05862']", 'authors': '["[\'Wenda Zhou\', \'Victor Veitch\', \'Morgane Austern\', \'Ryan P. Adams\', \'Peter Orbanz\']"]', 'raw_generated_text': '\n## repository\nvivoutlaw/tcbp\n## files\nTCBP.py, video_ordering_dataset/Audio_Visual_Place_Temporal.py, video_ordering_dataset/lsmdc_glove_300D_text_feats.py, generate_start_end_index, generic_layer_feat_extractor, load_pretrained_soundnet8, load_audio_librosa, load_audio_all_methods, SoundNet8, load_lua, get_duration, InceptionI3d, register_forward_hook\n## tags\n[action recognition]\n\n## repository\ndatamllab/pyodds\n## files\npyodds/algo/algorithm_utils.py, pyodds/algo/autoencoder.py, pyodds/algo/base.py, pyodds/algo/cblof.py, pyodds/algo/dagmm.py, construct_search_space, test_io_static, test_io_time_serie, deepBase, query_demo_data, visualize_distribution_static, visualize_outlierscore, visualize_distribution_time_serie, IFOREST, luminolDet\n## tags\n[outlier detection]\n\n## repository\nwendazhou/nnet-compression-generalization\n## files\nnnet/compression/bounds.py, nnet/compression/coding.py, nnet/compression/mnist.py, nnet/compression/weights_noise.py, nnet/compression/_arithmetic_coding.py, _compress_default, _decompress_default, decompress_variable, compress_checkpoint, _is_pointwise_variable, scale_dense, scale_depthwise, scale_pointwise, scale_prior, scale_posterior\n## 3 tags\n[network compression]\n\n## repository\nSmilingYan/DeepCosine', 'raw_reference_text': 'generalization bounds', 'reference_text': 'generalization bounds', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8677192330360413, 'bertscore_recall': 0.8546034693717957, 'bertscore_f1': 0.8611114025115967, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.22154103219509125}, {'repo': 'Hadisalman/smoothing-adversarial', 'tasks_x': 'Other, text generation, machine learning, ', 'true_tasks': ['adversarial defense', 'adversarial attack'], 'generated_text': 'text generation', 'prompt_info': {'content': 'code/train_pgd.py, code/generate_github_result.py, code/zipdata.py, code/attacks.py, code/certify.py, _imagenet_on_philly, TiTop50KDataset, MultiDatasetsDataLoader, _imagenet32, InputCenterLayer, noise_sd, init_logfile, get_normalize_layer, get_architecture, outdir', 'generated_text': '\n## repository\nNingMiao/CGMH\n## files\ncorrection/config.py, correction/correction.py, correction/reader.py, correction/utils.py, key_gen/key_gen.py, sen2sta_vec, generate_change_candidate, cut_from_point, cut_from_point, cut_from_point, _output_prob, use_log_path, backward_log_path, forward_log_path, use_output_path\n## tags\n[text generation]\n\n## repository\npiazzesiNiccolo/myLbc\n## files\nbird_view/models/attack.py, bird_view/augmenter.py, bird_view/models/agent.py, bird_view/models/baseline.py, bird_view/models/birdview.py, load_bim, load_hopskip, load_newton, load_spatial, load_image_model, get_highest_k, init_new_weights, DoubleTable, _img_size, _img_size\n## tags\n[autonomous driving]\n\n## repository\nHadisalman/smoothing-adversarial\n## files\ncode/train_pgd.py, code/generate_github_result.py, code/zipdata.py, code/attacks.py, code/certify.py, _imagenet_on_philly, TiTop50KDataset, MultiDatasetsDataLoader, _imagenet32, InputCenterLayer, noise_sd, init_logfile, get_normalize_layer, get_architecture, outdir\n## 3 tags\n[Other]\n[text generation]\n[machine learning]\n\n## repository', 'id': '24a3b81abfc757ee1d519f37c758703d', 'name': 'Hadisalman/smoothing-adversarial', 'tasks': 'Other, text generation, machine learning, ', 'true_text': 'adversarial defense, adversarial attack'}, 'generation': 0, 'input_text': '\n## repository\nNingMiao/CGMH\n## files\ncorrection/config.py, correction/correction.py, correction/reader.py, correction/utils.py, key_gen/key_gen.py, sen2sta_vec, generate_change_candidate, cut_from_point, cut_from_point, cut_from_point, _output_prob, use_log_path, backward_log_path, forward_log_path, use_output_path\n## tags\n[text generation]\n\n## repository\npiazzesiNiccolo/myLbc\n## files\nbird_view/models/attack.py, bird_view/augmenter.py, bird_view/models/agent.py, bird_view/models/baseline.py, bird_view/models/birdview.py, load_bim, load_hopskip, load_newton, load_spatial, load_image_model, get_highest_k, init_new_weights, DoubleTable, _img_size, _img_size\n## tags\n[autonomous driving]\n\n## repository\nHadisalman/smoothing-adversarial\n## files\ncode/train_pgd.py, code/generate_github_result.py, code/zipdata.py, code/attacks.py, code/certify.py, _imagenet_on_philly, TiTop50KDataset, MultiDatasetsDataLoader, _imagenet32, InputCenterLayer, noise_sd, init_logfile, get_normalize_layer, get_architecture, outdir\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/provably-robust-deep-learning-via']", 'paper_titles': "['Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers']", 'arxiv_ids': "['1906.04584']", 'authors': '["[\'Hadi Salman\', \'Greg Yang\', \'Jerry Li\', \'Pengchuan Zhang\', \'huan zhang\', \'Ilya Razenshteyn\', \'Sebastien Bubeck\']"]', 'raw_generated_text': '\n## repository\nNingMiao/CGMH\n## files\ncorrection/config.py, correction/correction.py, correction/reader.py, correction/utils.py, key_gen/key_gen.py, sen2sta_vec, generate_change_candidate, cut_from_point, cut_from_point, cut_from_point, _output_prob, use_log_path, backward_log_path, forward_log_path, use_output_path\n## tags\n[text generation]\n\n## repository\npiazzesiNiccolo/myLbc\n## files\nbird_view/models/attack.py, bird_view/augmenter.py, bird_view/models/agent.py, bird_view/models/baseline.py, bird_view/models/birdview.py, load_bim, load_hopskip, load_newton, load_spatial, load_image_model, get_highest_k, init_new_weights, DoubleTable, _img_size, _img_size\n## tags\n[autonomous driving]\n\n## repository\nHadisalman/smoothing-adversarial\n## files\ncode/train_pgd.py, code/generate_github_result.py, code/zipdata.py, code/attacks.py, code/certify.py, _imagenet_on_philly, TiTop50KDataset, MultiDatasetsDataLoader, _imagenet32, InputCenterLayer, noise_sd, init_logfile, get_normalize_layer, get_architecture, outdir\n## 3 tags\n[Other]\n[text generation]\n[machine learning]\n\n## repository', 'raw_reference_text': 'adversarial defense, adversarial attack', 'reference_text': 'adversarial defense adversarial attack', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.854667067527771, 'bertscore_recall': 0.7969131469726562, 'bertscore_f1': 0.824780285358429, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.010831729508936405}, {'repo': 'larsbratholm/champs_kaggle', 'tasks_x': 'kaggle, ', 'true_tasks': ['molecular property prediction'], 'generated_text': 'gaussian processes', 'prompt_info': {'content': 'analysis/blend.py, analysis/make_auxiliary_plots.py, analysis/make_submission_plots.py, analysis/preprocess.py, dataset_generation/convert_exyz.py, add_extra_mol_features, make_teams_vs_prize_plot, make_progression_plots, make_ensemble_plots, _create_embedding, schduler, global_cg_dict, num_top_levels, wsteps, structure_file', 'generated_text': '\n## repository\nciarapb/recovering_bandits\n## files\nbasic_alg.py, rgpts_ds_opplan_l4.py, rgpts_manyl.py, rgpucb_manyl.py, rogueucbtuned.py, RGPUCBds, RGPTSds, RGPTSop, RGPTS, RGPUCB, indicies, posterior_samples_f, GetUCBSeq, GetUCBSeqRep, GetTSSeq\n## tags\n[gaussian processes]\n\n## repository\ngpauloski/kfac_pytorch\n## files\nexamples/horovod_cifar10_resnet.py, examples/horovod_imagenet_resnet.py, examples/torch_cifar10_resnet.py, examples/torch_imagenet_resnet.py, examples/torch_language_model.py, load_balance, WorkerAllocator, partition_inv_ranks, partition_grad_ranks, append_bias_ones, resume_from_epoch, COMM_OPT, HYBRID_OPT, MEM_OPT, wikitext_2_dataset\n## tags\n[stochastic optimization]\n\n## repository\nlarsbratholm/champs_kaggle\n## files\nanalysis/blend.py, analysis/make_auxiliary_plots.py, analysis/make_submission_plots.py, analysis/preprocess.py, dataset_generation/convert_exyz.py, add_extra_mol_features, make_teams_vs_prize_plot, make_progression_plots, make_ensemble_plots, _create_embedding, schduler, global_cg_dict, num_top_levels, wsteps, structure_file\n## 3 tags\n[kaggle]\n\n## repository\nlmartins/deepchem_tutorial', 'id': '0964765a4537d9a76624eb001a7e8d48', 'name': 'larsbratholm/champs_kaggle', 'tasks': 'kaggle, ', 'true_text': 'molecular property prediction'}, 'generation': 0, 'input_text': '\n## repository\nciarapb/recovering_bandits\n## files\nbasic_alg.py, rgpts_ds_opplan_l4.py, rgpts_manyl.py, rgpucb_manyl.py, rogueucbtuned.py, RGPUCBds, RGPTSds, RGPTSop, RGPTS, RGPUCB, indicies, posterior_samples_f, GetUCBSeq, GetUCBSeqRep, GetTSSeq\n## tags\n[gaussian processes]\n\n## repository\ngpauloski/kfac_pytorch\n## files\nexamples/horovod_cifar10_resnet.py, examples/horovod_imagenet_resnet.py, examples/torch_cifar10_resnet.py, examples/torch_imagenet_resnet.py, examples/torch_language_model.py, load_balance, WorkerAllocator, partition_inv_ranks, partition_grad_ranks, append_bias_ones, resume_from_epoch, COMM_OPT, HYBRID_OPT, MEM_OPT, wikitext_2_dataset\n## tags\n[stochastic optimization]\n\n## repository\nlarsbratholm/champs_kaggle\n## files\nanalysis/blend.py, analysis/make_auxiliary_plots.py, analysis/make_submission_plots.py, analysis/preprocess.py, dataset_generation/convert_exyz.py, add_extra_mol_features, make_teams_vs_prize_plot, make_progression_plots, make_ensemble_plots, _create_embedding, schduler, global_cg_dict, num_top_levels, wsteps, structure_file\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-community-powered-search-of-machine']", 'paper_titles': "['A community-powered search of machine learning strategy space to find NMR property prediction models']", 'arxiv_ids': "['2008.05994']", 'authors': '["[\'Lars A. Bratholm\', \'Will Gerrard\', \'Brandon Anderson\', \'Shaojie Bai\', \'Sunghwan Choi\', \'Lam Dang\', \'Pavel Hanchar\', \'Addison Howard\', \'Guillaume Huard\', \'Sanghoon Kim\', \'Zico Kolter\', \'Risi Kondor\', \'Mordechai Kornbluth\', \'Youhan Lee\', \'Youngsoo Lee\', \'Jonathan P. Mailoa\', \'Thanh Tu Nguyen\', \'Milos Popovic\', \'Goran Rakocevic\', \'Walter Reade\', \'Wonho Song\', \'Luka Stojanovic\', \'Erik H. Thiede\', \'Nebojsa Tijanic\', \'Andres Torrubia\', \'Devin Willmott\', \'Craig P. Butts\', \'David R. Glowacki\', \'Kaggle participants\']"]', 'raw_generated_text': '\n## repository\nciarapb/recovering_bandits\n## files\nbasic_alg.py, rgpts_ds_opplan_l4.py, rgpts_manyl.py, rgpucb_manyl.py, rogueucbtuned.py, RGPUCBds, RGPTSds, RGPTSop, RGPTS, RGPUCB, indicies, posterior_samples_f, GetUCBSeq, GetUCBSeqRep, GetTSSeq\n## tags\n[gaussian processes]\n\n## repository\ngpauloski/kfac_pytorch\n## files\nexamples/horovod_cifar10_resnet.py, examples/horovod_imagenet_resnet.py, examples/torch_cifar10_resnet.py, examples/torch_imagenet_resnet.py, examples/torch_language_model.py, load_balance, WorkerAllocator, partition_inv_ranks, partition_grad_ranks, append_bias_ones, resume_from_epoch, COMM_OPT, HYBRID_OPT, MEM_OPT, wikitext_2_dataset\n## tags\n[stochastic optimization]\n\n## repository\nlarsbratholm/champs_kaggle\n## files\nanalysis/blend.py, analysis/make_auxiliary_plots.py, analysis/make_submission_plots.py, analysis/preprocess.py, dataset_generation/convert_exyz.py, add_extra_mol_features, make_teams_vs_prize_plot, make_progression_plots, make_ensemble_plots, _create_embedding, schduler, global_cg_dict, num_top_levels, wsteps, structure_file\n## 3 tags\n[kaggle]\n\n## repository\nlmartins/deepchem_tutorial', 'raw_reference_text': 'molecular property prediction', 'reference_text': 'molecular property prediction', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8193141222000122, 'bertscore_recall': 0.8605149984359741, 'bertscore_f1': 0.8394092917442322, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.15033388137817383}, {'repo': 'gump88/SAN-CWS', 'tasks_x': 'sentiment analysis, natural language processing, ', 'true_tasks': ['chinese word segmentation'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'model/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip', 'generated_text': '\n## repository\nskiler07/mst-tf\n## files\ntrainer/MST.py, trainer/train.py, trainer/utils.py, setup.py, get_Fcs, load_weights_from_gcs, train_mst, restore_original_image, copy_file_to_gcs, restore_original_image, copy_file_to_gcs, save_decoder_weights, plot_test_images, get_content_feature_map\n## tags\n[style transfer]\n\n## repository\nfyumoto/EIF\n## files\neif.py, version.py, setup.py, all_branches, iTree, c_factor, iForest, PathFactor, right, left, append\n## tags\n[anomaly detection]\n\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## 3 tags\n[sentiment analysis] [natural language processing]\n\n## repository\nBoulderAI', 'id': '5d36dac841c5748a2add7359f94ba674', 'name': 'gump88/SAN-CWS', 'tasks': 'sentiment analysis, natural language processing, ', 'true_text': 'chinese word segmentation'}, 'generation': 0, 'input_text': '\n## repository\nskiler07/mst-tf\n## files\ntrainer/MST.py, trainer/train.py, trainer/utils.py, setup.py, get_Fcs, load_weights_from_gcs, train_mst, restore_original_image, copy_file_to_gcs, restore_original_image, copy_file_to_gcs, save_decoder_weights, plot_test_images, get_content_feature_map\n## tags\n[style transfer]\n\n## repository\nfyumoto/EIF\n## files\neif.py, version.py, setup.py, all_branches, iTree, c_factor, iForest, PathFactor, right, left, append\n## tags\n[anomaly detection]\n\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/investigating-self-attention-network-for']", 'paper_titles': "['Investigating Self-Attention Network for Chinese Word Segmentation']", 'arxiv_ids': "['1907.11512']", 'authors': '["[\'Leilei Gan\', \'Yue Zhang\']"]', 'raw_generated_text': '\n## repository\nskiler07/mst-tf\n## files\ntrainer/MST.py, trainer/train.py, trainer/utils.py, setup.py, get_Fcs, load_weights_from_gcs, train_mst, restore_original_image, copy_file_to_gcs, restore_original_image, copy_file_to_gcs, save_decoder_weights, plot_test_images, get_content_feature_map\n## tags\n[style transfer]\n\n## repository\nfyumoto/EIF\n## files\neif.py, version.py, setup.py, all_branches, iTree, c_factor, iForest, PathFactor, right, left, append\n## tags\n[anomaly detection]\n\n## repository\ngump88/SAN-CWS\n## files\nmodel/cws.py, model/sequence.py, model/word_attention.py, model/optim.py, model/cnn.py, load_tencent_dic, load_pos_to_idx, load_external_pos, load_token_pos_prob, FNNLayer, use_warmup_adam, use_adadelta, word_alphabet_size, biword_alphabet_size, HP_clip\n## 3 tags\n[sentiment analysis] [natural language processing]\n\n## repository\nBoulderAI', 'raw_reference_text': 'chinese word segmentation', 'reference_text': 'chinese word segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.866952121257782, 'bertscore_recall': 0.8229269981384277, 'bertscore_f1': 0.8443660736083984, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.0385374017059803}, {'repo': 'rajeevyasarla/UMRL--using-Cycle-Spinning', 'tasks_x': 'training, ', 'true_tasks': ['single image deraining'], 'generated_text': 'session based recommendations', 'prompt_info': {'content': 'umrl_cycspn_test.py, umrl_cycspn_train.py, umrl_test.py, umrl_train.py, models/derain_mulcmp.py, Multi_scale2, UMRL, scale_residue_conf, scale_residue_est, imageblocks, slct, commonDataset, tensortype, tensortype, load_lua', 'generated_text': '\n## repository\nkunwuz/DGTN\n## files\ndata/multi_sess_graph.py, model/InOutGat.py, model/multi_sess.py, model/srgnn.py, neigh_retrieval/knn.py, ItemFusing, Embedding2Score, CNNFusing, InOutGATConv_intra, InOutGATConv, MultiSessionsGraph, filter_test, split_train, process_seqs, get_sequence\n## tags\n[session based recommendations]\n\n## repository\nAdelaide-AI-Group/MCVL\n## files\nlibs/vlfeat-0.9.21/docsrc/doxytag.py, libs/vlfeat-0.9.21/docsrc/formatter.py, libs/vlfeat-0.9.21/docsrc/mdoc.py, libs/vlfeat-0.9.21/docsrc/webdoc.py, libs/vlfeat-0.9.21/docsrc/wikidoc.py, xscan, breadCrumb, towiki, depth_first, MFile, addMFile, addChildNode, inner_content, bullet, MFile\n## tags\n[visual place recognition, visual localization]\n\n## repository\nrajeevyasarla/UMRL--using-Cycle-Spinning\n## files\numrl_cycspn_test.py, umrl_cycspn_train.py, umrl_test.py, umrl_train.py, models/derain_mulcmp.py, Multi_scale2, UMRL, scale_residue_conf, scale_residue_est, imageblocks, slct, commonDataset, tensortype, tensortype, load_lua\n## 3 tags\n[training]\n\n## repository\nawreyes/iclr202', 'id': '709c77819df1e0565d00b30d1d4a6fab', 'name': 'rajeevyasarla/UMRL--using-Cycle-Spinning', 'tasks': 'training, ', 'true_text': 'single image deraining'}, 'generation': 0, 'input_text': '\n## repository\nkunwuz/DGTN\n## files\ndata/multi_sess_graph.py, model/InOutGat.py, model/multi_sess.py, model/srgnn.py, neigh_retrieval/knn.py, ItemFusing, Embedding2Score, CNNFusing, InOutGATConv_intra, InOutGATConv, MultiSessionsGraph, filter_test, split_train, process_seqs, get_sequence\n## tags\n[session based recommendations]\n\n## repository\nAdelaide-AI-Group/MCVL\n## files\nlibs/vlfeat-0.9.21/docsrc/doxytag.py, libs/vlfeat-0.9.21/docsrc/formatter.py, libs/vlfeat-0.9.21/docsrc/mdoc.py, libs/vlfeat-0.9.21/docsrc/webdoc.py, libs/vlfeat-0.9.21/docsrc/wikidoc.py, xscan, breadCrumb, towiki, depth_first, MFile, addMFile, addChildNode, inner_content, bullet, MFile\n## tags\n[visual place recognition, visual localization]\n\n## repository\nrajeevyasarla/UMRL--using-Cycle-Spinning\n## files\numrl_cycspn_test.py, umrl_cycspn_train.py, umrl_test.py, umrl_train.py, models/derain_mulcmp.py, Multi_scale2, UMRL, scale_residue_conf, scale_residue_est, imageblocks, slct, commonDataset, tensortype, tensortype, load_lua\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/uncertainty-guided-multi-scale-residual-1']", 'paper_titles': "['Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining']", 'arxiv_ids': "['1906.11129']", 'authors': '["[\'Rajeev Yasarla\', \'Vishal M. Patel\']"]', 'raw_generated_text': '\n## repository\nkunwuz/DGTN\n## files\ndata/multi_sess_graph.py, model/InOutGat.py, model/multi_sess.py, model/srgnn.py, neigh_retrieval/knn.py, ItemFusing, Embedding2Score, CNNFusing, InOutGATConv_intra, InOutGATConv, MultiSessionsGraph, filter_test, split_train, process_seqs, get_sequence\n## tags\n[session based recommendations]\n\n## repository\nAdelaide-AI-Group/MCVL\n## files\nlibs/vlfeat-0.9.21/docsrc/doxytag.py, libs/vlfeat-0.9.21/docsrc/formatter.py, libs/vlfeat-0.9.21/docsrc/mdoc.py, libs/vlfeat-0.9.21/docsrc/webdoc.py, libs/vlfeat-0.9.21/docsrc/wikidoc.py, xscan, breadCrumb, towiki, depth_first, MFile, addMFile, addChildNode, inner_content, bullet, MFile\n## tags\n[visual place recognition, visual localization]\n\n## repository\nrajeevyasarla/UMRL--using-Cycle-Spinning\n## files\numrl_cycspn_test.py, umrl_cycspn_train.py, umrl_test.py, umrl_train.py, models/derain_mulcmp.py, Multi_scale2, UMRL, scale_residue_conf, scale_residue_est, imageblocks, slct, commonDataset, tensortype, tensortype, load_lua\n## 3 tags\n[training]\n\n## repository\nawreyes/iclr202', 'raw_reference_text': 'single image deraining', 'reference_text': 'single image deraining', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8203203678131104, 'bertscore_recall': 0.7878900170326233, 'bertscore_f1': 0.8037782907485962, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1387699544429779}, {'repo': 'yiskw713/boundary_loss_for_remote_sensing', 'tasks_x': 'loss function, pytorch, segmentation, ', 'true_tasks': ['boundary detection', 'semantic segmentation'], 'generated_text': 'multiple instance learning', 'prompt_info': {'content': 'boundary_loss.py, BoundaryLoss, one_hot, device, transpose', 'generated_text': '\n## repository\ndurandtibo/weldon.resnet.pytorch\n## files\nweldon/weldon.py, weldon/weldon_resnet.py, weldon/__init__.py, resnet18_weldon, resnet34_weldon, resnet101_weldon, resnet152_weldon, resnet50_weldon, resnet50_weldon, WeldonPool2d, WeldonPool2d, WeldonPool2d, WeldonPool2d\n## tags\n[multiple instance learning]\n\n## repository\nNon-biri/ml-agents-0.9.3\n## files\nml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/extrinsic/__init__.py, test_simple_rl, test_add_rewards_output, test_bc_trainer_step, test_bc_trainer_add_proc_experiences, test_bc_trainer_end_episode, layer_ranks, model_tensors, map_ignored_layer_to_its_input, construct_curr_info, previous_all_brain_info\n## tags\n[unity]\n\n## repository\nyiskw713/boundary_loss_for_remote_sensing\n## files\nboundary_loss.py, BoundaryLoss, one_hot, device, transpose\n## 3 tags\n[loss function], [pytorch], [segmentation]\n\n## repository\nKwent', 'id': '275adb0b7b1d02b10121c55fbeb70b03', 'name': 'yiskw713/boundary_loss_for_remote_sensing', 'tasks': 'loss function, pytorch, segmentation, ', 'true_text': 'boundary detection, semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\ndurandtibo/weldon.resnet.pytorch\n## files\nweldon/weldon.py, weldon/weldon_resnet.py, weldon/__init__.py, resnet18_weldon, resnet34_weldon, resnet101_weldon, resnet152_weldon, resnet50_weldon, resnet50_weldon, WeldonPool2d, WeldonPool2d, WeldonPool2d, WeldonPool2d\n## tags\n[multiple instance learning]\n\n## repository\nNon-biri/ml-agents-0.9.3\n## files\nml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/extrinsic/__init__.py, test_simple_rl, test_add_rewards_output, test_bc_trainer_step, test_bc_trainer_add_proc_experiences, test_bc_trainer_end_episode, layer_ranks, model_tensors, map_ignored_layer_to_its_input, construct_curr_info, previous_all_brain_info\n## tags\n[unity]\n\n## repository\nyiskw713/boundary_loss_for_remote_sensing\n## files\nboundary_loss.py, BoundaryLoss, one_hot, device, transpose\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/boundary-loss-for-remote-sensing-imagery']", 'paper_titles': "['Boundary Loss for Remote Sensing Imagery Semantic Segmentation']", 'arxiv_ids': "['1905.07852']", 'authors': '["[\'Alexey Bokhovkin\', \'Evgeny Burnaev\']"]', 'raw_generated_text': '\n## repository\ndurandtibo/weldon.resnet.pytorch\n## files\nweldon/weldon.py, weldon/weldon_resnet.py, weldon/__init__.py, resnet18_weldon, resnet34_weldon, resnet101_weldon, resnet152_weldon, resnet50_weldon, resnet50_weldon, WeldonPool2d, WeldonPool2d, WeldonPool2d, WeldonPool2d\n## tags\n[multiple instance learning]\n\n## repository\nNon-biri/ml-agents-0.9.3\n## files\nml-agents/mlagents/trainers/components/reward_signals/reward_signal.py, ml-agents/mlagents/trainers/components/bc/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/gail/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/curiosity/__init__.py, ml-agents/mlagents/trainers/components/reward_signals/extrinsic/__init__.py, test_simple_rl, test_add_rewards_output, test_bc_trainer_step, test_bc_trainer_add_proc_experiences, test_bc_trainer_end_episode, layer_ranks, model_tensors, map_ignored_layer_to_its_input, construct_curr_info, previous_all_brain_info\n## tags\n[unity]\n\n## repository\nyiskw713/boundary_loss_for_remote_sensing\n## files\nboundary_loss.py, BoundaryLoss, one_hot, device, transpose\n## 3 tags\n[loss function], [pytorch], [segmentation]\n\n## repository\nKwent', 'raw_reference_text': 'boundary detection, semantic segmentation', 'reference_text': 'semantic segmentation boundary detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8833088278770447, 'bertscore_recall': 0.8428836464881897, 'bertscore_f1': 0.862622857093811, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.10563808679580688}, {'repo': 'ctr4si/A-Hierarchical-Latent-Structure-for-Variational-Conversation-Modeling', 'tasks_x': 'language models, word embeddings, ', 'true_tasks': ['response generation'], 'generated_text': 'sequential image classification time series', 'prompt_info': {'content': 'cornell_preprocess.py, ubuntu_preprocess.py, model/eval_embed.py, model/solver.py, model/layers/beam_search.py, prepare_ubuntu_data, prepare_ubuntu_data, prepare_ubuntu_data, get_dialog_path_list, get_dialog_path_list, pad_conversation, DialogDataset, to_var, to_var, to_var', 'generated_text': '\n## repository\nmlech26l/learning-long-term-irregular-ts\n## files\nirregular_sampled_datasets.py, node_cell.py, pt_trainer.py, torch_node_cell.py, xor_task.py, CTRNNCell, GRUODE, HawkLSTMCell, Walker2dImitationData, IrregularSequenceLearner, train_elapsed, train_events, test_elapsed, train_t, ETSMnistData\n## tags\n[time series, sequential image classification]\n\n## repository\ncasperhansen/CPTW\n## files\nCPTW.py, loadDataWithPreDefTest, constructAndSave, computeBOW, testFunctions, constructCorpus, makeBOW, computeW2W, loadDataWithTest, allmeasures, orderData\n## tags\n[word embeddings]\n\n## repository\nctr4si/A-Hierarchical-Latent-Structure-for-Variational-Conversation-Modeling\n## files\ncornell_preprocess.py, ubuntu_preprocess.py, model/eval_embed.py, model/solver.py, model/layers/beam_search.py, prepare_ubuntu_data, prepare_ubuntu_data, prepare_ubuntu_data, get_dialog_path_list, get_dialog_path_list, pad_conversation, DialogDataset, to_var, to_var, to_var\n## 3 tags\n[language models, word embeddings]\n\n## repository\ndavek44', 'id': 'bd06cf7ac0c098019421354452f623b4', 'name': 'ctr4si/A-Hierarchical-Latent-Structure-for-Variational-Conversation-Modeling', 'tasks': 'language models, word embeddings, ', 'true_text': 'response generation'}, 'generation': 0, 'input_text': '\n## repository\nmlech26l/learning-long-term-irregular-ts\n## files\nirregular_sampled_datasets.py, node_cell.py, pt_trainer.py, torch_node_cell.py, xor_task.py, CTRNNCell, GRUODE, HawkLSTMCell, Walker2dImitationData, IrregularSequenceLearner, train_elapsed, train_events, test_elapsed, train_t, ETSMnistData\n## tags\n[time series, sequential image classification]\n\n## repository\ncasperhansen/CPTW\n## files\nCPTW.py, loadDataWithPreDefTest, constructAndSave, computeBOW, testFunctions, constructCorpus, makeBOW, computeW2W, loadDataWithTest, allmeasures, orderData\n## tags\n[word embeddings]\n\n## repository\nctr4si/A-Hierarchical-Latent-Structure-for-Variational-Conversation-Modeling\n## files\ncornell_preprocess.py, ubuntu_preprocess.py, model/eval_embed.py, model/solver.py, model/layers/beam_search.py, prepare_ubuntu_data, prepare_ubuntu_data, prepare_ubuntu_data, get_dialog_path_list, get_dialog_path_list, pad_conversation, DialogDataset, to_var, to_var, to_var\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-hierarchical-latent-variable-encoder']", 'paper_titles': "['A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues']", 'arxiv_ids': "['1605.06069']", 'authors': '["[\'Iulian Vlad Serban\', \'Alessandro Sordoni\', \'Ryan Lowe\', \'Laurent Charlin\', \'Joelle Pineau\', \'Aaron Courville\', \'Yoshua Bengio\']"]', 'raw_generated_text': '\n## repository\nmlech26l/learning-long-term-irregular-ts\n## files\nirregular_sampled_datasets.py, node_cell.py, pt_trainer.py, torch_node_cell.py, xor_task.py, CTRNNCell, GRUODE, HawkLSTMCell, Walker2dImitationData, IrregularSequenceLearner, train_elapsed, train_events, test_elapsed, train_t, ETSMnistData\n## tags\n[time series, sequential image classification]\n\n## repository\ncasperhansen/CPTW\n## files\nCPTW.py, loadDataWithPreDefTest, constructAndSave, computeBOW, testFunctions, constructCorpus, makeBOW, computeW2W, loadDataWithTest, allmeasures, orderData\n## tags\n[word embeddings]\n\n## repository\nctr4si/A-Hierarchical-Latent-Structure-for-Variational-Conversation-Modeling\n## files\ncornell_preprocess.py, ubuntu_preprocess.py, model/eval_embed.py, model/solver.py, model/layers/beam_search.py, prepare_ubuntu_data, prepare_ubuntu_data, prepare_ubuntu_data, get_dialog_path_list, get_dialog_path_list, pad_conversation, DialogDataset, to_var, to_var, to_var\n## 3 tags\n[language models, word embeddings]\n\n## repository\ndavek44', 'raw_reference_text': 'response generation', 'reference_text': 'response generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8376193642616272, 'bertscore_recall': 0.8616762161254883, 'bertscore_f1': 0.8494775295257568, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1910526156425476}, {'repo': 'KnollFrank/automl_zero', 'tasks_x': 'classification, ', 'true_tasks': ['automl'], 'generated_text': 'network pruning', 'prompt_info': {'content': 'task_pb2.py, generate_datasets.py, load_projected_binary_dataset, create_projected_binary_dataset, train_valid_test_split, get_dataset, main, max_data_seed, min_data_seed, num_test_examples, projected_dim, ScalarLabelDataset', 'generated_text': '\n## repository\nkenichdietrich/LotteryTicketHypothesis\n## files\nlib/model_base.py, lib/pruning.py, lib/layers.py, LotteryConv2D, LotteryLinear, iteration_pruning, random_reset_weights, jitter_reset, k_init, pruning_criterion, pruning_criterion, quantile, quantile\n## tags\n[network pruning]\n\n## repository\nRanlot/single-parameter-fit\n## files\nhelperFunctions.py, findInitialCondition, decimalToBinary, dyadicDecoder, logisticDecoder, binaryReducer, binaryToDecFaster, mpf, phi, partial, map\n## tags\n[time series]\n\n## repository\nKnollFrank/automl_zero\n## files\ntask_pb2.py, generate_datasets.py, load_projected_binary_dataset, create_projected_binary_dataset, train_valid_test_split, get_dataset, main, max_data_seed, min_data_seed, num_test_examples, projected_dim, ScalarLabelDataset\n## 3 tags\n[classification]\n\n## repository\nreplicate-research-experiment/sli', 'id': '4cfcad8f2034ccaa9d6cef6724ca55c7', 'name': 'KnollFrank/automl_zero', 'tasks': 'classification, ', 'true_text': 'automl'}, 'generation': 0, 'input_text': '\n## repository\nkenichdietrich/LotteryTicketHypothesis\n## files\nlib/model_base.py, lib/pruning.py, lib/layers.py, LotteryConv2D, LotteryLinear, iteration_pruning, random_reset_weights, jitter_reset, k_init, pruning_criterion, pruning_criterion, quantile, quantile\n## tags\n[network pruning]\n\n## repository\nRanlot/single-parameter-fit\n## files\nhelperFunctions.py, findInitialCondition, decimalToBinary, dyadicDecoder, logisticDecoder, binaryReducer, binaryToDecFaster, mpf, phi, partial, map\n## tags\n[time series]\n\n## repository\nKnollFrank/automl_zero\n## files\ntask_pb2.py, generate_datasets.py, load_projected_binary_dataset, create_projected_binary_dataset, train_valid_test_split, get_dataset, main, max_data_seed, min_data_seed, num_test_examples, projected_dim, ScalarLabelDataset\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/automl-zero-evolving-machine-learning']", 'paper_titles': "['AutoML-Zero: Evolving Machine Learning Algorithms From Scratch']", 'arxiv_ids': "['2003.03384']", 'authors': '["[\'Esteban Real\', \'Chen Liang\', \'David R. So\', \'Quoc V. Le\']"]', 'raw_generated_text': '\n## repository\nkenichdietrich/LotteryTicketHypothesis\n## files\nlib/model_base.py, lib/pruning.py, lib/layers.py, LotteryConv2D, LotteryLinear, iteration_pruning, random_reset_weights, jitter_reset, k_init, pruning_criterion, pruning_criterion, quantile, quantile\n## tags\n[network pruning]\n\n## repository\nRanlot/single-parameter-fit\n## files\nhelperFunctions.py, findInitialCondition, decimalToBinary, dyadicDecoder, logisticDecoder, binaryReducer, binaryToDecFaster, mpf, phi, partial, map\n## tags\n[time series]\n\n## repository\nKnollFrank/automl_zero\n## files\ntask_pb2.py, generate_datasets.py, load_projected_binary_dataset, create_projected_binary_dataset, train_valid_test_split, get_dataset, main, max_data_seed, min_data_seed, num_test_examples, projected_dim, ScalarLabelDataset\n## 3 tags\n[classification]\n\n## repository\nreplicate-research-experiment/sli', 'raw_reference_text': 'automl', 'reference_text': 'automl', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8591580390930176, 'bertscore_recall': 0.8297238349914551, 'bertscore_f1': 0.8441844582557678, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.13581699132919312}, {'repo': 'jtiger958/style-transfer-pytorch', 'tasks_x': 'image stylization, python, vision, ', 'true_tasks': ['style transfer'], 'generated_text': 'anomaly detection', 'prompt_info': {'content': 'model/transfer_net.py, utils/download_dataset.py, make_sample.py, dataloader/dataloader.py, model/vgg.py, unzip_tar_file, unzip_zip_file, ResidualNetwork, TransferNet, DownloadProgressBar, style_image_name, unzip_zip_file, sample_batch_size, sample_dir, download_dataset', 'generated_text': '\n## repository\nsharpenb/Uncertainty-Event-Prediction\n## files\nsrc/models/dirichlet.py, src/models/dpp.py, src/models/gp.py, src/models/base.py, src/data.py, split_on_sequence, break_down_sequences, split_on_data, DirichletPointProcess, list_datasets, split_on_sequence, break_down_sequences, split_on_data, pad_sequence, from_tensor_slices\n## tags\n[anomaly detection]\n\n## repository\nmit-acl/clear\n## files\nCLEAR_Python/CLEAR.py, CLEAR_Python/util.py, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd\n## tags\n[graph clustering]\n\n## repository\njtiger958/style-transfer-pytorch\n## files\nmodel/transfer_net.py, utils/download_dataset.py, make_sample.py, dataloader/dataloader.py, model/vgg.py, unzip_tar_file, unzip_zip_file, ResidualNetwork, TransferNet, DownloadProgressBar, style_image_name, unzip_zip_file, sample_batch_size, sample_dir, download_dataset\n## 3 tags\n[image stylization], [python], [vision]\n\n## repository\n', 'id': 'e5451ed2042a8b574abbb830b1056f4c', 'name': 'jtiger958/style-transfer-pytorch', 'tasks': 'image stylization, python, vision, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\nsharpenb/Uncertainty-Event-Prediction\n## files\nsrc/models/dirichlet.py, src/models/dpp.py, src/models/gp.py, src/models/base.py, src/data.py, split_on_sequence, break_down_sequences, split_on_data, DirichletPointProcess, list_datasets, split_on_sequence, break_down_sequences, split_on_data, pad_sequence, from_tensor_slices\n## tags\n[anomaly detection]\n\n## repository\nmit-acl/clear\n## files\nCLEAR_Python/CLEAR.py, CLEAR_Python/util.py, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd\n## tags\n[graph clustering]\n\n## repository\njtiger958/style-transfer-pytorch\n## files\nmodel/transfer_net.py, utils/download_dataset.py, make_sample.py, dataloader/dataloader.py, model/vgg.py, unzip_tar_file, unzip_zip_file, ResidualNetwork, TransferNet, DownloadProgressBar, style_image_name, unzip_zip_file, sample_batch_size, sample_dir, download_dataset\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-neural-algorithm-of-artistic-style']", 'paper_titles': "['A Neural Algorithm of Artistic Style']", 'arxiv_ids': "['1508.06576']", 'authors': '["[\'Leon A. Gatys\', \'Alexander S. Ecker\', \'Matthias Bethge\']"]', 'raw_generated_text': '\n## repository\nsharpenb/Uncertainty-Event-Prediction\n## files\nsrc/models/dirichlet.py, src/models/dpp.py, src/models/gp.py, src/models/base.py, src/data.py, split_on_sequence, break_down_sequences, split_on_data, DirichletPointProcess, list_datasets, split_on_sequence, break_down_sequences, split_on_data, pad_sequence, from_tensor_slices\n## tags\n[anomaly detection]\n\n## repository\nmit-acl/clear\n## files\nCLEAR_Python/CLEAR.py, CLEAR_Python/util.py, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd, P2L, estimate_num_obj, suboptimal_assignment, L2P, block_svd\n## tags\n[graph clustering]\n\n## repository\njtiger958/style-transfer-pytorch\n## files\nmodel/transfer_net.py, utils/download_dataset.py, make_sample.py, dataloader/dataloader.py, model/vgg.py, unzip_tar_file, unzip_zip_file, ResidualNetwork, TransferNet, DownloadProgressBar, style_image_name, unzip_zip_file, sample_batch_size, sample_dir, download_dataset\n## 3 tags\n[image stylization], [python], [vision]\n\n## repository\n', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8665617108345032, 'bertscore_recall': 0.8937678337097168, 'bertscore_f1': 0.8799545168876648, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1248253583908081}, {'repo': 'ddkang/advex-uar', 'tasks_x': 'adversarial robustness, image inversion, ', 'true_tasks': ['adversarial defense'], 'generated_text': 'word embeddings', 'prompt_info': {'content': 'advex_uar/analysis/calibrate_eps.py, advex_uar/analysis/compute_ata.py, advex_uar/analysis/compute_uar.py, advex_uar/attacks/attacks.py, advex_uar/attacks/elastic.py, make_quantization_tables, jpeg_compress_decode, dct_8x8_ref, idct_8x8_ref, norm_to_pil_image, use_max_step, class_downsample_factor, wandb_ckpt_project, wandb_username, wandb_run_id', 'generated_text': '\n## repository\ndavidmatthews1uvm/2019-IROS\n## files\ndemos/word2vecDatabase.py, experiments/job.py, experiments/quadruped.py, experiments/spherebot.py, experiments/twig.py, shuffle_vec, create_new_job, Word2VecVectorSpace, SphereBot, Twig, get_internal_bot, rmtree, randint, makedirs, len\n## tags\n[word embeddings]\n\n## repository\nbel2scm/bel2scm\n## files\nsrc/bel2scm/causal_graph.py, src/bel2scm/gen_test_data.py, src/bel2scm/graph_node.py, src/bel2scm/version.py, src/bel2scm/generation/covid_scm_cf.py, all_parents_visited, get_parent_samples, get_sample_for_continuous_node, get_parent_tensor, get_exogenous_distribution, get_metadata_attribute, get_GEO, download_supplementary_files, parse_mtx, indep_vars\n## tags\n[counterfactual inference]\n\n## repository\nddkang/advex-uar\n## files\nadvex_uar/analysis/calibrate_eps.py, advex_uar/analysis/compute_ata.py, advex_uar/analysis/compute_uar.py, advex_uar/attacks/attacks.py, advex_uar/attacks/elastic.py, make_quantization_tables, jpeg_compress_decode, dct_8x8_ref, idct_8x8_ref, norm_to_pil_image, use_max_step, class_downsample_factor, wandb_ckpt_project, wandb_username, wandb_run_id\n## 3 tags\n[adversarial robustness]\n[image inversion]\n', 'id': '03fc6af10b6c119b6a38bcc9ea22a60a', 'name': 'ddkang/advex-uar', 'tasks': 'adversarial robustness, image inversion, ', 'true_text': 'adversarial defense'}, 'generation': 0, 'input_text': '\n## repository\ndavidmatthews1uvm/2019-IROS\n## files\ndemos/word2vecDatabase.py, experiments/job.py, experiments/quadruped.py, experiments/spherebot.py, experiments/twig.py, shuffle_vec, create_new_job, Word2VecVectorSpace, SphereBot, Twig, get_internal_bot, rmtree, randint, makedirs, len\n## tags\n[word embeddings]\n\n## repository\nbel2scm/bel2scm\n## files\nsrc/bel2scm/causal_graph.py, src/bel2scm/gen_test_data.py, src/bel2scm/graph_node.py, src/bel2scm/version.py, src/bel2scm/generation/covid_scm_cf.py, all_parents_visited, get_parent_samples, get_sample_for_continuous_node, get_parent_tensor, get_exogenous_distribution, get_metadata_attribute, get_GEO, download_supplementary_files, parse_mtx, indep_vars\n## tags\n[counterfactual inference]\n\n## repository\nddkang/advex-uar\n## files\nadvex_uar/analysis/calibrate_eps.py, advex_uar/analysis/compute_ata.py, advex_uar/analysis/compute_uar.py, advex_uar/attacks/attacks.py, advex_uar/attacks/elastic.py, make_quantization_tables, jpeg_compress_decode, dct_8x8_ref, idct_8x8_ref, norm_to_pil_image, use_max_step, class_downsample_factor, wandb_ckpt_project, wandb_username, wandb_run_id\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/190808016']", 'paper_titles': "['Testing Robustness Against Unforeseen Adversaries']", 'arxiv_ids': "['1908.08016']", 'authors': '["[\'Daniel Kang\', \'Yi Sun\', \'Dan Hendrycks\', \'Tom Brown\', \'Jacob Steinhardt\']"]', 'raw_generated_text': '\n## repository\ndavidmatthews1uvm/2019-IROS\n## files\ndemos/word2vecDatabase.py, experiments/job.py, experiments/quadruped.py, experiments/spherebot.py, experiments/twig.py, shuffle_vec, create_new_job, Word2VecVectorSpace, SphereBot, Twig, get_internal_bot, rmtree, randint, makedirs, len\n## tags\n[word embeddings]\n\n## repository\nbel2scm/bel2scm\n## files\nsrc/bel2scm/causal_graph.py, src/bel2scm/gen_test_data.py, src/bel2scm/graph_node.py, src/bel2scm/version.py, src/bel2scm/generation/covid_scm_cf.py, all_parents_visited, get_parent_samples, get_sample_for_continuous_node, get_parent_tensor, get_exogenous_distribution, get_metadata_attribute, get_GEO, download_supplementary_files, parse_mtx, indep_vars\n## tags\n[counterfactual inference]\n\n## repository\nddkang/advex-uar\n## files\nadvex_uar/analysis/calibrate_eps.py, advex_uar/analysis/compute_ata.py, advex_uar/analysis/compute_uar.py, advex_uar/attacks/attacks.py, advex_uar/attacks/elastic.py, make_quantization_tables, jpeg_compress_decode, dct_8x8_ref, idct_8x8_ref, norm_to_pil_image, use_max_step, class_downsample_factor, wandb_ckpt_project, wandb_username, wandb_run_id\n## 3 tags\n[adversarial robustness]\n[image inversion]\n', 'raw_reference_text': 'adversarial defense', 'reference_text': 'adversarial defense', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7900650501251221, 'bertscore_recall': 0.7801551818847656, 'bertscore_f1': 0.7850788831710815, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.06690993905067444}, {'repo': 'thoailinh/Sentiment-Analysis-using-BERT', 'tasks_x': 'sentiment analysis, bert, semeval, ', 'true_tasks': ['sentiment analysis'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'Interface/main.py, Interface/output.py, Interface/predict_text.py, Interface/vd1.py, bert_rcnn, mywindow, NonMasking, resource_path, resource_path, _MEIPASS, resource_path, outputs, inputs, load_weights', 'generated_text': '\n## repository\ngurkanwar-singh/Style_Transfer\n## files\nutils/style_util.py, utils/vgg_util.py, vgg_func, content_loss_calculate, style_loss_calculate, layers_and_mean, pooling, layers_and_mean, pooling, gram_matrix, conv_relu, pow\n## tags\n[style transfer]\n\n## repository\nczk32611/GEDDnet\n## files\ncode/GEDDnet.py, code/PreProcess.py, code/PreProcess_eyecenter.py, code/tf_utils.py, code/infer.py, creatIter, dilated2d, randomRotate, preprocess_eye_image, preprocess_face_image, camera_mat, num_subject, creatIter, pre_process_eye_images, pre_process_face_images\n## tags\n[gaze estimation]\n\n## repository\nthoailinh/Sentiment-Analysis-using-BERT\n## files\nInterface/main.py, Interface/output.py, Interface/predict_text.py, Interface/vd1.py, bert_rcnn, mywindow, NonMasking, resource_path, resource_path, _MEIPASS, resource_path, outputs, inputs, load_weights\n## 3 tags\n[sentiment analysis]\n[bert]\n[semeval]\n\n##', 'id': '4638ed8eb850552398f9ddd9c7c9bbc6', 'name': 'thoailinh/Sentiment-Analysis-using-BERT', 'tasks': 'sentiment analysis, bert, semeval, ', 'true_text': 'sentiment analysis'}, 'generation': 0, 'input_text': '\n## repository\ngurkanwar-singh/Style_Transfer\n## files\nutils/style_util.py, utils/vgg_util.py, vgg_func, content_loss_calculate, style_loss_calculate, layers_and_mean, pooling, layers_and_mean, pooling, gram_matrix, conv_relu, pow\n## tags\n[style transfer]\n\n## repository\nczk32611/GEDDnet\n## files\ncode/GEDDnet.py, code/PreProcess.py, code/PreProcess_eyecenter.py, code/tf_utils.py, code/infer.py, creatIter, dilated2d, randomRotate, preprocess_eye_image, preprocess_face_image, camera_mat, num_subject, creatIter, pre_process_eye_images, pre_process_face_images\n## tags\n[gaze estimation]\n\n## repository\nthoailinh/Sentiment-Analysis-using-BERT\n## files\nInterface/main.py, Interface/output.py, Interface/predict_text.py, Interface/vd1.py, bert_rcnn, mywindow, NonMasking, resource_path, resource_path, _MEIPASS, resource_path, outputs, inputs, load_weights\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/fine-tuning-bert-for-sentiment-analysis-of']", 'paper_titles': "['Fine-Tuning BERT for Sentiment Analysis of Vietnamese Reviews']", 'arxiv_ids': "['2011.10426']", 'authors': '["[\'Quoc Thai Nguyen\', \'Thoai Linh Nguyen\', \'Ngoc Hoang Luong\', \'Quoc Hung Ngo\']"]', 'raw_generated_text': '\n## repository\ngurkanwar-singh/Style_Transfer\n## files\nutils/style_util.py, utils/vgg_util.py, vgg_func, content_loss_calculate, style_loss_calculate, layers_and_mean, pooling, layers_and_mean, pooling, gram_matrix, conv_relu, pow\n## tags\n[style transfer]\n\n## repository\nczk32611/GEDDnet\n## files\ncode/GEDDnet.py, code/PreProcess.py, code/PreProcess_eyecenter.py, code/tf_utils.py, code/infer.py, creatIter, dilated2d, randomRotate, preprocess_eye_image, preprocess_face_image, camera_mat, num_subject, creatIter, pre_process_eye_images, pre_process_face_images\n## tags\n[gaze estimation]\n\n## repository\nthoailinh/Sentiment-Analysis-using-BERT\n## files\nInterface/main.py, Interface/output.py, Interface/predict_text.py, Interface/vd1.py, bert_rcnn, mywindow, NonMasking, resource_path, resource_path, _MEIPASS, resource_path, outputs, inputs, load_weights\n## 3 tags\n[sentiment analysis]\n[bert]\n[semeval]\n\n##', 'raw_reference_text': 'sentiment analysis', 'reference_text': 'sentiment analysis', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8895980715751648, 'bertscore_recall': 0.8990515470504761, 'bertscore_f1': 0.8942998051643372, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.13596737384796143}, {'repo': 'manyunya/bitcoin-rnn', 'tasks_x': 'privacy, ', 'true_tasks': ['learning to execute'], 'generated_text': 'scene text recognition', 'prompt_info': {'content': 'gru-addr2priv.py, mk-privaddr-pair.py, priv2pub.py, seed2hpriv, CharacterTable, colors, HMAC, encode', 'generated_text': '\n## repository\nModelBunker/MORAN-PyTorch\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## tags\n[scene text recognition]\n\n## repository\nfyangneil/pavement-crack-detection\n## files\nexamples/fphb/CustomSigmoidCrossEntropyLossLayer.py, examples/fphb/solve_fphb_crack.py, examples/fphb/solve_fpn_crack.py, python/caffe/proto/caffe_pb2.py, python/caffe/test/test_layer_type_list.py, save_single_scale, CustomSigmoidCrossEntropyLossLayer, plot_single_scale, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[semantic segmentation, edge detection]\n\n## repository\nmanyunya/bitcoin-rnn\n## files\ngru-addr2priv.py, mk-privaddr-pair.py, priv2pub.py, seed2hpriv, CharacterTable, colors, HMAC, encode\n## 3 tags\n[privacy]\n\n', 'id': '238c3b47361dbb54f5e72519f2248c8e', 'name': 'manyunya/bitcoin-rnn', 'tasks': 'privacy, ', 'true_text': 'learning to execute'}, 'generation': 0, 'input_text': '\n## repository\nModelBunker/MORAN-PyTorch\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## tags\n[scene text recognition]\n\n## repository\nfyangneil/pavement-crack-detection\n## files\nexamples/fphb/CustomSigmoidCrossEntropyLossLayer.py, examples/fphb/solve_fphb_crack.py, examples/fphb/solve_fpn_crack.py, python/caffe/proto/caffe_pb2.py, python/caffe/test/test_layer_type_list.py, save_single_scale, CustomSigmoidCrossEntropyLossLayer, plot_single_scale, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[semantic segmentation, edge detection]\n\n## repository\nmanyunya/bitcoin-rnn\n## files\ngru-addr2priv.py, mk-privaddr-pair.py, priv2pub.py, seed2hpriv, CharacterTable, colors, HMAC, encode\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/learning-to-execute']", 'paper_titles': "['Learning to Execute']", 'arxiv_ids': "['1410.4615']", 'authors': '["[\'Wojciech Zaremba\', \'Ilya Sutskever\']"]', 'raw_generated_text': '\n## repository\nModelBunker/MORAN-PyTorch\n## files\nmodels/asrn_res.py, models/fracPickup.py, models/moran.py, models/morn.py, tools/dataset.py, strLabelConverterForAttention, ASRN, fracPickup, MORN, MORAN, BidirDecoder, BidirDecoder, MORAN, MORAN, get_torch_version\n## tags\n[scene text recognition]\n\n## repository\nfyangneil/pavement-crack-detection\n## files\nexamples/fphb/CustomSigmoidCrossEntropyLossLayer.py, examples/fphb/solve_fphb_crack.py, examples/fphb/solve_fpn_crack.py, python/caffe/proto/caffe_pb2.py, python/caffe/test/test_layer_type_list.py, save_single_scale, CustomSigmoidCrossEntropyLossLayer, plot_single_scale, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[semantic segmentation, edge detection]\n\n## repository\nmanyunya/bitcoin-rnn\n## files\ngru-addr2priv.py, mk-privaddr-pair.py, priv2pub.py, seed2hpriv, CharacterTable, colors, HMAC, encode\n## 3 tags\n[privacy]\n\n', 'raw_reference_text': 'learning to execute', 'reference_text': 'learning to execute', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8310765027999878, 'bertscore_recall': 0.872825026512146, 'bertscore_f1': 0.8514392971992493, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.10940250754356384}, {'repo': 'speakstone/TextSnake', 'tasks_x': 'semantic segmentation, text detection, ', 'true_tasks': ['scene text detection'], 'generated_text': 'stochastic optimization', 'prompt_info': {'content': 'bai_pachong/Img_Deduplication.py, bai_pachong/index.py, bai_pachong/main.py, dataset/data_coco.py, bai_pachong/test.py, regularize_sin_cos, vector_sin, find_bottom, disjoint_merge, vector_cos, rescale_result, mgpu, merge_two_polygon, TextLoss, visualize_network_output', 'generated_text': '\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\npquochuy/sasegan\n## files\nsasegan/bnorm.py, sasegan/data_loader.py, sasegan/discriminator.py, sasegan/generator.py, sasegan/main.py, sn_downconv, sn_deconv, nn_deconv, sn_conv1x1, sample_random_walk, noisy_dir, synthesis_path, wav_dir, encoder_proc, read_and_slice\n## tags\n[speech enhancement]\n\n## repository\nspeakstone/TextSnake\n## files\nbai_pachong/Img_Deduplication.py, bai_pachong/index.py, bai_pachong/main.py, dataset/data_coco.py, bai_pachong/test.py, regularize_sin_cos, vector_sin, find_bottom, disjoint_merge, vector_cos, rescale_result, mgpu, merge_two_polygon, TextLoss, visualize_network_output\n## 3 tags\n[semantic segmentation, text detection]\n\n## repository\nmaertens/G', 'id': '958bfcfb51b7ebf43894788513eca852', 'name': 'speakstone/TextSnake', 'tasks': 'semantic segmentation, text detection, ', 'true_text': 'scene text detection'}, 'generation': 0, 'input_text': '\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\npquochuy/sasegan\n## files\nsasegan/bnorm.py, sasegan/data_loader.py, sasegan/discriminator.py, sasegan/generator.py, sasegan/main.py, sn_downconv, sn_deconv, nn_deconv, sn_conv1x1, sample_random_walk, noisy_dir, synthesis_path, wav_dir, encoder_proc, read_and_slice\n## tags\n[speech enhancement]\n\n## repository\nspeakstone/TextSnake\n## files\nbai_pachong/Img_Deduplication.py, bai_pachong/index.py, bai_pachong/main.py, dataset/data_coco.py, bai_pachong/test.py, regularize_sin_cos, vector_sin, find_bottom, disjoint_merge, vector_cos, rescale_result, mgpu, merge_two_polygon, TextLoss, visualize_network_output\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/textsnake-a-flexible-representation-for']", 'paper_titles': "['TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes']", 'arxiv_ids': "['1807.01544']", 'authors': '["[\'Shangbang Long\', \'Jiaqiang Ruan\', \'Wenjie Zhang\', \'Xin He\', \'Wenhao Wu\', \'Cong Yao\']"]', 'raw_generated_text': '\n## repository\nmadkn/HistogramLoss\n## files\npython_layers/distribution_loss_layer.py, python_layers/label_shuffling_layer.py, validation/data_preprocessing.py, validation/ranking.py, caffe-master/examples/finetune_flickr_style/assemble_data.py, transpose_for_show, break_the_image, getPlace, transpose_for_storage, parseRecord, break_the_image, getPlace, transpose_for_storage, ResetSection, prepareImage\n## tags\n[stochastic optimization]\n\n## repository\npquochuy/sasegan\n## files\nsasegan/bnorm.py, sasegan/data_loader.py, sasegan/discriminator.py, sasegan/generator.py, sasegan/main.py, sn_downconv, sn_deconv, nn_deconv, sn_conv1x1, sample_random_walk, noisy_dir, synthesis_path, wav_dir, encoder_proc, read_and_slice\n## tags\n[speech enhancement]\n\n## repository\nspeakstone/TextSnake\n## files\nbai_pachong/Img_Deduplication.py, bai_pachong/index.py, bai_pachong/main.py, dataset/data_coco.py, bai_pachong/test.py, regularize_sin_cos, vector_sin, find_bottom, disjoint_merge, vector_cos, rescale_result, mgpu, merge_two_polygon, TextLoss, visualize_network_output\n## 3 tags\n[semantic segmentation, text detection]\n\n## repository\nmaertens/G', 'raw_reference_text': 'scene text detection', 'reference_text': 'scene text detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7729374170303345, 'bertscore_recall': 0.8609505891799927, 'bertscore_f1': 0.8145735263824463, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23385527729988098}, {'repo': 'LightDXY/GreedyFool', 'tasks_x': 'neural networks, Reconstruction, unsupervised, ', 'true_tasks': ['adversarial attack'], 'generated_text': 'information retrieval table detection', 'prompt_info': {'content': 'nips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator', 'generated_text': '\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## tags\n[information retrieval, table detection]\n\n## repository\nnithyadurai87/pottan-ocr-tamil\n## files\nmisc/keras_model.py, ocropy/ocrolib/chars.py, ocropy/ocrolib/common.py, ocropy/ocrolib/default.py, ocropy/ocrolib/edist.py, project_text, write_image_gray, read_image_binary, write_image_binary, read_line_segmentation, testEncoding, testencoding, crnnModel, font_description_from_string, get_font_description\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## 3 tags\n[neural networks, Reconstruction, unsupervised]\n\n## repository\ndorag', 'id': '609861310c1aa4d889e1dc2b7eff5ffa', 'name': 'LightDXY/GreedyFool', 'tasks': 'neural networks, Reconstruction, unsupervised, ', 'true_text': 'adversarial attack'}, 'generation': 0, 'input_text': '\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## tags\n[information retrieval, table detection]\n\n## repository\nnithyadurai87/pottan-ocr-tamil\n## files\nmisc/keras_model.py, ocropy/ocrolib/chars.py, ocropy/ocrolib/common.py, ocropy/ocrolib/default.py, ocropy/ocrolib/edist.py, project_text, write_image_gray, read_image_binary, write_image_binary, read_line_segmentation, testEncoding, testencoding, crnnModel, font_description_from_string, get_font_description\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/greedyfool-distortion-aware-sparse']", 'paper_titles': "['GreedyFool: Distortion-Aware Sparse Adversarial Attack']", 'arxiv_ids': "['2010.13773']", 'authors': '["[\'Xiaoyi Dong\', \'Dongdong Chen\', \'Jianmin Bao\', \'Chuan Qin\', \'Lu Yuan\', \'Weiming Zhang\', \'Nenghai Yu\', \'Dong Chen\']"]', 'raw_generated_text': '\n## repository\nnikolamilosevic86/TabInOut\n## files\nAnalyzePattern.py, CreateTableDataset.py, QueryDBClass.py, QueryDBClassESG.py, Data/Article.py, CreateFolderStructure, BlackListWindow, SemanticListWindowEdit, WhiteListWindowEdit, SaveWhiteListSemantic, bl_look_header, bl_look_superrow, wl_look_header, bl_look_data, idPMC\n## tags\n[information retrieval, table detection]\n\n## repository\nnithyadurai87/pottan-ocr-tamil\n## files\nmisc/keras_model.py, ocropy/ocrolib/chars.py, ocropy/ocrolib/common.py, ocropy/ocrolib/default.py, ocropy/ocrolib/edist.py, project_text, write_image_gray, read_image_binary, write_image_binary, read_line_segmentation, testEncoding, testencoding, crnnModel, font_description_from_string, get_font_description\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## 3 tags\n[neural networks, Reconstruction, unsupervised]\n\n## repository\ndorag', 'raw_reference_text': 'adversarial attack', 'reference_text': 'adversarial attack', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8383511304855347, 'bertscore_recall': 0.8086541891098022, 'bertscore_f1': 0.8232349157333374, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.15672728419303894}, {'repo': 'KAIST-vilab/SpherPHD_public', 'tasks_x': 'speaker localization, speech feature, each-pair classifier, ', 'true_tasks': ['semantic segmentation'], 'generated_text': 'grammatical error correction', 'prompt_info': {'content': 'loadTable.py, MNIST_example.py, loadData.py, params.py, network.py, loadTrainData, loadTestData, avgpool, conv_net, maxpool, avgpool, maxpool, tf_record_iterator, tf_record_iterator, ParseFromString', 'generated_text': '\n## repository\nkeisks/reassess-gec\n## files\ninter_annotator_agreements/computeIAA.py, trueskill_rankings/cluster.py, add_trinary, compute_kappa, rank_by_mu, shorten_name, sort_by_mu, kappa, combinations, round, sort, sort\n## tags\n[grammatical error correction]\n\n## repository\nYeeU/InverseRenderNet\n## files\nmodel/lambSH_layer.py, model/pred_illuDecomp_layer.py, model/reproj_layer.py, model/SfMNet.py, model/sup_illuDecomp_layer.py, _read_pk_function, md_read_func, md_preprocess_func, megaDepth_dataPipeline, conv2d_nosum_2ch, megaDepth_dataPipeline, loss_formulate, cvtLab, conv2d_nosum, md_construct_inputPipeline\n## tags\n[intrinsic image decomposition]\n\n## repository\nKAIST-vilab/SpherPHD_public\n## files\nloadTable.py, MNIST_example.py, loadData.py, params.py, network.py, loadTrainData, loadTestData, avgpool, conv_net, maxpool, avgpool, maxpool, tf_record_iterator, tf_record_iterator, ParseFromString\n## 3 tags\n[speaker localization] [speech feature] [each-pair classifier]\n\n##', 'id': 'c8d95bc4b3ccecc9a961602b2bc1ca1e', 'name': 'KAIST-vilab/SpherPHD_public', 'tasks': 'speaker localization, speech feature, each-pair classifier, ', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\nkeisks/reassess-gec\n## files\ninter_annotator_agreements/computeIAA.py, trueskill_rankings/cluster.py, add_trinary, compute_kappa, rank_by_mu, shorten_name, sort_by_mu, kappa, combinations, round, sort, sort\n## tags\n[grammatical error correction]\n\n## repository\nYeeU/InverseRenderNet\n## files\nmodel/lambSH_layer.py, model/pred_illuDecomp_layer.py, model/reproj_layer.py, model/SfMNet.py, model/sup_illuDecomp_layer.py, _read_pk_function, md_read_func, md_preprocess_func, megaDepth_dataPipeline, conv2d_nosum_2ch, megaDepth_dataPipeline, loss_formulate, cvtLab, conv2d_nosum, md_construct_inputPipeline\n## tags\n[intrinsic image decomposition]\n\n## repository\nKAIST-vilab/SpherPHD_public\n## files\nloadTable.py, MNIST_example.py, loadData.py, params.py, network.py, loadTrainData, loadTestData, avgpool, conv_net, maxpool, avgpool, maxpool, tf_record_iterator, tf_record_iterator, ParseFromString\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/spherephd-applying-cnns-on-a-spherical']", 'paper_titles': "['SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360 degree Images']", 'arxiv_ids': "['1811.08196']", 'authors': '["[\'Yeonkun Lee\', \'Jaeseok Jeong\', \'Jongseob Yun\', \'Wonjune Cho\', \'Kuk-Jin Yoon\']"]', 'raw_generated_text': '\n## repository\nkeisks/reassess-gec\n## files\ninter_annotator_agreements/computeIAA.py, trueskill_rankings/cluster.py, add_trinary, compute_kappa, rank_by_mu, shorten_name, sort_by_mu, kappa, combinations, round, sort, sort\n## tags\n[grammatical error correction]\n\n## repository\nYeeU/InverseRenderNet\n## files\nmodel/lambSH_layer.py, model/pred_illuDecomp_layer.py, model/reproj_layer.py, model/SfMNet.py, model/sup_illuDecomp_layer.py, _read_pk_function, md_read_func, md_preprocess_func, megaDepth_dataPipeline, conv2d_nosum_2ch, megaDepth_dataPipeline, loss_formulate, cvtLab, conv2d_nosum, md_construct_inputPipeline\n## tags\n[intrinsic image decomposition]\n\n## repository\nKAIST-vilab/SpherPHD_public\n## files\nloadTable.py, MNIST_example.py, loadData.py, params.py, network.py, loadTrainData, loadTestData, avgpool, conv_net, maxpool, avgpool, maxpool, tf_record_iterator, tf_record_iterator, ParseFromString\n## 3 tags\n[speaker localization] [speech feature] [each-pair classifier]\n\n##', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8578636646270752, 'bertscore_recall': 0.8581114411354065, 'bertscore_f1': 0.8579875826835632, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.21544617414474487}, {'repo': 'podgorskiy/GPND', 'tasks_x': 'variational autoencoders, Object classification, ', 'true_tasks': ['one class classifier'], 'generated_text': 'response generation data augmentation speech enhancement', 'prompt_info': {'content': 'novelty_detector.py, partition_mnist.py, save_to_csv.py, train_AAE.py, utils/save_plot.py, ZDiscriminator, ZDiscriminator_mergebatch, create_set_with_outlier_percentage, _get_gpu_count, set_numpy_treads, Z_DISCRIMINATOR_CROSS_BATCH, EPOCH_COUNT, register_means, PERCENTAGES, INPUT_IMAGE_CHANNELS', 'generated_text': '\n## repository\nSRPOL-AUI/storir\n## files\nstorir/audio_utils.py, storir/ir.py, storir/rir_utils.py, storir/__init__.py, example.py, calculate_drr_energy_ratio, thin_out_reflections, peak_norm, decibels_to_gain, ImpulseResponse, log10, choice, sum, int, len\n## tags\n[response generation, data augmentation, speech enhancement]\n\n## repository\nth2l/Eye_VR_Segmentation\n## files\nsrc/test.py, src/train.py, src/utils.py, generalised_dice_loss_ce, OpenEDS, MobileNetV2_CS, weighted_CrossEntropyLoss, weighted_CrossEntropyLoss, eye_refinement, re_label, pixel_acc, pixel_acc, mean_iou\n## tags\n[semantic segmentation]\n\n## repository\npodgorskiy/GPND\n## files\nnovelty_detector.py, partition_mnist.py, save_to_csv.py, train_AAE.py, utils/save_plot.py, ZDiscriminator, ZDiscriminator_mergebatch, create_set_with_outlier_percentage, _get_gpu_count, set_numpy_treads, Z_DISCRIMINATOR_CROSS_BATCH, EPOCH_COUNT, register_means, PERCENTAGES, INPUT_IMAGE_CHANNELS\n## 3 tags\n[variational autoencoders, Object classification]\n\n## repository\nlmot', 'id': 'beb30dd60972e04817f908a240454c59', 'name': 'podgorskiy/GPND', 'tasks': 'variational autoencoders, Object classification, ', 'true_text': 'one class classifier'}, 'generation': 0, 'input_text': '\n## repository\nSRPOL-AUI/storir\n## files\nstorir/audio_utils.py, storir/ir.py, storir/rir_utils.py, storir/__init__.py, example.py, calculate_drr_energy_ratio, thin_out_reflections, peak_norm, decibels_to_gain, ImpulseResponse, log10, choice, sum, int, len\n## tags\n[response generation, data augmentation, speech enhancement]\n\n## repository\nth2l/Eye_VR_Segmentation\n## files\nsrc/test.py, src/train.py, src/utils.py, generalised_dice_loss_ce, OpenEDS, MobileNetV2_CS, weighted_CrossEntropyLoss, weighted_CrossEntropyLoss, eye_refinement, re_label, pixel_acc, pixel_acc, mean_iou\n## tags\n[semantic segmentation]\n\n## repository\npodgorskiy/GPND\n## files\nnovelty_detector.py, partition_mnist.py, save_to_csv.py, train_AAE.py, utils/save_plot.py, ZDiscriminator, ZDiscriminator_mergebatch, create_set_with_outlier_percentage, _get_gpu_count, set_numpy_treads, Z_DISCRIMINATOR_CROSS_BATCH, EPOCH_COUNT, register_means, PERCENTAGES, INPUT_IMAGE_CHANNELS\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/generative-probabilistic-novelty-detection']", 'paper_titles': "['Generative Probabilistic Novelty Detection with Adversarial Autoencoders']", 'arxiv_ids': "['1807.02588']", 'authors': '["[\'Stanislav Pidhorskyi\', \'Ranya Almohsen\', \'Donald A. Adjeroh\', \'Gianfranco Doretto\']"]', 'raw_generated_text': '\n## repository\nSRPOL-AUI/storir\n## files\nstorir/audio_utils.py, storir/ir.py, storir/rir_utils.py, storir/__init__.py, example.py, calculate_drr_energy_ratio, thin_out_reflections, peak_norm, decibels_to_gain, ImpulseResponse, log10, choice, sum, int, len\n## tags\n[response generation, data augmentation, speech enhancement]\n\n## repository\nth2l/Eye_VR_Segmentation\n## files\nsrc/test.py, src/train.py, src/utils.py, generalised_dice_loss_ce, OpenEDS, MobileNetV2_CS, weighted_CrossEntropyLoss, weighted_CrossEntropyLoss, eye_refinement, re_label, pixel_acc, pixel_acc, mean_iou\n## tags\n[semantic segmentation]\n\n## repository\npodgorskiy/GPND\n## files\nnovelty_detector.py, partition_mnist.py, save_to_csv.py, train_AAE.py, utils/save_plot.py, ZDiscriminator, ZDiscriminator_mergebatch, create_set_with_outlier_percentage, _get_gpu_count, set_numpy_treads, Z_DISCRIMINATOR_CROSS_BATCH, EPOCH_COUNT, register_means, PERCENTAGES, INPUT_IMAGE_CHANNELS\n## 3 tags\n[variational autoencoders, Object classification]\n\n## repository\nlmot', 'raw_reference_text': 'one class classifier', 'reference_text': 'one class classifier', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8457558155059814, 'bertscore_recall': 0.871486485004425, 'bertscore_f1': 0.858428418636322, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.00455520860850811}, {'repo': 'LouieYang/stroke-controllable-fast-style-transfer', 'tasks_x': 'neural network, machine learning, classification, ', 'true_tasks': ['style transfer'], 'generated_text': 'depth estimation superpixels monocular depth estimation', 'prompt_info': {'content': 'netdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars', 'generated_text': '\n## repository\nimran3180/depth-map-prediction\n## files\nimage_helper.py, test_run.py, evaluate.py, data.py, model.py, TransposeDepthInput, fineNet, coarseNet, threeshold_percentage, rmse_linear, fine_model, fine_model, coarse_model, coarse_model, coarse_model\n## tags\n[superpixels, monocular depth estimation, depth estimation]\n\n## repository\nGuyTevet/diversity-eval\n## files\ncon_test.py, dec_test.py, diversity_metrics.py, similarity_metrics.py, metrics_test.py, parse_path_list, lines_to_ngrams, download_and_place_data, optimal_classification_accuracy, CamleCase2snake_case, dict_print, ignore_cache, fieldnames, writeheader, DictWriter\n## tags\n[text generation]\n\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## 3 tags\n[neural network, machine learning, classification]\n\n## repository\nHiroyuki', 'id': '8c8e24520e06756c5d078b7d8ac417c9', 'name': 'LouieYang/stroke-controllable-fast-style-transfer', 'tasks': 'neural network, machine learning, classification, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\nimran3180/depth-map-prediction\n## files\nimage_helper.py, test_run.py, evaluate.py, data.py, model.py, TransposeDepthInput, fineNet, coarseNet, threeshold_percentage, rmse_linear, fine_model, fine_model, coarse_model, coarse_model, coarse_model\n## tags\n[superpixels, monocular depth estimation, depth estimation]\n\n## repository\nGuyTevet/diversity-eval\n## files\ncon_test.py, dec_test.py, diversity_metrics.py, similarity_metrics.py, metrics_test.py, parse_path_list, lines_to_ngrams, download_and_place_data, optimal_classification_accuracy, CamleCase2snake_case, dict_print, ignore_cache, fieldnames, writeheader, DictWriter\n## tags\n[text generation]\n\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/stroke-controllable-fast-style-transfer-with']", 'paper_titles': "['Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields']", 'arxiv_ids': "['1802.07101']", 'authors': '["[\'Yongcheng Jing\', \'Yang Liu\', \'Yezhou Yang\', \'Zunlei Feng\', \'Yizhou Yu\', \'DaCheng Tao\', \'Mingli Song\']"]', 'raw_generated_text': '\n## repository\nimran3180/depth-map-prediction\n## files\nimage_helper.py, test_run.py, evaluate.py, data.py, model.py, TransposeDepthInput, fineNet, coarseNet, threeshold_percentage, rmse_linear, fine_model, fine_model, coarse_model, coarse_model, coarse_model\n## tags\n[superpixels, monocular depth estimation, depth estimation]\n\n## repository\nGuyTevet/diversity-eval\n## files\ncon_test.py, dec_test.py, diversity_metrics.py, similarity_metrics.py, metrics_test.py, parse_path_list, lines_to_ngrams, download_and_place_data, optimal_classification_accuracy, CamleCase2snake_case, dict_print, ignore_cache, fieldnames, writeheader, DictWriter\n## tags\n[text generation]\n\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## 3 tags\n[neural network, machine learning, classification]\n\n## repository\nHiroyuki', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8118640780448914, 'bertscore_recall': 0.8728811740875244, 'bertscore_f1': 0.8412676453590393, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.00851425714790821}, {'repo': 'gongzhitaao/adversarial-classifier', 'tasks_x': 'source code, Show Augmentation for Adversarial Attacks on Deep Convolution', 'true_tasks': ['adversarial attack'], 'generated_text': 'stereo lidar fusion', 'prompt_info': {'content': 'src/figure_2.py, src/table_1_svhn.py, src/figure_1.py, src/table_1_cifar10.py, src/table_1_mnist.py, random_orthogonal, maybe_download, random, norm, dot, makedirs, shape', 'generated_text': '\n## repository\nzyf12389/GC-Net\n## files\ngc_net.py, read_sceneflow.py, read_testFile.py, readFile.py, python_pfm.py, normalizeRGB, sceneDisp, GcNet, ThreeDConv, GC_NET, sceneDisp, sceneDisp, byteorder, kaiming_normal, L1Loss\n## tags\n[stereo lidar fusion]\n\n## repository\nzachary2940/ml-agents-assignment\n## files\nml-agents/mlagents/tf_utils/globals.py, ml-agents/mlagents/trainers/model_saver/tf_model_saver.py, ml-agents/mlagents/trainers/ppo/optimizer_tf.py, ml-agents/mlagents/trainers/sac/optimizer_tf.py, ml-agents/mlagents/trainers/tests/tensorflow/test_barracuda_converter.py, test_process_pixels_multi_png, CompletionCriteriaSettings, Lesson, MultiRangeUniformSettings, ParameterRandomizationSettings, get_minimum_reward_buffer_size, _split_decision_step, use_continuous_act, layer_ranks, get_trajectory_value_estimates\n## tags\n[unity]\n\n## repository\ngongzhitaao/adversarial-classifier\n## files\nsrc/figure_2.py, src/table_1_svhn.py, src/figure_1.py, src/table_1_cifar10.py, src/table_1_mnist.py, random_orthogonal, maybe_download, random, norm, dot, makedirs, shape\n## 3 tags\n[source code][Show Augmentation for Adversarial Attacks on Deep Convolution', 'id': '7fda69355f1ab06aedfd220dfaee98e3', 'name': 'gongzhitaao/adversarial-classifier', 'tasks': 'source code, Show Augmentation for Adversarial Attacks on Deep Convolution', 'true_text': 'adversarial attack'}, 'generation': 0, 'input_text': '\n## repository\nzyf12389/GC-Net\n## files\ngc_net.py, read_sceneflow.py, read_testFile.py, readFile.py, python_pfm.py, normalizeRGB, sceneDisp, GcNet, ThreeDConv, GC_NET, sceneDisp, sceneDisp, byteorder, kaiming_normal, L1Loss\n## tags\n[stereo lidar fusion]\n\n## repository\nzachary2940/ml-agents-assignment\n## files\nml-agents/mlagents/tf_utils/globals.py, ml-agents/mlagents/trainers/model_saver/tf_model_saver.py, ml-agents/mlagents/trainers/ppo/optimizer_tf.py, ml-agents/mlagents/trainers/sac/optimizer_tf.py, ml-agents/mlagents/trainers/tests/tensorflow/test_barracuda_converter.py, test_process_pixels_multi_png, CompletionCriteriaSettings, Lesson, MultiRangeUniformSettings, ParameterRandomizationSettings, get_minimum_reward_buffer_size, _split_decision_step, use_continuous_act, layer_ranks, get_trajectory_value_estimates\n## tags\n[unity]\n\n## repository\ngongzhitaao/adversarial-classifier\n## files\nsrc/figure_2.py, src/table_1_svhn.py, src/figure_1.py, src/table_1_cifar10.py, src/table_1_mnist.py, random_orthogonal, maybe_download, random, norm, dot, makedirs, shape\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/adversarial-and-clean-data-are-not-twins']", 'paper_titles': "['Adversarial and Clean Data Are Not Twins']", 'arxiv_ids': "['1704.04960']", 'authors': '["[\'Zhitao Gong\', \'Wenlu Wang\', \'Wei-Shinn Ku\']"]', 'raw_generated_text': '\n## repository\nzyf12389/GC-Net\n## files\ngc_net.py, read_sceneflow.py, read_testFile.py, readFile.py, python_pfm.py, normalizeRGB, sceneDisp, GcNet, ThreeDConv, GC_NET, sceneDisp, sceneDisp, byteorder, kaiming_normal, L1Loss\n## tags\n[stereo lidar fusion]\n\n## repository\nzachary2940/ml-agents-assignment\n## files\nml-agents/mlagents/tf_utils/globals.py, ml-agents/mlagents/trainers/model_saver/tf_model_saver.py, ml-agents/mlagents/trainers/ppo/optimizer_tf.py, ml-agents/mlagents/trainers/sac/optimizer_tf.py, ml-agents/mlagents/trainers/tests/tensorflow/test_barracuda_converter.py, test_process_pixels_multi_png, CompletionCriteriaSettings, Lesson, MultiRangeUniformSettings, ParameterRandomizationSettings, get_minimum_reward_buffer_size, _split_decision_step, use_continuous_act, layer_ranks, get_trajectory_value_estimates\n## tags\n[unity]\n\n## repository\ngongzhitaao/adversarial-classifier\n## files\nsrc/figure_2.py, src/table_1_svhn.py, src/figure_1.py, src/table_1_cifar10.py, src/table_1_mnist.py, random_orthogonal, maybe_download, random, norm, dot, makedirs, shape\n## 3 tags\n[source code][Show Augmentation for Adversarial Attacks on Deep Convolution', 'raw_reference_text': 'adversarial attack', 'reference_text': 'adversarial attack', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8479024767875671, 'bertscore_recall': 0.8184067606925964, 'bertscore_f1': 0.8328936100006104, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.22116872668266296}, {'repo': 'visinf/1-stage-wseg', 'tasks_x': 'object detector, segmentation, object segmentation, ', 'true_tasks': ['semantic segmentation'], 'generated_text': 'outlier detection word embeddings', 'prompt_info': {'content': 'infer_val.py, datasets/pascal_voc_ms.py, models/stage_net.py, models/backbones/base_net.py, models/backbones/resnet38d.py, time_call, DecTrainer, mask_loss_ce, MLHingeLoss, summarise_one, summarise_per_class, add_global_arguments, check_global_arguments, reset_stage, network_factory', 'generated_text': '\n## repository\ndnguyen1196/word-embedding-cp\n## files\nembedding_comparison.py, embedding_evaluation.py, gensim_utils.py, tensor_decomp.py, tensor_embedding.py, TestPhrasesCommon, test_decomp, test_symmetric_decomp, test_joint_decomp, JointSymmetricCPDecomp, clear_sims, add_topic_posting, csc_matvecs, GensimSandbox, write_embedding_to_file\n## tags\n[word embeddings, outlier detection]\n\n## repository\nlmb-freiburg/deeptam\n## files\nmapping/example/mapping_test_deeptam.py, mapping/example/mapping_test_deeptam_2.py, mapping/python/deeptam_mapper/mapper.py, mapping/python/deeptam_mapper/evaluation/metrics.py, mapping/python/deeptam_mapper/models/blocks.py, compute_confidence_for_costvolume_2, get_depth_label_list, create_border_mask_for_image, compute_sad_volume_with_confidence, create_depthsweep_images_tensor, compute_keyframe_depth, compute_current_pose, save_associations, second_file, first_file\n## tags\n[depth estimation]\n\n## repository\nvisinf/1-stage-wseg\n## files\ninfer_val.py, datasets/pascal_voc_ms.py, models/stage_net.py, models/backbones/base_net.py, models/backbones/resnet38d.py, time_call, DecTrainer, mask_loss_ce, MLHingeLoss, summarise_one, summarise_per_class, add_global_arguments, check_global_arguments, reset_stage, network_factory\n## 3 tags\n[object detector, segmentation, object segmentation]\n\n## repository\ncvpr2', 'id': 'b514c92fdde915816792550a991d8421', 'name': 'visinf/1-stage-wseg', 'tasks': 'object detector, segmentation, object segmentation, ', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\ndnguyen1196/word-embedding-cp\n## files\nembedding_comparison.py, embedding_evaluation.py, gensim_utils.py, tensor_decomp.py, tensor_embedding.py, TestPhrasesCommon, test_decomp, test_symmetric_decomp, test_joint_decomp, JointSymmetricCPDecomp, clear_sims, add_topic_posting, csc_matvecs, GensimSandbox, write_embedding_to_file\n## tags\n[word embeddings, outlier detection]\n\n## repository\nlmb-freiburg/deeptam\n## files\nmapping/example/mapping_test_deeptam.py, mapping/example/mapping_test_deeptam_2.py, mapping/python/deeptam_mapper/mapper.py, mapping/python/deeptam_mapper/evaluation/metrics.py, mapping/python/deeptam_mapper/models/blocks.py, compute_confidence_for_costvolume_2, get_depth_label_list, create_border_mask_for_image, compute_sad_volume_with_confidence, create_depthsweep_images_tensor, compute_keyframe_depth, compute_current_pose, save_associations, second_file, first_file\n## tags\n[depth estimation]\n\n## repository\nvisinf/1-stage-wseg\n## files\ninfer_val.py, datasets/pascal_voc_ms.py, models/stage_net.py, models/backbones/base_net.py, models/backbones/resnet38d.py, time_call, DecTrainer, mask_loss_ce, MLHingeLoss, summarise_one, summarise_per_class, add_global_arguments, check_global_arguments, reset_stage, network_factory\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/single-stage-semantic-segmentation-from-image']", 'paper_titles': "['Single-Stage Semantic Segmentation from Image Labels']", 'arxiv_ids': "['2005.08104']", 'authors': '["[\'Nikita Araslanov\', \'Stefan Roth\']"]', 'raw_generated_text': '\n## repository\ndnguyen1196/word-embedding-cp\n## files\nembedding_comparison.py, embedding_evaluation.py, gensim_utils.py, tensor_decomp.py, tensor_embedding.py, TestPhrasesCommon, test_decomp, test_symmetric_decomp, test_joint_decomp, JointSymmetricCPDecomp, clear_sims, add_topic_posting, csc_matvecs, GensimSandbox, write_embedding_to_file\n## tags\n[word embeddings, outlier detection]\n\n## repository\nlmb-freiburg/deeptam\n## files\nmapping/example/mapping_test_deeptam.py, mapping/example/mapping_test_deeptam_2.py, mapping/python/deeptam_mapper/mapper.py, mapping/python/deeptam_mapper/evaluation/metrics.py, mapping/python/deeptam_mapper/models/blocks.py, compute_confidence_for_costvolume_2, get_depth_label_list, create_border_mask_for_image, compute_sad_volume_with_confidence, create_depthsweep_images_tensor, compute_keyframe_depth, compute_current_pose, save_associations, second_file, first_file\n## tags\n[depth estimation]\n\n## repository\nvisinf/1-stage-wseg\n## files\ninfer_val.py, datasets/pascal_voc_ms.py, models/stage_net.py, models/backbones/base_net.py, models/backbones/resnet38d.py, time_call, DecTrainer, mask_loss_ce, MLHingeLoss, summarise_one, summarise_per_class, add_global_arguments, check_global_arguments, reset_stage, network_factory\n## 3 tags\n[object detector, segmentation, object segmentation]\n\n## repository\ncvpr2', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8459111452102661, 'bertscore_recall': 0.874753475189209, 'bertscore_f1': 0.8600905537605286, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.21624258160591125}, {'repo': 'tmiranda101/OC-NN_remote', 'tasks_x': 'OMT, OMT-cdn, AD, ', 'true_tasks': ['anomaly detection'], 'generated_text': 'adversarial attack', 'prompt_info': {'content': 'src/data/GTSRB.py, src/data/main.py, src/data/__local__.py, src/debug/debug_mnist.py, src/debug/debug_ocnn.py, func_getDecision_Scores_USPS, func_getDecision_Scores_FAKE_NEWS, func_getDecision_Scores_SPAM_VS_HAM, func_getDecision_Scores_CIFAR_10, func_getDecision_Scores_mnist, dropout_architecture, R_update_lp_obj, c_mean_init, warm_up_n_epochs, ae_lr_drop', 'generated_text': '\n## repository\nnaufalso/distributed-blackbox-adv-attack\n## files\nattack/google_attack.py, attack/mgrr_pso_attack.py, configure/server_configuration.py, helpers/cifar10.py, helpers/dataset_helper.py, mnist_predict, cifar10_predict, mnist_cw_predict, cifar10_cw_predict, Base64_Utils, decode_numpy, decode_numpy, decode_numpy, decode_numpy, decode_numpy\n## tags\n[adversarial attack]\n\n## repository\nadambielski/perturbed-seg\n## files\nperturber.py, perturber_utils.py, prepare_processed_lmdb.py, renderer.py, stylegan.py, LogArtifactThread, FakeImageSampler, UniformNoiseSampler, RealImageSampler, GeneratorNOutputs, used_sample, perturbs, real_penalty, lr_disc_mult, same_mixing\n## tags\n[semantic segmentation, unsupervised object segmentation]\n\n## repository\ntmiranda101/OC-NN_remote\n## files\nsrc/data/GTSRB.py, src/data/main.py, src/data/__local__.py, src/debug/debug_mnist.py, src/debug/debug_ocnn.py, func_getDecision_Scores_USPS, func_getDecision_Scores_FAKE_NEWS, func_getDecision_Scores_SPAM_VS_HAM, func_getDecision_Scores_CIFAR_10, func_getDecision_Scores_mnist, dropout_architecture, R_update_lp_obj, c_mean_init, warm_up_n_epochs, ae_lr_drop\n## 3 tags\n[OMT, OMT-cdn]\n[AD]\n## repository\n', 'id': '8aa1c39925cc1d8ba8d341628b88e559', 'name': 'tmiranda101/OC-NN_remote', 'tasks': 'OMT, OMT-cdn, AD, ', 'true_text': 'anomaly detection'}, 'generation': 0, 'input_text': '\n## repository\nnaufalso/distributed-blackbox-adv-attack\n## files\nattack/google_attack.py, attack/mgrr_pso_attack.py, configure/server_configuration.py, helpers/cifar10.py, helpers/dataset_helper.py, mnist_predict, cifar10_predict, mnist_cw_predict, cifar10_cw_predict, Base64_Utils, decode_numpy, decode_numpy, decode_numpy, decode_numpy, decode_numpy\n## tags\n[adversarial attack]\n\n## repository\nadambielski/perturbed-seg\n## files\nperturber.py, perturber_utils.py, prepare_processed_lmdb.py, renderer.py, stylegan.py, LogArtifactThread, FakeImageSampler, UniformNoiseSampler, RealImageSampler, GeneratorNOutputs, used_sample, perturbs, real_penalty, lr_disc_mult, same_mixing\n## tags\n[semantic segmentation, unsupervised object segmentation]\n\n## repository\ntmiranda101/OC-NN_remote\n## files\nsrc/data/GTSRB.py, src/data/main.py, src/data/__local__.py, src/debug/debug_mnist.py, src/debug/debug_ocnn.py, func_getDecision_Scores_USPS, func_getDecision_Scores_FAKE_NEWS, func_getDecision_Scores_SPAM_VS_HAM, func_getDecision_Scores_CIFAR_10, func_getDecision_Scores_mnist, dropout_architecture, R_update_lp_obj, c_mean_init, warm_up_n_epochs, ae_lr_drop\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/anomaly-detection-using-one-class-neural', 'https://paperswithcode.com/paper/robust-deep-and-inductive-anomaly-detection']", 'paper_titles': "['Anomaly Detection using One-Class Neural Networks', 'Robust, Deep and Inductive Anomaly Detection']", 'arxiv_ids': "['1802.06360', '1704.06743']", 'authors': '["[\'Raghavendra Chalapathy\', \'Aditya Krishna Menon\', \'Sanjay Chawla\']", "[\'Raghavendra Chalapathy\', \'Aditya Krishna Menon\', \'Sanjay Chawla\']"]', 'raw_generated_text': '\n## repository\nnaufalso/distributed-blackbox-adv-attack\n## files\nattack/google_attack.py, attack/mgrr_pso_attack.py, configure/server_configuration.py, helpers/cifar10.py, helpers/dataset_helper.py, mnist_predict, cifar10_predict, mnist_cw_predict, cifar10_cw_predict, Base64_Utils, decode_numpy, decode_numpy, decode_numpy, decode_numpy, decode_numpy\n## tags\n[adversarial attack]\n\n## repository\nadambielski/perturbed-seg\n## files\nperturber.py, perturber_utils.py, prepare_processed_lmdb.py, renderer.py, stylegan.py, LogArtifactThread, FakeImageSampler, UniformNoiseSampler, RealImageSampler, GeneratorNOutputs, used_sample, perturbs, real_penalty, lr_disc_mult, same_mixing\n## tags\n[semantic segmentation, unsupervised object segmentation]\n\n## repository\ntmiranda101/OC-NN_remote\n## files\nsrc/data/GTSRB.py, src/data/main.py, src/data/__local__.py, src/debug/debug_mnist.py, src/debug/debug_ocnn.py, func_getDecision_Scores_USPS, func_getDecision_Scores_FAKE_NEWS, func_getDecision_Scores_SPAM_VS_HAM, func_getDecision_Scores_CIFAR_10, func_getDecision_Scores_mnist, dropout_architecture, R_update_lp_obj, c_mean_init, warm_up_n_epochs, ae_lr_drop\n## 3 tags\n[OMT, OMT-cdn]\n[AD]\n## repository\n', 'raw_reference_text': 'anomaly detection', 'reference_text': 'anomaly detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8197053670883179, 'bertscore_recall': 0.8764622807502747, 'bertscore_f1': 0.8471341729164124, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2141210436820984}, {'repo': 'isht7/pytorch-deeplab-resnet', 'tasks_x': 'segmentation, segmentation loss, vgg16, ', 'true_tasks': ['semantic segmentation'], 'generated_text': 'adversarial attack', 'prompt_info': {'content': 'convert_deeplab_resnet.py, deeplab_resnet.py, evalpyt2.py, evalpyt.py, init_net_surgery.py, get_data_from_chunk_v2, resize_label_batch, get_1x_lr_params_NOscale, scale_gt, scale_im, scale_gt, scale_im, UpsamplingBilinear2d, NLLLoss2d, outS', 'generated_text': '\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## tags\n[adversarial attack]\n\n## repository\nDoubleML/doubleml-for-py\n## files\ndoubleml/datasets.py, doubleml/double_ml.py, doubleml/double_ml_data.py, doubleml/double_ml_iivm.py, doubleml/double_ml_irm.py, test_doubleml_exception_data, test_doubleml_exception_smpls, test_doubleml_exception_bootstrap, test_doubleml_exception_confint, test_doubleml_exception_p_adjust, set_ml_nuisance_params, pred_fun, _est_causal_pars_and_se, plr_score, irm_score\n## tags\n[causal inference]\n\n## repository\nisht7/pytorch-deeplab-resnet\n## files\nconvert_deeplab_resnet.py, deeplab_resnet.py, evalpyt2.py, evalpyt.py, init_net_surgery.py, get_data_from_chunk_v2, resize_label_batch, get_1x_lr_params_NOscale, scale_gt, scale_im, scale_gt, scale_im, UpsamplingBilinear2d, NLLLoss2d, outS\n## 3 tags\n[segmentation] [segmentation loss] [vgg16]\n\n## repository', 'id': 'da164e3a394e4fcabb92290366156834', 'name': 'isht7/pytorch-deeplab-resnet', 'tasks': 'segmentation, segmentation loss, vgg16, ', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## tags\n[adversarial attack]\n\n## repository\nDoubleML/doubleml-for-py\n## files\ndoubleml/datasets.py, doubleml/double_ml.py, doubleml/double_ml_data.py, doubleml/double_ml_iivm.py, doubleml/double_ml_irm.py, test_doubleml_exception_data, test_doubleml_exception_smpls, test_doubleml_exception_bootstrap, test_doubleml_exception_confint, test_doubleml_exception_p_adjust, set_ml_nuisance_params, pred_fun, _est_causal_pars_and_se, plr_score, irm_score\n## tags\n[causal inference]\n\n## repository\nisht7/pytorch-deeplab-resnet\n## files\nconvert_deeplab_resnet.py, deeplab_resnet.py, evalpyt2.py, evalpyt.py, init_net_surgery.py, get_data_from_chunk_v2, resize_label_batch, get_1x_lr_params_NOscale, scale_gt, scale_im, scale_gt, scale_im, UpsamplingBilinear2d, NLLLoss2d, outS\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/deeplab-semantic-image-segmentation-with-deep']", 'paper_titles': "['DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs']", 'arxiv_ids': "['1606.00915']", 'authors': '["[\'Liang-Chieh Chen\', \'George Papandreou\', \'Iasonas Kokkinos\', \'Kevin Murphy\', \'Alan L. Yuille\']"]', 'raw_generated_text': '\n## repository\nLightDXY/GreedyFool\n## files\nnips_black_gd.py, nips_gd.py, inception_v3.py, generators.py, options.py, Weight_Res_ResnetGenerator, conWeight_ResnetGenerator, Res_ResnetGenerator, Weight_ResnetGenerator, EX_Weight_Res_ResnetGenerator, RecursiveUnetGenerator, UnetGeneratorSC, UnetGenerator, Inception3, ResnetGenerator\n## tags\n[adversarial attack]\n\n## repository\nDoubleML/doubleml-for-py\n## files\ndoubleml/datasets.py, doubleml/double_ml.py, doubleml/double_ml_data.py, doubleml/double_ml_iivm.py, doubleml/double_ml_irm.py, test_doubleml_exception_data, test_doubleml_exception_smpls, test_doubleml_exception_bootstrap, test_doubleml_exception_confint, test_doubleml_exception_p_adjust, set_ml_nuisance_params, pred_fun, _est_causal_pars_and_se, plr_score, irm_score\n## tags\n[causal inference]\n\n## repository\nisht7/pytorch-deeplab-resnet\n## files\nconvert_deeplab_resnet.py, deeplab_resnet.py, evalpyt2.py, evalpyt.py, init_net_surgery.py, get_data_from_chunk_v2, resize_label_batch, get_1x_lr_params_NOscale, scale_gt, scale_im, scale_gt, scale_im, UpsamplingBilinear2d, NLLLoss2d, outS\n## 3 tags\n[segmentation] [segmentation loss] [vgg16]\n\n## repository', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8256839513778687, 'bertscore_recall': 0.8634448051452637, 'bertscore_f1': 0.8441423177719116, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.17320874333381653}, {'repo': 'GuoShi28/GCP-Net', 'tasks_x': 'pytorch, image processing, ', 'true_tasks': ['demosaicking', 'denoising'], 'generated_text': 'semantic correspondence', 'prompt_info': {'content': 'codes/test_real.py, codes/data/test_JDDB.py, codes/data/Vimeo90K_JDDB.py, codes/models/module_util.py, codes/models/archs/arch_gcpnet.py, rggb_tensor2img, rggb2bgr, RGB2RGGB, RGB2RGGB2, func_degamma, optimize_parameters_rgb, bayer2bgr, GCPNet, GCPNet, unprocess_meta_gt', 'generated_text': '\n## repository\nignacio-rocco/weakalign\n## files\ntrain_strong.py, data/pascal_parts_dataset.py, data/weak_dataset.py, data/caltech_dataset.py, data/tss_dataset.py, WeakInlierCount, pascal_parts_metrics, ComposedGeometricTnf, SynthTwoStageTwoPairTnf, SynthTwoPairTnf, tgrl, TpsGridRegularityLoss, pck_alpha, theta_to_sampling_grid, batch_preprocessing_fn\n## tags\n[semantic correspondence]\n\n## repository\napexrl/CoDAIL\n## files\nmulti-agent-irl/irl/mack/codail.py, multi-agent-irl/irl/mack/kfac_discriminator_codail.py, multi-agent-irl/irl/mack/kfac_discriminator_ncdail.py, multi-agent-irl/irl/mack/ncdail.py, multi-agent-irl/irl/mack/run_mack_codail.py, multionehot, lsf01, CategoricalPdOneHotType, CategoricalPdOneHot, kde_prob, kde_prob, calcloglik, param_placeholder, MAVecNormalize, MADataSet\n## tags\n[imitation learning]\n\n## repository\nGuoShi28/GCP-Net\n## files\ncodes/test_real.py, codes/data/test_JDDB.py, codes/data/Vimeo90K_JDDB.py, codes/models/module_util.py, codes/models/archs/arch_gcpnet.py, rggb_tensor2img, rggb2bgr, RGB2RGGB, RGB2RGGB2, func_degamma, optimize_parameters_rgb, bayer2bgr, GCPNet, GCPNet, unprocess_meta_gt\n## 3 tags\n[pytorch, image processing]\n\n## repository\nshihuiji/ph', 'id': 'a2809917f058e7e7842ead740166bb50', 'name': 'GuoShi28/GCP-Net', 'tasks': 'pytorch, image processing, ', 'true_text': 'demosaicking, denoising'}, 'generation': 0, 'input_text': '\n## repository\nignacio-rocco/weakalign\n## files\ntrain_strong.py, data/pascal_parts_dataset.py, data/weak_dataset.py, data/caltech_dataset.py, data/tss_dataset.py, WeakInlierCount, pascal_parts_metrics, ComposedGeometricTnf, SynthTwoStageTwoPairTnf, SynthTwoPairTnf, tgrl, TpsGridRegularityLoss, pck_alpha, theta_to_sampling_grid, batch_preprocessing_fn\n## tags\n[semantic correspondence]\n\n## repository\napexrl/CoDAIL\n## files\nmulti-agent-irl/irl/mack/codail.py, multi-agent-irl/irl/mack/kfac_discriminator_codail.py, multi-agent-irl/irl/mack/kfac_discriminator_ncdail.py, multi-agent-irl/irl/mack/ncdail.py, multi-agent-irl/irl/mack/run_mack_codail.py, multionehot, lsf01, CategoricalPdOneHotType, CategoricalPdOneHot, kde_prob, kde_prob, calcloglik, param_placeholder, MAVecNormalize, MADataSet\n## tags\n[imitation learning]\n\n## repository\nGuoShi28/GCP-Net\n## files\ncodes/test_real.py, codes/data/test_JDDB.py, codes/data/Vimeo90K_JDDB.py, codes/models/module_util.py, codes/models/archs/arch_gcpnet.py, rggb_tensor2img, rggb2bgr, RGB2RGGB, RGB2RGGB2, func_degamma, optimize_parameters_rgb, bayer2bgr, GCPNet, GCPNet, unprocess_meta_gt\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/joint-denoising-and-demosaicking-with-green']", 'paper_titles': "['Joint Denoising and Demosaicking with Green Channel Prior for Real-world Burst Images']", 'arxiv_ids': "['2101.09870']", 'authors': '["[\'Shi Guo\', \'Zhetong Liang\', \'Lei Zhang\']"]', 'raw_generated_text': '\n## repository\nignacio-rocco/weakalign\n## files\ntrain_strong.py, data/pascal_parts_dataset.py, data/weak_dataset.py, data/caltech_dataset.py, data/tss_dataset.py, WeakInlierCount, pascal_parts_metrics, ComposedGeometricTnf, SynthTwoStageTwoPairTnf, SynthTwoPairTnf, tgrl, TpsGridRegularityLoss, pck_alpha, theta_to_sampling_grid, batch_preprocessing_fn\n## tags\n[semantic correspondence]\n\n## repository\napexrl/CoDAIL\n## files\nmulti-agent-irl/irl/mack/codail.py, multi-agent-irl/irl/mack/kfac_discriminator_codail.py, multi-agent-irl/irl/mack/kfac_discriminator_ncdail.py, multi-agent-irl/irl/mack/ncdail.py, multi-agent-irl/irl/mack/run_mack_codail.py, multionehot, lsf01, CategoricalPdOneHotType, CategoricalPdOneHot, kde_prob, kde_prob, calcloglik, param_placeholder, MAVecNormalize, MADataSet\n## tags\n[imitation learning]\n\n## repository\nGuoShi28/GCP-Net\n## files\ncodes/test_real.py, codes/data/test_JDDB.py, codes/data/Vimeo90K_JDDB.py, codes/models/module_util.py, codes/models/archs/arch_gcpnet.py, rggb_tensor2img, rggb2bgr, RGB2RGGB, RGB2RGGB2, func_degamma, optimize_parameters_rgb, bayer2bgr, GCPNet, GCPNet, unprocess_meta_gt\n## 3 tags\n[pytorch, image processing]\n\n## repository\nshihuiji/ph', 'raw_reference_text': 'demosaicking, denoising', 'reference_text': 'demosaicking denoising', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8437513113021851, 'bertscore_recall': 0.7619706392288208, 'bertscore_f1': 0.8007783889770508, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1398511528968811}, {'repo': 'icc2115/Neural-GC', 'tasks_x': 'neural network, projection, learning, recurrent neural network, convolutional neural network', 'true_tasks': ['sparse learning'], 'generated_text': 'mri reconstruction', 'prompt_info': {'content': 'models/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update', 'generated_text': '\n## repository\ngeopi1/DeepMRI\n## files\nsave_raw_data_to_pickle.py, ismrmrd/generate_cartesian_shepp_logan_dataset.py, ismrmrd/ismrmrdtools/coils.py, ismrmrd/ismrmrdtools/grappa.py, ismrmrd/ismrmrdtools/imageviewer.py, RAKIDataHandler, SpatialDataHandler, RAKINetwork, read_json_with_line_comments, calculate_prewhitening, kspace_encode_step_2, coils, ismrmrd_file, ismrmrd_group, ACS_size\n## tags\n[mri reconstruction]\n\n## repository\nCoolgiserz/NLP_starter\n## files\nCodePratices/MachineLearning/pos_tagging_hmm_project/main.py, CodePratices/MachineLearning/pos_tagging_hmm_project/models/hmm.py, CodePratices/MachineLearning/pos_tagging_hmm_project/utils/corpus.py, HMMPOSTagger, CorpusHelper, test, decode, print\n## tags\n[text classification, stochastic optimization]\n\n## repository\nicc2115/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## 3 tags\n[neural network, projection, learning, recurrent neural network, convolutional neural network', 'id': '50d204fac478e41ce77d203d9c3e7006', 'name': 'icc2115/Neural-GC', 'tasks': 'neural network, projection, learning, recurrent neural network, convolutional neural network', 'true_text': 'sparse learning'}, 'generation': 0, 'input_text': '\n## repository\ngeopi1/DeepMRI\n## files\nsave_raw_data_to_pickle.py, ismrmrd/generate_cartesian_shepp_logan_dataset.py, ismrmrd/ismrmrdtools/coils.py, ismrmrd/ismrmrdtools/grappa.py, ismrmrd/ismrmrdtools/imageviewer.py, RAKIDataHandler, SpatialDataHandler, RAKINetwork, read_json_with_line_comments, calculate_prewhitening, kspace_encode_step_2, coils, ismrmrd_file, ismrmrd_group, ACS_size\n## tags\n[mri reconstruction]\n\n## repository\nCoolgiserz/NLP_starter\n## files\nCodePratices/MachineLearning/pos_tagging_hmm_project/main.py, CodePratices/MachineLearning/pos_tagging_hmm_project/models/hmm.py, CodePratices/MachineLearning/pos_tagging_hmm_project/utils/corpus.py, HMMPOSTagger, CorpusHelper, test, decode, print\n## tags\n[text classification, stochastic optimization]\n\n## repository\nicc2115/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-general-iterative-shrinkage-and']", 'paper_titles': "['A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems']", 'arxiv_ids': "['1303.4434']", 'authors': '["[\'Pinghua Gong\', \'Chang-Shui Zhang\', \'Zhaosong Lu\', \'Jianhua Huang\', \'Jieping Ye\']"]', 'raw_generated_text': '\n## repository\ngeopi1/DeepMRI\n## files\nsave_raw_data_to_pickle.py, ismrmrd/generate_cartesian_shepp_logan_dataset.py, ismrmrd/ismrmrdtools/coils.py, ismrmrd/ismrmrdtools/grappa.py, ismrmrd/ismrmrdtools/imageviewer.py, RAKIDataHandler, SpatialDataHandler, RAKINetwork, read_json_with_line_comments, calculate_prewhitening, kspace_encode_step_2, coils, ismrmrd_file, ismrmrd_group, ACS_size\n## tags\n[mri reconstruction]\n\n## repository\nCoolgiserz/NLP_starter\n## files\nCodePratices/MachineLearning/pos_tagging_hmm_project/main.py, CodePratices/MachineLearning/pos_tagging_hmm_project/models/hmm.py, CodePratices/MachineLearning/pos_tagging_hmm_project/utils/corpus.py, HMMPOSTagger, CorpusHelper, test, decode, print\n## tags\n[text classification, stochastic optimization]\n\n## repository\nicc2115/Neural-GC\n## files\nmodels/cmlp.py, models/clstm.py, models/model_helper.py, models/crnn.py, synthetic.py, activation_helper, cRNNSparse, cRNN, cLSTMSparse, cMLPSparse, cmlp, net_copy, make_var_stationary, ridge_regularize, prox_update\n## 3 tags\n[neural network, projection, learning, recurrent neural network, convolutional neural network', 'raw_reference_text': 'sparse learning', 'reference_text': 'sparse learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8249754309654236, 'bertscore_recall': 0.8729161620140076, 'bertscore_f1': 0.848268985748291, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14394252002239227}, {'repo': 'PkuRainBow/OCNet.pytorch', 'tasks_x': 'segmentation, pytorch, speech enhancement, ', 'true_tasks': ['semantic segmentation', 'scene parsing'], 'generated_text': 'multiple object tracking', 'prompt_info': {'content': 'generate_submit.py, inplace_abn_03/modules/bn.py, inplace_abn_03/modules/dense.py, inplace_abn_03/modules/functions.py, inplace_abn_03/modules/misc.py, reshape_predict_target, _quick_countless, down_sample_target_count, predict_whole_img_w_label, CriterionOhemDSN_single, fix_lr, ohem, use_flip, predict_whole_img_w_label, elu_inv_cuda', 'generated_text': '\n## repository\ndeshumake/SORT\n## files\nsort.py, convert_bbox_to_z, convert_x_to_bbox, associate_detections_to_trackers, KalmanBoxTracker, Sort, linear_assignment, iou, minimum, maximum, ArgumentParser\n## tags\n[multiple object tracking]\n\n## repository\nsremes/nssm-gp\n## files\nexperiments/get_best_results.py, experiments/run-experiment.py, nssm_gp/initializers.py, nssm_gp/neural.py, nssm_gp/spectral_kernels.py, Motorcycle, Skin, Mocap2, Sunspots, SolarRadiance, BSMKernelComponent, BSMKernel, jitter_level, self_adjoint_eigvals, self_adjoint_eigvals\n## tags\n[gaussian processes]\n\n## repository\nPkuRainBow/OCNet.pytorch\n## files\ngenerate_submit.py, inplace_abn_03/modules/bn.py, inplace_abn_03/modules/dense.py, inplace_abn_03/modules/functions.py, inplace_abn_03/modules/misc.py, reshape_predict_target, _quick_countless, down_sample_target_count, predict_whole_img_w_label, CriterionOhemDSN_single, fix_lr, ohem, use_flip, predict_whole_img_w_label, elu_inv_cuda\n## 3 tags\n[segmentation, pytorch]\n[speech enhancement]\n\n## repository\nfbcot', 'id': '8840170be5763c9f899b721d0715db84', 'name': 'PkuRainBow/OCNet.pytorch', 'tasks': 'segmentation, pytorch, speech enhancement, ', 'true_text': 'semantic segmentation, scene parsing'}, 'generation': 0, 'input_text': '\n## repository\ndeshumake/SORT\n## files\nsort.py, convert_bbox_to_z, convert_x_to_bbox, associate_detections_to_trackers, KalmanBoxTracker, Sort, linear_assignment, iou, minimum, maximum, ArgumentParser\n## tags\n[multiple object tracking]\n\n## repository\nsremes/nssm-gp\n## files\nexperiments/get_best_results.py, experiments/run-experiment.py, nssm_gp/initializers.py, nssm_gp/neural.py, nssm_gp/spectral_kernels.py, Motorcycle, Skin, Mocap2, Sunspots, SolarRadiance, BSMKernelComponent, BSMKernel, jitter_level, self_adjoint_eigvals, self_adjoint_eigvals\n## tags\n[gaussian processes]\n\n## repository\nPkuRainBow/OCNet.pytorch\n## files\ngenerate_submit.py, inplace_abn_03/modules/bn.py, inplace_abn_03/modules/dense.py, inplace_abn_03/modules/functions.py, inplace_abn_03/modules/misc.py, reshape_predict_target, _quick_countless, down_sample_target_count, predict_whole_img_w_label, CriterionOhemDSN_single, fix_lr, ohem, use_flip, predict_whole_img_w_label, elu_inv_cuda\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/interlaced-sparse-self-attention-for-semantic', 'https://paperswithcode.com/paper/ocnet-object-context-network-for-scene']", 'paper_titles': "['Interlaced Sparse Self-Attention for Semantic Segmentation', 'OCNet: Object Context Network for Scene Parsing']", 'arxiv_ids': "['1907.12273', '1809.00916']", 'authors': '["[\'Lang Huang\', \'Yuhui Yuan\', \'Jianyuan Guo\', \'Chao Zhang\', \'Xilin Chen\', \'Jingdong Wang\']", "[\'Yuhui Yuan\', \'Lang Huang\', \'Jianyuan Guo\', \'Chao Zhang\', \'Xilin Chen\', \'Jingdong Wang\']"]', 'raw_generated_text': '\n## repository\ndeshumake/SORT\n## files\nsort.py, convert_bbox_to_z, convert_x_to_bbox, associate_detections_to_trackers, KalmanBoxTracker, Sort, linear_assignment, iou, minimum, maximum, ArgumentParser\n## tags\n[multiple object tracking]\n\n## repository\nsremes/nssm-gp\n## files\nexperiments/get_best_results.py, experiments/run-experiment.py, nssm_gp/initializers.py, nssm_gp/neural.py, nssm_gp/spectral_kernels.py, Motorcycle, Skin, Mocap2, Sunspots, SolarRadiance, BSMKernelComponent, BSMKernel, jitter_level, self_adjoint_eigvals, self_adjoint_eigvals\n## tags\n[gaussian processes]\n\n## repository\nPkuRainBow/OCNet.pytorch\n## files\ngenerate_submit.py, inplace_abn_03/modules/bn.py, inplace_abn_03/modules/dense.py, inplace_abn_03/modules/functions.py, inplace_abn_03/modules/misc.py, reshape_predict_target, _quick_countless, down_sample_target_count, predict_whole_img_w_label, CriterionOhemDSN_single, fix_lr, ohem, use_flip, predict_whole_img_w_label, elu_inv_cuda\n## 3 tags\n[segmentation, pytorch]\n[speech enhancement]\n\n## repository\nfbcot', 'raw_reference_text': 'semantic segmentation, scene parsing', 'reference_text': 'semantic segmentation scene parsing', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.86430424451828, 'bertscore_recall': 0.8513598442077637, 'bertscore_f1': 0.8577831983566284, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.27639439702033997}, {'repo': 'bhatiasiddharth/MIDAS', 'tasks_x': 'sentiment analysis, part of speech tagging, ', 'true_tasks': ['anomaly detection'], 'generated_text': 'part of speech tagging word embeddings morphological tagging', 'prompt_info': {'content': 'util/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype', 'generated_text': '\n## repository\nqGentry/MetaBiLSTM\n## files\nsrc/meta_bilstm/meta_wrapper.py, src/meta_bilstm/bin/train_model.py, src/meta_bilstm/models/char_model.py, src/meta_bilstm/models/meta_model.py, src/meta_bilstm/models/word_model.py, WordBiLSTM, WordTrainableEmbeddings, WordPretrainedEmbbedings, seq_loss, MetaBiLSTM, train_data, train_model, setup, Trainer, DataLoader\n## tags\n[part of speech tagging, morphological tagging, word embeddings]\n\n## repository\ninfolab-usc/bdr-tweet\n## files\nhash_tag.py, categorization.py, convent_hash_tag.py, create_vocab_from_tweets.py, doc2vec_train.py, mbr_to_cellids, cell_coord, angle_bwn_two_points, distance_km, mbr_to_path, checkCorrectness, ZIPF_STEPS, label_folder, __is_intersect, performed_task\n## tags\n[sentiment analysis]\n\n## repository\nbhatiasiddharth/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[sentiment analysis, part of speech tagging]\n### \\# train_CNN.py', 'id': '121cca95d093b3a7ac751248f5f7aeeb', 'name': 'bhatiasiddharth/MIDAS', 'tasks': 'sentiment analysis, part of speech tagging, ', 'true_text': 'anomaly detection'}, 'generation': 0, 'input_text': '\n## repository\nqGentry/MetaBiLSTM\n## files\nsrc/meta_bilstm/meta_wrapper.py, src/meta_bilstm/bin/train_model.py, src/meta_bilstm/models/char_model.py, src/meta_bilstm/models/meta_model.py, src/meta_bilstm/models/word_model.py, WordBiLSTM, WordTrainableEmbeddings, WordPretrainedEmbbedings, seq_loss, MetaBiLSTM, train_data, train_model, setup, Trainer, DataLoader\n## tags\n[part of speech tagging, morphological tagging, word embeddings]\n\n## repository\ninfolab-usc/bdr-tweet\n## files\nhash_tag.py, categorization.py, convent_hash_tag.py, create_vocab_from_tweets.py, doc2vec_train.py, mbr_to_cellids, cell_coord, angle_bwn_two_points, distance_km, mbr_to_path, checkCorrectness, ZIPF_STEPS, label_folder, __is_intersect, performed_task\n## tags\n[sentiment analysis]\n\n## repository\nbhatiasiddharth/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/real-time-streaming-anomaly-detection-in', 'https://paperswithcode.com/paper/midas-microcluster-based-detector-of']", 'paper_titles': "['Real-Time Anomaly Detection in Edge Streams', 'MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams']", 'arxiv_ids': "['2009.08452', '1911.04464']", 'authors': '["[\'Siddharth Bhatia\', \'Rui Liu\', \'Bryan Hooi\', \'Minji Yoon\', \'Kijung Shin\', \'Christos Faloutsos\']", "[\'Siddharth Bhatia\', \'Bryan Hooi\', \'Minji Yoon\', \'Kijung Shin\', \'Christos Faloutsos\']"]', 'raw_generated_text': '\n## repository\nqGentry/MetaBiLSTM\n## files\nsrc/meta_bilstm/meta_wrapper.py, src/meta_bilstm/bin/train_model.py, src/meta_bilstm/models/char_model.py, src/meta_bilstm/models/meta_model.py, src/meta_bilstm/models/word_model.py, WordBiLSTM, WordTrainableEmbeddings, WordPretrainedEmbbedings, seq_loss, MetaBiLSTM, train_data, train_model, setup, Trainer, DataLoader\n## tags\n[part of speech tagging, morphological tagging, word embeddings]\n\n## repository\ninfolab-usc/bdr-tweet\n## files\nhash_tag.py, categorization.py, convent_hash_tag.py, create_vocab_from_tweets.py, doc2vec_train.py, mbr_to_cellids, cell_coord, angle_bwn_two_points, distance_km, mbr_to_path, checkCorrectness, ZIPF_STEPS, label_folder, __is_intersect, performed_task\n## tags\n[sentiment analysis]\n\n## repository\nbhatiasiddharth/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[sentiment analysis, part of speech tagging]\n### \\# train_CNN.py', 'raw_reference_text': 'anomaly detection', 'reference_text': 'anomaly detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8271052241325378, 'bertscore_recall': 0.8765003681182861, 'bertscore_f1': 0.8510867357254028, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.21734079718589783}, {'repo': 'zhegan27/TSBN_code_NIPS2015', 'tasks_x': 'variational autoencoder (vae), cnn, train vae on', 'true_tasks': ['time series'], 'generated_text': 'depth estimation', 'prompt_info': {'content': 'bouncing_balls/data/data_handler_bouncing_balls.py, bounce_mat, bounce_vec, new_speeds, bounce_n, show_sample, new_speeds, bounce_n, bounce_n, matricize, matricize', 'generated_text': '\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## tags\n[depth estimation]\n\n## repository\nZelgunn/MANN_tf_keras\n## files\ndatasets/omniglot/OmniglotGenerator.py, datasets/omniglot/OmniglotRawDataset.py, layers/DeepLayer.py, layers/ExternalMemoryCell.py, layers/MemoryAugmentedLayer.py, bot_k_mask, entries_in, accuracy_at_n, dirs_in, acc_at_5th, OmniglotRawDataset, MemoryAugmentedLayer, save_model_json, top_k_mask, save_model_info\n## tags\n[one shot learning]\n\n## repository\nzhegan27/TSBN_code_NIPS2015\n## files\nbouncing_balls/data/data_handler_bouncing_balls.py, bounce_mat, bounce_vec, new_speeds, bounce_n, show_sample, new_speeds, bounce_n, bounce_n, matricize, matricize\n## 3 tags\n[variational autoencoder (vae)], [cnn], [train vae on', 'id': '9b392d7a224c3fcc45fdaa0b7b081dbb', 'name': 'zhegan27/TSBN_code_NIPS2015', 'tasks': 'variational autoencoder (vae), cnn, train vae on', 'true_text': 'time series'}, 'generation': 0, 'input_text': '\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## tags\n[depth estimation]\n\n## repository\nZelgunn/MANN_tf_keras\n## files\ndatasets/omniglot/OmniglotGenerator.py, datasets/omniglot/OmniglotRawDataset.py, layers/DeepLayer.py, layers/ExternalMemoryCell.py, layers/MemoryAugmentedLayer.py, bot_k_mask, entries_in, accuracy_at_n, dirs_in, acc_at_5th, OmniglotRawDataset, MemoryAugmentedLayer, save_model_json, top_k_mask, save_model_info\n## tags\n[one shot learning]\n\n## repository\nzhegan27/TSBN_code_NIPS2015\n## files\nbouncing_balls/data/data_handler_bouncing_balls.py, bounce_mat, bounce_vec, new_speeds, bounce_n, show_sample, new_speeds, bounce_n, bounce_n, matricize, matricize\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/deep-temporal-sigmoid-belief-networks-for']", 'paper_titles': "['Deep Temporal Sigmoid Belief Networks for Sequence Modeling']", 'arxiv_ids': "['1509.07087']", 'authors': '["[\'Zhe Gan\', \'Chunyuan Li\', \'Ricardo Henao\', \'David Carlson\', \'Lawrence Carin\']"]', 'raw_generated_text': '\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## tags\n[depth estimation]\n\n## repository\nZelgunn/MANN_tf_keras\n## files\ndatasets/omniglot/OmniglotGenerator.py, datasets/omniglot/OmniglotRawDataset.py, layers/DeepLayer.py, layers/ExternalMemoryCell.py, layers/MemoryAugmentedLayer.py, bot_k_mask, entries_in, accuracy_at_n, dirs_in, acc_at_5th, OmniglotRawDataset, MemoryAugmentedLayer, save_model_json, top_k_mask, save_model_info\n## tags\n[one shot learning]\n\n## repository\nzhegan27/TSBN_code_NIPS2015\n## files\nbouncing_balls/data/data_handler_bouncing_balls.py, bounce_mat, bounce_vec, new_speeds, bounce_n, show_sample, new_speeds, bounce_n, bounce_n, matricize, matricize\n## 3 tags\n[variational autoencoder (vae)], [cnn], [train vae on', 'raw_reference_text': 'time series', 'reference_text': 'time series', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8830480575561523, 'bertscore_recall': 0.898566484451294, 'bertscore_f1': 0.8907397389411926, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2489660084247589}, {'repo': 'li3cmz/GRADE', 'tasks_x': 'semantic segmentation, human evaluation, ', 'true_tasks': ['dialogue evaluation'], 'generated_text': 'generalization bounds gaussian processes', 'prompt_info': {'content': 'main_for_metric_grade.py, main_grade.py, evaluation/human_correlation.py, evaluation/merge_keyword_and_text.py, model/evaluation_model/GRADE/model_grade_K1.py, add_loss_accu_msg, print_loss_accu_predlabel, get_adjs1, get_adjs2, maybe_create_file, hyp_format, eval_metric_name, reduced_results_path, _do_metrics, embedding_init_value', 'generated_text': '\n## repository\nboschresearch/PAC_GP\n## files\npac_gp/pac_gp_example.py, pac_gp/sparseGP_study.py, pac_gp/gp/conditionals.py, pac_gp/gp/gpr.py, pac_gp/gp/kerns.py, epsilon_loss, gibbs_risk_noiseless, clamp_and_round, PAC_INDUCING_HYP_GP, PAC_HYP_GP, binary_kl_inv, Kdiag, plot_lines, matrix_triangular_solve, matrix_band_part\n## tags\n[gaussian processes, generalization bounds]\n\n## repository\nJunMa11/SegLoss\n## files\nlosses_pytorch/boundary_loss.py, losses_pytorch/dice_loss.py, losses_pytorch/focal_loss.py, losses_pytorch/hausdorff.py, losses_pytorch/lovasz_loss.py, gt2onehot, DC_topk_ce_loss, DC_topk_focal_loss, compute_edts_forhdloss, compute_gt_dtm, AppendChannelsTransform, RemoveLabelTransform, RenameTransform, MirrorTransform, SegChannelSelectionTransform\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nli3cmz/GRADE\n## files\nmain_for_metric_grade.py, main_grade.py, evaluation/human_correlation.py, evaluation/merge_keyword_and_text.py, model/evaluation_model/GRADE/model_grade_K1.py, add_loss_accu_msg, print_loss_accu_predlabel, get_adjs1, get_adjs2, maybe_create_file, hyp_format, eval_metric_name, reduced_results_path, _do_metrics, embedding_init_value\n## 3 tags\n[semantic segmentation, human evaluation]\n\n## repository\nxinlei31', 'id': '7c9f594b8c8de6c86f9193f5c18a68b0', 'name': 'li3cmz/GRADE', 'tasks': 'semantic segmentation, human evaluation, ', 'true_text': 'dialogue evaluation'}, 'generation': 0, 'input_text': '\n## repository\nboschresearch/PAC_GP\n## files\npac_gp/pac_gp_example.py, pac_gp/sparseGP_study.py, pac_gp/gp/conditionals.py, pac_gp/gp/gpr.py, pac_gp/gp/kerns.py, epsilon_loss, gibbs_risk_noiseless, clamp_and_round, PAC_INDUCING_HYP_GP, PAC_HYP_GP, binary_kl_inv, Kdiag, plot_lines, matrix_triangular_solve, matrix_band_part\n## tags\n[gaussian processes, generalization bounds]\n\n## repository\nJunMa11/SegLoss\n## files\nlosses_pytorch/boundary_loss.py, losses_pytorch/dice_loss.py, losses_pytorch/focal_loss.py, losses_pytorch/hausdorff.py, losses_pytorch/lovasz_loss.py, gt2onehot, DC_topk_ce_loss, DC_topk_focal_loss, compute_edts_forhdloss, compute_gt_dtm, AppendChannelsTransform, RemoveLabelTransform, RenameTransform, MirrorTransform, SegChannelSelectionTransform\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nli3cmz/GRADE\n## files\nmain_for_metric_grade.py, main_grade.py, evaluation/human_correlation.py, evaluation/merge_keyword_and_text.py, model/evaluation_model/GRADE/model_grade_K1.py, add_loss_accu_msg, print_loss_accu_predlabel, get_adjs1, get_adjs2, maybe_create_file, hyp_format, eval_metric_name, reduced_results_path, _do_metrics, embedding_init_value\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/grade-automatic-graph-enhanced-coherence']", 'paper_titles': "['GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems']", 'arxiv_ids': "['2010.03994']", 'authors': '["[\'Lishan Huang\', \'Zheng Ye\', \'Jinghui Qin\', \'Liang Lin\', \'Xiaodan Liang\']"]', 'raw_generated_text': '\n## repository\nboschresearch/PAC_GP\n## files\npac_gp/pac_gp_example.py, pac_gp/sparseGP_study.py, pac_gp/gp/conditionals.py, pac_gp/gp/gpr.py, pac_gp/gp/kerns.py, epsilon_loss, gibbs_risk_noiseless, clamp_and_round, PAC_INDUCING_HYP_GP, PAC_HYP_GP, binary_kl_inv, Kdiag, plot_lines, matrix_triangular_solve, matrix_band_part\n## tags\n[gaussian processes, generalization bounds]\n\n## repository\nJunMa11/SegLoss\n## files\nlosses_pytorch/boundary_loss.py, losses_pytorch/dice_loss.py, losses_pytorch/focal_loss.py, losses_pytorch/hausdorff.py, losses_pytorch/lovasz_loss.py, gt2onehot, DC_topk_ce_loss, DC_topk_focal_loss, compute_edts_forhdloss, compute_gt_dtm, AppendChannelsTransform, RemoveLabelTransform, RenameTransform, MirrorTransform, SegChannelSelectionTransform\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nli3cmz/GRADE\n## files\nmain_for_metric_grade.py, main_grade.py, evaluation/human_correlation.py, evaluation/merge_keyword_and_text.py, model/evaluation_model/GRADE/model_grade_K1.py, add_loss_accu_msg, print_loss_accu_predlabel, get_adjs1, get_adjs2, maybe_create_file, hyp_format, eval_metric_name, reduced_results_path, _do_metrics, embedding_init_value\n## 3 tags\n[semantic segmentation, human evaluation]\n\n## repository\nxinlei31', 'raw_reference_text': 'dialogue evaluation', 'reference_text': 'dialogue evaluation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8158953785896301, 'bertscore_recall': 0.8742592334747314, 'bertscore_f1': 0.8440696001052856, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.16390365362167358}, {'repo': 'samaonline/spatial-transformer-for-3d-point-clouds', 'tasks_x': 'classification, segmentation, semantic segmentation, ', 'true_tasks': ['semantic segmentation'], 'generated_text': 'denoising relation extraction', 'prompt_info': {'content': 'point_based/part_seg/part_seg_model_deform.py, point_based/part_seg/provider.py, point_based/part_seg/test_deform.py, point_based/part_seg/train_deform.py, point_based/part_seg/transform_nets.py, offset2d, offset2d2, offsetaffine, offsetxplusb, offsetAx, g_label2color, mpi_fin, mpi_init, prob_raw, FACADE_CATEGORIES', 'generated_text': '\n## repository\nAlbertChen1991/nEM\n## files\ndata_loader.py, model.py, main.py, Y_S, BagEncoder, Z_Y, get_pre_recall, pred_RE, get_pre_recall, pred_RE, train_RE, train_epoch_RE, pr_with_threshold\n## tags\n[relation extraction, denoising]\n\n## repository\nabhi4ssj/relaynet_pytorch\n## files\nrelaynet_pytorch/data_utils.py, relaynet_pytorch/relay_net.py, relaynet_pytorch/solver.py, relaynet_pytorch/net_api/losses.py, relaynet_pytorch/net_api/sub_module.py, per_class_dice, per_class_dice, ReLayNet, ReLayNet, get_imdb_data, forward, is_cuda, File, zero_, tile\n## tags\n[semantic segmentation]\n\n## repository\nsamaonline/spatial-transformer-for-3d-point-clouds\n## files\npoint_based/part_seg/part_seg_model_deform.py, point_based/part_seg/provider.py, point_based/part_seg/test_deform.py, point_based/part_seg/train_deform.py, point_based/part_seg/transform_nets.py, offset2d, offset2d2, offsetaffine, offsetxplusb, offsetAx, g_label2color, mpi_fin, mpi_init, prob_raw, FACADE_CATEGORIES\n## 3 tags\n[classification, segmentation, semantic segmentation]\n\n## repository\nwjhjh6', 'id': 'd9f4120c6c5d10124a5d7cd99c98b83b', 'name': 'samaonline/spatial-transformer-for-3d-point-clouds', 'tasks': 'classification, segmentation, semantic segmentation, ', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\nAlbertChen1991/nEM\n## files\ndata_loader.py, model.py, main.py, Y_S, BagEncoder, Z_Y, get_pre_recall, pred_RE, get_pre_recall, pred_RE, train_RE, train_epoch_RE, pr_with_threshold\n## tags\n[relation extraction, denoising]\n\n## repository\nabhi4ssj/relaynet_pytorch\n## files\nrelaynet_pytorch/data_utils.py, relaynet_pytorch/relay_net.py, relaynet_pytorch/solver.py, relaynet_pytorch/net_api/losses.py, relaynet_pytorch/net_api/sub_module.py, per_class_dice, per_class_dice, ReLayNet, ReLayNet, get_imdb_data, forward, is_cuda, File, zero_, tile\n## tags\n[semantic segmentation]\n\n## repository\nsamaonline/spatial-transformer-for-3d-point-clouds\n## files\npoint_based/part_seg/part_seg_model_deform.py, point_based/part_seg/provider.py, point_based/part_seg/test_deform.py, point_based/part_seg/train_deform.py, point_based/part_seg/transform_nets.py, offset2d, offset2d2, offsetaffine, offsetxplusb, offsetAx, g_label2color, mpi_fin, mpi_init, prob_raw, FACADE_CATEGORIES\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/spatial-transformer-for-3d-points']", 'paper_titles': "['Spatial Transformer for 3D Point Clouds']", 'arxiv_ids': "['1906.10887']", 'authors': '["[\'Jiayun Wang\', \'Rudrasis Chakraborty\', \'Stella X. Yu\']"]', 'raw_generated_text': '\n## repository\nAlbertChen1991/nEM\n## files\ndata_loader.py, model.py, main.py, Y_S, BagEncoder, Z_Y, get_pre_recall, pred_RE, get_pre_recall, pred_RE, train_RE, train_epoch_RE, pr_with_threshold\n## tags\n[relation extraction, denoising]\n\n## repository\nabhi4ssj/relaynet_pytorch\n## files\nrelaynet_pytorch/data_utils.py, relaynet_pytorch/relay_net.py, relaynet_pytorch/solver.py, relaynet_pytorch/net_api/losses.py, relaynet_pytorch/net_api/sub_module.py, per_class_dice, per_class_dice, ReLayNet, ReLayNet, get_imdb_data, forward, is_cuda, File, zero_, tile\n## tags\n[semantic segmentation]\n\n## repository\nsamaonline/spatial-transformer-for-3d-point-clouds\n## files\npoint_based/part_seg/part_seg_model_deform.py, point_based/part_seg/provider.py, point_based/part_seg/test_deform.py, point_based/part_seg/train_deform.py, point_based/part_seg/transform_nets.py, offset2d, offset2d2, offsetaffine, offsetxplusb, offsetAx, g_label2color, mpi_fin, mpi_init, prob_raw, FACADE_CATEGORIES\n## 3 tags\n[classification, segmentation, semantic segmentation]\n\n## repository\nwjhjh6', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8132151365280151, 'bertscore_recall': 0.8645235300064087, 'bertscore_f1': 0.8380848169326782, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1184476912021637}, {'repo': 'saitcakmak/gp-sampling', 'tasks_x': 'python, torch, experiments, ', 'true_tasks': ['gaussian processes'], 'generated_text': 'text generation human detection', 'prompt_info': {'content': 'gp_sampling/base.py, gp_sampling/basis_functions.py, gp_sampling/decoupled_samplers.py, gp_sampling/thompson_samplers.py, gp_sampling/utils/layer.py, decoupled_ts, exact_ts, continuous_decoupled_ts, BayesianLinearSampler, decoupled_sampler, posterior_sampler, _create_prior_fn, _create_update_fn, batched_multinomial, _input_batch_shape', 'generated_text': '\n## repository\ntgao1337/detecting-fake-text\n## files\njson_analyze_test.py, text_analyze_test.py, backend/class_register.py, backend/__init__.py, backend/api.py, redir, get_top_k_count, get_frac_p, AbstractLanguageChecker, remove_symbols_from_text, sample_unconditional, check_probabilities, check_probabilities, BERTLM, LM\n## tags\n[human detection, text generation]\n\n## repository\nZernach/Fashion-MNIST\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## tags\n[data augmentation]\n\n## repository\nsaitcakmak/gp-sampling\n## files\ngp_sampling/base.py, gp_sampling/basis_functions.py, gp_sampling/decoupled_samplers.py, gp_sampling/thompson_samplers.py, gp_sampling/utils/layer.py, decoupled_ts, exact_ts, continuous_decoupled_ts, BayesianLinearSampler, decoupled_sampler, posterior_sampler, _create_prior_fn, _create_update_fn, batched_multinomial, _input_batch_shape\n## 3 tags\n[python, torch, experiments]\n\n## repository\npatvark/memo', 'id': 'b10da1e7f9d824f1084d143904ad51f6', 'name': 'saitcakmak/gp-sampling', 'tasks': 'python, torch, experiments, ', 'true_text': 'gaussian processes'}, 'generation': 0, 'input_text': '\n## repository\ntgao1337/detecting-fake-text\n## files\njson_analyze_test.py, text_analyze_test.py, backend/class_register.py, backend/__init__.py, backend/api.py, redir, get_top_k_count, get_frac_p, AbstractLanguageChecker, remove_symbols_from_text, sample_unconditional, check_probabilities, check_probabilities, BERTLM, LM\n## tags\n[human detection, text generation]\n\n## repository\nZernach/Fashion-MNIST\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## tags\n[data augmentation]\n\n## repository\nsaitcakmak/gp-sampling\n## files\ngp_sampling/base.py, gp_sampling/basis_functions.py, gp_sampling/decoupled_samplers.py, gp_sampling/thompson_samplers.py, gp_sampling/utils/layer.py, decoupled_ts, exact_ts, continuous_decoupled_ts, BayesianLinearSampler, decoupled_sampler, posterior_sampler, _create_prior_fn, _create_update_fn, batched_multinomial, _input_batch_shape\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/efficiently-sampling-functions-from-gaussian']", 'paper_titles': "['Efficiently Sampling Functions from Gaussian Process Posteriors']", 'arxiv_ids': "['2002.09309']", 'authors': '["[\'James T. Wilson\', \'Viacheslav Borovitskiy\', \'Alexander Terenin\', \'Peter Mostowsky\', \'Marc Peter Deisenroth\']"]', 'raw_generated_text': '\n## repository\ntgao1337/detecting-fake-text\n## files\njson_analyze_test.py, text_analyze_test.py, backend/class_register.py, backend/__init__.py, backend/api.py, redir, get_top_k_count, get_frac_p, AbstractLanguageChecker, remove_symbols_from_text, sample_unconditional, check_probabilities, check_probabilities, BERTLM, LM\n## tags\n[human detection, text generation]\n\n## repository\nZernach/Fashion-MNIST\n## files\nbenchmark/convnet.py, benchmark/runner.py, visualization/project_zalando.py, utils/mnist_reader.py, utils/argparser.py, now_int, upload_result_s3, get_sprite_image, start_s3_sync, get_json_logger, UploadS3Thread, vector_to_matrix_mnist, invert_grayscale, touch_dir, numpy_input_fn\n## tags\n[data augmentation]\n\n## repository\nsaitcakmak/gp-sampling\n## files\ngp_sampling/base.py, gp_sampling/basis_functions.py, gp_sampling/decoupled_samplers.py, gp_sampling/thompson_samplers.py, gp_sampling/utils/layer.py, decoupled_ts, exact_ts, continuous_decoupled_ts, BayesianLinearSampler, decoupled_sampler, posterior_sampler, _create_prior_fn, _create_update_fn, batched_multinomial, _input_batch_shape\n## 3 tags\n[python, torch, experiments]\n\n## repository\npatvark/memo', 'raw_reference_text': 'gaussian processes', 'reference_text': 'gaussian processes', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8475437760353088, 'bertscore_recall': 0.8306044936180115, 'bertscore_f1': 0.8389886021614075, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14070861041545868}, {'repo': 'lucasmaystre/ChessAnalysis', 'tasks_x': 'data_analysis, data_visualization, data_wrangling, ', 'true_tasks': ['time series'], 'generated_text': 'semantic parsing', 'prompt_info': {'content': 'visualization/viz.py, plot_skills, read_csv, tab10, cycle, fill_between, set_title, set_xlabel', 'generated_text': '\n## repository\nnaver/sqlova\n## files\nadd_csv.py, add_question.py, annotate_ws.py, train_decoder_layer.py, train_shallow_layer.py, get_g_wvi_bert_from_sql_i, get_cnt_sw, hs_to_idx, make_w2i_wemb, generate_w2i_wemb_e2k_headers, EG_forward, forward_EG, get_cnt_lx_list_s2s, get_cnt_lx_list_s2s, get_bert_output_s2s\n## tags\n[semantic parsing]\n\n## repository\ns3nh/pytorch-tacotron2\n## files\nutils/audio_process.py, utils/text_process.py, utils/solver.py, src/optimizer.py, hyperparams.py, FeaturePredictNet, FeaturePredictNetLoss, _inv_preemphasis, RandomBucketBatchSampler, TextAudioCollate, FeaturePredictNet, FeaturePredictNetLoss, num_chars, RandomBucketBatchSampler, TextAudioCollate\n## tags\n[speech synthesis]\n\n## repository\nlucasmaystre/ChessAnalysis\n## files\nvisualization/viz.py, plot_skills, read_csv, tab10, cycle, fill_between, set_title, set_xlabel\n## 3 tags\n[data_analysis, data_visualization, data_wrangling]\n\n##', 'id': '4ac3976ba66d54423586ab3b5b7d488f', 'name': 'lucasmaystre/ChessAnalysis', 'tasks': 'data_analysis, data_visualization, data_wrangling, ', 'true_text': 'time series'}, 'generation': 0, 'input_text': '\n## repository\nnaver/sqlova\n## files\nadd_csv.py, add_question.py, annotate_ws.py, train_decoder_layer.py, train_shallow_layer.py, get_g_wvi_bert_from_sql_i, get_cnt_sw, hs_to_idx, make_w2i_wemb, generate_w2i_wemb_e2k_headers, EG_forward, forward_EG, get_cnt_lx_list_s2s, get_cnt_lx_list_s2s, get_bert_output_s2s\n## tags\n[semantic parsing]\n\n## repository\ns3nh/pytorch-tacotron2\n## files\nutils/audio_process.py, utils/text_process.py, utils/solver.py, src/optimizer.py, hyperparams.py, FeaturePredictNet, FeaturePredictNetLoss, _inv_preemphasis, RandomBucketBatchSampler, TextAudioCollate, FeaturePredictNet, FeaturePredictNetLoss, num_chars, RandomBucketBatchSampler, TextAudioCollate\n## tags\n[speech synthesis]\n\n## repository\nlucasmaystre/ChessAnalysis\n## files\nvisualization/viz.py, plot_skills, read_csv, tab10, cycle, fill_between, set_title, set_xlabel\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/trueskill-through-time-revisiting-the-history']", 'paper_titles': "['TrueSkill Through Time: Revisiting the History of Chess']", 'arxiv_ids': '[nan]', 'authors': '["[\'Pierre Dangauthier\', \'Ralf Herbrich\', \'Tom Minka\', \'Thore Graepel\']"]', 'raw_generated_text': '\n## repository\nnaver/sqlova\n## files\nadd_csv.py, add_question.py, annotate_ws.py, train_decoder_layer.py, train_shallow_layer.py, get_g_wvi_bert_from_sql_i, get_cnt_sw, hs_to_idx, make_w2i_wemb, generate_w2i_wemb_e2k_headers, EG_forward, forward_EG, get_cnt_lx_list_s2s, get_cnt_lx_list_s2s, get_bert_output_s2s\n## tags\n[semantic parsing]\n\n## repository\ns3nh/pytorch-tacotron2\n## files\nutils/audio_process.py, utils/text_process.py, utils/solver.py, src/optimizer.py, hyperparams.py, FeaturePredictNet, FeaturePredictNetLoss, _inv_preemphasis, RandomBucketBatchSampler, TextAudioCollate, FeaturePredictNet, FeaturePredictNetLoss, num_chars, RandomBucketBatchSampler, TextAudioCollate\n## tags\n[speech synthesis]\n\n## repository\nlucasmaystre/ChessAnalysis\n## files\nvisualization/viz.py, plot_skills, read_csv, tab10, cycle, fill_between, set_title, set_xlabel\n## 3 tags\n[data_analysis, data_visualization, data_wrangling]\n\n##', 'raw_reference_text': 'time series', 'reference_text': 'time series', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8656574487686157, 'bertscore_recall': 0.8747984170913696, 'bertscore_f1': 0.8702039122581482, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14157241582870483}, {'repo': 'JeongsolKim/BiS400_term_project', 'tasks_x': 'image translation, Speech enhancement, music genre classification, ', 'true_tasks': ['style transfer'], 'generated_text': 'speech enhancement', 'prompt_info': {'content': 'Trainer_with_discriminator.py, BatchGenerator.py, Decoder.py, StyleTransfer.py, Encoder.py, MinMax_Scale, deprocessing, batchgenerator, trainer_with_discriminator, styletransfer, reduce_min, preprocess_input, reduce_max, reverse, tile', 'generated_text': '\n## repository\naleXiehta/PhoneFortifiedPerceptualLoss\n## files\nperceptual_loss/losses.py, optimizers.py, generate.py, dataset.py, models.py, rnn_collate, DeepConvolutionalUNet, DCUNet20, VoiceBankDemandDataset, cal_pesq, pesq, starmap, Pool, LongTensor, manual_seed\n## tags\n[speech enhancement]\n\n## repository\nshivaat/VMWE-Identification\n## files\nbin/average_of_evaluations.py, bin/parsemetsv2cupt.py, bin/tsvlib.py, bin/validate_cupt.py, bin/bmc_munkres/munkres.py, Alpha_Weights, GraphMaxPool, GraphAveragePool, parse_statline, tokbased_pairing, Train_Test, TSVToken, global_last_lineno, TSVSentence, record_gradient\n## tags\n[word embeddings]\n\n## repository\nJeongsolKim/BiS400_term_project\n## files\nTrainer_with_discriminator.py, BatchGenerator.py, Decoder.py, StyleTransfer.py, Encoder.py, MinMax_Scale, deprocessing, batchgenerator, trainer_with_discriminator, styletransfer, reduce_min, preprocess_input, reduce_max, reverse, tile\n## 3 tags\n[image translation] [Speech enhancement] [music genre classification]\n', 'id': '11345e9731def301e9fa39a15d4278ad', 'name': 'JeongsolKim/BiS400_term_project', 'tasks': 'image translation, Speech enhancement, music genre classification, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\naleXiehta/PhoneFortifiedPerceptualLoss\n## files\nperceptual_loss/losses.py, optimizers.py, generate.py, dataset.py, models.py, rnn_collate, DeepConvolutionalUNet, DCUNet20, VoiceBankDemandDataset, cal_pesq, pesq, starmap, Pool, LongTensor, manual_seed\n## tags\n[speech enhancement]\n\n## repository\nshivaat/VMWE-Identification\n## files\nbin/average_of_evaluations.py, bin/parsemetsv2cupt.py, bin/tsvlib.py, bin/validate_cupt.py, bin/bmc_munkres/munkres.py, Alpha_Weights, GraphMaxPool, GraphAveragePool, parse_statline, tokbased_pairing, Train_Test, TSVToken, global_last_lineno, TSVSentence, record_gradient\n## tags\n[word embeddings]\n\n## repository\nJeongsolKim/BiS400_term_project\n## files\nTrainer_with_discriminator.py, BatchGenerator.py, Decoder.py, StyleTransfer.py, Encoder.py, MinMax_Scale, deprocessing, batchgenerator, trainer_with_discriminator, styletransfer, reduce_min, preprocess_input, reduce_max, reverse, tile\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/arbitrary-style-transfer-in-real-time-with']", 'paper_titles': "['Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization']", 'arxiv_ids': "['1703.06868']", 'authors': '["[\'Xun Huang\', \'Serge Belongie\']"]', 'raw_generated_text': '\n## repository\naleXiehta/PhoneFortifiedPerceptualLoss\n## files\nperceptual_loss/losses.py, optimizers.py, generate.py, dataset.py, models.py, rnn_collate, DeepConvolutionalUNet, DCUNet20, VoiceBankDemandDataset, cal_pesq, pesq, starmap, Pool, LongTensor, manual_seed\n## tags\n[speech enhancement]\n\n## repository\nshivaat/VMWE-Identification\n## files\nbin/average_of_evaluations.py, bin/parsemetsv2cupt.py, bin/tsvlib.py, bin/validate_cupt.py, bin/bmc_munkres/munkres.py, Alpha_Weights, GraphMaxPool, GraphAveragePool, parse_statline, tokbased_pairing, Train_Test, TSVToken, global_last_lineno, TSVSentence, record_gradient\n## tags\n[word embeddings]\n\n## repository\nJeongsolKim/BiS400_term_project\n## files\nTrainer_with_discriminator.py, BatchGenerator.py, Decoder.py, StyleTransfer.py, Encoder.py, MinMax_Scale, deprocessing, batchgenerator, trainer_with_discriminator, styletransfer, reduce_min, preprocess_input, reduce_max, reverse, tile\n## 3 tags\n[image translation] [Speech enhancement] [music genre classification]\n', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.862939715385437, 'bertscore_recall': 0.8629177808761597, 'bertscore_f1': 0.8629287481307983, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2536904811859131}, {'repo': 'fromm-m/aaai2021-am-peer-reviews', 'tasks_x': 'apps, python3, threePythonIntervals threePythonIntervals.INTERVAL', 'true_tasks': ['argument mining'], 'generated_text': 'semantic correspondence', 'prompt_info': {'content': '1_scrape_data/executables/scrap_data.py, 2_clean_data/executables/cleaning_helper.py, 2_clean_data/executables/clean_dataset_graph20.py, 2_clean_data/executables/clean_dataset_iclr19.py, 2_clean_data/executables/clean_dataset_iclr20.py, n_of_tokens, iterate_over_all_reviews, merge_review_scores, merge_aurc_scores, construct_reviews, use_weight, use_topic, save_confidence, set_class_weights, tmdate', 'generated_text': '\n## repository\nliuf1990/Implicit_Dense_Correspondence\n## files\nevaluation/datasetBHCP.py, evaluation/evaluateBHCP.py, train_stage3.py, train_stage2.py, train_stage1.py, ImplicitFun, InverseImplicitFun, CD_normal_loss, EMD_loss, selfrec_loss, cate_name, nnd_dist, emd_dist, cosine_loss, sample_and_group\n## tags\n[semantic correspondence]\n\n## repository\nyaojieliu/ECCV20-STDN\n## files\nmodel/warp.py, model/dataset.py, model/config.py, model/loss.py, model/utils.py, Disc_s, generate_offset_map, tf_repeat_2d, warping, PRelu, STEPS_PER_EPOCH, _gather_pixel, nextit, Disc_s, _interpx\n## tags\n[face anti spoofing]\n\n## repository\nfromm-m/aaai2021-am-peer-reviews\n## files\n1_scrape_data/executables/scrap_data.py, 2_clean_data/executables/cleaning_helper.py, 2_clean_data/executables/clean_dataset_graph20.py, 2_clean_data/executables/clean_dataset_iclr19.py, 2_clean_data/executables/clean_dataset_iclr20.py, n_of_tokens, iterate_over_all_reviews, merge_review_scores, merge_aurc_scores, construct_reviews, use_weight, use_topic, save_confidence, set_class_weights, tmdate\n## 3 tags\n[apps], [python3]\nthreePythonIntervals\nthreePythonIntervals.INTERVAL', 'id': 'dc84702d7d11138bedb41da1ecaa9f81', 'name': 'fromm-m/aaai2021-am-peer-reviews', 'tasks': 'apps, python3, threePythonIntervals threePythonIntervals.INTERVAL', 'true_text': 'argument mining'}, 'generation': 0, 'input_text': '\n## repository\nliuf1990/Implicit_Dense_Correspondence\n## files\nevaluation/datasetBHCP.py, evaluation/evaluateBHCP.py, train_stage3.py, train_stage2.py, train_stage1.py, ImplicitFun, InverseImplicitFun, CD_normal_loss, EMD_loss, selfrec_loss, cate_name, nnd_dist, emd_dist, cosine_loss, sample_and_group\n## tags\n[semantic correspondence]\n\n## repository\nyaojieliu/ECCV20-STDN\n## files\nmodel/warp.py, model/dataset.py, model/config.py, model/loss.py, model/utils.py, Disc_s, generate_offset_map, tf_repeat_2d, warping, PRelu, STEPS_PER_EPOCH, _gather_pixel, nextit, Disc_s, _interpx\n## tags\n[face anti spoofing]\n\n## repository\nfromm-m/aaai2021-am-peer-reviews\n## files\n1_scrape_data/executables/scrap_data.py, 2_clean_data/executables/cleaning_helper.py, 2_clean_data/executables/clean_dataset_graph20.py, 2_clean_data/executables/clean_dataset_iclr19.py, 2_clean_data/executables/clean_dataset_iclr20.py, n_of_tokens, iterate_over_all_reviews, merge_review_scores, merge_aurc_scores, construct_reviews, use_weight, use_topic, save_confidence, set_class_weights, tmdate\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/argument-mining-driven-analysis-of-peer']", 'paper_titles': "['Argument Mining Driven Analysis of Peer-Reviews']", 'arxiv_ids': "['2012.07743']", 'authors': '["[\'Michael Fromm\', \'Evgeniy Faerman\', \'Max Berrendorf\', \'Siddharth Bhargava\', \'Ruoxia Qi\', \'Yao Zhang\', \'Lukas Dennert\', \'Sophia Selle\', \'Yang Mao\', \'Thomas Seidl\']"]', 'raw_generated_text': '\n## repository\nliuf1990/Implicit_Dense_Correspondence\n## files\nevaluation/datasetBHCP.py, evaluation/evaluateBHCP.py, train_stage3.py, train_stage2.py, train_stage1.py, ImplicitFun, InverseImplicitFun, CD_normal_loss, EMD_loss, selfrec_loss, cate_name, nnd_dist, emd_dist, cosine_loss, sample_and_group\n## tags\n[semantic correspondence]\n\n## repository\nyaojieliu/ECCV20-STDN\n## files\nmodel/warp.py, model/dataset.py, model/config.py, model/loss.py, model/utils.py, Disc_s, generate_offset_map, tf_repeat_2d, warping, PRelu, STEPS_PER_EPOCH, _gather_pixel, nextit, Disc_s, _interpx\n## tags\n[face anti spoofing]\n\n## repository\nfromm-m/aaai2021-am-peer-reviews\n## files\n1_scrape_data/executables/scrap_data.py, 2_clean_data/executables/cleaning_helper.py, 2_clean_data/executables/clean_dataset_graph20.py, 2_clean_data/executables/clean_dataset_iclr19.py, 2_clean_data/executables/clean_dataset_iclr20.py, n_of_tokens, iterate_over_all_reviews, merge_review_scores, merge_aurc_scores, construct_reviews, use_weight, use_topic, save_confidence, set_class_weights, tmdate\n## 3 tags\n[apps], [python3]\nthreePythonIntervals\nthreePythonIntervals.INTERVAL', 'raw_reference_text': 'argument mining', 'reference_text': 'argument mining', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8772391080856323, 'bertscore_recall': 0.881006121635437, 'bertscore_f1': 0.8791185617446899, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.16403579711914062}, {'repo': 'dgattiwsu/Res-CR-Net', 'tasks_x': 'human pose estimation; unsupervised center-view object detection, ', 'true_tasks': ['semantic segmentation'], 'generated_text': 'human object interaction detection', 'prompt_info': {'content': 'MODULES/Blocks.py, MODULES/Constants.py, MODULES/Generators.py, MODULES/Losses.py, MODULES/Networks.py, tani_coeff_nc, cce_tani_loss, weighted_tani_loss, weighted_bce_tani_loss, weighted_cce_loss, tani_loss, tani_coeff, weighted_tani_coeff, weighted_tani_coeff, weighted_dice_coeff', 'generated_text': '\n## repository\nMVIG-SJTU/DIRV\n## files\nGenerate_HICO_detection.py, test_hico-det.py, test_vcoco.py, efficientdet/help_function.py, efficientdet/hico_det_dataset.py, postprocess_hoi, postprocess_dense, postprocess_dense_union, postprocess_hoi_flip, postprocess_dense_union_flip, accumulate_batch, freeze_object_detection, freeze_bn_backbone, freeze_bn_object_detection, visual_demo\n## tags\n[human object interaction detection]\n\n## repository\nWeijian-li/unsupervised_inter_intra_landmark\n## files\nMTFAN.py, databases.py, Train_options.py, extract_data.py, losses.py, interpolated_sum_multicontour, interpolated_sum_multicontour, savetorchimg, savetorchimg, savetorchimgandpts, _set_batch, lr_fan, lr_gan, bSize, tight\n## tags\n[graph learning]\n\n## repository\ndgattiwsu/Res-CR-Net\n## files\nMODULES/Blocks.py, MODULES/Constants.py, MODULES/Generators.py, MODULES/Losses.py, MODULES/Networks.py, tani_coeff_nc, cce_tani_loss, weighted_tani_loss, weighted_bce_tani_loss, weighted_cce_loss, tani_loss, tani_coeff, weighted_tani_coeff, weighted_tani_coeff, weighted_dice_coeff\n## 3 tags\n[human pose estimation; unsupervised center-view object detection]\n\n## repository', 'id': 'ec04547cbddcb1b1b6b7ed07a7a4d17c', 'name': 'dgattiwsu/Res-CR-Net', 'tasks': 'human pose estimation; unsupervised center-view object detection, ', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\nMVIG-SJTU/DIRV\n## files\nGenerate_HICO_detection.py, test_hico-det.py, test_vcoco.py, efficientdet/help_function.py, efficientdet/hico_det_dataset.py, postprocess_hoi, postprocess_dense, postprocess_dense_union, postprocess_hoi_flip, postprocess_dense_union_flip, accumulate_batch, freeze_object_detection, freeze_bn_backbone, freeze_bn_object_detection, visual_demo\n## tags\n[human object interaction detection]\n\n## repository\nWeijian-li/unsupervised_inter_intra_landmark\n## files\nMTFAN.py, databases.py, Train_options.py, extract_data.py, losses.py, interpolated_sum_multicontour, interpolated_sum_multicontour, savetorchimg, savetorchimg, savetorchimgandpts, _set_batch, lr_fan, lr_gan, bSize, tight\n## tags\n[graph learning]\n\n## repository\ndgattiwsu/Res-CR-Net\n## files\nMODULES/Blocks.py, MODULES/Constants.py, MODULES/Generators.py, MODULES/Losses.py, MODULES/Networks.py, tani_coeff_nc, cce_tani_loss, weighted_tani_loss, weighted_bce_tani_loss, weighted_cce_loss, tani_loss, tani_coeff, weighted_tani_coeff, weighted_tani_coeff, weighted_dice_coeff\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/res-cr-net-a-residual-network-with-a-novel']", 'paper_titles': "['Res-CR-Net, a residual network with a novel architecture optimized for the semantic segmentation of microscopy images']", 'arxiv_ids': "['2004.08246']", 'authors': '["[\'Hassan Abdallah\', \'Asiri Liyanaarachchi\', \'Maranda Saigh\', \'Samantha Silvers\', \'Suzan Arslanturk\', \'Douglas J. Taatjes\', \'Lars Larsson\', \'Bhanu P. Jena\', \'Domenico L. Gatti\']"]', 'raw_generated_text': '\n## repository\nMVIG-SJTU/DIRV\n## files\nGenerate_HICO_detection.py, test_hico-det.py, test_vcoco.py, efficientdet/help_function.py, efficientdet/hico_det_dataset.py, postprocess_hoi, postprocess_dense, postprocess_dense_union, postprocess_hoi_flip, postprocess_dense_union_flip, accumulate_batch, freeze_object_detection, freeze_bn_backbone, freeze_bn_object_detection, visual_demo\n## tags\n[human object interaction detection]\n\n## repository\nWeijian-li/unsupervised_inter_intra_landmark\n## files\nMTFAN.py, databases.py, Train_options.py, extract_data.py, losses.py, interpolated_sum_multicontour, interpolated_sum_multicontour, savetorchimg, savetorchimg, savetorchimgandpts, _set_batch, lr_fan, lr_gan, bSize, tight\n## tags\n[graph learning]\n\n## repository\ndgattiwsu/Res-CR-Net\n## files\nMODULES/Blocks.py, MODULES/Constants.py, MODULES/Generators.py, MODULES/Losses.py, MODULES/Networks.py, tani_coeff_nc, cce_tani_loss, weighted_tani_loss, weighted_bce_tani_loss, weighted_cce_loss, tani_loss, tani_coeff, weighted_tani_coeff, weighted_tani_coeff, weighted_dice_coeff\n## 3 tags\n[human pose estimation; unsupervised center-view object detection]\n\n## repository', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8465425372123718, 'bertscore_recall': 0.8661289811134338, 'bertscore_f1': 0.8562237620353699, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.06688365340232849}, {'repo': 'lwgkzl/MedDG', 'tasks_x': 'medical, distance, ', 'true_tasks': ['response generation'], 'generated_text': 'stock market prediction', 'prompt_info': {'content': 'MedDG/generation/CY_DataReadandMetric.py, MedDG/generation/hred.py, MedDG/generation/moban.py, MedDG/generation/seq2seq.py, MedDG/topic_predict/baseline.py, get_fan, get_tong_pinglv, get_other_sym, get_youyin, get_tong, span, span, span, scatter_, long', 'generated_text': '\n## repository\nhjeffreywang/Failed_LSTM_network\n## files\nIntraday-240,1-LSTM.py, Intraday-240,1-RF.py, Intraday-240,3-LSTM.py, Intraday-240,3-RF.py, NextDay-240,1-LSTM.py, makeSimpleLSTM, makeCuDNNLSTM, scalar_normalize, scalar_normalize, makeLSTM, makeSimpleLSTM, makeCuDNNLSTM, makeLSTM, callbacks_req, pct_change\n## tags\n[stock market prediction]\n\n## repository\nmdabashar/TAnoGAN\n## files\nnab_dataset.py, models/recurrent_models_pyramid.py, NabDataset, DataSettings, LSTMDiscriminator, LSTMGenerator, main, NabDataset, DataSettings, print\n## tags\n[time series, anomaly detection]\n\n## repository\nlwgkzl/MedDG\n## files\nMedDG/generation/CY_DataReadandMetric.py, MedDG/generation/hred.py, MedDG/generation/moban.py, MedDG/generation/seq2seq.py, MedDG/topic_predict/baseline.py, get_fan, get_tong_pinglv, get_other_sym, get_youyin, get_tong, span, span, span, scatter_, long\n## 3 tags\n[medical, distance]\n\n## repository\nGTNL-SAS/Time-', 'id': 'e6ab3e69e1dc7d14a27eb2e6debbe35c', 'name': 'lwgkzl/MedDG', 'tasks': 'medical, distance, ', 'true_text': 'response generation'}, 'generation': 0, 'input_text': '\n## repository\nhjeffreywang/Failed_LSTM_network\n## files\nIntraday-240,1-LSTM.py, Intraday-240,1-RF.py, Intraday-240,3-LSTM.py, Intraday-240,3-RF.py, NextDay-240,1-LSTM.py, makeSimpleLSTM, makeCuDNNLSTM, scalar_normalize, scalar_normalize, makeLSTM, makeSimpleLSTM, makeCuDNNLSTM, makeLSTM, callbacks_req, pct_change\n## tags\n[stock market prediction]\n\n## repository\nmdabashar/TAnoGAN\n## files\nnab_dataset.py, models/recurrent_models_pyramid.py, NabDataset, DataSettings, LSTMDiscriminator, LSTMGenerator, main, NabDataset, DataSettings, print\n## tags\n[time series, anomaly detection]\n\n## repository\nlwgkzl/MedDG\n## files\nMedDG/generation/CY_DataReadandMetric.py, MedDG/generation/hred.py, MedDG/generation/moban.py, MedDG/generation/seq2seq.py, MedDG/topic_predict/baseline.py, get_fan, get_tong_pinglv, get_other_sym, get_youyin, get_tong, span, span, span, scatter_, long\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/meddg-a-large-scale-medical-consultation']", 'paper_titles': "['MedDG: A Large-scale Medical Consultation Dataset for Building Medical Dialogue System']", 'arxiv_ids': "['2010.07497']", 'authors': '["[\'Wenge Liu\', \'Jianheng Tang\', \'Jinghui Qin\', \'Lin Xu\', \'Zhen Li\', \'Xiaodan Liang\']"]', 'raw_generated_text': '\n## repository\nhjeffreywang/Failed_LSTM_network\n## files\nIntraday-240,1-LSTM.py, Intraday-240,1-RF.py, Intraday-240,3-LSTM.py, Intraday-240,3-RF.py, NextDay-240,1-LSTM.py, makeSimpleLSTM, makeCuDNNLSTM, scalar_normalize, scalar_normalize, makeLSTM, makeSimpleLSTM, makeCuDNNLSTM, makeLSTM, callbacks_req, pct_change\n## tags\n[stock market prediction]\n\n## repository\nmdabashar/TAnoGAN\n## files\nnab_dataset.py, models/recurrent_models_pyramid.py, NabDataset, DataSettings, LSTMDiscriminator, LSTMGenerator, main, NabDataset, DataSettings, print\n## tags\n[time series, anomaly detection]\n\n## repository\nlwgkzl/MedDG\n## files\nMedDG/generation/CY_DataReadandMetric.py, MedDG/generation/hred.py, MedDG/generation/moban.py, MedDG/generation/seq2seq.py, MedDG/topic_predict/baseline.py, get_fan, get_tong_pinglv, get_other_sym, get_youyin, get_tong, span, span, span, scatter_, long\n## 3 tags\n[medical, distance]\n\n## repository\nGTNL-SAS/Time-', 'raw_reference_text': 'response generation', 'reference_text': 'response generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8693913221359253, 'bertscore_recall': 0.8717927932739258, 'bertscore_f1': 0.8705903887748718, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23397953808307648}, {'repo': 'MohdDanish/FastPhotoStyle1', 'tasks_x': 'segmentation, presentation, style transfer, ', 'true_tasks': ['image stylization'], 'generated_text': 'imitation learning', 'prompt_info': {'content': 'demo_with_ade20k_ssn.py, photo_gif.py, process_stylization.py, process_stylization_ade20k_ssn.py, process_stylization_folder.py, photo_wct_loader, SegReMapping, memory_limit_image_resize, ReMapping, weight_assign, padding_constant, _bilateral_smooth_kernel, _best_local_affine_kernel, _reconstruction_best_kernel, imgSize', 'generated_text': '\n## repository\njerrylin1121/BCO\n## files\nmodels/bco_cartpole.py, models/bco.py, models/utils.py, get_shuffle_idx, BCO_cartpole, BCO, bias_initializer, weight_initializer, shuffle, arange, append\n## tags\n[imitation learning]\n\n## repository\nnikhgarg/EmbeddingDynamicStereotypes\n## files\nchanges_over_time.py, create_final_plots_all.py, latexify.py, plot_creation.py, variance_only_over_time.py, occupation_func_female_logitprop, occupation_func_female_percent, occupation_func_whitehispanic_logitprop, occupation_func_whitehispanic_percent, occupation_func_whiteasian_logitprop, set_plots_folder, load_mturkstereotype_data, test_phase_shift_heatmap, as_latex, as_latex\n## tags\n[word embeddings]\n\n## repository\nMohdDanish/FastPhotoStyle1\n## files\ndemo_with_ade20k_ssn.py, photo_gif.py, process_stylization.py, process_stylization_ade20k_ssn.py, process_stylization_folder.py, photo_wct_loader, SegReMapping, memory_limit_image_resize, ReMapping, weight_assign, padding_constant, _bilateral_smooth_kernel, _best_local_affine_kernel, _reconstruction_best_kernel, imgSize\n## 3 tags\n[segmentation]\n\n[presentation]\n\n[style transfer]\n\n##', 'id': '3ffc634daece40e8005da0933c3b71a5', 'name': 'MohdDanish/FastPhotoStyle1', 'tasks': 'segmentation, presentation, style transfer, ', 'true_text': 'image stylization'}, 'generation': 0, 'input_text': '\n## repository\njerrylin1121/BCO\n## files\nmodels/bco_cartpole.py, models/bco.py, models/utils.py, get_shuffle_idx, BCO_cartpole, BCO, bias_initializer, weight_initializer, shuffle, arange, append\n## tags\n[imitation learning]\n\n## repository\nnikhgarg/EmbeddingDynamicStereotypes\n## files\nchanges_over_time.py, create_final_plots_all.py, latexify.py, plot_creation.py, variance_only_over_time.py, occupation_func_female_logitprop, occupation_func_female_percent, occupation_func_whitehispanic_logitprop, occupation_func_whitehispanic_percent, occupation_func_whiteasian_logitprop, set_plots_folder, load_mturkstereotype_data, test_phase_shift_heatmap, as_latex, as_latex\n## tags\n[word embeddings]\n\n## repository\nMohdDanish/FastPhotoStyle1\n## files\ndemo_with_ade20k_ssn.py, photo_gif.py, process_stylization.py, process_stylization_ade20k_ssn.py, process_stylization_folder.py, photo_wct_loader, SegReMapping, memory_limit_image_resize, ReMapping, weight_assign, padding_constant, _bilateral_smooth_kernel, _best_local_affine_kernel, _reconstruction_best_kernel, imgSize\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-closed-form-solution-to-photorealistic']", 'paper_titles': "['A Closed-form Solution to Photorealistic Image Stylization']", 'arxiv_ids': "['1802.06474']", 'authors': '["[\'Yijun Li\', \'Ming-Yu Liu\', \'Xueting Li\', \'Ming-Hsuan Yang\', \'Jan Kautz\']"]', 'raw_generated_text': '\n## repository\njerrylin1121/BCO\n## files\nmodels/bco_cartpole.py, models/bco.py, models/utils.py, get_shuffle_idx, BCO_cartpole, BCO, bias_initializer, weight_initializer, shuffle, arange, append\n## tags\n[imitation learning]\n\n## repository\nnikhgarg/EmbeddingDynamicStereotypes\n## files\nchanges_over_time.py, create_final_plots_all.py, latexify.py, plot_creation.py, variance_only_over_time.py, occupation_func_female_logitprop, occupation_func_female_percent, occupation_func_whitehispanic_logitprop, occupation_func_whitehispanic_percent, occupation_func_whiteasian_logitprop, set_plots_folder, load_mturkstereotype_data, test_phase_shift_heatmap, as_latex, as_latex\n## tags\n[word embeddings]\n\n## repository\nMohdDanish/FastPhotoStyle1\n## files\ndemo_with_ade20k_ssn.py, photo_gif.py, process_stylization.py, process_stylization_ade20k_ssn.py, process_stylization_folder.py, photo_wct_loader, SegReMapping, memory_limit_image_resize, ReMapping, weight_assign, padding_constant, _bilateral_smooth_kernel, _best_local_affine_kernel, _reconstruction_best_kernel, imgSize\n## 3 tags\n[segmentation]\n\n[presentation]\n\n[style transfer]\n\n##', 'raw_reference_text': 'image stylization', 'reference_text': 'image stylization', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8944851160049438, 'bertscore_recall': 0.8719388842582703, 'bertscore_f1': 0.8830681443214417, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.16277527809143066}, {'repo': 'shrebox/Personified-Chatbot-I-am-Kalam', 'tasks_x': 'dynamic consumption, uri normalization, extract questions, ', 'true_tasks': ['response generation'], 'generated_text': 'video prediction', 'prompt_info': {'content': 'code/evaluation/pipeline.py, code/evaluation/sent2vec_my.py, code/IR-IE/sent2vec_my.py, code/preprocess/S_extract_answers.py, code/preprocess/S_goodreadsscrapper.py, filter_dataown, filter_unkown, zero_padown, sentence_similarilty_wmd, get_qna, sequences_add_end_id, remove_pad_sequences, remove_pad_sequences, Sent2vecModel, find_author', 'generated_text': '\n## repository\nryanhoque/fabric-vsf\n## files\nvismpc/cost_functions.py, vismpc/mpc.py, vismpc/SV2P.py, vismpc/visualize.py, vismpc/dqn/dqn.py, OracleCornerWrinklePolicy, mask_pos_lt, mask_pos_lt, mask_pos_gt, mask_pos_gt, policy_and_value_get_params, policy_and_value_optimizer_fn, policy_and_value_net_fn, sparse_passthrough_c, sparse_passthrough_r\n## tags\n[video prediction]\n\n## repository\nbordesf/Infusion\n## files\nparzen.py, normalization.py, run.py, create_MNIST_data_streams, get_probabilites, build_MLP_mnist_bn, run_chaine_train_for, run_chaine_samples_for, Compute_probs_gaussian, get_first_term, compute_step_train, compute_step_samples, samples_mix\n## tags\n[denoising]\n\n## repository\nshrebox/Personified-Chatbot-I-am-Kalam\n## files\ncode/evaluation/pipeline.py, code/evaluation/sent2vec_my.py, code/IR-IE/sent2vec_my.py, code/preprocess/S_extract_answers.py, code/preprocess/S_goodreadsscrapper.py, filter_dataown, filter_unkown, zero_padown, sentence_similarilty_wmd, get_qna, sequences_add_end_id, remove_pad_sequences, remove_pad_sequences, Sent2vecModel, find_author\n## 3 tags\n[dynamic consumption]\n[uri normalization]\n[extract questions]\n\n##', 'id': '8f267907ad8cb68caf85978f1b4ed056', 'name': 'shrebox/Personified-Chatbot-I-am-Kalam', 'tasks': 'dynamic consumption, uri normalization, extract questions, ', 'true_text': 'response generation'}, 'generation': 0, 'input_text': '\n## repository\nryanhoque/fabric-vsf\n## files\nvismpc/cost_functions.py, vismpc/mpc.py, vismpc/SV2P.py, vismpc/visualize.py, vismpc/dqn/dqn.py, OracleCornerWrinklePolicy, mask_pos_lt, mask_pos_lt, mask_pos_gt, mask_pos_gt, policy_and_value_get_params, policy_and_value_optimizer_fn, policy_and_value_net_fn, sparse_passthrough_c, sparse_passthrough_r\n## tags\n[video prediction]\n\n## repository\nbordesf/Infusion\n## files\nparzen.py, normalization.py, run.py, create_MNIST_data_streams, get_probabilites, build_MLP_mnist_bn, run_chaine_train_for, run_chaine_samples_for, Compute_probs_gaussian, get_first_term, compute_step_train, compute_step_samples, samples_mix\n## tags\n[denoising]\n\n## repository\nshrebox/Personified-Chatbot-I-am-Kalam\n## files\ncode/evaluation/pipeline.py, code/evaluation/sent2vec_my.py, code/IR-IE/sent2vec_my.py, code/preprocess/S_extract_answers.py, code/preprocess/S_goodreadsscrapper.py, filter_dataown, filter_unkown, zero_padown, sentence_similarilty_wmd, get_qna, sequences_add_end_id, remove_pad_sequences, remove_pad_sequences, Sent2vecModel, find_author\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-persona-based-neural-conversation-model']", 'paper_titles': "['A Persona-Based Neural Conversation Model']", 'arxiv_ids': "['1603.06155']", 'authors': '["[\'Jiwei Li\', \'Michel Galley\', \'Chris Brockett\', \'Georgios P. Spithourakis\', \'Jianfeng Gao\', \'Bill Dolan\']"]', 'raw_generated_text': '\n## repository\nryanhoque/fabric-vsf\n## files\nvismpc/cost_functions.py, vismpc/mpc.py, vismpc/SV2P.py, vismpc/visualize.py, vismpc/dqn/dqn.py, OracleCornerWrinklePolicy, mask_pos_lt, mask_pos_lt, mask_pos_gt, mask_pos_gt, policy_and_value_get_params, policy_and_value_optimizer_fn, policy_and_value_net_fn, sparse_passthrough_c, sparse_passthrough_r\n## tags\n[video prediction]\n\n## repository\nbordesf/Infusion\n## files\nparzen.py, normalization.py, run.py, create_MNIST_data_streams, get_probabilites, build_MLP_mnist_bn, run_chaine_train_for, run_chaine_samples_for, Compute_probs_gaussian, get_first_term, compute_step_train, compute_step_samples, samples_mix\n## tags\n[denoising]\n\n## repository\nshrebox/Personified-Chatbot-I-am-Kalam\n## files\ncode/evaluation/pipeline.py, code/evaluation/sent2vec_my.py, code/IR-IE/sent2vec_my.py, code/preprocess/S_extract_answers.py, code/preprocess/S_goodreadsscrapper.py, filter_dataown, filter_unkown, zero_padown, sentence_similarilty_wmd, get_qna, sequences_add_end_id, remove_pad_sequences, remove_pad_sequences, Sent2vecModel, find_author\n## 3 tags\n[dynamic consumption]\n[uri normalization]\n[extract questions]\n\n##', 'raw_reference_text': 'response generation', 'reference_text': 'response generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8991152048110962, 'bertscore_recall': 0.8973630666732788, 'bertscore_f1': 0.8982383012771606, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.31021612882614136}, {'repo': 'mickeystroller/TaxoExpan', 'tasks_x': 'KG entity prediction, knowledge graph embedding, kg entity prediction, knowledge', 'true_tasks': ['product recommendation'], 'generated_text': 'depth estimation monocular depth estimation autonomous driving', 'prompt_info': {'content': 'generate_dataset_binary.py, test_fast.py, baselines/simple_parent.py, baselines/simple_structure.py, baselines/simple_MLP/data_loader.py, calculate_ranks_from_similarities, micro_mr, combined_metrics, TaxoExpan, square_exp_loss, node2parents, all_positions, train_node_ids, test_node_ids, validation_node_ids', 'generated_text': '\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDeep-Imaging-Group/RED-WGAN\n## files\nmixdata.py, model5.py, model7.py, model2.py, preprocessing.py, generate_noised_mri, patch_test_img, merge_test_img, MRIValidDataset, MRIValidDataset, add_rice_noise, BatchNorm3d, Nifti1Image, kaiming_normal_, affine\n## tags\n[denoising]\n\n## repository\nmickeystroller/TaxoExpan\n## files\ngenerate_dataset_binary.py, test_fast.py, baselines/simple_parent.py, baselines/simple_structure.py, baselines/simple_MLP/data_loader.py, calculate_ranks_from_similarities, micro_mr, combined_metrics, TaxoExpan, square_exp_loss, node2parents, all_positions, train_node_ids, test_node_ids, validation_node_ids\n## 3 tags\n[KG entity prediction, knowledge graph embedding]\n[kg entity prediction, , knowledge', 'id': '8109ba65784c0343a57a73c056106e07', 'name': 'mickeystroller/TaxoExpan', 'tasks': 'KG entity prediction, knowledge graph embedding, kg entity prediction, knowledge', 'true_text': 'product recommendation'}, 'generation': 0, 'input_text': '\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDeep-Imaging-Group/RED-WGAN\n## files\nmixdata.py, model5.py, model7.py, model2.py, preprocessing.py, generate_noised_mri, patch_test_img, merge_test_img, MRIValidDataset, MRIValidDataset, add_rice_noise, BatchNorm3d, Nifti1Image, kaiming_normal_, affine\n## tags\n[denoising]\n\n## repository\nmickeystroller/TaxoExpan\n## files\ngenerate_dataset_binary.py, test_fast.py, baselines/simple_parent.py, baselines/simple_structure.py, baselines/simple_MLP/data_loader.py, calculate_ranks_from_similarities, micro_mr, combined_metrics, TaxoExpan, square_exp_loss, node2parents, all_positions, train_node_ids, test_node_ids, validation_node_ids\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/taxoexpan-self-supervised-taxonomy-expansion']", 'paper_titles': "['TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network']", 'arxiv_ids': "['2001.09522']", 'authors': '["[\'Jiaming Shen\', \'Zhihong Shen\', \'Chenyan Xiong\', \'Chi Wang\', \'Kuansan Wang\', \'Jiawei Han\']"]', 'raw_generated_text': '\n## repository\nYichengWu/PhaseCam3D\n## files\ndepth_estimation_test.py, depth_estimation.py, Network.py, UNet_2, cost_grad, add_SDGN, cnnLayer, cnn3x3, cnnLayer, cnn3x3, cost_rms, image_gradients, data_augment\n## tags\n[autonomous driving, monocular depth estimation, depth estimation]\n\n## repository\nDeep-Imaging-Group/RED-WGAN\n## files\nmixdata.py, model5.py, model7.py, model2.py, preprocessing.py, generate_noised_mri, patch_test_img, merge_test_img, MRIValidDataset, MRIValidDataset, add_rice_noise, BatchNorm3d, Nifti1Image, kaiming_normal_, affine\n## tags\n[denoising]\n\n## repository\nmickeystroller/TaxoExpan\n## files\ngenerate_dataset_binary.py, test_fast.py, baselines/simple_parent.py, baselines/simple_structure.py, baselines/simple_MLP/data_loader.py, calculate_ranks_from_similarities, micro_mr, combined_metrics, TaxoExpan, square_exp_loss, node2parents, all_positions, train_node_ids, test_node_ids, validation_node_ids\n## 3 tags\n[KG entity prediction, knowledge graph embedding]\n[kg entity prediction, , knowledge', 'raw_reference_text': 'product recommendation', 'reference_text': 'product recommendation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7994191646575928, 'bertscore_recall': 0.8464093208312988, 'bertscore_f1': 0.8222434520721436, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.07788597047328949}, {'repo': 'AaltoVision/MaskMVS', 'tasks_x': 'image/sparse matching, ', 'true_tasks': ['depth estimation'], 'generated_text': 'selection bias', 'prompt_info': {'content': 'generate_volume.py, models/MaskNet.py, models/DispNet.py, models/__init__.py, generate_volume, gen_mask_gt, mask_layer, warping_neighbor, up_conv_layer, warping_neighbor, grid_sample, bmm, FloatTensor, expand', 'generated_text': '\n## repository\nghzhang233/Leakage-Neutral-Learning-for-QuoraQP\n## files\ndebias/main.py, debias/make_data.py, debias/utils.py, quantify/leaky_predict.py, quantify/propensity.py, make_model_f, make_model_g, make_model_k, make_model_adn, extract_leakage, extract_leakage, extract_unlexicalized, extract_deepwalk, extract_network_based, model_k\n## tags\n[selection bias]\n\n## repository\nv1n337/semeval2017-task5\n## files\nfpb_semeval_doc2vec_hl.py, semeval_hl.py, entities/fpb_tagged_line_document.py, entities/semeval_tagged_line_document.py, processors/bigram_processor.py, get_article_details, annotate_test_set, train_svm_regressor, train_gnb_classifier, train_xgboost_regressor, DocvecProcessorCrossval, FPBDocvecProcessor, get_articles_list, LinearSVR, Doc2Vec\n## tags\n[sentiment analysis, data augmentation]\n\n## repository\nAaltoVision/MaskMVS\n## files\ngenerate_volume.py, models/MaskNet.py, models/DispNet.py, models/__init__.py, generate_volume, gen_mask_gt, mask_layer, warping_neighbor, up_conv_layer, warping_neighbor, grid_sample, bmm, FloatTensor, expand\n## 3 tags\n[image/sparse matching]\n\n## repository\nlwwang/RNLB', 'id': '53b529153a07c5a7ad76f444f6849bf0', 'name': 'AaltoVision/MaskMVS', 'tasks': 'image/sparse matching, ', 'true_text': 'depth estimation'}, 'generation': 0, 'input_text': '\n## repository\nghzhang233/Leakage-Neutral-Learning-for-QuoraQP\n## files\ndebias/main.py, debias/make_data.py, debias/utils.py, quantify/leaky_predict.py, quantify/propensity.py, make_model_f, make_model_g, make_model_k, make_model_adn, extract_leakage, extract_leakage, extract_unlexicalized, extract_deepwalk, extract_network_based, model_k\n## tags\n[selection bias]\n\n## repository\nv1n337/semeval2017-task5\n## files\nfpb_semeval_doc2vec_hl.py, semeval_hl.py, entities/fpb_tagged_line_document.py, entities/semeval_tagged_line_document.py, processors/bigram_processor.py, get_article_details, annotate_test_set, train_svm_regressor, train_gnb_classifier, train_xgboost_regressor, DocvecProcessorCrossval, FPBDocvecProcessor, get_articles_list, LinearSVR, Doc2Vec\n## tags\n[sentiment analysis, data augmentation]\n\n## repository\nAaltoVision/MaskMVS\n## files\ngenerate_volume.py, models/MaskNet.py, models/DispNet.py, models/__init__.py, generate_volume, gen_mask_gt, mask_layer, warping_neighbor, up_conv_layer, warping_neighbor, grid_sample, bmm, FloatTensor, expand\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/unstructured-multi-view-depth-estimation']", 'paper_titles': "['Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane Representation']", 'arxiv_ids': "['1902.02166']", 'authors': '["[\'Yuxin Hou\', \'Arno Solin\', \'Juho Kannala\']"]', 'raw_generated_text': '\n## repository\nghzhang233/Leakage-Neutral-Learning-for-QuoraQP\n## files\ndebias/main.py, debias/make_data.py, debias/utils.py, quantify/leaky_predict.py, quantify/propensity.py, make_model_f, make_model_g, make_model_k, make_model_adn, extract_leakage, extract_leakage, extract_unlexicalized, extract_deepwalk, extract_network_based, model_k\n## tags\n[selection bias]\n\n## repository\nv1n337/semeval2017-task5\n## files\nfpb_semeval_doc2vec_hl.py, semeval_hl.py, entities/fpb_tagged_line_document.py, entities/semeval_tagged_line_document.py, processors/bigram_processor.py, get_article_details, annotate_test_set, train_svm_regressor, train_gnb_classifier, train_xgboost_regressor, DocvecProcessorCrossval, FPBDocvecProcessor, get_articles_list, LinearSVR, Doc2Vec\n## tags\n[sentiment analysis, data augmentation]\n\n## repository\nAaltoVision/MaskMVS\n## files\ngenerate_volume.py, models/MaskNet.py, models/DispNet.py, models/__init__.py, generate_volume, gen_mask_gt, mask_layer, warping_neighbor, up_conv_layer, warping_neighbor, grid_sample, bmm, FloatTensor, expand\n## 3 tags\n[image/sparse matching]\n\n## repository\nlwwang/RNLB', 'raw_reference_text': 'depth estimation', 'reference_text': 'depth estimation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8794615268707275, 'bertscore_recall': 0.8960597515106201, 'bertscore_f1': 0.8876830339431763, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.321125328540802}, {'repo': 'mounicam/hashtag_master', 'tasks_x': 'entity recogntion, cardinal digits, ', 'true_tasks': ['sentiment analysis'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'neural_ranker/config.py, neural_ranker/main.py, neural_ranker/metrics.py, neural_ranker/rerank.py, neural_ranker/features/counts.py, segment_word, expand_gold_truths, _expand_gold_truths, CountFeatures, Hashtag, train_topk, segment_word, expand_gold_truths, MRMultiRanker, MRRanker', 'generated_text': '\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## tags\n[style transfer]\n\n## repository\ngombru/TextFCN\n## files\ncompute_heatmaps.py, draw_net_structure.py, voc-fcn8s-atonce/net.py, voc-fcn8s-atonce/solve.py, surgery.py, SYNTHTEXTDataLayer, COCODataLayer, ICDARDataLayer, VOCSegDataLayer, SBDDSegDataLayer, scale_pool3, scale_pool4, score_fr, score_pool4c, score_pool4\n## tags\n[scene text recognition]\n\n## repository\nmounicam/hashtag_master\n## files\nneural_ranker/config.py, neural_ranker/main.py, neural_ranker/metrics.py, neural_ranker/rerank.py, neural_ranker/features/counts.py, segment_word, expand_gold_truths, _expand_gold_truths, CountFeatures, Hashtag, train_topk, segment_word, expand_gold_truths, MRMultiRanker, MRRanker\n## 3 tags\n[entity recogntion]\n[cardinal digits]\n## repository\n', 'id': '64eb0379ca1e5cdc548ba2dd077869f5', 'name': 'mounicam/hashtag_master', 'tasks': 'entity recogntion, cardinal digits, ', 'true_text': 'sentiment analysis'}, 'generation': 0, 'input_text': '\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## tags\n[style transfer]\n\n## repository\ngombru/TextFCN\n## files\ncompute_heatmaps.py, draw_net_structure.py, voc-fcn8s-atonce/net.py, voc-fcn8s-atonce/solve.py, surgery.py, SYNTHTEXTDataLayer, COCODataLayer, ICDARDataLayer, VOCSegDataLayer, SBDDSegDataLayer, scale_pool3, scale_pool4, score_fr, score_pool4c, score_pool4\n## tags\n[scene text recognition]\n\n## repository\nmounicam/hashtag_master\n## files\nneural_ranker/config.py, neural_ranker/main.py, neural_ranker/metrics.py, neural_ranker/rerank.py, neural_ranker/features/counts.py, segment_word, expand_gold_truths, _expand_gold_truths, CountFeatures, Hashtag, train_topk, segment_word, expand_gold_truths, MRMultiRanker, MRRanker\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/190600790']", 'paper_titles': "['Multi-task Pairwise Neural Ranking for Hashtag Segmentation']", 'arxiv_ids': "['1906.00790']", 'authors': '["[\'Mounica Maddela\', \'Wei Xu\', \'Daniel Preoiuc-Pietro\']"]', 'raw_generated_text': '\n## repository\nLouieYang/stroke-controllable-fast-style-transfer\n## files\nnetdef.py, inference_style_transfer.py, pack_model.py, vgg19/vgg.py, model.py, shortcut_interpolation, unpreprocess, _conv_tranpose_layer, _conv_init_vars, _instance_norm, continue_train, finetune_model, _conv_tranpose_layer, _conv_init_vars, _conv_init_vars\n## tags\n[style transfer]\n\n## repository\ngombru/TextFCN\n## files\ncompute_heatmaps.py, draw_net_structure.py, voc-fcn8s-atonce/net.py, voc-fcn8s-atonce/solve.py, surgery.py, SYNTHTEXTDataLayer, COCODataLayer, ICDARDataLayer, VOCSegDataLayer, SBDDSegDataLayer, scale_pool3, scale_pool4, score_fr, score_pool4c, score_pool4\n## tags\n[scene text recognition]\n\n## repository\nmounicam/hashtag_master\n## files\nneural_ranker/config.py, neural_ranker/main.py, neural_ranker/metrics.py, neural_ranker/rerank.py, neural_ranker/features/counts.py, segment_word, expand_gold_truths, _expand_gold_truths, CountFeatures, Hashtag, train_topk, segment_word, expand_gold_truths, MRMultiRanker, MRRanker\n## 3 tags\n[entity recogntion]\n[cardinal digits]\n## repository\n', 'raw_reference_text': 'sentiment analysis', 'reference_text': 'sentiment analysis', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8895980715751648, 'bertscore_recall': 0.8990515470504761, 'bertscore_f1': 0.8942998051643372, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1360015869140625}, {'repo': 'xiyou3368/DGVAE', 'tasks_x': 'spectrum-value, graph neural network, disordered graph, ', 'true_tasks': ['link prediction', 'graph generation', 'graph clustering'], 'generated_text': 'scene text recognition optical character recognition', 'prompt_info': {'content': 'dgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph', 'generated_text': '\n## repository\nsgenza/tf_crnn\n## files\nmodel/crnn.py, model/dataset.py, model/utils.py, model/__init__.py, evaluate.py, arrays2labels, DataLoaderSVT, DataLoaderSynth90K, DataLoaderIIIT5K, sparse_tuple_from, _get_args, read_config, set_random_seed, config, ArgumentParser\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nmberr/ea-active-learning\n## files\nexecutables/collate_results.py, executables/evaluate_active_learning_heuristic.py, src/kgm/active_learning/base.py, src/kgm/active_learning/bayesian.py, src/kgm/active_learning/graph.py, DWY100k, DBP15kFull, generalized_k_means, CSLSNormalization, SimilarityNormalization, norm_class, tracking_uri, init_class, dataset_loader_cls, with_inverse_triples\n## tags\n[active learning, entity alignment]\n\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## 3 tags\n[spectrum-value, graph neural network, disordered graph]\n\n\n\n## single', 'id': '50a73ddd15af88eb86ead5f838a2c973', 'name': 'xiyou3368/DGVAE', 'tasks': 'spectrum-value, graph neural network, disordered graph, ', 'true_text': 'link prediction, graph generation, graph clustering'}, 'generation': 0, 'input_text': '\n## repository\nsgenza/tf_crnn\n## files\nmodel/crnn.py, model/dataset.py, model/utils.py, model/__init__.py, evaluate.py, arrays2labels, DataLoaderSVT, DataLoaderSynth90K, DataLoaderIIIT5K, sparse_tuple_from, _get_args, read_config, set_random_seed, config, ArgumentParser\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nmberr/ea-active-learning\n## files\nexecutables/collate_results.py, executables/evaluate_active_learning_heuristic.py, src/kgm/active_learning/base.py, src/kgm/active_learning/bayesian.py, src/kgm/active_learning/graph.py, DWY100k, DBP15kFull, generalized_k_means, CSLSNormalization, SimilarityNormalization, norm_class, tracking_uri, init_class, dataset_loader_cls, with_inverse_triples\n## tags\n[active learning, entity alignment]\n\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/variational-graph-auto-encoders', 'https://paperswithcode.com/paper/dirichlet-graph-variational-autoencoder']", 'paper_titles': "['Variational Graph Auto-Encoders', 'Dirichlet Graph Variational Autoencoder']", 'arxiv_ids': "['1611.07308', '2010.04408']", 'authors': '["[\'Thomas N. Kipf\', \'Max Welling\']", "[\'Jia Li\', \'Tomasyu Yu\', \'Jiajin Li\', \'Honglei Zhang\', \'Kangfei Zhao\', \'Yu Rong\', \'Hong Cheng\', \'Junzhou Huang\']"]', 'raw_generated_text': '\n## repository\nsgenza/tf_crnn\n## files\nmodel/crnn.py, model/dataset.py, model/utils.py, model/__init__.py, evaluate.py, arrays2labels, DataLoaderSVT, DataLoaderSynth90K, DataLoaderIIIT5K, sparse_tuple_from, _get_args, read_config, set_random_seed, config, ArgumentParser\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nmberr/ea-active-learning\n## files\nexecutables/collate_results.py, executables/evaluate_active_learning_heuristic.py, src/kgm/active_learning/base.py, src/kgm/active_learning/bayesian.py, src/kgm/active_learning/graph.py, DWY100k, DBP15kFull, generalized_k_means, CSLSNormalization, SimilarityNormalization, norm_class, tracking_uri, init_class, dataset_loader_cls, with_inverse_triples\n## tags\n[active learning, entity alignment]\n\n## repository\nxiyou3368/DGVAE\n## files\ndgvae/initializations.py, dgvae/input_data.py, dgvae/layers.py, dgvae/model.py, dgvae/model_graphite.py, preprocess_graph_generate, preprocess_graph_generate_e, graph_padding, mask_test_graphs, preprocess_graph_e, reconstructions_noiseless, powerlaw_cluster_graph, random_geometric_graph, sparse_retain, random_regular_graph\n## 3 tags\n[spectrum-value, graph neural network, disordered graph]\n\n\n\n## single', 'raw_reference_text': 'link prediction, graph generation, graph clustering', 'reference_text': 'link prediction graph generation graph clustering', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8605233430862427, 'bertscore_recall': 0.8609253168106079, 'bertscore_f1': 0.8607242703437805, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23884974420070648}, {'repo': 'JamesZhutheThird/Rb-PaStaNet', 'tasks_x': 'multi-label classification, detection, nlp, ', 'true_tasks': ['human object interaction detection'], 'generated_text': 'machine translation', 'prompt_info': {'content': 'lib/models/test_Solver_AVA_pasta.py, lib/models/test_Solver_HICO_DET_pasta.py, lib/models/train_Solver_AVA_pasta.py, lib/models/train_Solver_HICO_DET_pasta.py, lib/networks/pasta_AVA.py, Generate_part_score, Get_Next_Instance_HO_HICO_DET_for_only_PVP, draw_bounding_boxes_PVP, Generate_action_rule, Generate_action_PVP, test_image_HO, Generate_action_rule, Generate_action_PVP, Generate_relation_bbox, Augmented_HO_Neg_HICO_DET_for_only_PVP76', 'generated_text': '\n## repository\nhhexiy/interpretese\n## files\nlib/bleu.py, lib/vw.py, src/count_statistics.py, lib/corpus.py, lib/vocabulary.py, which_bin, plot_stacked_histogram, bin_val, filter_by_alignments, intersect_alignments, ja_sent, en_sent, get_labeled_sent, get_word_rank, count_num_sents\n## tags\n[machine translation]\n\n## repository\ndesh2608/dover-lap\n## files\ndover_lap/dover_lap.py, dover_lap/libs/rttm.py, dover_lap/libs/turn.py, dover_lap/libs/uem.py, dover_lap/libs/utils.py, command_required_option, compute_spk_overlap, get_speaker_keys, DOVERLap, LabelVoting, combine_turns_list, envelop, merge_overlaps, load_rttms, load_uem\n## tags\n[graph partitioning, speaker diarization, speech recognition]\n\n## repository\nJamesZhutheThird/Rb-PaStaNet\n## files\nlib/models/test_Solver_AVA_pasta.py, lib/models/test_Solver_HICO_DET_pasta.py, lib/models/train_Solver_AVA_pasta.py, lib/models/train_Solver_HICO_DET_pasta.py, lib/networks/pasta_AVA.py, Generate_part_score, Get_Next_Instance_HO_HICO_DET_for_only_PVP, draw_bounding_boxes_PVP, Generate_action_rule, Generate_action_PVP, test_image_HO, Generate_action_rule, Generate_action_PVP, Generate_relation_bbox, Augmented_HO_Neg_HICO_DET_for_only_PVP76\n## 3 tags\n[multi-label classification, detection, nlp]\n\n## repository\nweak', 'id': '6a9a11e6418495d8f7dc03ea7ecc237a', 'name': 'JamesZhutheThird/Rb-PaStaNet', 'tasks': 'multi-label classification, detection, nlp, ', 'true_text': 'human object interaction detection'}, 'generation': 0, 'input_text': '\n## repository\nhhexiy/interpretese\n## files\nlib/bleu.py, lib/vw.py, src/count_statistics.py, lib/corpus.py, lib/vocabulary.py, which_bin, plot_stacked_histogram, bin_val, filter_by_alignments, intersect_alignments, ja_sent, en_sent, get_labeled_sent, get_word_rank, count_num_sents\n## tags\n[machine translation]\n\n## repository\ndesh2608/dover-lap\n## files\ndover_lap/dover_lap.py, dover_lap/libs/rttm.py, dover_lap/libs/turn.py, dover_lap/libs/uem.py, dover_lap/libs/utils.py, command_required_option, compute_spk_overlap, get_speaker_keys, DOVERLap, LabelVoting, combine_turns_list, envelop, merge_overlaps, load_rttms, load_uem\n## tags\n[graph partitioning, speaker diarization, speech recognition]\n\n## repository\nJamesZhutheThird/Rb-PaStaNet\n## files\nlib/models/test_Solver_AVA_pasta.py, lib/models/test_Solver_HICO_DET_pasta.py, lib/models/train_Solver_AVA_pasta.py, lib/models/train_Solver_HICO_DET_pasta.py, lib/networks/pasta_AVA.py, Generate_part_score, Get_Next_Instance_HO_HICO_DET_for_only_PVP, draw_bounding_boxes_PVP, Generate_action_rule, Generate_action_PVP, test_image_HO, Generate_action_rule, Generate_action_PVP, Generate_relation_bbox, Augmented_HO_Neg_HICO_DET_for_only_PVP76\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/rb-pastanet-a-few-shot-human-object']", 'paper_titles': "['Rb-PaStaNet: A Few-Shot Human-Object Interaction Detection Based on Rules and Part States']", 'arxiv_ids': "['2008.06285']", 'authors': '["[\'Shenyu Zhang\', \'Zichen Zhu\', \'Qingquan Bao\']"]', 'raw_generated_text': '\n## repository\nhhexiy/interpretese\n## files\nlib/bleu.py, lib/vw.py, src/count_statistics.py, lib/corpus.py, lib/vocabulary.py, which_bin, plot_stacked_histogram, bin_val, filter_by_alignments, intersect_alignments, ja_sent, en_sent, get_labeled_sent, get_word_rank, count_num_sents\n## tags\n[machine translation]\n\n## repository\ndesh2608/dover-lap\n## files\ndover_lap/dover_lap.py, dover_lap/libs/rttm.py, dover_lap/libs/turn.py, dover_lap/libs/uem.py, dover_lap/libs/utils.py, command_required_option, compute_spk_overlap, get_speaker_keys, DOVERLap, LabelVoting, combine_turns_list, envelop, merge_overlaps, load_rttms, load_uem\n## tags\n[graph partitioning, speaker diarization, speech recognition]\n\n## repository\nJamesZhutheThird/Rb-PaStaNet\n## files\nlib/models/test_Solver_AVA_pasta.py, lib/models/test_Solver_HICO_DET_pasta.py, lib/models/train_Solver_AVA_pasta.py, lib/models/train_Solver_HICO_DET_pasta.py, lib/networks/pasta_AVA.py, Generate_part_score, Get_Next_Instance_HO_HICO_DET_for_only_PVP, draw_bounding_boxes_PVP, Generate_action_rule, Generate_action_PVP, test_image_HO, Generate_action_rule, Generate_action_PVP, Generate_relation_bbox, Augmented_HO_Neg_HICO_DET_for_only_PVP76\n## 3 tags\n[multi-label classification, detection, nlp]\n\n## repository\nweak', 'raw_reference_text': 'human object interaction detection', 'reference_text': 'human object interaction detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8336323499679565, 'bertscore_recall': 0.8129996657371521, 'bertscore_f1': 0.8231868147850037, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.20913636684417725}, {'repo': 'kundajelab/deeplift', 'tasks_x': 'DeepTomics, pytorch, Tensorflow, ', 'true_tasks': ['text classification'], 'generated_text': 'word embeddings', 'prompt_info': {'content': 'deeplift/dinuc_shuffle.py, deeplift/models.py, deeplift/util.py, deeplift/conversion/kerasapi_conversion.py, deeplift/layers/activations.py, conv2d_conversion, conv1d_conversion, dense_conversion, batchnorm_conversion, maxpool2d_conversion, nuc_content, insert_weights_into_nested_model_config, sequential_container_conversion, dinuc_content, dna_to_one_hot', 'generated_text': '\n## repository\nfvancesco/emoji_modifiers\n## files\nexample.py, print_nn, print_emo_nn, conv_sw2v_to_emo, conv_sw2v_to_emo, conv_sw2v_to_emo, startswith, startswith, join\n## tags\n[word embeddings]\n\n## repository\nlyn1874/region_based_active_learning\n## files\nselect_images.py, select_regions.py, Train_Active_Full_Im.py, Train_Active_Region_Im.py, visualize_calibration_score.py, give_args, give_score_path, give_first_figure, give_figure_e2, give_figure_5, give_acquired_full_image_uncertainty, show_uncertainty_distribution, get_region_uncert, sklearn_cm, compare_acq_at_certain_point_line\n## tags\n[active learning, medical image segmentation, semantic segmentation]\n\n## repository\nkundajelab/deeplift\n## files\ndeeplift/dinuc_shuffle.py, deeplift/models.py, deeplift/util.py, deeplift/conversion/kerasapi_conversion.py, deeplift/layers/activations.py, conv2d_conversion, conv1d_conversion, dense_conversion, batchnorm_conversion, maxpool2d_conversion, nuc_content, insert_weights_into_nested_model_config, sequential_container_conversion, dinuc_content, dna_to_one_hot\n## 3 tags\n[DeepTomics, pytorch, Tensorflow]\n\n## repository\nanne', 'id': '881a83246dd86b083909501ccefed3e0', 'name': 'kundajelab/deeplift', 'tasks': 'DeepTomics, pytorch, Tensorflow, ', 'true_text': 'text classification'}, 'generation': 0, 'input_text': '\n## repository\nfvancesco/emoji_modifiers\n## files\nexample.py, print_nn, print_emo_nn, conv_sw2v_to_emo, conv_sw2v_to_emo, conv_sw2v_to_emo, startswith, startswith, join\n## tags\n[word embeddings]\n\n## repository\nlyn1874/region_based_active_learning\n## files\nselect_images.py, select_regions.py, Train_Active_Full_Im.py, Train_Active_Region_Im.py, visualize_calibration_score.py, give_args, give_score_path, give_first_figure, give_figure_e2, give_figure_5, give_acquired_full_image_uncertainty, show_uncertainty_distribution, get_region_uncert, sklearn_cm, compare_acq_at_certain_point_line\n## tags\n[active learning, medical image segmentation, semantic segmentation]\n\n## repository\nkundajelab/deeplift\n## files\ndeeplift/dinuc_shuffle.py, deeplift/models.py, deeplift/util.py, deeplift/conversion/kerasapi_conversion.py, deeplift/layers/activations.py, conv2d_conversion, conv1d_conversion, dense_conversion, batchnorm_conversion, maxpool2d_conversion, nuc_content, insert_weights_into_nested_model_config, sequential_container_conversion, dinuc_content, dna_to_one_hot\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/towards-better-understanding-of-gradient']", 'paper_titles': "['Towards better understanding of gradient-based attribution methods for Deep Neural Networks']", 'arxiv_ids': "['1711.06104']", 'authors': '["[\'Marco Ancona\', \'Enea Ceolini\', \'Cengiz ztireli\', \'Markus Gross\']"]', 'raw_generated_text': '\n## repository\nfvancesco/emoji_modifiers\n## files\nexample.py, print_nn, print_emo_nn, conv_sw2v_to_emo, conv_sw2v_to_emo, conv_sw2v_to_emo, startswith, startswith, join\n## tags\n[word embeddings]\n\n## repository\nlyn1874/region_based_active_learning\n## files\nselect_images.py, select_regions.py, Train_Active_Full_Im.py, Train_Active_Region_Im.py, visualize_calibration_score.py, give_args, give_score_path, give_first_figure, give_figure_e2, give_figure_5, give_acquired_full_image_uncertainty, show_uncertainty_distribution, get_region_uncert, sklearn_cm, compare_acq_at_certain_point_line\n## tags\n[active learning, medical image segmentation, semantic segmentation]\n\n## repository\nkundajelab/deeplift\n## files\ndeeplift/dinuc_shuffle.py, deeplift/models.py, deeplift/util.py, deeplift/conversion/kerasapi_conversion.py, deeplift/layers/activations.py, conv2d_conversion, conv1d_conversion, dense_conversion, batchnorm_conversion, maxpool2d_conversion, nuc_content, insert_weights_into_nested_model_config, sequential_container_conversion, dinuc_content, dna_to_one_hot\n## 3 tags\n[DeepTomics, pytorch, Tensorflow]\n\n## repository\nanne', 'raw_reference_text': 'text classification', 'reference_text': 'text classification', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8133010864257812, 'bertscore_recall': 0.8757869005203247, 'bertscore_f1': 0.8433881998062134, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.425577849149704}, {'repo': 'kagaminccino/LAVSE', 'tasks_x': 'speech recognition, deep learning, recognition, ', 'true_tasks': ['data compression', 'denoising', 'speech enhancement'], 'generated_text': 'link prediction discourse parsing', 'prompt_info': {'content': 'main/build_model.py, main/data_generator.py, main/prepare_path_list.py, main/scoring.py, main/utils.py, model_detail_string, prepare_scoring_list, prepare_path_list, stft2spec, AV_Dataset, model_detail_string, cal_time, cal_time, cal_time, scoring_dir', 'generated_text': '\n## repository\nshizhouxing/DialogueDiscourseParsing\n## files\nbaseline/ilp.py, baseline/Sentence_Encoder.py, NonStructured_Encoder.py, Structured_Encoder.py, libs/dropout.py, init_grad, preview_data, get_summary_sum, pretty_data, get_batched_data_test, dim_embed_word, scip_path, get_batched_data_test, get_batched_data_test, dump_scores_to_dat_files\n## tags\n[link prediction, discourse parsing]\n\n## repository\nperia1/VAEconvMNIST\n## files\nsrc/vanilla_vae.py, src/util.py, see2, disp_to_term, perform_latent_space_arithmatics, latent_space_transition, rand_faces, see2, latent_variable_size, get_latent_var, get_latent_var, reconstruction_function\n## tags\n[style transfer]\n\n## repository\nkagaminccino/LAVSE\n## files\nmain/build_model.py, main/data_generator.py, main/prepare_path_list.py, main/scoring.py, main/utils.py, model_detail_string, prepare_scoring_list, prepare_path_list, stft2spec, AV_Dataset, model_detail_string, cal_time, cal_time, cal_time, scoring_dir\n## 3 tags\n[speech recognition, deep learning, recognition]\n\n## repository\ncambridgelt', 'id': 'ba1f8f9d2cc03cffdbc1865fe69aef4b', 'name': 'kagaminccino/LAVSE', 'tasks': 'speech recognition, deep learning, recognition, ', 'true_text': 'data compression, denoising, speech enhancement'}, 'generation': 0, 'input_text': '\n## repository\nshizhouxing/DialogueDiscourseParsing\n## files\nbaseline/ilp.py, baseline/Sentence_Encoder.py, NonStructured_Encoder.py, Structured_Encoder.py, libs/dropout.py, init_grad, preview_data, get_summary_sum, pretty_data, get_batched_data_test, dim_embed_word, scip_path, get_batched_data_test, get_batched_data_test, dump_scores_to_dat_files\n## tags\n[link prediction, discourse parsing]\n\n## repository\nperia1/VAEconvMNIST\n## files\nsrc/vanilla_vae.py, src/util.py, see2, disp_to_term, perform_latent_space_arithmatics, latent_space_transition, rand_faces, see2, latent_variable_size, get_latent_var, get_latent_var, reconstruction_function\n## tags\n[style transfer]\n\n## repository\nkagaminccino/LAVSE\n## files\nmain/build_model.py, main/data_generator.py, main/prepare_path_list.py, main/scoring.py, main/utils.py, model_detail_string, prepare_scoring_list, prepare_path_list, stft2spec, AV_Dataset, model_detail_string, cal_time, cal_time, cal_time, scoring_dir\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/lite-audio-visual-speech-enhancement']", 'paper_titles': "['Lite Audio-Visual Speech Enhancement']", 'arxiv_ids': "['2005.11769']", 'authors': '["[\'Shang-Yi Chuang\', \'Yu Tsao\', \'Chen-Chou Lo\', \'Hsin-Min Wang\']"]', 'raw_generated_text': '\n## repository\nshizhouxing/DialogueDiscourseParsing\n## files\nbaseline/ilp.py, baseline/Sentence_Encoder.py, NonStructured_Encoder.py, Structured_Encoder.py, libs/dropout.py, init_grad, preview_data, get_summary_sum, pretty_data, get_batched_data_test, dim_embed_word, scip_path, get_batched_data_test, get_batched_data_test, dump_scores_to_dat_files\n## tags\n[link prediction, discourse parsing]\n\n## repository\nperia1/VAEconvMNIST\n## files\nsrc/vanilla_vae.py, src/util.py, see2, disp_to_term, perform_latent_space_arithmatics, latent_space_transition, rand_faces, see2, latent_variable_size, get_latent_var, get_latent_var, reconstruction_function\n## tags\n[style transfer]\n\n## repository\nkagaminccino/LAVSE\n## files\nmain/build_model.py, main/data_generator.py, main/prepare_path_list.py, main/scoring.py, main/utils.py, model_detail_string, prepare_scoring_list, prepare_path_list, stft2spec, AV_Dataset, model_detail_string, cal_time, cal_time, cal_time, scoring_dir\n## 3 tags\n[speech recognition, deep learning, recognition]\n\n## repository\ncambridgelt', 'raw_reference_text': 'data compression, denoising, speech enhancement', 'reference_text': 'speech enhancement denoising data compression', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8829361796379089, 'bertscore_recall': 0.8626928329467773, 'bertscore_f1': 0.872697114944458, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.29303377866744995}, {'repo': 'sfu-gruvi-3dv/deep_human', 'tasks_x': 'rgbd, depth, 3Dpose - rgbd, ', 'true_tasks': ['depth estimation'], 'generated_text': 'anomaly detection', 'prompt_info': {'content': 'function/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator', 'generated_text': '\n## repository\noeberle/BiLRP_explain_similarity\n## files\nmodel/bilrp.py, visualization/plotting.py, utils.py, proc_image, newlayer, plot_relevances, vgg_gamma, VggLayers, clip_func, g, load_image, bias, xlim\n## tags\n[anomaly detection]\n\n## repository\nzijundeng/BDRAR\n## files\nresnext/resnext101_regular.py, resnext/resnext_101_32x4d_.py, resnext/config.py, resnext/__init__.py, joint_transforms.py, BDRAR, _AttentionModule, crf_refine, ResNeXt101, check_mkdir, bce_logit, addPairwiseBilateral, addPairwiseGaussian, DenseCRF2D, check_mkdir\n## tags\n[shadow detection]\n\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## 3 tags\n[rgbd] [depth]\n[3Dpose - rgbd] \n', 'id': '3b02b4f89e72315b58afa2cfa5938638', 'name': 'sfu-gruvi-3dv/deep_human', 'tasks': 'rgbd, depth, 3Dpose - rgbd, ', 'true_text': 'depth estimation'}, 'generation': 0, 'input_text': '\n## repository\noeberle/BiLRP_explain_similarity\n## files\nmodel/bilrp.py, visualization/plotting.py, utils.py, proc_image, newlayer, plot_relevances, vgg_gamma, VggLayers, clip_func, g, load_image, bias, xlim\n## tags\n[anomaly detection]\n\n## repository\nzijundeng/BDRAR\n## files\nresnext/resnext101_regular.py, resnext/resnext_101_32x4d_.py, resnext/config.py, resnext/__init__.py, joint_transforms.py, BDRAR, _AttentionModule, crf_refine, ResNeXt101, check_mkdir, bce_logit, addPairwiseBilateral, addPairwiseGaussian, DenseCRF2D, check_mkdir\n## tags\n[shadow detection]\n\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-neural-network-for-detailed-human-depth']", 'paper_titles': "['A Neural Network for Detailed Human Depth Estimation from a Single Image']", 'arxiv_ids': "['1910.01275']", 'authors': '["[\'Sicong Tang\', \'Feitong Tan\', \'Kelvin Cheng\', \'Zhaoyang Li\', \'Siyu Zhu\', \'Ping Tan\']"]', 'raw_generated_text': '\n## repository\noeberle/BiLRP_explain_similarity\n## files\nmodel/bilrp.py, visualization/plotting.py, utils.py, proc_image, newlayer, plot_relevances, vgg_gamma, VggLayers, clip_func, g, load_image, bias, xlim\n## tags\n[anomaly detection]\n\n## repository\nzijundeng/BDRAR\n## files\nresnext/resnext101_regular.py, resnext/resnext_101_32x4d_.py, resnext/config.py, resnext/__init__.py, joint_transforms.py, BDRAR, _AttentionModule, crf_refine, ResNeXt101, check_mkdir, bce_logit, addPairwiseBilateral, addPairwiseGaussian, DenseCRF2D, check_mkdir\n## tags\n[shadow detection]\n\n## repository\nsfu-gruvi-3dv/deep_human\n## files\nfunction/integral_loss.py, function/opt_depth.py, function/glob2/compat.py, function/glob2/fnmatch.py, function/glob2/__init__.py, draw_normal_sphere, smooth_depth, cropfor3d_intrinsic, persp_depth_opt_stereo, optimize_depth, getScale_detail, getCenter_detail, naive_pcd, dump_pointcloud, batch_neighbor_index_generator\n## 3 tags\n[rgbd] [depth]\n[3Dpose - rgbd] \n', 'raw_reference_text': 'depth estimation', 'reference_text': 'depth estimation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.906574010848999, 'bertscore_recall': 0.906574010848999, 'bertscore_f1': 0.9065740704536438, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.30901214480400085}, {'repo': 'EvZissel/Residual-Flow', 'tasks_x': 'approximate second orderacles, anomaly detection, ', 'true_tasks': ['out of distribution detection'], 'generated_text': 'style transfer', 'prompt_info': {'content': 'ADV_Generate_Mahalanobis.py, ADV_Samples_FGSM.py, OOD_Regression_Mahalanobis_FGSM_validation.py, OOD_Regression_Residual _flow.py, OOD_Regression_Residual_flow_FGSM_validation.py, getFGSM, get_resflow_score, get_resflow_score_FGSM, make_roc, reture_length_hidden, length_hidden, cuda_index, getFGSM, get_resflow_score, get_resflow_score_FGSM', 'generated_text': '\n## repository\njaydeepthik/neural-style-transfer\n## files\nnst_tf.py, nst_utils.py, layer_style_cost, content_cost, style_cost, reshape_and_normalize_image, load_vgg_model, layer_style_cost, _conv2d_relu, _avgpool, MEANS, MEANS\n## tags\n[style transfer]\n\n## repository\nucsd-dsc-arts/dsc160-final-dsc160_final_group4\n## files\nCode/closed_form_matting.py, Code/deep_photostyle.py, Code/photo_style.py, Code/smooth_local_affine.py, affine_loss, getlaplacian1, load_seg, getLaplacian, smooth_local_affine, affine_weight, content_seg_path, style_seg_path, init_image_path, _extract_mask\n## tags\n[style transfer]\n\n## repository\nEvZissel/Residual-Flow\n## files\nADV_Generate_Mahalanobis.py, ADV_Samples_FGSM.py, OOD_Regression_Mahalanobis_FGSM_validation.py, OOD_Regression_Residual _flow.py, OOD_Regression_Residual_flow_FGSM_validation.py, getFGSM, get_resflow_score, get_resflow_score_FGSM, make_roc, reture_length_hidden, length_hidden, cuda_index, getFGSM, get_resflow_score, get_resflow_score_FGSM\n## 3 tags\n[approximate second orderacles, anomaly detection]\n\n## repository\nsree', 'id': '94a1647874fce4fb113448850d1b1ae1', 'name': 'EvZissel/Residual-Flow', 'tasks': 'approximate second orderacles, anomaly detection, ', 'true_text': 'out of distribution detection'}, 'generation': 0, 'input_text': '\n## repository\njaydeepthik/neural-style-transfer\n## files\nnst_tf.py, nst_utils.py, layer_style_cost, content_cost, style_cost, reshape_and_normalize_image, load_vgg_model, layer_style_cost, _conv2d_relu, _avgpool, MEANS, MEANS\n## tags\n[style transfer]\n\n## repository\nucsd-dsc-arts/dsc160-final-dsc160_final_group4\n## files\nCode/closed_form_matting.py, Code/deep_photostyle.py, Code/photo_style.py, Code/smooth_local_affine.py, affine_loss, getlaplacian1, load_seg, getLaplacian, smooth_local_affine, affine_weight, content_seg_path, style_seg_path, init_image_path, _extract_mask\n## tags\n[style transfer]\n\n## repository\nEvZissel/Residual-Flow\n## files\nADV_Generate_Mahalanobis.py, ADV_Samples_FGSM.py, OOD_Regression_Mahalanobis_FGSM_validation.py, OOD_Regression_Residual _flow.py, OOD_Regression_Residual_flow_FGSM_validation.py, getFGSM, get_resflow_score, get_resflow_score_FGSM, make_roc, reture_length_hidden, length_hidden, cuda_index, getFGSM, get_resflow_score, get_resflow_score_FGSM\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/deep-residual-flow-for-novelty-detection']", 'paper_titles': "['Deep Residual Flow for Out of Distribution Detection']", 'arxiv_ids': "['2001.05419']", 'authors': '["[\'Ev Zisselman\', \'Aviv Tamar\']"]', 'raw_generated_text': '\n## repository\njaydeepthik/neural-style-transfer\n## files\nnst_tf.py, nst_utils.py, layer_style_cost, content_cost, style_cost, reshape_and_normalize_image, load_vgg_model, layer_style_cost, _conv2d_relu, _avgpool, MEANS, MEANS\n## tags\n[style transfer]\n\n## repository\nucsd-dsc-arts/dsc160-final-dsc160_final_group4\n## files\nCode/closed_form_matting.py, Code/deep_photostyle.py, Code/photo_style.py, Code/smooth_local_affine.py, affine_loss, getlaplacian1, load_seg, getLaplacian, smooth_local_affine, affine_weight, content_seg_path, style_seg_path, init_image_path, _extract_mask\n## tags\n[style transfer]\n\n## repository\nEvZissel/Residual-Flow\n## files\nADV_Generate_Mahalanobis.py, ADV_Samples_FGSM.py, OOD_Regression_Mahalanobis_FGSM_validation.py, OOD_Regression_Residual _flow.py, OOD_Regression_Residual_flow_FGSM_validation.py, getFGSM, get_resflow_score, get_resflow_score_FGSM, make_roc, reture_length_hidden, length_hidden, cuda_index, getFGSM, get_resflow_score, get_resflow_score_FGSM\n## 3 tags\n[approximate second orderacles, anomaly detection]\n\n## repository\nsree', 'raw_reference_text': 'out of distribution detection', 'reference_text': 'out of distribution detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.861237108707428, 'bertscore_recall': 0.849219799041748, 'bertscore_f1': 0.8551862835884094, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.16739286482334137}, {'repo': 'jamesrobertlloyd/gpss-research', 'tasks_x': 'FASTA input, ', 'true_tasks': ['time series', 'gaussian processes'], 'generated_text': 'data augmentation', 'prompt_info': {'content': 'source/flexible_function.py, source/test_everything.py, source/test_slow.py, source/translation.py, experiments/2014-04-14-class-breast.py, gen_all_results, gen_all_results, make_all_1d_figures, make_all_1d_figures, add_jitter_k, additive_form, jitter_sd, add_jitter_to_models, PREDICT_AND_SAVE_CODE, n_rand', 'generated_text': '\n## repository\nhongweilibran/wmh_ibbmTum\n## files\nsubmission_sysu_.py, test_leave_one_out.py, train_leave_one_out.py, evaluation.py, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, GE3T_postprocessing, getResultFilename, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, SetFullyConnected, sitkUInt32\n## tags\n[data augmentation]\n\n## repository\ns9xie/hed\n## files\nexamples/hed/solve.py, python/caffe/test/test_layer_type_list.py, python/caffe/test/test_python_layer_with_param_str.py, python/caffe/net_spec.py, python/caffe/test/test_net_spec.py, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, FindNextMatchingAngleBracket, FindPreviousMatchingAngleBracket, CheckCaffeRandom, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[boundary detection, edge detection]\n\n## repository\njamesrobertlloyd/gpss-research\n## files\nsource/flexible_function.py, source/test_everything.py, source/test_slow.py, source/translation.py, experiments/2014-04-14-class-breast.py, gen_all_results, gen_all_results, make_all_1d_figures, make_all_1d_figures, add_jitter_k, additive_form, jitter_sd, add_jitter_to_models, PREDICT_AND_SAVE_CODE, n_rand\n## 3 tags\n[FASTA input]\n\n## repository\ndmbeath/omnia\n', 'id': '443148c3f7918971d8532220f1955988', 'name': 'jamesrobertlloyd/gpss-research', 'tasks': 'FASTA input, ', 'true_text': 'time series, gaussian processes'}, 'generation': 0, 'input_text': '\n## repository\nhongweilibran/wmh_ibbmTum\n## files\nsubmission_sysu_.py, test_leave_one_out.py, train_leave_one_out.py, evaluation.py, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, GE3T_postprocessing, getResultFilename, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, SetFullyConnected, sitkUInt32\n## tags\n[data augmentation]\n\n## repository\ns9xie/hed\n## files\nexamples/hed/solve.py, python/caffe/test/test_layer_type_list.py, python/caffe/test/test_python_layer_with_param_str.py, python/caffe/net_spec.py, python/caffe/test/test_net_spec.py, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, FindNextMatchingAngleBracket, FindPreviousMatchingAngleBracket, CheckCaffeRandom, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[boundary detection, edge detection]\n\n## repository\njamesrobertlloyd/gpss-research\n## files\nsource/flexible_function.py, source/test_everything.py, source/test_slow.py, source/translation.py, experiments/2014-04-14-class-breast.py, gen_all_results, gen_all_results, make_all_1d_figures, make_all_1d_figures, add_jitter_k, additive_form, jitter_sd, add_jitter_to_models, PREDICT_AND_SAVE_CODE, n_rand\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/structure-discovery-in-nonparametric', 'https://paperswithcode.com/paper/automatic-construction-and-natural-language']", 'paper_titles': "['Structure Discovery in Nonparametric Regression through Compositional Kernel Search', 'Automatic Construction and Natural-Language Description of Nonparametric Regression Models']", 'arxiv_ids': "['1302.4922', '1402.4304']", 'authors': '["[\'David Duvenaud\', \'James Robert Lloyd\', \'Roger Grosse\', \'Joshua B. Tenenbaum\', \'Zoubin Ghahramani\']", "[\'James Robert Lloyd\', \'David Duvenaud\', \'Roger Grosse\', \'Joshua B. Tenenbaum\', \'Zoubin Ghahramani\']"]', 'raw_generated_text': '\n## repository\nhongweilibran/wmh_ibbmTum\n## files\nsubmission_sysu_.py, test_leave_one_out.py, train_leave_one_out.py, evaluation.py, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, GE3T_postprocessing, getResultFilename, test_leave_one_out, train_leave_one_out, GE3T_preprocessing, SetFullyConnected, sitkUInt32\n## tags\n[data augmentation]\n\n## repository\ns9xie/hed\n## files\nexamples/hed/solve.py, python/caffe/test/test_layer_type_list.py, python/caffe/test/test_python_layer_with_param_str.py, python/caffe/net_spec.py, python/caffe/test/test_net_spec.py, CheckCaffeAlternatives, CheckCaffeDataLayerSetUp, FindNextMatchingAngleBracket, FindPreviousMatchingAngleBracket, CheckCaffeRandom, ResetSection, CheckCompletedBlocks, SetLastHeader, CanonicalizeAlphabeticalOrder, CheckNextIncludeOrder\n## tags\n[boundary detection, edge detection]\n\n## repository\njamesrobertlloyd/gpss-research\n## files\nsource/flexible_function.py, source/test_everything.py, source/test_slow.py, source/translation.py, experiments/2014-04-14-class-breast.py, gen_all_results, gen_all_results, make_all_1d_figures, make_all_1d_figures, add_jitter_k, additive_form, jitter_sd, add_jitter_to_models, PREDICT_AND_SAVE_CODE, n_rand\n## 3 tags\n[FASTA input]\n\n## repository\ndmbeath/omnia\n', 'raw_reference_text': 'time series, gaussian processes', 'reference_text': 'time series gaussian processes', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.860176146030426, 'bertscore_recall': 0.8232179880142212, 'bertscore_f1': 0.8412913680076599, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.1932118833065033}, {'repo': 'guxinqian/TKP', 'tasks_x': 'single object occlusion segmentation, ', 'true_tasks': ['person re identification', 'video based person re identification'], 'generated_text': 'semantic segmentation medical image segmentation', 'prompt_info': {'content': 'main_ilids.py, utils/video_loader.py, utils/eval_metrics.py, utils/samplers.py, transforms/spatial_transforms.py, ImgResNet50, VidNonLocalResNet50, HeterogeneousTripletLoss, SimilarityBasedTKP, FeatureBasedTKP, criterion_tkp_f, criterion_i2v, criterion_tkp_d, img_arch, vid_arch', 'generated_text': '\n## repository\nRoichatulJannah21/neural-odes-segmentation\n## files\ninference_utils.py, augmentations.py, train_utils.py, model_utils.py, metrics.py, RandomRotationWithMask, ElasticTransformations, ObjectHausdorff, ObjectDice, ConvResUNet, remove_small_object, grow_to_fill_borders, hole_filling_per_object, resize_to_size, split_objects\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nzju3dv/RVL-Dynamic\n## files\nattentionmapnet/AttentionPoseNet.py, attentionmapnet/config.py, attentionmapnet/eval.py, attentionmapnet/resnet.py, attentionmapnet/train.py, get_rnn_test_dataloader, get_posenet_train_dataloader, get_maskmapnet_train_dataloader, get_mapnet_train_dataloader, np_qlog_t, MapNetConfigurator, get_mapnet_train_dataloader, compose_pose_quaternion, invert_pose_quaternion, process_with_params\n## tags\n[camera localization, visual localization]\n\n## repository\nguxinqian/TKP\n## files\nmain_ilids.py, utils/video_loader.py, utils/eval_metrics.py, utils/samplers.py, transforms/spatial_transforms.py, ImgResNet50, VidNonLocalResNet50, HeterogeneousTripletLoss, SimilarityBasedTKP, FeatureBasedTKP, criterion_tkp_f, criterion_i2v, criterion_tkp_d, img_arch, vid_arch\n## 3 tags\n[single object occlusion segmentation]\n\n## repository\nLearn-Hierarchical-Features', 'id': 'ee13d54918f5446e5a970f0f3294a105', 'name': 'guxinqian/TKP', 'tasks': 'single object occlusion segmentation, ', 'true_text': 'person re identification, video based person re identification'}, 'generation': 0, 'input_text': '\n## repository\nRoichatulJannah21/neural-odes-segmentation\n## files\ninference_utils.py, augmentations.py, train_utils.py, model_utils.py, metrics.py, RandomRotationWithMask, ElasticTransformations, ObjectHausdorff, ObjectDice, ConvResUNet, remove_small_object, grow_to_fill_borders, hole_filling_per_object, resize_to_size, split_objects\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nzju3dv/RVL-Dynamic\n## files\nattentionmapnet/AttentionPoseNet.py, attentionmapnet/config.py, attentionmapnet/eval.py, attentionmapnet/resnet.py, attentionmapnet/train.py, get_rnn_test_dataloader, get_posenet_train_dataloader, get_maskmapnet_train_dataloader, get_mapnet_train_dataloader, np_qlog_t, MapNetConfigurator, get_mapnet_train_dataloader, compose_pose_quaternion, invert_pose_quaternion, process_with_params\n## tags\n[camera localization, visual localization]\n\n## repository\nguxinqian/TKP\n## files\nmain_ilids.py, utils/video_loader.py, utils/eval_metrics.py, utils/samplers.py, transforms/spatial_transforms.py, ImgResNet50, VidNonLocalResNet50, HeterogeneousTripletLoss, SimilarityBasedTKP, FeatureBasedTKP, criterion_tkp_f, criterion_i2v, criterion_tkp_d, img_arch, vid_arch\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/temporal-knowledge-propagation-for-image-to']", 'paper_titles': "['Temporal Knowledge Propagation for Image-to-Video Person Re-identification']", 'arxiv_ids': "['1908.03885']", 'authors': '["[\'Xinqian Gu\', \'Bingpeng Ma\', \'Hong Chang\', \'Shiguang Shan\', \'Xilin Chen\']"]', 'raw_generated_text': '\n## repository\nRoichatulJannah21/neural-odes-segmentation\n## files\ninference_utils.py, augmentations.py, train_utils.py, model_utils.py, metrics.py, RandomRotationWithMask, ElasticTransformations, ObjectHausdorff, ObjectDice, ConvResUNet, remove_small_object, grow_to_fill_borders, hole_filling_per_object, resize_to_size, split_objects\n## tags\n[medical image segmentation, semantic segmentation]\n\n## repository\nzju3dv/RVL-Dynamic\n## files\nattentionmapnet/AttentionPoseNet.py, attentionmapnet/config.py, attentionmapnet/eval.py, attentionmapnet/resnet.py, attentionmapnet/train.py, get_rnn_test_dataloader, get_posenet_train_dataloader, get_maskmapnet_train_dataloader, get_mapnet_train_dataloader, np_qlog_t, MapNetConfigurator, get_mapnet_train_dataloader, compose_pose_quaternion, invert_pose_quaternion, process_with_params\n## tags\n[camera localization, visual localization]\n\n## repository\nguxinqian/TKP\n## files\nmain_ilids.py, utils/video_loader.py, utils/eval_metrics.py, utils/samplers.py, transforms/spatial_transforms.py, ImgResNet50, VidNonLocalResNet50, HeterogeneousTripletLoss, SimilarityBasedTKP, FeatureBasedTKP, criterion_tkp_f, criterion_i2v, criterion_tkp_d, img_arch, vid_arch\n## 3 tags\n[single object occlusion segmentation]\n\n## repository\nLearn-Hierarchical-Features', 'raw_reference_text': 'person re identification, video based person re identification', 'reference_text': 'person re identification video based person re identification', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8733424544334412, 'bertscore_recall': 0.8580645322799683, 'bertscore_f1': 0.8656361103057861, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.164481520652771}, {'repo': 'kushaltirumala/callibratable_style_consistency_flies', 'tasks_x': 'image classification, generation, image processing, ', 'true_tasks': ['imitation learning'], 'generated_text': 'semantic correspondence', 'prompt_info': {'content': 'scripts/eval_cop_policy.py, scripts/sample_ctvae.py, scripts/sample_ctvae_two_model.py, transforming_data/transform_data.py, util/datasets/fruit_fly/core.py, FruitFlyDataset, overlap_interval, FruitFlyEnv, BBallDataset, LabelFunction, prepare_stage, requires_environment, requires_environment, label_dim, filter_and_load_state_dict', 'generated_text': '\n## repository\nthunlp/scorp\n## files\nd-sepr.py, scorp.py, lstm_inference, SPBERT, sp_collate_fn, sp_collate_fn, BiSentenceLSTM, write_mean_std_max_min_absmean, SPBERT, get_bert_map, build_word2sememe, build_word2sememe\n## tags\n[semantic correspondence]\n\n## repository\ndeepsuncode/RNN-CME-prediction\n## files\nCMEpredict/CMEpredict.py, get_n_features_thresh, attention_3d_block, output_result, gru, lstm, attention_3d_block, attention_3d_block, Input, Input, read_csv\n## tags\n[time series]\n\n## repository\nkushaltirumala/callibratable_style_consistency_flies\n## files\nscripts/eval_cop_policy.py, scripts/sample_ctvae.py, scripts/sample_ctvae_two_model.py, transforming_data/transform_data.py, util/datasets/fruit_fly/core.py, FruitFlyDataset, overlap_interval, FruitFlyEnv, BBallDataset, LabelFunction, prepare_stage, requires_environment, requires_environment, label_dim, filter_and_load_state_dict\n## 3 tags\n[image classification], [generation], [image processing]\n\n## repository\ntj', 'id': 'bef36045a85935d0b361d98dc3e91450', 'name': 'kushaltirumala/callibratable_style_consistency_flies', 'tasks': 'image classification, generation, image processing, ', 'true_text': 'imitation learning'}, 'generation': 0, 'input_text': '\n## repository\nthunlp/scorp\n## files\nd-sepr.py, scorp.py, lstm_inference, SPBERT, sp_collate_fn, sp_collate_fn, BiSentenceLSTM, write_mean_std_max_min_absmean, SPBERT, get_bert_map, build_word2sememe, build_word2sememe\n## tags\n[semantic correspondence]\n\n## repository\ndeepsuncode/RNN-CME-prediction\n## files\nCMEpredict/CMEpredict.py, get_n_features_thresh, attention_3d_block, output_result, gru, lstm, attention_3d_block, attention_3d_block, Input, Input, read_csv\n## tags\n[time series]\n\n## repository\nkushaltirumala/callibratable_style_consistency_flies\n## files\nscripts/eval_cop_policy.py, scripts/sample_ctvae.py, scripts/sample_ctvae_two_model.py, transforming_data/transform_data.py, util/datasets/fruit_fly/core.py, FruitFlyDataset, overlap_interval, FruitFlyEnv, BBallDataset, LabelFunction, prepare_stage, requires_environment, requires_environment, label_dim, filter_and_load_state_dict\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/learning-calibratable-policies-using']", 'paper_titles': "['Learning Calibratable Policies using Programmatic Style-Consistency']", 'arxiv_ids': "['1910.01179']", 'authors': '["[\'Eric Zhan\', \'Albert Tseng\', \'Yisong Yue\', \'Adith Swaminathan\', \'Matthew Hausknecht\']"]', 'raw_generated_text': '\n## repository\nthunlp/scorp\n## files\nd-sepr.py, scorp.py, lstm_inference, SPBERT, sp_collate_fn, sp_collate_fn, BiSentenceLSTM, write_mean_std_max_min_absmean, SPBERT, get_bert_map, build_word2sememe, build_word2sememe\n## tags\n[semantic correspondence]\n\n## repository\ndeepsuncode/RNN-CME-prediction\n## files\nCMEpredict/CMEpredict.py, get_n_features_thresh, attention_3d_block, output_result, gru, lstm, attention_3d_block, attention_3d_block, Input, Input, read_csv\n## tags\n[time series]\n\n## repository\nkushaltirumala/callibratable_style_consistency_flies\n## files\nscripts/eval_cop_policy.py, scripts/sample_ctvae.py, scripts/sample_ctvae_two_model.py, transforming_data/transform_data.py, util/datasets/fruit_fly/core.py, FruitFlyDataset, overlap_interval, FruitFlyEnv, BBallDataset, LabelFunction, prepare_stage, requires_environment, requires_environment, label_dim, filter_and_load_state_dict\n## 3 tags\n[image classification], [generation], [image processing]\n\n## repository\ntj', 'raw_reference_text': 'imitation learning', 'reference_text': 'imitation learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8749340772628784, 'bertscore_recall': 0.8801934719085693, 'bertscore_f1': 0.8775559067726135, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14648288488388062}, {'repo': 'PingEnLu/Time-dependent_SIR_COVID-19', 'tasks_x': 'subsampling method, c9695510/Sto', 'true_tasks': ['time series'], 'generated_text': 'autonomous driving', 'prompt_info': {'content': 'TimeSIR_COVID-19.py, data_spilt, ridge, Ridge, GridSearchCV, delete, fit, empty', 'generated_text': '\n## repository\nAI-secure/VeriGauge\n## files\nadaptor/adaptor.py, adaptor/basic_adaptor.py, adaptor/cnncert_adaptor.py, adaptor/crown_adaptor.py, adaptor/eran_adaptor.py, DeeppolyMulNodeIntermediate, DeeppolySubNodeIntermediate, DeeppolyMulNodeFirst, DeeppolySubNodeFirst, Krelu, save_with_configs, residual_counter, numproc, dense_w, dense_bias\n## tags\n[autonomous driving]\n\n## repository\ncf020031308/mad-learning\n## files\ncifar_mnist.py, citations.py, mad.py, ogbl-ddi.py, weekday.py, MADGraph, MADGraph, MAD, MAD, MAD, MADGraph, get_edge_split, LinkPropPredDataset, eval_steps, runs\n## tags\n[link prediction]\n\n## repository\nPingEnLu/Time-dependent_SIR_COVID-19\n## files\nTimeSIR_COVID-19.py, data_spilt, ridge, Ridge, GridSearchCV, delete, fit, empty\n## 3 tags\n[subsampling method]\n\nc9695510/Sto', 'id': '4909686b8258683b12fc82d4d0a4f381', 'name': 'PingEnLu/Time-dependent_SIR_COVID-19', 'tasks': 'subsampling method, c9695510/Sto', 'true_text': 'time series'}, 'generation': 0, 'input_text': '\n## repository\nAI-secure/VeriGauge\n## files\nadaptor/adaptor.py, adaptor/basic_adaptor.py, adaptor/cnncert_adaptor.py, adaptor/crown_adaptor.py, adaptor/eran_adaptor.py, DeeppolyMulNodeIntermediate, DeeppolySubNodeIntermediate, DeeppolyMulNodeFirst, DeeppolySubNodeFirst, Krelu, save_with_configs, residual_counter, numproc, dense_w, dense_bias\n## tags\n[autonomous driving]\n\n## repository\ncf020031308/mad-learning\n## files\ncifar_mnist.py, citations.py, mad.py, ogbl-ddi.py, weekday.py, MADGraph, MADGraph, MAD, MAD, MAD, MADGraph, get_edge_split, LinkPropPredDataset, eval_steps, runs\n## tags\n[link prediction]\n\n## repository\nPingEnLu/Time-dependent_SIR_COVID-19\n## files\nTimeSIR_COVID-19.py, data_spilt, ridge, Ridge, GridSearchCV, delete, fit, empty\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-time-dependent-sir-model-for-covid-19']", 'paper_titles': "['A Time-dependent SIR model for COVID-19 with Undetectable Infected Persons']", 'arxiv_ids': "['2003.00122']", 'authors': '["[\'Yi-Cheng Chen\', \'Ping-En Lu\', \'Cheng-Shang Chang\', \'Tzu-Hsuan Liu\']"]', 'raw_generated_text': '\n## repository\nAI-secure/VeriGauge\n## files\nadaptor/adaptor.py, adaptor/basic_adaptor.py, adaptor/cnncert_adaptor.py, adaptor/crown_adaptor.py, adaptor/eran_adaptor.py, DeeppolyMulNodeIntermediate, DeeppolySubNodeIntermediate, DeeppolyMulNodeFirst, DeeppolySubNodeFirst, Krelu, save_with_configs, residual_counter, numproc, dense_w, dense_bias\n## tags\n[autonomous driving]\n\n## repository\ncf020031308/mad-learning\n## files\ncifar_mnist.py, citations.py, mad.py, ogbl-ddi.py, weekday.py, MADGraph, MADGraph, MAD, MAD, MAD, MADGraph, get_edge_split, LinkPropPredDataset, eval_steps, runs\n## tags\n[link prediction]\n\n## repository\nPingEnLu/Time-dependent_SIR_COVID-19\n## files\nTimeSIR_COVID-19.py, data_spilt, ridge, Ridge, GridSearchCV, delete, fit, empty\n## 3 tags\n[subsampling method]\n\nc9695510/Sto', 'raw_reference_text': 'time series', 'reference_text': 'time series', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8721299767494202, 'bertscore_recall': 0.8827935457229614, 'bertscore_f1': 0.8774293065071106, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.06865072250366211}, {'repo': 'mataney/BootstrappingRelationExtractors', 'tasks_x': 'relation extraction, specific entity recognition, standardization, ', 'true_tasks': ['relation extraction', 'data augmentation'], 'generated_text': 'anomaly detection one class classifier outlier detection', 'prompt_info': {'content': 'classification/docred.py, classification/re_processors.py, classification/tacred.py, classification/test_docred.py, classification/test_tacred.py, remove_same_sent_id, seperate_entities, merge_and_save_examples, download_from_spike_search, update_file_lengths, input_features_class, special_tokens_file, e2_entities, generation_file, e1_entities', 'generated_text': '\n## repository\nkhalooei/ALOCC-CVPR2018\n## files\nkh_tools.py, ops.py, models.py, test.py, utils.py, get_image_Slicization, get_image_SlicizationWithShape, kh_make_patches, check_some_assertions, check_some_assertions, f_test_frozen_model, check_some_assertions, read_lst_images, kh_isDirExist, kh_isDirExist\n## tags\n[one class classifier, anomaly detection, outlier detection]\n\n## repository\nDarg-Iztech/gender-prediction-from-tweets\n## files\nparameters_ar.py, parameters_en.py, parameters_es.py, modelDeleter.py, visualizer.py, readGloveEmbeddings, readCharEmbeddings, readCaptions, partite_dataset_vectors, prepVectorBatchData, tweet_per_user, tweet_per_user, model_tuple, model_tuple, validation_set_size\n## tags\n[gender prediction]\n\n## repository\nmataney/BootstrappingRelationExtractors\n## files\nclassification/docred.py, classification/re_processors.py, classification/tacred.py, classification/test_docred.py, classification/test_tacred.py, remove_same_sent_id, seperate_entities, merge_and_save_examples, download_from_spike_search, update_file_lengths, input_features_class, special_tokens_file, e2_entities, generation_file, e1_entities\n## 3 tags\n[relation extraction, specific entity recognition, standardization]\n\n## repository\ncd', 'id': 'c3819a309cc9fa7b3354b310f507f196', 'name': 'mataney/BootstrappingRelationExtractors', 'tasks': 'relation extraction, specific entity recognition, standardization, ', 'true_text': 'relation extraction, data augmentation'}, 'generation': 0, 'input_text': '\n## repository\nkhalooei/ALOCC-CVPR2018\n## files\nkh_tools.py, ops.py, models.py, test.py, utils.py, get_image_Slicization, get_image_SlicizationWithShape, kh_make_patches, check_some_assertions, check_some_assertions, f_test_frozen_model, check_some_assertions, read_lst_images, kh_isDirExist, kh_isDirExist\n## tags\n[one class classifier, anomaly detection, outlier detection]\n\n## repository\nDarg-Iztech/gender-prediction-from-tweets\n## files\nparameters_ar.py, parameters_en.py, parameters_es.py, modelDeleter.py, visualizer.py, readGloveEmbeddings, readCharEmbeddings, readCaptions, partite_dataset_vectors, prepVectorBatchData, tweet_per_user, tweet_per_user, model_tuple, model_tuple, validation_set_size\n## tags\n[gender prediction]\n\n## repository\nmataney/BootstrappingRelationExtractors\n## files\nclassification/docred.py, classification/re_processors.py, classification/tacred.py, classification/test_docred.py, classification/test_tacred.py, remove_same_sent_id, seperate_entities, merge_and_save_examples, download_from_spike_search, update_file_lengths, input_features_class, special_tokens_file, e2_entities, generation_file, e1_entities\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/bootstrapping-relation-extractors-using']", 'paper_titles': "['Bootstrapping Relation Extractors using Syntactic Search by Examples']", 'arxiv_ids': "['2102.05007']", 'authors': '["[\'Matan Eyal\', \'Asaf Amrami\', \'Hillel Taub-Tabib\', \'Yoav Goldberg\']"]', 'raw_generated_text': '\n## repository\nkhalooei/ALOCC-CVPR2018\n## files\nkh_tools.py, ops.py, models.py, test.py, utils.py, get_image_Slicization, get_image_SlicizationWithShape, kh_make_patches, check_some_assertions, check_some_assertions, f_test_frozen_model, check_some_assertions, read_lst_images, kh_isDirExist, kh_isDirExist\n## tags\n[one class classifier, anomaly detection, outlier detection]\n\n## repository\nDarg-Iztech/gender-prediction-from-tweets\n## files\nparameters_ar.py, parameters_en.py, parameters_es.py, modelDeleter.py, visualizer.py, readGloveEmbeddings, readCharEmbeddings, readCaptions, partite_dataset_vectors, prepVectorBatchData, tweet_per_user, tweet_per_user, model_tuple, model_tuple, validation_set_size\n## tags\n[gender prediction]\n\n## repository\nmataney/BootstrappingRelationExtractors\n## files\nclassification/docred.py, classification/re_processors.py, classification/tacred.py, classification/test_docred.py, classification/test_tacred.py, remove_same_sent_id, seperate_entities, merge_and_save_examples, download_from_spike_search, update_file_lengths, input_features_class, special_tokens_file, e2_entities, generation_file, e1_entities\n## 3 tags\n[relation extraction, specific entity recognition, standardization]\n\n## repository\ncd', 'raw_reference_text': 'relation extraction, data augmentation', 'reference_text': 'data augmentation relation extraction', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8389986157417297, 'bertscore_recall': 0.8575412034988403, 'bertscore_f1': 0.8481685519218445, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.19783398509025574}, {'repo': 'snap-stanford/GraphRNN', 'tasks_x': 'graph neural networks, tag:graph%20neural%20networks),', 'true_tasks': ['graph generation'], 'generated_text': 'anomaly detection', 'prompt_info': {'content': 'test_MMD.py, eval/mmd.py, eval/stats.py, dataset/ENZYMES/load_data.py, eval/setup.py, test_mlp_partial_epoch, test_mlp_partial_epoch, test_mlp_partial_epoch, train_mlp_forward_epoch, train_mlp_forward_epoch, max_num_nodes, node_embedding_size, test_graph_num, timing_save_path, timing_save_path', 'generated_text': '\n## repository\nvsaase/simpleBAD\n## files\nauc_delong_xu.py, inversecovariance.py, mri_dataloaders.py, pcakernel.py, sampleanalysis.py, create_patho_sample44, antsAffineToOrthogonal, inverse_MNI_T2, allRegistrations, auc_ci_Delong, voxnorm, addmin, maskloss, anon, Dataset3dH5\n## tags\n[anomaly detection]\n\n## repository\nopengm/opengm\n## files\nsrc/interfaces/python/test.py, src/interfaces/python/examples/denoise.py, src/interfaces/python/examples/interpixel_boundary_segmentation.py, src/interfaces/python/examples/pure_python_icm.py, src/interfaces/python/examples/python_function.py, to_native_boost_python_enum_converter, to_native_boost_python_enum_converter, to_native_build_in_simple_class_converter, to_native_build_in_simple_class_converter, to_native_tribool_converter, getIterations, timingVisitor, variableIndices, solverClass, factorIndices\n## tags\n[semantic segmentation]\n\n## repository\nsnap-stanford/GraphRNN\n## files\ntest_MMD.py, eval/mmd.py, eval/stats.py, dataset/ENZYMES/load_data.py, eval/setup.py, test_mlp_partial_epoch, test_mlp_partial_epoch, test_mlp_partial_epoch, train_mlp_forward_epoch, train_mlp_forward_epoch, max_num_nodes, node_embedding_size, test_graph_num, timing_save_path, timing_save_path\n## 3 tags\n[graph neural networks](tag:graph%20neural%20networks),', 'id': '74b734979b3f63ac27e210be445bfa28', 'name': 'snap-stanford/GraphRNN', 'tasks': 'graph neural networks, tag:graph%20neural%20networks),', 'true_text': 'graph generation'}, 'generation': 0, 'input_text': '\n## repository\nvsaase/simpleBAD\n## files\nauc_delong_xu.py, inversecovariance.py, mri_dataloaders.py, pcakernel.py, sampleanalysis.py, create_patho_sample44, antsAffineToOrthogonal, inverse_MNI_T2, allRegistrations, auc_ci_Delong, voxnorm, addmin, maskloss, anon, Dataset3dH5\n## tags\n[anomaly detection]\n\n## repository\nopengm/opengm\n## files\nsrc/interfaces/python/test.py, src/interfaces/python/examples/denoise.py, src/interfaces/python/examples/interpixel_boundary_segmentation.py, src/interfaces/python/examples/pure_python_icm.py, src/interfaces/python/examples/python_function.py, to_native_boost_python_enum_converter, to_native_boost_python_enum_converter, to_native_build_in_simple_class_converter, to_native_build_in_simple_class_converter, to_native_tribool_converter, getIterations, timingVisitor, variableIndices, solverClass, factorIndices\n## tags\n[semantic segmentation]\n\n## repository\nsnap-stanford/GraphRNN\n## files\ntest_MMD.py, eval/mmd.py, eval/stats.py, dataset/ENZYMES/load_data.py, eval/setup.py, test_mlp_partial_epoch, test_mlp_partial_epoch, test_mlp_partial_epoch, train_mlp_forward_epoch, train_mlp_forward_epoch, max_num_nodes, node_embedding_size, test_graph_num, timing_save_path, timing_save_path\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/graphrnn-generating-realistic-graphs-with']", 'paper_titles': "['GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models']", 'arxiv_ids': "['1802.08773']", 'authors': '["[\'Jiaxuan You\', \'Rex Ying\', \'Xiang Ren\', \'William L. Hamilton\', \'Jure Leskovec\']"]', 'raw_generated_text': '\n## repository\nvsaase/simpleBAD\n## files\nauc_delong_xu.py, inversecovariance.py, mri_dataloaders.py, pcakernel.py, sampleanalysis.py, create_patho_sample44, antsAffineToOrthogonal, inverse_MNI_T2, allRegistrations, auc_ci_Delong, voxnorm, addmin, maskloss, anon, Dataset3dH5\n## tags\n[anomaly detection]\n\n## repository\nopengm/opengm\n## files\nsrc/interfaces/python/test.py, src/interfaces/python/examples/denoise.py, src/interfaces/python/examples/interpixel_boundary_segmentation.py, src/interfaces/python/examples/pure_python_icm.py, src/interfaces/python/examples/python_function.py, to_native_boost_python_enum_converter, to_native_boost_python_enum_converter, to_native_build_in_simple_class_converter, to_native_build_in_simple_class_converter, to_native_tribool_converter, getIterations, timingVisitor, variableIndices, solverClass, factorIndices\n## tags\n[semantic segmentation]\n\n## repository\nsnap-stanford/GraphRNN\n## files\ntest_MMD.py, eval/mmd.py, eval/stats.py, dataset/ENZYMES/load_data.py, eval/setup.py, test_mlp_partial_epoch, test_mlp_partial_epoch, test_mlp_partial_epoch, train_mlp_forward_epoch, train_mlp_forward_epoch, max_num_nodes, node_embedding_size, test_graph_num, timing_save_path, timing_save_path\n## 3 tags\n[graph neural networks](tag:graph%20neural%20networks),', 'raw_reference_text': 'graph generation', 'reference_text': 'graph generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.9034320116043091, 'bertscore_recall': 0.9034320116043091, 'bertscore_f1': 0.9034320116043091, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.22617287933826447}, {'repo': 'AADeLucia/gpt2-narrative-decoding', 'tasks_x': 'question generation, riddle generation, gentle language style transfer, ', 'true_tasks': ['response generation'], 'generated_text': 'semantic segmentation', 'prompt_info': {'content': 'mturk_human_evaluation/cohen.py, mturk_human_evaluation/create_html_survey.py, mturk_human_evaluation/fleiss.py, mturk_human_evaluation/format_generated_narratives_csv.py, data/count_length.py, sentBERT, distinct_1, distinct_2, create_option_list, create_evaluation_display, pdist, multiply, findall, device, encode', 'generated_text': '\n## repository\nabhineet123/river_ice_segmentation\n## files\nctc_info.py, evolutionary_assignment.py, evolutionary_assignment_multi_type.py, plotIceConcentration.py, random_assignment.py, demo_imagenet, resize_ar_old, put_text_with_background, delete_weights_and_model, save_weights_and_model, seg_paths, seg_cols, labels_col, load_ice_conc_diff, seg_labels\n## tags\n[semantic segmentation]\n\n## repository\nRayhane-mamah/Tacotron-2\n## files\npaper_hparams.py, test_wavenet_feeder.py, wavenet_preprocess.py, datasets/wavenet_preprocessor.py, tacotron/feeder.py, MaskedMeanSquaredError, ResizeConvolution, NearestNeighborUpsample, inv_mel_spectrogram_tensorflow, SubPixelConvolution, GL_on_GPU, speakers_path, tacotron_input, predict_linear, wavenet_random_seed\n## tags\n[speech synthesis]\n\n## repository\nAADeLucia/gpt2-narrative-decoding\n## files\nmturk_human_evaluation/cohen.py, mturk_human_evaluation/create_html_survey.py, mturk_human_evaluation/fleiss.py, mturk_human_evaluation/format_generated_narratives_csv.py, data/count_length.py, sentBERT, distinct_1, distinct_2, create_option_list, create_evaluation_display, pdist, multiply, findall, device, encode\n## 3 tags\n[question generation], [riddle generation], [gentle language style transfer]\n', 'id': '935243d2c3e6dcd2f6393bca10a97d3b', 'name': 'AADeLucia/gpt2-narrative-decoding', 'tasks': 'question generation, riddle generation, gentle language style transfer, ', 'true_text': 'response generation'}, 'generation': 0, 'input_text': '\n## repository\nabhineet123/river_ice_segmentation\n## files\nctc_info.py, evolutionary_assignment.py, evolutionary_assignment_multi_type.py, plotIceConcentration.py, random_assignment.py, demo_imagenet, resize_ar_old, put_text_with_background, delete_weights_and_model, save_weights_and_model, seg_paths, seg_cols, labels_col, load_ice_conc_diff, seg_labels\n## tags\n[semantic segmentation]\n\n## repository\nRayhane-mamah/Tacotron-2\n## files\npaper_hparams.py, test_wavenet_feeder.py, wavenet_preprocess.py, datasets/wavenet_preprocessor.py, tacotron/feeder.py, MaskedMeanSquaredError, ResizeConvolution, NearestNeighborUpsample, inv_mel_spectrogram_tensorflow, SubPixelConvolution, GL_on_GPU, speakers_path, tacotron_input, predict_linear, wavenet_random_seed\n## tags\n[speech synthesis]\n\n## repository\nAADeLucia/gpt2-narrative-decoding\n## files\nmturk_human_evaluation/cohen.py, mturk_human_evaluation/create_html_survey.py, mturk_human_evaluation/fleiss.py, mturk_human_evaluation/format_generated_narratives_csv.py, data/count_length.py, sentBERT, distinct_1, distinct_2, create_option_list, create_evaluation_display, pdist, multiply, findall, device, encode\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/decoding-methods-for-neural-narrative']", 'paper_titles': "['Decoding Methods for Neural Narrative Generation']", 'arxiv_ids': "['2010.07375']", 'authors': '["[\'Alexandra DeLucia\', \'Aaron Mueller\', \'Xiang Lisa Li\', \'Joo Sedoc\']"]', 'raw_generated_text': '\n## repository\nabhineet123/river_ice_segmentation\n## files\nctc_info.py, evolutionary_assignment.py, evolutionary_assignment_multi_type.py, plotIceConcentration.py, random_assignment.py, demo_imagenet, resize_ar_old, put_text_with_background, delete_weights_and_model, save_weights_and_model, seg_paths, seg_cols, labels_col, load_ice_conc_diff, seg_labels\n## tags\n[semantic segmentation]\n\n## repository\nRayhane-mamah/Tacotron-2\n## files\npaper_hparams.py, test_wavenet_feeder.py, wavenet_preprocess.py, datasets/wavenet_preprocessor.py, tacotron/feeder.py, MaskedMeanSquaredError, ResizeConvolution, NearestNeighborUpsample, inv_mel_spectrogram_tensorflow, SubPixelConvolution, GL_on_GPU, speakers_path, tacotron_input, predict_linear, wavenet_random_seed\n## tags\n[speech synthesis]\n\n## repository\nAADeLucia/gpt2-narrative-decoding\n## files\nmturk_human_evaluation/cohen.py, mturk_human_evaluation/create_html_survey.py, mturk_human_evaluation/fleiss.py, mturk_human_evaluation/format_generated_narratives_csv.py, data/count_length.py, sentBERT, distinct_1, distinct_2, create_option_list, create_evaluation_display, pdist, multiply, findall, device, encode\n## 3 tags\n[question generation], [riddle generation], [gentle language style transfer]\n', 'raw_reference_text': 'response generation', 'reference_text': 'response generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8621120452880859, 'bertscore_recall': 0.8779730796813965, 'bertscore_f1': 0.8699702620506287, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.19703514873981476}, {'repo': 'Stream-AD/MIDAS', 'tasks_x': 'graph, structured data, sequence labeling, ', 'true_tasks': ['anomaly detection'], 'generated_text': 'semantic parsing', 'prompt_info': {'content': 'util/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype', 'generated_text': '\n## repository\nzechagl/NLPCC2019-Semantic-Parsing\n## files\nsrc/multitask/deal_with_multi_task.py, src/multitask/get_multi_task_data.py, src/multitask/mspars_multi_task.py, src/patternpair/comb.py, src/patternpair/get_new_train_file.py, MsparsProcessor, return_to_raw, combp, combp2, value_filter_for_superlative, entity_label, reference_label_ids, sketch_label2id, entity_label2id, get_entity_labels\n## tags\n[semantic parsing]\n\n## repository\nhphuongdhsp/Q-Newton-method\n## files\nStochasticGriewank.py, src/cubic_reg.py, src/cubic_reg2.py, src/functionsDirectRun.py, src/functionsDirectRun6.py, f39, f39Der, f39Hessian, f40, f40Der, NewQNewton, RandomNewQNewton, BacktrackingGD, InertialNewtonM, f1Der\n## tags\n[protein folding, stochastic optimization]\n\n## repository\nStream-AD/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[graph, structured data]\n[sequence labeling]\n\n## repository\n', 'id': '9d21b64428be42d1059cec05e6b68130', 'name': 'Stream-AD/MIDAS', 'tasks': 'graph, structured data, sequence labeling, ', 'true_text': 'anomaly detection'}, 'generation': 0, 'input_text': '\n## repository\nzechagl/NLPCC2019-Semantic-Parsing\n## files\nsrc/multitask/deal_with_multi_task.py, src/multitask/get_multi_task_data.py, src/multitask/mspars_multi_task.py, src/patternpair/comb.py, src/patternpair/get_new_train_file.py, MsparsProcessor, return_to_raw, combp, combp2, value_filter_for_superlative, entity_label, reference_label_ids, sketch_label2id, entity_label2id, get_entity_labels\n## tags\n[semantic parsing]\n\n## repository\nhphuongdhsp/Q-Newton-method\n## files\nStochasticGriewank.py, src/cubic_reg.py, src/cubic_reg2.py, src/functionsDirectRun.py, src/functionsDirectRun6.py, f39, f39Der, f39Hessian, f40, f40Der, NewQNewton, RandomNewQNewton, BacktrackingGD, InertialNewtonM, f1Der\n## tags\n[protein folding, stochastic optimization]\n\n## repository\nStream-AD/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/real-time-streaming-anomaly-detection-in', 'https://paperswithcode.com/paper/midas-microcluster-based-detector-of']", 'paper_titles': "['Real-Time Anomaly Detection in Edge Streams', 'MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams']", 'arxiv_ids': "['2009.08452', '1911.04464']", 'authors': '["[\'Siddharth Bhatia\', \'Rui Liu\', \'Bryan Hooi\', \'Minji Yoon\', \'Kijung Shin\', \'Christos Faloutsos\']", "[\'Siddharth Bhatia\', \'Bryan Hooi\', \'Minji Yoon\', \'Kijung Shin\', \'Christos Faloutsos\']"]', 'raw_generated_text': '\n## repository\nzechagl/NLPCC2019-Semantic-Parsing\n## files\nsrc/multitask/deal_with_multi_task.py, src/multitask/get_multi_task_data.py, src/multitask/mspars_multi_task.py, src/patternpair/comb.py, src/patternpair/get_new_train_file.py, MsparsProcessor, return_to_raw, combp, combp2, value_filter_for_superlative, entity_label, reference_label_ids, sketch_label2id, entity_label2id, get_entity_labels\n## tags\n[semantic parsing]\n\n## repository\nhphuongdhsp/Q-Newton-method\n## files\nStochasticGriewank.py, src/cubic_reg.py, src/cubic_reg2.py, src/functionsDirectRun.py, src/functionsDirectRun6.py, f39, f39Der, f39Hessian, f40, f40Der, NewQNewton, RandomNewQNewton, BacktrackingGD, InertialNewtonM, f1Der\n## tags\n[protein folding, stochastic optimization]\n\n## repository\nStream-AD/MIDAS\n## files\nutil/PreprocessData.py, util/ReproduceROC.py, util/EvaluateScore.py, util/DeleteTempFile.py, darpa_original, codes, to_csv, read_csv, concat, astype\n## 3 tags\n[graph, structured data]\n[sequence labeling]\n\n## repository\n', 'raw_reference_text': 'anomaly detection', 'reference_text': 'anomaly detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8885875940322876, 'bertscore_recall': 0.8885875940322876, 'bertscore_f1': 0.8885875940322876, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.3470976650714874}, {'repo': 'zjunlp/DiagnoseRE', 'tasks_x': 'RNN, attention, sequence prediction, ', 'true_tasks': ['relation extraction'], 'generated_text': 'time series', 'prompt_info': {'content': 'Q1/random_perturb.py, Q1/en_core_web_sm-2.3.1/setup.py, Q1/en_core_web_sm-2.3.1/en_core_web_sm/__init__.py, Q2/attack.py, Q2/victim.py, sample2dataset, perturb_context_by_type, dataset2sample, REClassifier, list_requirements, InvokeLimitedAttackEval, relation_path, dataset2sample, REClassifier, generate_adv', 'generated_text': '\n## repository\nmpc-msri/EzPC\n## files\nAthos/CompileONNXGraph.py, Athos/CompileRandomForests.py, Athos/CompileSampleNetworks.py, Athos/CompileTFGraph.py, Athos/CompilerScripts/change_onnx_output.py, prog_merge, add_idx_priv, generateBroadcastLoopBOp, getFlatArrIdxExpr, shrDefault, saveImgAndWtData, savePreTrainedWeightsInt, scalingFac, savePreTrainedWeightsFloat, gemm_reference_implementation\n## tags\n[time series]\n\n## repository\nElliottYan/DS_Temporal\n## files\ncnn_att.py, cnn_ave.py, cnn_one.py, cnn_rel_mem.py, cnn_word_mem.py, multi_hot_label, precision_recall_compute_multi, compute_max_f1, compute_average_f1, logging_existing_tensor, read_in_en2id, filter_by_dataset, save_wiki_time, fp16_utils, time_signature\n## tags\n[relation extraction]\n\n## repository\nzjunlp/DiagnoseRE\n## files\nQ1/random_perturb.py, Q1/en_core_web_sm-2.3.1/setup.py, Q1/en_core_web_sm-2.3.1/en_core_web_sm/__init__.py, Q2/attack.py, Q2/victim.py, sample2dataset, perturb_context_by_type, dataset2sample, REClassifier, list_requirements, InvokeLimitedAttackEval, relation_path, dataset2sample, REClassifier, generate_adv\n## 3 tags\n[RNN], [attention], [sequence prediction]\n\n## repository\nhind', 'id': 'df048b0e3828a3e4497d378511ea3833', 'name': 'zjunlp/DiagnoseRE', 'tasks': 'RNN, attention, sequence prediction, ', 'true_text': 'relation extraction'}, 'generation': 0, 'input_text': '\n## repository\nmpc-msri/EzPC\n## files\nAthos/CompileONNXGraph.py, Athos/CompileRandomForests.py, Athos/CompileSampleNetworks.py, Athos/CompileTFGraph.py, Athos/CompilerScripts/change_onnx_output.py, prog_merge, add_idx_priv, generateBroadcastLoopBOp, getFlatArrIdxExpr, shrDefault, saveImgAndWtData, savePreTrainedWeightsInt, scalingFac, savePreTrainedWeightsFloat, gemm_reference_implementation\n## tags\n[time series]\n\n## repository\nElliottYan/DS_Temporal\n## files\ncnn_att.py, cnn_ave.py, cnn_one.py, cnn_rel_mem.py, cnn_word_mem.py, multi_hot_label, precision_recall_compute_multi, compute_max_f1, compute_average_f1, logging_existing_tensor, read_in_en2id, filter_by_dataset, save_wiki_time, fp16_utils, time_signature\n## tags\n[relation extraction]\n\n## repository\nzjunlp/DiagnoseRE\n## files\nQ1/random_perturb.py, Q1/en_core_web_sm-2.3.1/setup.py, Q1/en_core_web_sm-2.3.1/en_core_web_sm/__init__.py, Q2/attack.py, Q2/victim.py, sample2dataset, perturb_context_by_type, dataset2sample, REClassifier, list_requirements, InvokeLimitedAttackEval, relation_path, dataset2sample, REClassifier, generate_adv\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/can-fine-tuning-pre-trained-models-lead-to']", 'paper_titles': "['On Robustness and Bias Analysis of BERT-based Relation Extraction']", 'arxiv_ids': "['2009.06206']", 'authors': '["[\'Luoqiu Li\', \'Xiang Chen\', \'Hongbin Ye\', \'Zhen Bi\', \'Shumin Deng\', \'Ningyu Zhang\', \'Huajun Chen\']"]', 'raw_generated_text': '\n## repository\nmpc-msri/EzPC\n## files\nAthos/CompileONNXGraph.py, Athos/CompileRandomForests.py, Athos/CompileSampleNetworks.py, Athos/CompileTFGraph.py, Athos/CompilerScripts/change_onnx_output.py, prog_merge, add_idx_priv, generateBroadcastLoopBOp, getFlatArrIdxExpr, shrDefault, saveImgAndWtData, savePreTrainedWeightsInt, scalingFac, savePreTrainedWeightsFloat, gemm_reference_implementation\n## tags\n[time series]\n\n## repository\nElliottYan/DS_Temporal\n## files\ncnn_att.py, cnn_ave.py, cnn_one.py, cnn_rel_mem.py, cnn_word_mem.py, multi_hot_label, precision_recall_compute_multi, compute_max_f1, compute_average_f1, logging_existing_tensor, read_in_en2id, filter_by_dataset, save_wiki_time, fp16_utils, time_signature\n## tags\n[relation extraction]\n\n## repository\nzjunlp/DiagnoseRE\n## files\nQ1/random_perturb.py, Q1/en_core_web_sm-2.3.1/setup.py, Q1/en_core_web_sm-2.3.1/en_core_web_sm/__init__.py, Q2/attack.py, Q2/victim.py, sample2dataset, perturb_context_by_type, dataset2sample, REClassifier, list_requirements, InvokeLimitedAttackEval, relation_path, dataset2sample, REClassifier, generate_adv\n## 3 tags\n[RNN], [attention], [sequence prediction]\n\n## repository\nhind', 'raw_reference_text': 'relation extraction', 'reference_text': 'relation extraction', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.889580488204956, 'bertscore_recall': 0.8742378950119019, 'bertscore_f1': 0.8818424344062805, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.20988193154335022}, {'repo': 'jraiskin/arxiv2kindle_pdf', 'tasks_x': 'nlp, kindle, pdf, ', 'true_tasks': ['style transfer'], 'generated_text': 'video text retrieval multiple instance learning cross modal retrieval', 'prompt_info': {'content': 'arxiv2kindle_pdf.py, yn_to_bool, get_args, main, line_filter_gen, yn_to_bool, article_id, landscape, landscape', 'generated_text': '\n## repository\nyalesong/pvse\n## files\ndata/mrw/download_gulp_mrw.py, data/tgif/gulp_tgif.py, video_transforms.py, option.py, vocab.py, get_video_transform, get_uid_tgif, convert_old_state_dict, PIENet, PVSE, eval_on_gpu, eval_on_gpu, PVSE, PVSELoss, copy_input_args_from_ckpt\n## tags\n[multiple instance learning, cross modal retrieval, video text retrieval]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## tags\n[imitation learning]\n\n## repository\njraiskin/arxiv2kindle_pdf\n## files\narxiv2kindle_pdf.py, yn_to_bool, get_args, main, line_filter_gen, yn_to_bool, article_id, landscape, landscape\n## 3 tags\n[nlp, kindle, pdf]\n\n## repository\nlucasb-', 'id': '344142fa2df88aa4862571ef1277b6c3', 'name': 'jraiskin/arxiv2kindle_pdf', 'tasks': 'nlp, kindle, pdf, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\nyalesong/pvse\n## files\ndata/mrw/download_gulp_mrw.py, data/tgif/gulp_tgif.py, video_transforms.py, option.py, vocab.py, get_video_transform, get_uid_tgif, convert_old_state_dict, PIENet, PVSE, eval_on_gpu, eval_on_gpu, PVSE, PVSELoss, copy_input_args_from_ckpt\n## tags\n[multiple instance learning, cross modal retrieval, video text retrieval]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## tags\n[imitation learning]\n\n## repository\njraiskin/arxiv2kindle_pdf\n## files\narxiv2kindle_pdf.py, yn_to_bool, get_args, main, line_filter_gen, yn_to_bool, article_id, landscape, landscape\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-neural-algorithm-of-artistic-style']", 'paper_titles': "['A Neural Algorithm of Artistic Style']", 'arxiv_ids': "['1508.06576']", 'authors': '["[\'Leon A. Gatys\', \'Alexander S. Ecker\', \'Matthias Bethge\']"]', 'raw_generated_text': '\n## repository\nyalesong/pvse\n## files\ndata/mrw/download_gulp_mrw.py, data/tgif/gulp_tgif.py, video_transforms.py, option.py, vocab.py, get_video_transform, get_uid_tgif, convert_old_state_dict, PIENet, PVSE, eval_on_gpu, eval_on_gpu, PVSE, PVSELoss, copy_input_args_from_ckpt\n## tags\n[multiple instance learning, cross modal retrieval, video text retrieval]\n\n## repository\nezhan94/multiagent-programmatic-supervision\n## files\ndatasets/bball/core.py, datasets/bball/label_macro_intents.py, datasets/bball/visualize.py, datasets/boids/boid.py, datasets/boids/core.py, hyperparams_str, cudafy_list, index_by_agent, get_macro_ohe, entropy_gauss, SEQUENCE_DIMENSION, BOOST_MAX, R_LOCAL, R_CLOSE, BOOST_MIN\n## tags\n[imitation learning]\n\n## repository\njraiskin/arxiv2kindle_pdf\n## files\narxiv2kindle_pdf.py, yn_to_bool, get_args, main, line_filter_gen, yn_to_bool, article_id, landscape, landscape\n## 3 tags\n[nlp, kindle, pdf]\n\n## repository\nlucasb-', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8033071756362915, 'bertscore_recall': 0.8706811666488647, 'bertscore_f1': 0.8356383442878723, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.20290008187294006}, {'repo': 'acm-uiuc/racerbot.rl', 'tasks_x': 'reinforcement learning, tags#reinforcement-learning), neural network', 'true_tasks': ['imitation learning'], 'generated_text': 'link prediction', 'prompt_info': {'content': 'conditional_imitation/config/racecar/scripts/listener.py, conditional_imitation/config/racecar/scripts/talker.py, conditional_imitation/scripts/convert.py, conditional_imitation/scripts/dataloader.py, conditional_imitation/scripts/preprocess.py, get_MC_accuracy_loss, tinyimagenet_loader, upsample_resnet, load_cifar100_numpy, plot_sync_accuracies, registerCallback, get_dataset_from_loader, ApproximateTimeSynchronizer, create_val_folder, CvBridge', 'generated_text': '\n## repository\nsreycodes/YLR\n## files\ncode/graph_vae.py, code/graph_vae_orig.py, code/graph_vae_weighted.py, code/gae/initializations.py, code/gae/input_data.py, get_bridge_edges, mask_test_edges_directed, DirectedGCNModelVAE, GravityInspiredDecoder, get_graph_and_features, get_network_statistics, reconstructions, draw_spring, bridges, bridges\n## tags\n[link prediction]\n\n## repository\nchaofengc/Face-Sketch\n## files\nsketch_generate.py, style_generate.py, train_content_net.py, utils/compare_patch.py, utils/content_model.py, PCA_recognition, cal_DOF, save_train_feat, generate_target_style, build_feat_function, gen_position_map, get_out_var, cal_DOF, ContentNet, compare_patch\n## tags\n[style transfer]\n\n## repository\nacm-uiuc/racerbot.rl\n## files\nconditional_imitation/config/racecar/scripts/listener.py, conditional_imitation/config/racecar/scripts/talker.py, conditional_imitation/scripts/convert.py, conditional_imitation/scripts/dataloader.py, conditional_imitation/scripts/preprocess.py, get_MC_accuracy_loss, tinyimagenet_loader, upsample_resnet, load_cifar100_numpy, plot_sync_accuracies, registerCallback, get_dataset_from_loader, ApproximateTimeSynchronizer, create_val_folder, CvBridge\n## 3 tags\n[reinforcement learning](/tags#reinforcement-learning), [neural network', 'id': '8ee2c6139a930498f1424fa5763a69d7', 'name': 'acm-uiuc/racerbot.rl', 'tasks': 'reinforcement learning, tags#reinforcement-learning), neural network', 'true_text': 'imitation learning'}, 'generation': 0, 'input_text': '\n## repository\nsreycodes/YLR\n## files\ncode/graph_vae.py, code/graph_vae_orig.py, code/graph_vae_weighted.py, code/gae/initializations.py, code/gae/input_data.py, get_bridge_edges, mask_test_edges_directed, DirectedGCNModelVAE, GravityInspiredDecoder, get_graph_and_features, get_network_statistics, reconstructions, draw_spring, bridges, bridges\n## tags\n[link prediction]\n\n## repository\nchaofengc/Face-Sketch\n## files\nsketch_generate.py, style_generate.py, train_content_net.py, utils/compare_patch.py, utils/content_model.py, PCA_recognition, cal_DOF, save_train_feat, generate_target_style, build_feat_function, gen_position_map, get_out_var, cal_DOF, ContentNet, compare_patch\n## tags\n[style transfer]\n\n## repository\nacm-uiuc/racerbot.rl\n## files\nconditional_imitation/config/racecar/scripts/listener.py, conditional_imitation/config/racecar/scripts/talker.py, conditional_imitation/scripts/convert.py, conditional_imitation/scripts/dataloader.py, conditional_imitation/scripts/preprocess.py, get_MC_accuracy_loss, tinyimagenet_loader, upsample_resnet, load_cifar100_numpy, plot_sync_accuracies, registerCallback, get_dataset_from_loader, ApproximateTimeSynchronizer, create_val_folder, CvBridge\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/end-to-end-driving-via-conditional-imitation']", 'paper_titles': "['End-to-end Driving via Conditional Imitation Learning']", 'arxiv_ids': "['1710.02410']", 'authors': '["[\'Felipe Codevilla\', \'Matthias Mller\', \'Antonio Lpez\', \'Vladlen Koltun\', \'Alexey Dosovitskiy\']"]', 'raw_generated_text': '\n## repository\nsreycodes/YLR\n## files\ncode/graph_vae.py, code/graph_vae_orig.py, code/graph_vae_weighted.py, code/gae/initializations.py, code/gae/input_data.py, get_bridge_edges, mask_test_edges_directed, DirectedGCNModelVAE, GravityInspiredDecoder, get_graph_and_features, get_network_statistics, reconstructions, draw_spring, bridges, bridges\n## tags\n[link prediction]\n\n## repository\nchaofengc/Face-Sketch\n## files\nsketch_generate.py, style_generate.py, train_content_net.py, utils/compare_patch.py, utils/content_model.py, PCA_recognition, cal_DOF, save_train_feat, generate_target_style, build_feat_function, gen_position_map, get_out_var, cal_DOF, ContentNet, compare_patch\n## tags\n[style transfer]\n\n## repository\nacm-uiuc/racerbot.rl\n## files\nconditional_imitation/config/racecar/scripts/listener.py, conditional_imitation/config/racecar/scripts/talker.py, conditional_imitation/scripts/convert.py, conditional_imitation/scripts/dataloader.py, conditional_imitation/scripts/preprocess.py, get_MC_accuracy_loss, tinyimagenet_loader, upsample_resnet, load_cifar100_numpy, plot_sync_accuracies, registerCallback, get_dataset_from_loader, ApproximateTimeSynchronizer, create_val_folder, CvBridge\n## 3 tags\n[reinforcement learning](/tags#reinforcement-learning), [neural network', 'raw_reference_text': 'imitation learning', 'reference_text': 'imitation learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.9037057757377625, 'bertscore_recall': 0.8947645425796509, 'bertscore_f1': 0.8992129564285278, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14225131273269653}, {'repo': 'belph/wiki-sem-500', 'tasks_x': 'text embedding, inn, text processing, random forest, ', 'true_tasks': ['word embeddings', 'outlier detection', 'sentiment analysis'], 'generated_text': 'stochastic optimization', 'prompt_info': {'content': 'install_dependencies.py, src/outlier_test_group.py, src/lib/polyglot/base.py, src/lib/polyglot/utils.py, src/lib/polyglot/mapping/base.py, read_dataset_directory, score_embedding, phrase_gen, WrappedEmbedding, TextFiles, num_cases, num_total_groups, opp, __self__, __mro__', 'generated_text': '\n## repository\ncjc77/gaussian_processes_final_project\n## files\nacquisition/acquisition_functions.py, acquisition/acquisition_optimizers.py, hp_optimizers/hp_optimizer.py, util/defs.py, util/util.py, random_x_sample, scalar_or_1d_to_2d, singleton_to_scalar, RandomAcquisitionOpt, ConstrainedAcquisitionOpt, random, uniform, round, array\n## tags\n[stochastic optimization]\n\n## repository\nSharpenb/python_paris\n## files\npython_paris/cluster_cut_slicer.py, python_paris/distance_slicer.py, python_paris/heterogeneous_cut_slicer.py, python_paris/homogeneous_cut_slicer.py, python_paris/paris.py, clustering_from_cluster_cut, best_cluster_cut, ranking_cluster_cuts, clustering_from_distance, ranking_distances, best_heterogeneous_cut, best_cut, old_score, old_score, old_score\n## tags\n[graph clustering]\n\n## repository\nbelph/wiki-sem-500\n## files\ninstall_dependencies.py, src/outlier_test_group.py, src/lib/polyglot/base.py, src/lib/polyglot/utils.py, src/lib/polyglot/mapping/base.py, read_dataset_directory, score_embedding, phrase_gen, WrappedEmbedding, TextFiles, num_cases, num_total_groups, opp, __self__, __mro__\n## 3 tags\n[text embedding, inn,  text processing, random forest]\n\n## repository', 'id': 'ff59fb82e237e0999a5ecc453d3381d3', 'name': 'belph/wiki-sem-500', 'tasks': 'text embedding, inn, text processing, random forest, ', 'true_text': 'word embeddings, outlier detection, sentiment analysis'}, 'generation': 0, 'input_text': '\n## repository\ncjc77/gaussian_processes_final_project\n## files\nacquisition/acquisition_functions.py, acquisition/acquisition_optimizers.py, hp_optimizers/hp_optimizer.py, util/defs.py, util/util.py, random_x_sample, scalar_or_1d_to_2d, singleton_to_scalar, RandomAcquisitionOpt, ConstrainedAcquisitionOpt, random, uniform, round, array\n## tags\n[stochastic optimization]\n\n## repository\nSharpenb/python_paris\n## files\npython_paris/cluster_cut_slicer.py, python_paris/distance_slicer.py, python_paris/heterogeneous_cut_slicer.py, python_paris/homogeneous_cut_slicer.py, python_paris/paris.py, clustering_from_cluster_cut, best_cluster_cut, ranking_cluster_cuts, clustering_from_distance, ranking_distances, best_heterogeneous_cut, best_cut, old_score, old_score, old_score\n## tags\n[graph clustering]\n\n## repository\nbelph/wiki-sem-500\n## files\ninstall_dependencies.py, src/outlier_test_group.py, src/lib/polyglot/base.py, src/lib/polyglot/utils.py, src/lib/polyglot/mapping/base.py, read_dataset_directory, score_embedding, phrase_gen, WrappedEmbedding, TextFiles, num_cases, num_total_groups, opp, __self__, __mro__\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/automated-generation-of-multilingual-clusters']", 'paper_titles': "['Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations']", 'arxiv_ids': "['1611.01547']", 'authors': '["[\'Philip Blair\', \'Yuval Merhav\', \'Joel Barry\']"]', 'raw_generated_text': '\n## repository\ncjc77/gaussian_processes_final_project\n## files\nacquisition/acquisition_functions.py, acquisition/acquisition_optimizers.py, hp_optimizers/hp_optimizer.py, util/defs.py, util/util.py, random_x_sample, scalar_or_1d_to_2d, singleton_to_scalar, RandomAcquisitionOpt, ConstrainedAcquisitionOpt, random, uniform, round, array\n## tags\n[stochastic optimization]\n\n## repository\nSharpenb/python_paris\n## files\npython_paris/cluster_cut_slicer.py, python_paris/distance_slicer.py, python_paris/heterogeneous_cut_slicer.py, python_paris/homogeneous_cut_slicer.py, python_paris/paris.py, clustering_from_cluster_cut, best_cluster_cut, ranking_cluster_cuts, clustering_from_distance, ranking_distances, best_heterogeneous_cut, best_cut, old_score, old_score, old_score\n## tags\n[graph clustering]\n\n## repository\nbelph/wiki-sem-500\n## files\ninstall_dependencies.py, src/outlier_test_group.py, src/lib/polyglot/base.py, src/lib/polyglot/utils.py, src/lib/polyglot/mapping/base.py, read_dataset_directory, score_embedding, phrase_gen, WrappedEmbedding, TextFiles, num_cases, num_total_groups, opp, __self__, __mro__\n## 3 tags\n[text embedding, inn,  text processing, random forest]\n\n## repository', 'raw_reference_text': 'word embeddings, outlier detection, sentiment analysis', 'reference_text': 'outlier detection word embeddings sentiment analysis', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.7880303263664246, 'bertscore_recall': 0.810270369052887, 'bertscore_f1': 0.7989956140518188, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.2712787389755249}, {'repo': 'tensorfreitas/Siamese-Networks-for-One-Shot-Learning', 'tasks_x': 'bayesian optimization, image processing, neural network, BayesianHyperparameterOptimization', 'true_tasks': ['one shot learning'], 'generated_text': 'scene text recognition optical character recognition', 'prompt_info': {'content': 'bayesian_hyperparameter_optimization.py, image_augmentor.py, modified_sgd.py, siamese_network.py, omniglot_loader.py, Modified_SGD, OmniglotLoader, ImageAugmentor, SiameseNetwork, main, train_siamese_network, one_shot_test, fx_opt, x_opt, run_optimization', 'generated_text': '\n## repository\nL706077/OCR-CRNN\n## files\ntool/create_dataset.py, checkImageIsValid, writeCache, createDataset, writeCache, imdecode, IMREAD_GRAYSCALE, fromstring, xrange\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\ntingweidaniel/chemprop_atomicfp_save_depth_final\n## files\nINCHI_detection.py, TEST/test.py, web/wsgi.py, chemprop_structure_splitting_data/random_split.py, chemprop/random_forest.py, is_zwitterion, bond_in_member_rings, with_message_passing, num_atom_in_ring, num_bond_in_ring, use_compound_names, compound_names, separate_val_path, write_true_val, write_true_val\n## tags\n[molecular property prediction]\n\n## repository\ntensorfreitas/Siamese-Networks-for-One-Shot-Learning\n## files\nbayesian_hyperparameter_optimization.py, image_augmentor.py, modified_sgd.py, siamese_network.py, omniglot_loader.py, Modified_SGD, OmniglotLoader, ImageAugmentor, SiameseNetwork, main, train_siamese_network, one_shot_test, fx_opt, x_opt, run_optimization\n## 3 tags\n[bayesian optimization, image processing, neural network]\nBayesianHyperparameterOptimization', 'id': '0b82cbc6859aba8636273d64da608167', 'name': 'tensorfreitas/Siamese-Networks-for-One-Shot-Learning', 'tasks': 'bayesian optimization, image processing, neural network, BayesianHyperparameterOptimization', 'true_text': 'one shot learning'}, 'generation': 0, 'input_text': '\n## repository\nL706077/OCR-CRNN\n## files\ntool/create_dataset.py, checkImageIsValid, writeCache, createDataset, writeCache, imdecode, IMREAD_GRAYSCALE, fromstring, xrange\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\ntingweidaniel/chemprop_atomicfp_save_depth_final\n## files\nINCHI_detection.py, TEST/test.py, web/wsgi.py, chemprop_structure_splitting_data/random_split.py, chemprop/random_forest.py, is_zwitterion, bond_in_member_rings, with_message_passing, num_atom_in_ring, num_bond_in_ring, use_compound_names, compound_names, separate_val_path, write_true_val, write_true_val\n## tags\n[molecular property prediction]\n\n## repository\ntensorfreitas/Siamese-Networks-for-One-Shot-Learning\n## files\nbayesian_hyperparameter_optimization.py, image_augmentor.py, modified_sgd.py, siamese_network.py, omniglot_loader.py, Modified_SGD, OmniglotLoader, ImageAugmentor, SiameseNetwork, main, train_siamese_network, one_shot_test, fx_opt, x_opt, run_optimization\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/siamese-neural-networks-for-one-shot-image']", 'paper_titles': "['Siamese neural networks for one-shot image recognition']", 'arxiv_ids': '[nan]', 'authors': '["[\'Gregory Koch\', \'Richard Zemel\', \'Ruslan Salakhutdinov\']"]', 'raw_generated_text': '\n## repository\nL706077/OCR-CRNN\n## files\ntool/create_dataset.py, checkImageIsValid, writeCache, createDataset, writeCache, imdecode, IMREAD_GRAYSCALE, fromstring, xrange\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\ntingweidaniel/chemprop_atomicfp_save_depth_final\n## files\nINCHI_detection.py, TEST/test.py, web/wsgi.py, chemprop_structure_splitting_data/random_split.py, chemprop/random_forest.py, is_zwitterion, bond_in_member_rings, with_message_passing, num_atom_in_ring, num_bond_in_ring, use_compound_names, compound_names, separate_val_path, write_true_val, write_true_val\n## tags\n[molecular property prediction]\n\n## repository\ntensorfreitas/Siamese-Networks-for-One-Shot-Learning\n## files\nbayesian_hyperparameter_optimization.py, image_augmentor.py, modified_sgd.py, siamese_network.py, omniglot_loader.py, Modified_SGD, OmniglotLoader, ImageAugmentor, SiameseNetwork, main, train_siamese_network, one_shot_test, fx_opt, x_opt, run_optimization\n## 3 tags\n[bayesian optimization, image processing, neural network]\nBayesianHyperparameterOptimization', 'raw_reference_text': 'one shot learning', 'reference_text': 'one shot learning', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8383169770240784, 'bertscore_recall': 0.8497133255004883, 'bertscore_f1': 0.8439766764640808, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.0946904718875885}, {'repo': 'ankit-vaghela30/Cilia-Segmentation', 'tasks_x': 'diffusion tensor analysis,endoGastric Image Segmentation, saliency', 'true_tasks': ['semantic segmentation'], 'generated_text': 'gaussian processes', 'prompt_info': {'content': 'hastings/fit_predict_unet.py, hastings/io_support.py, hastings/optical_flow.py, hastings/prepare_data.py, hastings/preprocessing.py, call_prepare, call_prepare, modifyMask, modifyMask, test_load_all_video, modifyMask, load_all_video, load_all_video, up_path, transition_dn', 'generated_text': '\n## repository\nhqminh/gp_sketch_nips\n## files\nembedding_plot.py, mvae.py, parse_res.py, vaegp.py, covs/kernel.py, gas_sensor_data, nll_test, MixtureVAE, MixtureGaussianNet, VAEGP, create_std_plot, cluster_coloring, parse_res, get_cuda_device, get_cuda_device\n## tags\n[gaussian processes]\n\n## repository\njackie840129/STE-NVAN\n## files\ncreate_MARS_database.py, creat_DukeV_database.py, train_NL.py, util/cmc.py, net/models.py, Video_train_collate_fn, Get_Video_train_DataLoader, Video_test_collate_fn, Get_Video_test_DataLoader, np_norm_eudist, query_idx, Video_Cmc, Video_train_Dataset, Video_test_Dataset, np_cdist\n## tags\n[person re identification, video based person re identification]\n\n## repository\nankit-vaghela30/Cilia-Segmentation\n## files\nhastings/fit_predict_unet.py, hastings/io_support.py, hastings/optical_flow.py, hastings/prepare_data.py, hastings/preprocessing.py, call_prepare, call_prepare, modifyMask, modifyMask, test_load_all_video, modifyMask, load_all_video, load_all_video, up_path, transition_dn\n## 3 tags\n[diffusion tensor analysis,endoGastric Image Segmentation, saliency', 'id': '64af01b81da8fdfa67ba125bf3140120', 'name': 'ankit-vaghela30/Cilia-Segmentation', 'tasks': 'diffusion tensor analysis,endoGastric Image Segmentation, saliency', 'true_text': 'semantic segmentation'}, 'generation': 0, 'input_text': '\n## repository\nhqminh/gp_sketch_nips\n## files\nembedding_plot.py, mvae.py, parse_res.py, vaegp.py, covs/kernel.py, gas_sensor_data, nll_test, MixtureVAE, MixtureGaussianNet, VAEGP, create_std_plot, cluster_coloring, parse_res, get_cuda_device, get_cuda_device\n## tags\n[gaussian processes]\n\n## repository\njackie840129/STE-NVAN\n## files\ncreate_MARS_database.py, creat_DukeV_database.py, train_NL.py, util/cmc.py, net/models.py, Video_train_collate_fn, Get_Video_train_DataLoader, Video_test_collate_fn, Get_Video_test_DataLoader, np_norm_eudist, query_idx, Video_Cmc, Video_train_Dataset, Video_test_Dataset, np_cdist\n## tags\n[person re identification, video based person re identification]\n\n## repository\nankit-vaghela30/Cilia-Segmentation\n## files\nhastings/fit_predict_unet.py, hastings/io_support.py, hastings/optical_flow.py, hastings/prepare_data.py, hastings/preprocessing.py, call_prepare, call_prepare, modifyMask, modifyMask, test_load_all_video, modifyMask, load_all_video, load_all_video, up_path, transition_dn\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/the-one-hundred-layers-tiramisu-fully']", 'paper_titles': "['The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation']", 'arxiv_ids': "['1611.09326']", 'authors': '["[\'Simon Jgou\', \'Michal Drozdzal\', \'David Vazquez\', \'Adriana Romero\', \'Yoshua Bengio\']"]', 'raw_generated_text': '\n## repository\nhqminh/gp_sketch_nips\n## files\nembedding_plot.py, mvae.py, parse_res.py, vaegp.py, covs/kernel.py, gas_sensor_data, nll_test, MixtureVAE, MixtureGaussianNet, VAEGP, create_std_plot, cluster_coloring, parse_res, get_cuda_device, get_cuda_device\n## tags\n[gaussian processes]\n\n## repository\njackie840129/STE-NVAN\n## files\ncreate_MARS_database.py, creat_DukeV_database.py, train_NL.py, util/cmc.py, net/models.py, Video_train_collate_fn, Get_Video_train_DataLoader, Video_test_collate_fn, Get_Video_test_DataLoader, np_norm_eudist, query_idx, Video_Cmc, Video_train_Dataset, Video_test_Dataset, np_cdist\n## tags\n[person re identification, video based person re identification]\n\n## repository\nankit-vaghela30/Cilia-Segmentation\n## files\nhastings/fit_predict_unet.py, hastings/io_support.py, hastings/optical_flow.py, hastings/prepare_data.py, hastings/preprocessing.py, call_prepare, call_prepare, modifyMask, modifyMask, test_load_all_video, modifyMask, load_all_video, load_all_video, up_path, transition_dn\n## 3 tags\n[diffusion tensor analysis,endoGastric Image Segmentation, saliency', 'raw_reference_text': 'semantic segmentation', 'reference_text': 'semantic segmentation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8191936612129211, 'bertscore_recall': 0.8605769276618958, 'bertscore_f1': 0.8393755555152893, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.19573384523391724}, {'repo': 'hordiales/transferencia-estilo-sms', 'tasks_x': 'Huamn data, Spanish data, audio, ', 'true_tasks': ['style transfer'], 'generated_text': 'time series gaussian processes', 'prompt_info': {'content': 'visualizar.py, extra/extractTempo.py, sms.py, stft.py, logmag_stft_from_file, melspectrogram_from_file, reconstruccion_fase_griffin_lim, espectrograma, comparar_2_espectrogramas, DCRemoval, offset_filter, timelength, minSineDur, freqDevOffset', 'generated_text': '\n## repository\njik0730/Deep-Mixed-Effect-Model-using-Gaussian-Processes\n## files\nsrc/means.py, src/data_loader.py, src/model.py, evaluate.py, src/utils.py, get_mean_params, DMEGP, fetch_dataloaders_PhysioNet, MLP_embed, Warping_mean, forward_mean, PhysioNetDataset, PhysioNetDataset, FileHandler, Formatter\n## tags\n[time series, gaussian processes]\n\n## repository\nmasqm/Faster-Mean-Shift\n## files\nbin/clustering.py, bin/dataset.py, bin/postprocess.py, bin/segment_and_track.py, bin/color/tif_process.py, find_maximum_in_image, find_quadratic_subpixel_maximum_in_image, split_label_image_with_unknown_labels, relabel_ascending, smooth_label_images, ChangeLabelImageFilter, SetChangeMap, MinimumMaximumImageFilter, embeddings_normalization, unet_recurrent_cell_0\n## tags\n[semantic segmentation, cell segmentation, instance segmentation]\n\n## repository\nhordiales/transferencia-estilo-sms\n## files\nvisualizar.py, extra/extractTempo.py, sms.py, stft.py, logmag_stft_from_file, melspectrogram_from_file, reconstruccion_fase_griffin_lim, espectrograma, comparar_2_espectrogramas, DCRemoval, offset_filter, timelength, minSineDur, freqDevOffset\n## 3 tags\n[Huamn data, Spanish data, audio]\n-->', 'id': '9eb7a9bfd2885e28c8fb7cb866dc3930', 'name': 'hordiales/transferencia-estilo-sms', 'tasks': 'Huamn data, Spanish data, audio, ', 'true_text': 'style transfer'}, 'generation': 0, 'input_text': '\n## repository\njik0730/Deep-Mixed-Effect-Model-using-Gaussian-Processes\n## files\nsrc/means.py, src/data_loader.py, src/model.py, evaluate.py, src/utils.py, get_mean_params, DMEGP, fetch_dataloaders_PhysioNet, MLP_embed, Warping_mean, forward_mean, PhysioNetDataset, PhysioNetDataset, FileHandler, Formatter\n## tags\n[time series, gaussian processes]\n\n## repository\nmasqm/Faster-Mean-Shift\n## files\nbin/clustering.py, bin/dataset.py, bin/postprocess.py, bin/segment_and_track.py, bin/color/tif_process.py, find_maximum_in_image, find_quadratic_subpixel_maximum_in_image, split_label_image_with_unknown_labels, relabel_ascending, smooth_label_images, ChangeLabelImageFilter, SetChangeMap, MinimumMaximumImageFilter, embeddings_normalization, unet_recurrent_cell_0\n## tags\n[semantic segmentation, cell segmentation, instance segmentation]\n\n## repository\nhordiales/transferencia-estilo-sms\n## files\nvisualizar.py, extra/extractTempo.py, sms.py, stft.py, logmag_stft_from_file, melspectrogram_from_file, reconstruccion_fase_griffin_lim, espectrograma, comparar_2_espectrogramas, DCRemoval, offset_filter, timelength, minSineDur, freqDevOffset\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/a-neural-algorithm-of-artistic-style']", 'paper_titles': "['A Neural Algorithm of Artistic Style']", 'arxiv_ids': "['1508.06576']", 'authors': '["[\'Leon A. Gatys\', \'Alexander S. Ecker\', \'Matthias Bethge\']"]', 'raw_generated_text': '\n## repository\njik0730/Deep-Mixed-Effect-Model-using-Gaussian-Processes\n## files\nsrc/means.py, src/data_loader.py, src/model.py, evaluate.py, src/utils.py, get_mean_params, DMEGP, fetch_dataloaders_PhysioNet, MLP_embed, Warping_mean, forward_mean, PhysioNetDataset, PhysioNetDataset, FileHandler, Formatter\n## tags\n[time series, gaussian processes]\n\n## repository\nmasqm/Faster-Mean-Shift\n## files\nbin/clustering.py, bin/dataset.py, bin/postprocess.py, bin/segment_and_track.py, bin/color/tif_process.py, find_maximum_in_image, find_quadratic_subpixel_maximum_in_image, split_label_image_with_unknown_labels, relabel_ascending, smooth_label_images, ChangeLabelImageFilter, SetChangeMap, MinimumMaximumImageFilter, embeddings_normalization, unet_recurrent_cell_0\n## tags\n[semantic segmentation, cell segmentation, instance segmentation]\n\n## repository\nhordiales/transferencia-estilo-sms\n## files\nvisualizar.py, extra/extractTempo.py, sms.py, stft.py, logmag_stft_from_file, melspectrogram_from_file, reconstruccion_fase_griffin_lim, espectrograma, comparar_2_espectrogramas, DCRemoval, offset_filter, timelength, minSineDur, freqDevOffset\n## 3 tags\n[Huamn data, Spanish data, audio]\n-->', 'raw_reference_text': 'style transfer', 'reference_text': 'style transfer', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8098904490470886, 'bertscore_recall': 0.8762092590332031, 'bertscore_f1': 0.8417456150054932, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.16714930534362793}, {'repo': 'XuRen20/deepface', 'tasks_x': 'deep_learning, face_recognation, face_shape, ', 'true_tasks': ['face swapping'], 'generated_text': 'intent detection', 'prompt_info': {'content': 'core/cv2ex.py, core/osex.py, core/pathex.py, core/randomex.py, core/stdex.py, average_gv_list, average_tensor_list, resize2d_bilinear, tf_get_value, convert_98_to_68, BORDER_TRANSPARENT, face_enhancer_func, hist_match_threshold, image_denoise_power, motion_blur_power', 'generated_text': '\n## repository\ntnlin/SMDN\n## files\npreprocessing_swda.py, train_BiLSTM-DOC.py, train_BiLSTM.py, train_HCNN-DOC.py, train_HCNN.py, get_swda, Metrics_HCNN, set_allow_growth, get_speaker_change, HCNN, damsl_act_tag, iter_transcripts, dense1, utterances, CorpusReader\n## tags\n[intent detection]\n\n## repository\nCodeAchieveDream/crnn_model\n## files\ncrnn_train.py, imgaug_image.py, recog.py, tolmdb.py, net/crnn.py, get_batch_label, compute_std_mean, to_alphabet, CRNN_VGG, CRNN_MODEL, SomeOf, crnn, crnn, trainBatch, niter\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nXuRen20/deepface\n## files\ncore/cv2ex.py, core/osex.py, core/pathex.py, core/randomex.py, core/stdex.py, average_gv_list, average_tensor_list, resize2d_bilinear, tf_get_value, convert_98_to_68, BORDER_TRANSPARENT, face_enhancer_func, hist_match_threshold, image_denoise_power, motion_blur_power\n## 3 tags\n[deep_learning, face_recognation, face_shape]\n\n', 'id': '681afe38d69312d3ef8c49f3c468804f', 'name': 'XuRen20/deepface', 'tasks': 'deep_learning, face_recognation, face_shape, ', 'true_text': 'face swapping'}, 'generation': 0, 'input_text': '\n## repository\ntnlin/SMDN\n## files\npreprocessing_swda.py, train_BiLSTM-DOC.py, train_BiLSTM.py, train_HCNN-DOC.py, train_HCNN.py, get_swda, Metrics_HCNN, set_allow_growth, get_speaker_change, HCNN, damsl_act_tag, iter_transcripts, dense1, utterances, CorpusReader\n## tags\n[intent detection]\n\n## repository\nCodeAchieveDream/crnn_model\n## files\ncrnn_train.py, imgaug_image.py, recog.py, tolmdb.py, net/crnn.py, get_batch_label, compute_std_mean, to_alphabet, CRNN_VGG, CRNN_MODEL, SomeOf, crnn, crnn, trainBatch, niter\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nXuRen20/deepface\n## files\ncore/cv2ex.py, core/osex.py, core/pathex.py, core/randomex.py, core/stdex.py, average_gv_list, average_tensor_list, resize2d_bilinear, tf_get_value, convert_98_to_68, BORDER_TRANSPARENT, face_enhancer_func, hist_match_threshold, image_denoise_power, motion_blur_power\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/deepfacelab-a-simple-flexible-and-extensible']", 'paper_titles': "['DeepFaceLab: Integrated, flexible and extensible face-swapping framework']", 'arxiv_ids': "['2005.05535']", 'authors': '["[\'Ivan Perov\', \'Daiheng Gao\', \'Nikolay Chervoniy\', \'Kunlin Liu\', \'Sugasa Marangonda\', \'Chris Um\', \'Mr. Dpfks\', \'Carl Shift Facenheim\', \'Luis RP\', \'Jian Jiang\', \'Sheng Zhang\', \'Pingyu Wu\', \'Bo Zhou\', \'Weiming Zhang\']"]', 'raw_generated_text': '\n## repository\ntnlin/SMDN\n## files\npreprocessing_swda.py, train_BiLSTM-DOC.py, train_BiLSTM.py, train_HCNN-DOC.py, train_HCNN.py, get_swda, Metrics_HCNN, set_allow_growth, get_speaker_change, HCNN, damsl_act_tag, iter_transcripts, dense1, utterances, CorpusReader\n## tags\n[intent detection]\n\n## repository\nCodeAchieveDream/crnn_model\n## files\ncrnn_train.py, imgaug_image.py, recog.py, tolmdb.py, net/crnn.py, get_batch_label, compute_std_mean, to_alphabet, CRNN_VGG, CRNN_MODEL, SomeOf, crnn, crnn, trainBatch, niter\n## tags\n[optical character recognition, scene text recognition]\n\n## repository\nXuRen20/deepface\n## files\ncore/cv2ex.py, core/osex.py, core/pathex.py, core/randomex.py, core/stdex.py, average_gv_list, average_tensor_list, resize2d_bilinear, tf_get_value, convert_98_to_68, BORDER_TRANSPARENT, face_enhancer_func, hist_match_threshold, image_denoise_power, motion_blur_power\n## 3 tags\n[deep_learning, face_recognation, face_shape]\n\n', 'raw_reference_text': 'face swapping', 'reference_text': 'face swapping', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8764147758483887, 'bertscore_recall': 0.8923612833023071, 'bertscore_f1': 0.8843161463737488, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.23648777604103088}, {'repo': 'zhijing-jin/ARTS_TestSet', 'tasks_x': 'entity linking, long document, pending reward, ', 'true_tasks': ['sentiment analysis', 'aspect based sentiment analysis'], 'generated_text': 'point cloud generation', 'prompt_info': {'content': 'code/strategies.py, code/main.py, code/utils.py, revTgt, revNon, addDiff, Utils, get_args, get_postag, exaggerate, tokenize_term_list, tokenize_term_list, LanguageTool', 'generated_text': '\n## repository\nsamgregoost/Spectral-GAN\n## files\nGAN_B.py, GAN_F.py, point_net.py, spatial_train.py, point_utils.py, point_branch, GAN_B, GAN_F, get_tensor_size, add_to_regularization_and_summary, sample_Z, sample_Z, zero_fraction, _variable_on_cpu, truncated_normal_initializer\n## tags\n[point cloud generation]\n\n## repository\npencoa/state-lstm\n## files\nmodel/grn.py, model/tree.py, prepare_vocab.py, utils/scorer.py, model/trainer.py, GGNNRelationModel, GGNNClassifier, State_LSTM, GNNTrainer, MyAdagrad, wv_file, wv_dim, VOCAB_PREFIX, min_freq, vocab_dir\n## tags\n[relation extraction]\n\n## repository\nzhijing-jin/ARTS_TestSet\n## files\ncode/strategies.py, code/main.py, code/utils.py, revTgt, revNon, addDiff, Utils, get_args, get_postag, exaggerate, tokenize_term_list, tokenize_term_list, LanguageTool\n## 3 tags\n[entity linking], [long document], [pending reward]\n\n## repository\n', 'id': '13b4ffaad358735304969a69f5cb16ea', 'name': 'zhijing-jin/ARTS_TestSet', 'tasks': 'entity linking, long document, pending reward, ', 'true_text': 'sentiment analysis, aspect based sentiment analysis'}, 'generation': 0, 'input_text': '\n## repository\nsamgregoost/Spectral-GAN\n## files\nGAN_B.py, GAN_F.py, point_net.py, spatial_train.py, point_utils.py, point_branch, GAN_B, GAN_F, get_tensor_size, add_to_regularization_and_summary, sample_Z, sample_Z, zero_fraction, _variable_on_cpu, truncated_normal_initializer\n## tags\n[point cloud generation]\n\n## repository\npencoa/state-lstm\n## files\nmodel/grn.py, model/tree.py, prepare_vocab.py, utils/scorer.py, model/trainer.py, GGNNRelationModel, GGNNClassifier, State_LSTM, GNNTrainer, MyAdagrad, wv_file, wv_dim, VOCAB_PREFIX, min_freq, vocab_dir\n## tags\n[relation extraction]\n\n## repository\nzhijing-jin/ARTS_TestSet\n## files\ncode/strategies.py, code/main.py, code/utils.py, revTgt, revNon, addDiff, Utils, get_args, get_postag, exaggerate, tokenize_term_list, tokenize_term_list, LanguageTool\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/tasty-burgers-soggy-fries-probing-aspect']", 'paper_titles': "['Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis']", 'arxiv_ids': "['2009.07964']", 'authors': '["[\'Xiaoyu Xing\', \'Zhijing Jin\', \'Di Jin\', \'Bingning Wang\', \'Qi Zhang\', \'Xuanjing Huang\']"]', 'raw_generated_text': '\n## repository\nsamgregoost/Spectral-GAN\n## files\nGAN_B.py, GAN_F.py, point_net.py, spatial_train.py, point_utils.py, point_branch, GAN_B, GAN_F, get_tensor_size, add_to_regularization_and_summary, sample_Z, sample_Z, zero_fraction, _variable_on_cpu, truncated_normal_initializer\n## tags\n[point cloud generation]\n\n## repository\npencoa/state-lstm\n## files\nmodel/grn.py, model/tree.py, prepare_vocab.py, utils/scorer.py, model/trainer.py, GGNNRelationModel, GGNNClassifier, State_LSTM, GNNTrainer, MyAdagrad, wv_file, wv_dim, VOCAB_PREFIX, min_freq, vocab_dir\n## tags\n[relation extraction]\n\n## repository\nzhijing-jin/ARTS_TestSet\n## files\ncode/strategies.py, code/main.py, code/utils.py, revTgt, revNon, addDiff, Utils, get_args, get_postag, exaggerate, tokenize_term_list, tokenize_term_list, LanguageTool\n## 3 tags\n[entity linking], [long document], [pending reward]\n\n## repository\n', 'raw_reference_text': 'sentiment analysis, aspect based sentiment analysis', 'reference_text': 'aspect based sentiment analysis sentiment analysis', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8184217810630798, 'bertscore_recall': 0.729477047920227, 'bertscore_f1': 0.7713940143585205, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.14081183075904846}, {'repo': 'YeongHyeon/FARED_for_Anomaly_Detection', 'tasks_x': 'anomaly detection, applied matlab analytics platform, yet(one), ', 'true_tasks': ['anomaly detection'], 'generated_text': 'facial expression recognition', 'prompt_info': {'content': 'FARED_source/run.py, FARED_source/source/datamanager.py, FARED_source/source/developer.py, FARED_source/source/neuralnet.py, FARED_source/source/tf_process.py, complex2magnitude, LSTM_Model_TF1_04, data2canvas, LSTM_Model_TF1_14, spectrums2timeaverage, trkey, am_tot, key_tot, complex2magnitude, data2canvas', 'generated_text': '\n## repository\nshanxuanchen/FacialExpressionRecognition\n## files\nCenterLoss/generateData.py, CenterLoss/modelGenerator.py, IslandLoss/generateData.py, IslandLoss/modelGenerator.py, CenterLoss/CenterLoss.py, IslandLossLayer, getData2, getData2, generateModel2, generateModel2, resize_to_224, baseModel, float16, load_weights, output\n## tags\n[facial expression recognition]\n\n## repository\nHikkaV/StylizerML\n## files\napp/serving.py, app/utlis.py, app/predictor.py, stylize, Body, crop_center, to_bytes, Predictor, crop_center, crop_to_bounding_box, to_bytes, b64decode, BytesIO\n## tags\n[style transfer]\n\n## repository\nYeongHyeon/FARED_for_Anomaly_Detection\n## files\nFARED_source/run.py, FARED_source/source/datamanager.py, FARED_source/source/developer.py, FARED_source/source/neuralnet.py, FARED_source/source/tf_process.py, complex2magnitude, LSTM_Model_TF1_04, data2canvas, LSTM_Model_TF1_14, spectrums2timeaverage, trkey, am_tot, key_tot, complex2magnitude, data2canvas\n## 3 tags\n[anomaly detection], [applied matlab analytics platform], [yet(one)]\n', 'id': 'ad48cac7f657121033767facef265858', 'name': 'YeongHyeon/FARED_for_Anomaly_Detection', 'tasks': 'anomaly detection, applied matlab analytics platform, yet(one), ', 'true_text': 'anomaly detection'}, 'generation': 0, 'input_text': '\n## repository\nshanxuanchen/FacialExpressionRecognition\n## files\nCenterLoss/generateData.py, CenterLoss/modelGenerator.py, IslandLoss/generateData.py, IslandLoss/modelGenerator.py, CenterLoss/CenterLoss.py, IslandLossLayer, getData2, getData2, generateModel2, generateModel2, resize_to_224, baseModel, float16, load_weights, output\n## tags\n[facial expression recognition]\n\n## repository\nHikkaV/StylizerML\n## files\napp/serving.py, app/utlis.py, app/predictor.py, stylize, Body, crop_center, to_bytes, Predictor, crop_center, crop_to_bounding_box, to_bytes, b64decode, BytesIO\n## tags\n[style transfer]\n\n## repository\nYeongHyeon/FARED_for_Anomaly_Detection\n## files\nFARED_source/run.py, FARED_source/source/datamanager.py, FARED_source/source/developer.py, FARED_source/source/neuralnet.py, FARED_source/source/tf_process.py, complex2magnitude, LSTM_Model_TF1_04, data2canvas, LSTM_Model_TF1_14, spectrums2timeaverage, trkey, am_tot, key_tot, complex2magnitude, data2canvas\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/fast-adaptive-rnn-encoder-decoder-for-anomaly']", 'paper_titles': "['Fast Adaptive RNN Encoder-Decoder for Anomaly Detection in SMD Assembly Machine']", 'arxiv_ids': '[nan]', 'authors': '["[\'YeongHyeon Park\', \'Il Dong Yun\']"]', 'raw_generated_text': '\n## repository\nshanxuanchen/FacialExpressionRecognition\n## files\nCenterLoss/generateData.py, CenterLoss/modelGenerator.py, IslandLoss/generateData.py, IslandLoss/modelGenerator.py, CenterLoss/CenterLoss.py, IslandLossLayer, getData2, getData2, generateModel2, generateModel2, resize_to_224, baseModel, float16, load_weights, output\n## tags\n[facial expression recognition]\n\n## repository\nHikkaV/StylizerML\n## files\napp/serving.py, app/utlis.py, app/predictor.py, stylize, Body, crop_center, to_bytes, Predictor, crop_center, crop_to_bounding_box, to_bytes, b64decode, BytesIO\n## tags\n[style transfer]\n\n## repository\nYeongHyeon/FARED_for_Anomaly_Detection\n## files\nFARED_source/run.py, FARED_source/source/datamanager.py, FARED_source/source/developer.py, FARED_source/source/neuralnet.py, FARED_source/source/tf_process.py, complex2magnitude, LSTM_Model_TF1_04, data2canvas, LSTM_Model_TF1_14, spectrums2timeaverage, trkey, am_tot, key_tot, complex2magnitude, data2canvas\n## 3 tags\n[anomaly detection], [applied matlab analytics platform], [yet(one)]\n', 'raw_reference_text': 'anomaly detection', 'reference_text': 'anomaly detection', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8726255893707275, 'bertscore_recall': 0.9230330586433411, 'bertscore_f1': 0.897121787071228, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.3205142617225647}, {'repo': 'geek-ai/Texygen', 'tasks_x': 'language, gan, ', 'true_tasks': ['text generation'], 'generated_text': 'face anti spoofing', 'prompt_info': {'content': 'models/gsgan/Gsgan.py, models/gsgan/GsganDataLoader.py, models/gsgan/GsganDiscriminator.py, models/gsgan/GsganGenerator.py, models/maligan_basic/MailganDiscriminator.py, chinese_process, set_gan, TextganMmd, Gsgan, Maligan, gan_func, train_real, set_gan, train_oracle, train_oracle', 'generated_text': '\n## repository\nSoftwareGift/FeatheNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019\n## files\ngen_final_submission.py, model_onnx2IR.py, data/fileList.py, models/FeatherNet.py, models/MobileLiteNet.py, TalyorCrossEntroyLoss, MobileLiteNet54, MobileLiteNet54_se, MobileLiteNet102, MobileLiteNet105_se, ConvFlops, every_decay, MAdd, Flops, inference_memory\n## tags\n[face anti spoofing]\n\n## repository\nsnap-stanford/masa\n## files\nCASC_solver.py, paper_code/generateDatasets/generate_synthetic.py, paper_code/generateDatasets/generate_synthetic_util.py, paper_code/generateDatasets/snap.py, paper_code/scripts/aggregation_and_plotting/analyze_synthetic.py, TStrHashF_Murmur3, TStr_Base64Decode, TStrHashF_Murmur3_GetPrimHashCd, TStrHashF_Murmur3_GetSecHashCd, TStr_Base64Encode, negLLMatrix, TICCSolver, PerformFullTICC, TStopwatch_CopyNodes, TStopwatch_BuildSubgraph\n## tags\n[time series]\n\n## repository\ngeek-ai/Texygen\n## files\nmodels/gsgan/Gsgan.py, models/gsgan/GsganDataLoader.py, models/gsgan/GsganDiscriminator.py, models/gsgan/GsganGenerator.py, models/maligan_basic/MailganDiscriminator.py, chinese_process, set_gan, TextganMmd, Gsgan, Maligan, gan_func, train_real, set_gan, train_oracle, train_oracle\n## 3 tags\n[language]\n[gan]\n\n## repository\nbatra-projects/', 'id': '0062b9d8293ba79d88f3cb48d1bd5f1e', 'name': 'geek-ai/Texygen', 'tasks': 'language, gan, ', 'true_text': 'text generation'}, 'generation': 0, 'input_text': '\n## repository\nSoftwareGift/FeatheNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019\n## files\ngen_final_submission.py, model_onnx2IR.py, data/fileList.py, models/FeatherNet.py, models/MobileLiteNet.py, TalyorCrossEntroyLoss, MobileLiteNet54, MobileLiteNet54_se, MobileLiteNet102, MobileLiteNet105_se, ConvFlops, every_decay, MAdd, Flops, inference_memory\n## tags\n[face anti spoofing]\n\n## repository\nsnap-stanford/masa\n## files\nCASC_solver.py, paper_code/generateDatasets/generate_synthetic.py, paper_code/generateDatasets/generate_synthetic_util.py, paper_code/generateDatasets/snap.py, paper_code/scripts/aggregation_and_plotting/analyze_synthetic.py, TStrHashF_Murmur3, TStr_Base64Decode, TStrHashF_Murmur3_GetPrimHashCd, TStrHashF_Murmur3_GetSecHashCd, TStr_Base64Encode, negLLMatrix, TICCSolver, PerformFullTICC, TStopwatch_CopyNodes, TStopwatch_BuildSubgraph\n## tags\n[time series]\n\n## repository\ngeek-ai/Texygen\n## files\nmodels/gsgan/Gsgan.py, models/gsgan/GsganDataLoader.py, models/gsgan/GsganDiscriminator.py, models/gsgan/GsganGenerator.py, models/maligan_basic/MailganDiscriminator.py, chinese_process, set_gan, TextganMmd, Gsgan, Maligan, gan_func, train_real, set_gan, train_oracle, train_oracle\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/texygen-a-benchmarking-platform-for-text']", 'paper_titles': "['Texygen: A Benchmarking Platform for Text Generation Models']", 'arxiv_ids': "['1802.01886']", 'authors': '["[\'Yaoming Zhu\', \'Sidi Lu\', \'Lei Zheng\', \'Jiaxian Guo\', \'Wei-Nan Zhang\', \'Jun Wang\', \'Yong Yu\']"]', 'raw_generated_text': '\n## repository\nSoftwareGift/FeatheNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019\n## files\ngen_final_submission.py, model_onnx2IR.py, data/fileList.py, models/FeatherNet.py, models/MobileLiteNet.py, TalyorCrossEntroyLoss, MobileLiteNet54, MobileLiteNet54_se, MobileLiteNet102, MobileLiteNet105_se, ConvFlops, every_decay, MAdd, Flops, inference_memory\n## tags\n[face anti spoofing]\n\n## repository\nsnap-stanford/masa\n## files\nCASC_solver.py, paper_code/generateDatasets/generate_synthetic.py, paper_code/generateDatasets/generate_synthetic_util.py, paper_code/generateDatasets/snap.py, paper_code/scripts/aggregation_and_plotting/analyze_synthetic.py, TStrHashF_Murmur3, TStr_Base64Decode, TStrHashF_Murmur3_GetPrimHashCd, TStrHashF_Murmur3_GetSecHashCd, TStr_Base64Encode, negLLMatrix, TICCSolver, PerformFullTICC, TStopwatch_CopyNodes, TStopwatch_BuildSubgraph\n## tags\n[time series]\n\n## repository\ngeek-ai/Texygen\n## files\nmodels/gsgan/Gsgan.py, models/gsgan/GsganDataLoader.py, models/gsgan/GsganDiscriminator.py, models/gsgan/GsganGenerator.py, models/maligan_basic/MailganDiscriminator.py, chinese_process, set_gan, TextganMmd, Gsgan, Maligan, gan_func, train_real, set_gan, train_oracle, train_oracle\n## 3 tags\n[language]\n[gan]\n\n## repository\nbatra-projects/', 'raw_reference_text': 'text generation', 'reference_text': 'text generation', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8690876960754395, 'bertscore_recall': 0.8983287215232849, 'bertscore_f1': 0.8834663033485413, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.17218910157680511}, {'repo': 'da03/Attention-OCR', 'tasks_x': 'OCR, model, recurrent model, ', 'true_tasks': ['optical character recognition'], 'generated_text': 'data augmentation', 'prompt_info': {'content': 'src/exp_config.py, src/data_util/bucketdata.py, src/data_util/data_gen.py, src/model/seq2seq_model.py, src/data_util/__init__.py, var_random, tf_create_attention_map, max_2x1pool, max_2x2pool, ConvReluBN, EvalGen, output_size, output_size, process_args, assert_is_compatible_with', 'generated_text': '\n## repository\nsdv4/Predicting-Confusion-with-RNN_HAI-2019\n## files\nutils.py, train.py, optimal_threshold_sensitivity_specificity, get_grouped_k_fold_splits, check_metrics, get_train_val_split, pickle_loader, check_metrics, VPacker, TextArea, AnchoredOffsetbox, GroupShuffleSplit\n## tags\n[data augmentation]\n\n## repository\nLinZhuoChen/SGNet\n## files\nagents/sgnet_agent.py, data/nyudv2.py, data/transform/rgbd_transform.py, graphs/models/SGNet/SGNet.py, graphs/models/SGNet/SGNet_ASPP.py, get_currect_time, lr_poly_epoch, Normalize_PIL2numpy_depth2xyz, FixedResize_image, ToTensor_SUN, min_learining_rate, min_learining_rate, lr_poly_exp, lr_poly_exp, elu_inv_cuda\n## tags\n[semantic segmentation]\n\n## repository\nda03/Attention-OCR\n## files\nsrc/exp_config.py, src/data_util/bucketdata.py, src/data_util/data_gen.py, src/model/seq2seq_model.py, src/data_util/__init__.py, var_random, tf_create_attention_map, max_2x1pool, max_2x2pool, ConvReluBN, EvalGen, output_size, output_size, process_args, assert_is_compatible_with\n## 3 tags\n[OCR],[model],[recurrent model]\n\n## repository\nJudyZ', 'id': 'b40e04d28309fa2d2adf8ba231de9879', 'name': 'da03/Attention-OCR', 'tasks': 'OCR, model, recurrent model, ', 'true_text': 'optical character recognition'}, 'generation': 0, 'input_text': '\n## repository\nsdv4/Predicting-Confusion-with-RNN_HAI-2019\n## files\nutils.py, train.py, optimal_threshold_sensitivity_specificity, get_grouped_k_fold_splits, check_metrics, get_train_val_split, pickle_loader, check_metrics, VPacker, TextArea, AnchoredOffsetbox, GroupShuffleSplit\n## tags\n[data augmentation]\n\n## repository\nLinZhuoChen/SGNet\n## files\nagents/sgnet_agent.py, data/nyudv2.py, data/transform/rgbd_transform.py, graphs/models/SGNet/SGNet.py, graphs/models/SGNet/SGNet_ASPP.py, get_currect_time, lr_poly_epoch, Normalize_PIL2numpy_depth2xyz, FixedResize_image, ToTensor_SUN, min_learining_rate, min_learining_rate, lr_poly_exp, lr_poly_exp, elu_inv_cuda\n## tags\n[semantic segmentation]\n\n## repository\nda03/Attention-OCR\n## files\nsrc/exp_config.py, src/data_util/bucketdata.py, src/data_util/data_gen.py, src/model/seq2seq_model.py, src/data_util/__init__.py, var_random, tf_create_attention_map, max_2x1pool, max_2x2pool, ConvReluBN, EvalGen, output_size, output_size, process_args, assert_is_compatible_with\n## 3 tags\n[', 'paper_urls': "['https://paperswithcode.com/paper/image-to-markup-generation-with-coarse-to']", 'paper_titles': "['Image-to-Markup Generation with Coarse-to-Fine Attention']", 'arxiv_ids': "['1609.04938']", 'authors': '["[\'Yuntian Deng\', \'Anssi Kanervisto\', \'Jeffrey Ling\', \'Alexander M. Rush\']"]', 'raw_generated_text': '\n## repository\nsdv4/Predicting-Confusion-with-RNN_HAI-2019\n## files\nutils.py, train.py, optimal_threshold_sensitivity_specificity, get_grouped_k_fold_splits, check_metrics, get_train_val_split, pickle_loader, check_metrics, VPacker, TextArea, AnchoredOffsetbox, GroupShuffleSplit\n## tags\n[data augmentation]\n\n## repository\nLinZhuoChen/SGNet\n## files\nagents/sgnet_agent.py, data/nyudv2.py, data/transform/rgbd_transform.py, graphs/models/SGNet/SGNet.py, graphs/models/SGNet/SGNet_ASPP.py, get_currect_time, lr_poly_epoch, Normalize_PIL2numpy_depth2xyz, FixedResize_image, ToTensor_SUN, min_learining_rate, min_learining_rate, lr_poly_exp, lr_poly_exp, elu_inv_cuda\n## tags\n[semantic segmentation]\n\n## repository\nda03/Attention-OCR\n## files\nsrc/exp_config.py, src/data_util/bucketdata.py, src/data_util/data_gen.py, src/model/seq2seq_model.py, src/data_util/__init__.py, var_random, tf_create_attention_map, max_2x1pool, max_2x2pool, ConvReluBN, EvalGen, output_size, output_size, process_args, assert_is_compatible_with\n## 3 tags\n[OCR],[model],[recurrent model]\n\n## repository\nJudyZ', 'raw_reference_text': 'optical character recognition', 'reference_text': 'optical character recognition', 'edit_word': 1.0, 'jaccard_lst': 0.0, 'bertscore_precision': 0.8638339638710022, 'bertscore_recall': 0.8378814458847046, 'bertscore_f1': 0.8506597876548767, 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.34.1)', 'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0, 'sentence_transformer_similarity': 0.17582564055919647}]}

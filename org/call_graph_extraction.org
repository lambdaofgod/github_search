#+title: Call Graph Extraction

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
import pathlib
import pandas as pd
import tqdm


repos_path = pathlib.Path("~/Projects/github_search/data/repos/").expanduser()
repo_paths = list(repos_path.glob("*"))
example_repo_paths = list(repos_path.glob("008*"))
example_repo_paths = [p for p in example_repo_paths if not p.name.endswith("zip")]
example_repo_path = example_repo_paths[0]
example_repo_files = list(example_repo_path.expanduser().rglob("*py"))
example_repo_files
#+END_SRC

#+RESULTS:
| PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/speaker_id.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/inference.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/data_io.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/dnn_models.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/TIMIT_preparation.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/similarity.py) | PosixPath | (/home/kuba/Projects/github_search/data/repos/008karan_SincNet_demo/compute_d_vector.py) |

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
len(example_repo_paths)
#+END_SRC

#+RESULTS:
: 1

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
def get_repo_name_from_path(p):
    return str(p).replace("_", "/", 1)

p = example_repo_files[0].relative_to(repos_path).parent
get_repo_name_from_path(p)
#+END_SRC

#+RESULTS:
: 008karan/SincNet_demo

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
p
#+END_SRC

#+RESULTS:
: 008karan_SincNet_demo

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
def get_python_files_df(repo_path):
    repo_name = get_repo_name_from_path(repo_path.name)
    repo_file_paths = repo_path.rglob("**/*.py")
    repo_file_contents = {}
    for p in repo_file_paths:
        try:
            p_name = p.relative_to(repo_path)
            with open(p) as f:
                file_contents = f.read()
            repo_file_contents[str(p_name)] = file_contents
        except:
            print(f"error in repo {repo_name} processing file {p}")
    repo_file_contents_df = pd.Series(repo_file_contents).reset_index()
    repo_file_contents_df.columns = ["path", "content"]
    repo_file_contents_df["repo_name"] = repo_name
    return repo_file_contents_df
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
python_files_dfs = [get_python_files_df(p) for p in tqdm.tqdm(repo_paths)]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
python_files_df = pd.concat(python_files_dfs).sort_values("repo_name")
python_files_df.shape
#+END_SRC

#+RESULTS:
| 228871 | 3 |
#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
python_files_df.columns
#+END_SRC

#+RESULTS:
: Index(['path', 'content', 'repo_name'], dtype='object')

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
python_files_df["repo_name"].value_counts().describe()
#+END_SRC

#+RESULTS:
: count    6864.000000
: mean       33.343677
: std        79.376327
: min         1.000000
: 25%         7.000000
: 50%        14.000000
: 75%        31.000000
: max      1934.000000
: Name: count, dtype: float64

* Defining the call graph

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
import pandas as pd
from github_search.python_call_graph import GraphExtractor
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
graph_dependencies_df = GraphExtractor.extract_repo_dependencies_to_parquet(python_files_df, "../output/dependency_records.parquet")

graph_dependencies_df = pd.read_parquet("../output/dependency_records.parquet")
#+END_SRC

#+RESULTS:
: /tmp/babel-zaYAly/python-tCFaGB

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
#graph_dependencies_df.to_parquet("/tmp/deps.parquet")
#+END_SRC

#+RESULTS:
: /tmp/babel-rOv9H4/python-6UBx1T

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
import pandas as pd
from github_search.python_call_graph_analysis import GraphCentralityAnalyzer

#graph_dependencies_df = pd.read_csv("/tmp/deps.csv")
graph_dependencies_df = pd.read_parquet("../output/dependency_records.parquet")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
centralities_df = GraphCentralityAnalyzer(centrality_method="pagerank").analyze_centralities(graph_dependencies_df, {edge_type: 10 for edge_type in ["repo-file", "file-class", "file-function", "file-import"]})
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both :async
centralities_df.to_csv("../output/pagerank_centralities.csv")
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
centralities_df
#+END_SRC

#+RESULTS:
| 236752 | 6 |

#+BEGIN_SRC python :session call_graph_extraction.org  :exports both
graph_dependencies_df["repo_name"].value_counts().describe()
#+END_SRC

#+RESULTS:
: count     6780.000000
: mean      1743.188201
: std       4211.351139
: min          1.000000
: 25%        304.000000
: 50%        701.000000
: 75%       1603.250000
: max      97185.000000
: Name: count, dtype: float64

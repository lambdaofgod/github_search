#+title: Evaluation
#+PROPERTY: header-args :tangle evaluation_experiment.py


* Experiment setup
** Text generation
We sample 100 repositories, prepare 2 other repos and use LLaMa 13b model in few-shot manner.

Example prompt:

#+BEGIN_SRC python :session evaluation.org  :exports both
repository: 3DVisionISR/3DRegNet
files: ['data/readMat.py', 'main.py', 'test.py', 'data/data.py', 'registration/global_registration.py', 'registration/setupPly.py', 'registration/registration.py', 'config.py', 'ops.py', 'archs/arch.py']
tags: ['frame']

repository: 3P2S/arcface
files: ['train.py', 'modules/layers.py', 'utils.py', 'modules/dataset.py', 'modules/utils.py', 'evaluate.py', 'data/convert_train_binary_tfrecord.py', 'modules/evaluations.py', 'modules/losses.py', 'infer_t265.py']
tags: ['pedestrian attribute recognition', 'retinal oct disease classification', 'image classification', 'person re identification', 'pedestrian trajectory prediction', 'face identification', 'face recognition', 'face verification', 'object detection', 'image to image translation', 'semantic segmentation']
repository: 0three/Speech-Denoise-With-Feature-Loss
files: ['helper.py', 'senet_train.py', 'lossnet_train.py', 'model.py', 'data_import.py', 'data/extract.py', 'data/noise.py', 'dataset/seg.py', 'senet_infer.py', 'data/sox.py']
tags:
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session evaluation.org  :exports results
## load data
from pathlib import Path as P
import matplotlib.pyplot as plt
import seaborn as sns
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session evaluation.org  :exports results
import pandas as pd
pd.set_option("display.max_columns", 10)
pd.set_option("display.max_colwidth", 30)

#p = list(P("../data").rglob("*.jsonl"))[0]
#evaluated_df = pd.read_json(p, orient="records", lines=True).set_index("repo", drop=True)
evaluated_df = pd.read_csv("../data/sample_df.csv.gz")
#+END_SRC

#+RESULTS:
: None
** Metrics

We explore

- ROUGE
- BLEURT
- Word Mover's Distance
- sentence-transformers based similarity

* Records with highest ROUGE

#+BEGIN_SRC python :session evaluation.org  :exports results :results both
import pprint
import json

import tabulate
viewed_columns = ["reference_text", "predicted_text", "rougeL", "bleurt", "wmd", "sentence_transformer_similarity"]

#tabulate.tabulate(evaluated_df[viewed_columns].head(5), headers=viewed_columns, maxcolwidths=20)
#evaluated_df[viewed_columns].head(5).to_latex()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session evaluation.org  :exports both :results output
print(evaluated_df["reference_text"].iloc[[1,10]].to_list())
print(evaluated_df["predicted_text"].iloc[[1,10]].to_list())
#+END_SRC

#+RESULTS:

#+BEGIN_SRC latex :session evaluation.org  :exports results
\begin{tabular}{lllrrrr}
\toprule
{} &                 reference\_text &                 predicted\_text &    rougeL &    bleurt &       wmd &  sentence\_transformer\_similarity \\
repo                           &                                &                                &           &           &           &                                  \\
\midrule
EricArazo/PseudoLabeling       &           image classification &           image classification &  1.000000 &  1.033122 &  0.000000 &                       1.000000 \\
Evfro/recsys19\_hybridsvd       &  collaborative filtering, m... &        collaborative filtering &  0.666667 &  0.044577 &  0.615900 &                       0.664749 \\
Garfield35/Speach-Recogniti... &  language modelling, speech... &             speech recognition &  0.666667 &  0.096140 &  0.900432 &                       0.656970 \\
FenTechSolutions/CausalDisc... &  causal discovery, causal i... &  causality discovery, causa... &  0.615385 & -0.331163 &  0.517049 &                       0.713446 \\
GeoZcx/A-deeply-supervised-... &  change detection for remot... &  attention mechanism, chang... &  0.526316 & -0.151706 &  0.352550 &                       0.694007 \\
\bottomrule
\end{tabular}


#+END_SRC

#+RESULTS:
#+begin_export latex
\begin{tabular}{lllrrrr}
\toprule
{} &                 reference\_text &                 predicted\_text &    rougeL &    bleurt &       wmd &  sentence\_transformer\_similarity \\
repo                           &                                &                                &           &           &           &                                  \\
\midrule
EricArazo/PseudoLabeling       &           image classification &           image classification &  1.000000 &  1.033122 &  0.000000 &                       1.000000 \\
Evfro/recsys19\_hybridsvd       &  collaborative filtering, m... &        collaborative filtering &  0.666667 &  0.044577 &  0.615900 &                       0.664749 \\
Garfield35/Speach-Recogniti... &  language modelling, speech... &             speech recognition &  0.666667 &  0.096140 &  0.900432 &                       0.656970 \\
FenTechSolutions/CausalDisc... &  causal discovery, causal i... &  causality discovery, causa... &  0.615385 & -0.331163 &  0.517049 &                       0.713446 \\
GeoZcx/A-deeply-supervised-... &  change detection for remot... &  attention mechanism, chang... &  0.526316 & -0.151706 &  0.352550 &                       0.694007 \\
\bottomrule
\end{tabular}
#+end_export

** Records with lowest ROUGE

#+BEGIN_SRC python :session evaluation.org  :exports results :results both
#evaluated_df[viewed_columns].tail(5).to_latex()
pass
#+END_SRC

#+RESULTS:

#+BEGIN_SRC latex :session evaluation.org  :exports results
\begin{tabular}{lllrrrr}
\toprule
{} &                 reference\_text &                 predicted\_text &  rougeL &    bleurt &       wmd &  sentence\_transformer\_similarity \\
repo                          &                                &                                &         &           &           &                                  \\
\midrule
GanjinZero/CODER              &  semantic textual similarit... &         BERT, CODES, NLP, UMLS &     0.0 & -1.719004 &  4.207823 &                       0.365483 \\
EliasNehme/DeepSTORM3D        &               super resolution &  storm, 3d reconstruction, ... &     0.0 & -1.117862 &  0.894443 &                       0.068080 \\
ElementAI/N-BEATS             &  time series few shot learn... &  traffic analysis, tourist ... &     0.0 & -1.277228 &  1.113645 &                       0.168604 \\
GemsLab/H2GCN                 &            node classification &  graph convolutional networ... &     0.0 & -1.116135 &  1.023539 &                       0.269346 \\
HLR/LatentAlignmentProcedural &  reading comprehension, que... &  latent alignment, transfor... &     0.0 & -1.659781 &  1.251815 &                       0.177733 \\
\bottomrule
\end{tabular}
#+END_SRC

#+RESULTS:
#+begin_export latex
\begin{tabular}{lllrrrr}
\toprule
{} &                 reference\_text &                 predicted\_text &  rougeL &    bleurt &       wmd &  sentence\_transformer\_similarity \\
repo                          &                                &                                &         &           &           &                                  \\
\midrule
GanjinZero/CODER              &  semantic textual similarit... &         BERT, CODES, NLP, UMLS &     0.0 & -1.719004 &  4.207823 &                       0.365483 \\
EliasNehme/DeepSTORM3D        &               super resolution &  storm, 3d reconstruction, ... &     0.0 & -1.117862 &  0.894443 &                       0.068080 \\
ElementAI/N-BEATS             &  time series few shot learn... &  traffic analysis, tourist ... &     0.0 & -1.277228 &  1.113645 &                       0.168604 \\
GemsLab/H2GCN                 &            node classification &  graph convolutional networ... &     0.0 & -1.116135 &  1.023539 &                       0.269346 \\
HLR/LatentAlignmentProcedural &  reading comprehension, que... &  latent alignment, transfor... &     0.0 & -1.659781 &  1.251815 &                       0.177733 \\
\bottomrule
\end{tabular}
#+end_export

** Sample data summary
#+BEGIN_SRC python :session evaluation.org  :exports results :results output
#print(evaluated_df.drop(columns=["rouge1", "rouge2"]).describe().round(2).to_latex())
pass
#+END_SRC

#+RESULTS:

#+BEGIN_SRC latex :session evaluation.org  :exports results
\begin{tabular}{lrrrrrr}
\toprule
{} &  bleurt &  rougeL &  rougeLsum &     wmd &  sbert\_similarity &  token\_length \\
\midrule
count &  100.00 &  100.00 &     100.00 &  100.00 &                         100.00 &        100.00 \\
mean  &   -0.76 &    0.18 &       0.18 &    0.97 &                           0.44 &         12.74 \\
std   &    0.55 &    0.20 &       0.20 &    0.55 &                           0.21 &         11.47 \\
min   &   -1.97 &    0.00 &       0.00 &    0.00 &                           0.00 &          2.00 \\
25\%   &   -1.16 &    0.00 &       0.00 &    0.70 &                           0.27 &          5.00 \\
50\%   &   -0.78 &    0.15 &       0.15 &    0.88 &                           0.44 &          9.00 \\
75\%   &   -0.40 &    0.31 &       0.31 &    1.11 &                           0.62 &         16.00 \\
max   &    1.03 &    1.00 &       1.00 &    4.21 &                           1.00 &         66.00 \\
#+END_SRC

#+RESULTS:
#+begin_export latex
\begin{tabular}{lrrrrrr}
\toprule
{} &  bleurt &  rougeL &  rougeLsum &     wmd &  sbert\_similarity &  token\_length \\
\midrule
count &  100.00 &  100.00 &     100.00 &  100.00 &                         100.00 &        100.00 \\
mean  &   -0.76 &    0.18 &       0.18 &    0.97 &                           0.44 &         12.74 \\
std   &    0.55 &    0.20 &       0.20 &    0.55 &                           0.21 &         11.47 \\
min   &   -1.97 &    0.00 &       0.00 &    0.00 &                           0.00 &          2.00 \\
25\%   &   -1.16 &    0.00 &       0.00 &    0.70 &                           0.27 &          5.00 \\
50\%   &   -0.78 &    0.15 &       0.15 &    0.88 &                           0.44 &          9.00 \\
75\%   &   -0.40 &    0.31 &       0.31 &    1.11 &                           0.62 &         16.00 \\
max   &    1.03 &    1.00 &       1.00 &    4.21 &                           1.00 &         66.00 \\
#+end_export

#+BEGIN_SRC python :session evaluation.org  :exports results
llama_path = "/home/kuba/models/llama-7b-hf"
#+END_SRC

#+RESULTS:

*** Token lengths

#+BEGIN_SRC python :session evaluation.org  :exports results
from tgutil.evaluation_utils import ColumnEnricher

evaluated_df = ColumnEnricher(df=evaluated_df).add_token_length("reference_text", llama_path).df
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session evaluation.org  :exports results
evaluated_df["token_length"].describe()
#+END_SRC

#+RESULTS:

** True tasks length in tokens
#+BEGIN_SRC python :session evaluation.org  :exports results :results file :var f="plots/length_histogram.png"
import matplotlib.pyplot as plt
import seaborn as sns
fig=plt.figure(figsize=(8,6))

sns.histplot(data=evaluated_df, x="token_length", bins=20)
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]
* Correlations between distances

#+BEGIN_SRC python :session evaluation.org  :exports results :results file :var f="plots/metric_correlations.png"
correlations = evaluated_df[["bleurt", "rougeL", "wmd", "sentence_transformer_similarity"]].corr(method="kendall")
fig=plt.figure(figsize=(8,6))

sns.heatmap(data=correlations, annot=True)
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]

* Conclusion

The metrics do not seem to be strongly correlated (as measured with Kendall's Tau rank correlation metric).

Because of this and the different behavior with respect to text length we will explore all of these metrics.

* Additional plots
** Length vs ROUGEL

#+BEGIN_SRC python :session evaluation.org :exports results :results file :var f="plots/length_vs_rouge.png"
import matplotlib.pyplot as plt
import seaborn as sns
fig=plt.figure(figsize=(7,7))

sns.lmplot(data=evaluated_df, x="token_length", y="rougeL")
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]
* Length vs BLEURT

#+BEGIN_SRC python :session evaluation.org  :exports results :results file :var f="plots/length_vs_bleurt.png"
import matplotlib.pyplot as plt
fig=plt.figure(figsize=(7,7))

sns.lmplot(data=evaluated_df, x="token_length", y="bleurt")
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]
** Length vs Word Mover's Distance

Texts are embedded using FastText
#+BEGIN_SRC python :session evaluation.org  :exports results :results file :var f="plots/length_vs_wmdistance.png"
import matplotlib.pyplot as plt
fig=plt.figure(figsize=(7,7))

sns.lmplot(data=evaluated_df, x="token_length", y="wmd")
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]

** Length vs sentence transformer similarity


#+BEGIN_SRC python :session evaluation.org  :exports results :results file :var f="plots/length_vs_similarity.png"
import matplotlib.pyplot as plt
fig=plt.figure(figsize=(7,7))

sns.lmplot(data=evaluated_df, x="token_length", y="sentence_transformer_similarity")
plt.savefig(f)
f
#+END_SRC

#+RESULTS:
[[file:]]


#+BEGIN_SRC python :session evaluation.org  :exports both
evaluated_df.info()
#+END_SRC

#+RESULTS:

meta:
  extract_upstream: False 
  extract_product: False
  jupyter_hot_reload: True
  jupyter_functions_as_notebooks: True

tasks:
  # prepare data for token2vec (modified import2vec)
  - source: github_search.data_engineering.prepare_module_corpus
    params:
      python_file_paths: ["data/scraped_python_files.csv"]
    product: "output/module_corpus.csv"
  # train token2vec model
  - source: "github_search/token2vec.py"
    params:
      n_iterations: 100000
      n_positive_imports: 32
      embedding_dim: "{{word2vec_dimension}}" 
    product:
      nb: "output/token2vec_training.ipynb"
      model_path: "output/import2vec_module_vectors{{word2vec_dimension}}.bin"
  # prepare paper dataset with imports extracted per-project
  - source: github_search.data_engineering.prepare_paperswithcode_with_imports_df
    params:
      python_file_paths: ["data/crawled_python_files.csv"]
    upstream:
      prepare_module_corpus
    product: "output/papers_with_imports.csv"
  # prepare python dependency graph records
  - source: github_search.data_engineering.prepare_dependency_records
    params:
      python_file_paths: ["data/crawled_python_files.csv"]
    product: "output/dependency_records.csv"
  # make igraph from dependency records
  - source: github_search.data_engineering.make_igraph
    upstream:
      prepare_dependency_records
    product: "output/call_igraph.pkl"
  # train FastText model on Python files
  - source: github_search.data_engineering.train_python_token_fasttext
    params:
      python_file_path: "data/crawled_python_files.csv"
      dim: "{{fasttext_dimension}}" 
      epoch: "{{fasttext_epochs}}"
    product:
      "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"
  # prepare records df where repos have edges if they share at least one task
  - source: github_search.data_engineering.prepare_task_linked_repo_records_df
    upstream:
      prepare_paperswithcode_with_imports_df
    product: "output/shared_task_repos.csv"
  # train GraphSAGE model
  - source: github_search.pytorch_geometric_training.run_gnn_experiment
    upstream:
      prepare_task_linked_repo_records_df
    params:
      use_additional_records: True
      records_csv_path: "{{graph_records_path}}"
      fasttext_model_path: "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"
      epochs: "{{gnn_epochs}}" 
      hidden_channels: "{{gnn_hidden_channels}}" 
      batch_size: "{{gnn_batch_size}}" 
      num_layers: "{{gnn_layers}}"
      test_run: "{{test_run}}" 
      lr: 0.001
      model_name: "{{gnn_model_name}}"
    product:
      plot_file: "output/loss_{{gnn_model_name}}_fasttext_dim{{fasttext_dimension}}_epochs{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.png"
      gnn_token_embeddings: "output/{{gnn_model_name}}_embeddings_fasttext_dim{{fasttext_dimension}}_epochs{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.bin"
      model_path: "output/{{gnn_model_name}}_model_{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.pth"

executor:
  dotted_path: ploomber.executors.Serial
  build_in_subprocess: False

meta:
  extract_upstream: False 
  extract_product: False
  jupyter_hot_reload: True
  jupyter_functions_as_notebooks: True

tasks:

  # download area grouped tasks
  - source: github_search.paperswithcode_task_areas.prepare_area_grouped_tasks
    product: "data/paperswithcode_tasks.csv"

  # train-test split for tasks
  # tasks are stratified by paperswithcode area
  - source: github_search.data_engineering.prepare_task_train_test_split
    upstream:
      - prepare_area_grouped_tasks
    params:
      test_size: 0.1
    product:
      train: "output/tasks_train.csv"
      test: "output/tasks_test.csv"

  # get github readmes
  - source: github_search.data_engineering.make_readmes
    params:
      paperswithcode_with_tasks_path: "data/paperswithcode_with_tasks.csv"
      max_workers: 24
    upstream: prepare_paperswithcode_with_imports_df
    product: "output/papers_with_readmes.csv"

  #
  - source: github_search.data_engineering.prepare_repo_train_test_split
    upstream:
      - make_readmes
      - prepare_task_train_test_split
    product:
      train: "output/repos_train.csv"
      test: "output/repos_test.csv"

  # extract python tokens for BoW baseline
  - source: github_search.bow_baseline.extract_python_tokens
    product: "output/python_files_with_tokens_df.feather"

  # 
  - source: github_search.bow_baseline.prepare_bow_retrieval_evaluation_results
    name: prepare_bow_retrieval_evaluation_results_readme
    upstream:
      - prepare_task_train_test_split
    params:
      index: python_tokenized_files 
    product: "output/python_files_retrieval_results.csv"
  # 
  - source: github_search.bow_baseline.prepare_bow_retrieval_evaluation_results
    name: prepare_bow_retrieval_evaluation_results_python_files
    upstream:
      - prepare_task_train_test_split
    params:
      index: project_readmes 
    product: "output/readme_retrieval_results.csv"
  # run word2vec on natural language data
  - source: github_search.word2vec.train_abstract_readme_w2v
    upstream:
      - make_readmes
    params:
      embedding_dim: "{{word2vec.dimension}}"
      epochs: "{{word2vec.epochs}}"
    product:
      "output/abstract_readme_w2v{{word2vec.dimension}}.bin"

  # prepare data for token2vec (modified import2vec)
  - source: github_search.data_engineering.prepare_module_corpus
    params:
      python_file_paths: ["{{python_files_path}}"]
    product: "output/module_corpus.csv"

  # train token2vec model
  - source: github_search.token2vec.train_token2vec
    upstream:
      - prepare_module_corpus
    params:
      n_iterations: 10000
      n_positive_imports: 32
      embedding_dim: "{{word2vec.dimension}}" 
    product:
      model_path: "output/import2vec_module_vectors{{word2vec.dimension}}.bin"

  # prepare paper dataset with imports extracted per-project
  - source: github_search.data_engineering.prepare_paperswithcode_with_imports_df
    upstream:
      - prepare_module_corpus
    params:
      python_file_paths: ["{{python_files_path}}"]
    product: "output/papers_with_imports.csv"

  # prepare python dependency graph records
  - source: github_search.data_engineering.prepare_dependency_records
    params:
      sample_files_per_repo: 100
      add_filename_repo_label: True
      python_file_paths: ["{{python_files_path}}"]
    product: "output/dependency_records.csv"

  # additional information for dependency records
  - source: github_search.data_engineering.postprocess_dependency_records
    upstream:
      - prepare_dependency_records
      - prepare_paperswithcode_with_imports_df
    params:
      use_additional_records: False
      description_mode: "{{gnn.description_mode}}" 
    product: "output/processed_dependency_records.csv"

  # extract python function df
  # f has columns
  # ['repo_name', 'path', 'function_name', 'function_code']
  - source: github_search.data_engineering.prepare_function_code_df
    params:
      python_file_path: "{{python_files_path}}"
      max_depth: 10
    product:
      "output/python_functions.feather"

  # train FastText model on Python files
  - source: github_search.data_engineering.train_python_token_fasttext
    params:
      python_file_path: "{{python_files_path}}"
      dim: "{{fasttext.dimension}}" 
      epoch: "{{fasttext.epochs}}"
      n_cores: 16 
    product:
      "output/python_files_fasttext_dim{{fasttext.dimension}}.bin"

  #
  - source: github_search.summarization.prepare_function_df_with_summarized_code
    params:
      transformer_model_name: "{{summarization.transformer_model_name}}"
    upstream: prepare_function_code_df
    product:
      "output/python_files_descriptions_{{summarization.transformer_model_name}}.feather"

  - source: github_search.graphs.prepare_graph.prepare_from_function_code
    name: graph.prepare_from_function_code
    upstream:
      - prepare_function_code_df
    product: "output/function_code_igraph.pkl"

  # prepare graph dataloaders
  - source: github_search.graphs.data_preparation.prepare_dataset
    name: gnn.prepare_dataset
    params:
      sentence_transformer_model_name: "{{gnn.sentence_transformer_model_name}}"
      batch_size: 128
    upstream:
      - prepare_area_grouped_tasks
      - graph.prepare_from_function_code
    product: "output/graph_list.pkl"

  # train graph classification model
  - source: github_search.graphs.nn_training.run_area_classification
    name: gnn.train_area_classification
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset
    params:
      hidden_channels: 64
    product:
      model_path: "output/models/graph_nn_area_training.pth"
      plot_path: "output/graph_nn_area_training_losses.png"
  - source: github_search.graphs.nn_training.run_multilabel_classification
    name: gnn.train_multilabel_task_classification
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset
    params:
      hidden_channels: 64
    product:
      model_path: "output/models/graph_nn_task_training.pth"
      plot_path: "output/graph_nn_task_training_losses.png"
  - source: github_search.graphs.nn_training.run_label_similarity_model
    name: gnn.train_label_similarity_model
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset
    params:
      epochs: 10 
      hidden_channels: 128 
      sentence_transformer_model_name: "{{gnn.sentence_transformer_model_name}}"
    product:
      model_path: "output/models/similarity_gnn.pth"
      plot_path: "output/graph_nn_similarity_training_losses.png"

#  # train GraphSAGE model
#  - source: github_search.pytorch_geometric_training.run_gnn_experiment
#    upstream:
#      - postprocess_dependency_records
#      - train_python_token_fasttext
#    params:
#      fasttext_model_path: "output/python_files_fasttext_dim{{fasttext.dimension}}.bin"
#      epochs: "{{gnn.epochs}}" 
#      hidden_channels: "{{gnn.hidden_channels}}" 
#      batch_size: "{{gnn.batch_size}}" 
#      num_layers: "{{gnn.layers}}"
#      test_run: "{{test_run}}" 
#      lr: 0.001
#      model_name: "{{gnn.model_name}}"
#      use_self_connection: "{{gnn.use_self_connection}}"
#      description_mode: "{{gnn.description_mode}}" 
#    product:
#      plot_file: "output/loss_{{gnn.model_name}}_fasttext_dim{{fasttext.dimension}}_epochs{{gnn.epochs}}_dim{{gnn.hidden_channels}}_layers{{gnn.layers}}.png"
#      gnn_token_embeddings: "output/{{gnn.model_name}}_embeddings_fasttext_dim{fasttext.dimension}}_epochs{{gnn.epochs}}_dim{{gnn.hidden_channels}}_layers{{gnn.layers}}.bin"
#      model_path: "output/{{gnn.model_name}}_model_{{gnn.epochs}}_dim{{gnn.hidden_channels}}_layers{{gnn.layers}}.pth"

  # task embeddings
  # prepares data for task_embeddings_app streamlit app
  # depends on sentence_embeddings_main
  - source: github_search.embedding_utils.prepare_reduced_embeddings
    upstream:
      - train_abstract_readme_w2v
      - train_python_token_fasttext
    params:
      rnn_model_path: "output/sbert/sru2x256_epoch350"
      codebert_model_path: "output/sbert/codebert15"
    product: "output/reduced_features.pkl"

  # for each repo select files that are most similar
  # similarity is cosine similarity in provided model
  - source: github_search.python_code_analysis.select_repo_files
    upstream:
      - make_readmes 
    params:
      similar_col: "readme"
      bow_vectorizer_class: "CountVectorizer"
      files_per_repo: 10
      python_files_path: "{{python_files_path}}"
    product: "output/selected_python_files.feather"


  - source: github_search.python_code_analysis.extract_python_comments
    upstream:
      - select_repo_files
    params:
      line_neighbor_threshold: 2
    product:
      "output/selected_python_files_comments.feather"

  - source: github_search.python_code_analysis.extract_python_imports
    upstream:
      - select_repo_files
    product:
      "output/selected_python_files_imports.feather"

  - source: github_search.seq2seq_utils.prepare_path_docid_seq2seq_df
    upstream:
      - select_repo_files
      - make_readmes
    params:
      max_source_length: "{{transformer.max_source_length}}" 
      max_target_length: "{{transformer.max_target_length}}" 

    product:
      "output/path_docid_seq2seq.csv"

  - source: github_search.seq2seq_utils.prepare_seq2seq_dataset
    name: prepare_path_docid_seq2seq_dataset
    upstream:
      - prepare_path_docid_seq2seq_df
    params:
      base_model: "{{transformer.T5_model}}" 
      max_source_length: "{{transformer.max_source_length}}" 
      max_target_length: "{{transformer.max_target_length}}" 
    product:
      "output/seq2seq_hf_dataset"

meta:
  extract_upstream: False 
  extract_product: False
  jupyter_hot_reload: True
  jupyter_functions_as_notebooks: True

tasks:

  # download area grouped tasks
  - source: github_search.paperswithcode_task_areas.prepare_area_grouped_tasks
    product: "data/paperswithcode_tasks.csv"

  # run word2vec on natural language data
  - source: github_search.word2vec.train_abstract_readme_w2v
    upstream:
      - make_readmes
    params:
      embedding_dim: "{{word2vec_dimension}}"
      epochs: "{{word2vec_epochs}}"
    product:
      "output/abstract_readme_w2v{{word2vec_dimension}}.bin"

  # prepare data for token2vec (modified import2vec)
  - source: github_search.data_engineering.prepare_module_corpus
    params:
      python_file_paths: ["{{python_files_csv_path}}"]
    product: "output/module_corpus.csv"

  # train token2vec model
  - source: "github_search/token2vec.py"
    params:
      n_iterations: 100000
      n_positive_imports: 32
      embedding_dim: "{{word2vec_dimension}}" 
    product:
      nb: "output/token2vec_training.ipynb"
      model_path: "output/import2vec_module_vectors{{word2vec_dimension}}.bin"

  # prepare paper dataset with imports extracted per-project
  - source: github_search.data_engineering.prepare_paperswithcode_with_imports_df
    params:
      python_file_paths: ["{{python_files_csv_path}}"]
    upstream:
      prepare_module_corpus
    product: "output/papers_with_imports.csv"

  # prepare python dependency graph records
  - source: github_search.data_engineering.prepare_dependency_records
    params:
      sample_files_per_repo: 100
      add_filename_repo_label: True
      python_file_paths: ["{{python_files_csv_path}}"]
    product: "output/dependency_records.csv"

  # additional information for dependency records
  - source: github_search.data_engineering.postprocess_dependency_records
    upstream:
      - prepare_dependency_records
      - prepare_paperswithcode_with_imports_df
    params:
      use_additional_records: False
      description_mode: "{{gnn_description_mode}}" 
    product: "output/processed_dependency_records.csv"

  # make igraph from dependency records
  - source: github_search.data_engineering.make_igraph
    upstream:
      prepare_dependency_records
    product: "output/call_igraph.pkl"

  # extract python function df
  # df has columns
  # ['repo_name', 'path', 'function_name', 'function_code']
  - source: github_search.data_engineering.make_function_code_df
    params:
      python_file_path: "{{python_files_csv_path}}"
    product:
      "output/python_functions.csv"

  # get github readmes
  - source: github_search.data_engineering.make_readmes
    params:
      paperswithcode_with_tasks_path: "data/paperswithcode_with_tasks.csv"
      max_workers: 24
    upstream: prepare_paperswithcode_with_imports_df
    product: "output/papers_with_readmes.csv"

  # train FastText model on Python files
  - source: github_search.data_engineering.train_python_token_fasttext
    params:
      python_file_path: "{{python_files_csv_path}}"
      dim: "{{fasttext_dimension}}" 
      epoch: "{{fasttext_epochs}}"
      n_cores: 12
    product:
      "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"

  # train GraphSAGE model
  - source: github_search.pytorch_geometric_training.run_gnn_experiment
    upstream:
      - postprocess_dependency_records
      - train_python_token_fasttext
    params:
      fasttext_model_path: "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"
      epochs: "{{gnn_epochs}}" 
      hidden_channels: "{{gnn_hidden_channels}}" 
      batch_size: "{{gnn_batch_size}}" 
      num_layers: "{{gnn_layers}}"
      test_run: "{{test_run}}" 
      lr: 0.001
      model_name: "{{gnn_model_name}}"
      use_self_connection: "{{gnn_use_self_connection}}"
      description_mode: "{{gnn_description_mode}}" 
    product:
      plot_file: "output/loss_{{gnn_model_name}}_fasttext_dim{{fasttext_dimension}}_epochs{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.png"
      gnn_token_embeddings: "output/{{gnn_model_name}}_embeddings_fasttext_dim{{fasttext_dimension}}_epochs{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.bin"
      model_path: "output/{{gnn_model_name}}_model_{{gnn_epochs}}_dim{{gnn_hidden_channels}}_layers{{gnn_layers}}.pth"

  # train-test split for tasks
  - source: github_search.matching_zsl.prepare_task_train_test_split
    upstream:
      - postprocess_dependency_records
      - train_python_token_fasttext
      - prepare_area_grouped_tasks
    product:
      train: "output/tasks_train.csv"
      test: "output/tasks_test.csv"

  # task embeddings
  # prepares data for task_embeddings_app streamlit app
  # depends on sentence_embeddings_main
  - source: github_search.embedding_utils.prepare_reduced_embeddings
    params:
      w2v_model_path: "output/abstract_readme_w2v200.bin"
      fasttext_model_path: "output/python_files_fasttext_dim200.bin"
      lstm_model_path: "output/sbert/lstm2x256_epoch500"
      codebert_model_path: "output/sbert/codebert15"
    product: "output/reduced_features.pkl"

meta:
  extract_upstream: False 
  extract_product: False
  jupyter_hot_reload: True
  jupyter_functions_as_notebooks: True

tasks:
  # prepare data for token2vec (modified import2vec)
  - source: github_search.data_engineering.prepare_module_corpus
    params:
      python_file_paths: ["data/scraped_python_files.csv"]
    product: "output/module_corpus.csv"
  # train token2vec model
  - source: "github_search/token2vec.py"
    params:
      n_iterations: 100000
      n_positive_imports: 32
      embedding_dim: "{{word2vec_dimension}}" 
    product:
      nb: "output/token2vec_training.ipynb"
      model_path: "output/import2vec_module_vectors{{word2vec_dimension}}.bin"
  # prepare paper dataset with imports extracted per-project
  - source: github_search.data_engineering.prepare_paperswithcode_with_imports_df
    params:
      python_file_paths: ["data/crawled_python_files.csv"]
    upstream:
      prepare_module_corpus
    product: "output/papers_with_imports.csv"
  # prepare python dependency graph records
  - source: github_search.data_engineering.prepare_dependency_records
    params:
      python_file_paths: ["data/crawled_python_files.csv"]
    product: "output/dependency_records.csv"
  # make igraph from dependency records
  - source: github_search.data_engineering.make_igraph
    upstream:
      prepare_dependency_records
    product: "output/call_igraph.pkl"
  # train FastText model on Python files
  - source: github_search.data_engineering.train_python_token_fasttext
    params:
      python_file_path: "data/crawled_python_files.csv"
      dim: "{{fasttext_dimension}}" 
      epoch: "{{fasttext_epochs}}"
    product:
      "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"
  # train GraphSAGE model
  - source: github_search.pytorch_geometric_training.run_graphsage_experiment
    params:
      fasttext_model_path: "output/python_files_fasttext_dim{{fasttext_dimension}}.bin"
      epochs: "{{graphsage_epochs}}" 
      hidden_channels: "{{graphsage_hidden_channels}}" 
      num_layers: "{{graphsage_layers}}"
      lr: 0.001
      skip_connections: "{{graphsage_use_skip_connections}}"
    product:
      graphsage_token_embeddings: "output/graphsage_embeddings_fasttext_dim{{fasttext_dimension}}_epochs{{graphsage_epochs}}_dim{{graphsage_hidden_channels}}_layers{{graphsage_layers}}.bin"
      model_path: "output/graphsage_model_{{graphsage_epochs}}_dim{{graphsage_hidden_channels}}_layers{{graphsage_layers}}.pth"
      nb: "output/graphsage_training.ipynb"

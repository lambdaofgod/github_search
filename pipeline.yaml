executor:
  dotted_path: ploomber.executors.Serial
  build_in_subprocess: False

meta:
  extract_upstream: False 
  extract_product: False
  jupyter_hot_reload: True
  jupyter_functions_as_notebooks: True

tasks:

  # download area grouped tasks
  - source: github_search.papers_with_code.paperswithcode_task_areas.prepare_area_grouped_tasks
    product: "data/paperswithcode_tasks.csv"
  
  - source: github_search.papers_with_code.paperswithcode_tasks.prepare_paperswithcode_df
    params:
      paperswithcode_filename: "data/links-between-papers-and-code.json.gz"
      papers_filename: "data/papers-with-abstracts.json.gz"
    product:
      paperswithcode_path: "output/raw_paperswithcode_df.csv"

  - source: github_search.papers_with_code.paperswithcode_tasks.prepare_filtered_paperswithcode_df
    upstream:
      - prepare_paperswithcode_df
    params:
      min_task_count: 10
    product:
      paperswithcode_path: "{{paperswithcode_path}}"
      task_counts_path: "output/task_counts.csv"

  # train-test split for tasks
  # tasks are stratified by paperswithcode area
  - source: github_search.data_engineering.prepare_task_train_test_split
    upstream:
      - prepare_area_grouped_tasks
      - prepare_filtered_paperswithcode_df
    params:
     test_size: 0.4
    product:
      train: "output/tasks_train.csv"
      test: "output/tasks_test.csv"

  # get github readmes
  - source: github_search.data_engineering.make_readmes
    params:
      paperswithcode_with_tasks_path: "data/paperswithcode_with_tasks.csv"
      max_workers: 24
    upstream: prepare_paperswithcode_with_imports_df
    product: "output/papers_with_readmes.csv"

  # the stack
  - source: github_search.the_stack.prepare_the_stack_files
    name:
      the_stack.prepare_files
    params:
      paperswithcode_path: "data/paperswithcode_with_tasks.csv"
      delete_temporary_files: True
    product: "data/the_stack_paperswithcode_repos"

  - source: github_search.the_stack.prepare_the_stack_df
    name:
      the_stack.prepare_df
    upstream:
      - the_stack.prepare_files
    product: "output/the_stack_paperswithcode_files.parquet"

  # prepare data for similarity learning from paperswithcode
  - source: github_search.sentence_embeddings.datasets.prepare_paperswithcode_data
    name: sentence_embeddings.prepare_paperswithcode_data
    product:
      datasets: "data/datasets.json.gz"
      methods: "data/methods.json.gz"

  # prepare data for similarity learning from dbpedia 
  - source: github_search.sentence_embeddings.datasets.prepare_dbpedia_machine_learning_data
    name: sentence_embeddings.prepare_dbpedia_data
    product: "data/dbpedia_ml_records.csv" 

  - source: github_search.sentence_embeddings.datasets.prepare_data
    name: sentence_embeddings.prepare_data
    upstream:
      - sentence_embeddings.prepare_dbpedia_data
      - sentence_embeddings.prepare_paperswithcode_data
    product: "data/sentence_similarity_data.csv"

  #
  - source: github_search.data_engineering.prepare_repo_train_test_split
    params:
      paperswithcode_with_tasks_path: "data/paperswithcode_with_tasks.csv"
    upstream:
      - prepare_task_train_test_split
    product:
      train: "output/repos_train.csv"
      test: "output/repos_test.csv"

  # extract python tokens for BoW baseline
  - source: github_search.bow_baseline.extract_python_tokens
    product: "output/python_files_with_tokens_df.feather"

  # 
  - source: github_search.bow_baseline.prepare_bow_retrieval_evaluation_results
    name: prepare_bow_retrieval_evaluation_results_readme
    upstream:
      - prepare_task_train_test_split
    params:
      index: python_tokenized_files 
    product: "output/python_files_retrieval_results.csv"
  # 
  - source: github_search.bow_baseline.prepare_bow_retrieval_evaluation_results
    name: prepare_bow_retrieval_evaluation_results_python_files
    upstream:
      - prepare_task_train_test_split
    params:
      index: project_readmes 
    product: "output/readme_retrieval_results.csv"

  # run word2vec on natural language data
  - source: github_search.word2vec.train_abstract_readme_w2v
    upstream:
      - make_readmes
    params:
      embedding_dim: "{{word2vec.dimension}}"
      epochs: "{{word2vec.epochs}}"
    product:
      binary: "output/abstract_readme_w2v{{word2vec.dimension}}.bin"
      txt: "output/abstract_readme_w2v{{word2vec.dimension}}.txt"

  #
  # imports
  #
  - source: github_search.imports.prepare_data.prepare_file_imports
    name: imports.prepare_file_imports
    params:
      python_files_path: "{{python_files_path}}"
    product: "output/python_file_imports.feather"

  - source: github_search.imports.training.train_import_word2vec
    name: imports.train_w2v
    upstream:
      - imports.prepare_file_imports
    params:
      embedding_dim: "{{word2vec.dimension}}"
      epochs: "{{word2vec.epochs}}"
    product:
      binary: "output/imports_w2v{{word2vec.dimension}}.bin"
      txt: "output/imports_w2v{{word2vec.dimension}}.txt"

  - source: github_search.imports.training.train_import_rnn_file_similarity_model
    name: imports.train_rnn
    upstream:
      - imports.prepare_file_imports
      - imports.train_w2v
    params:
      epochs: 2
      batch_size: 256 
      rnn_config: "{{rnn_config}}"
    product: "output/models/import_lstm"

  #
  # sentence embeddings
  #

  # make word2vec aggregator model
  - source: github_search.sentence_embeddings.models.prepare_word2vec_sentence_embedding_model
    name: sentence_embeddings.prepare_w2v_model
    upstream:
      - train_abstract_readme_w2v
    product:
      "output/abstract_readme_embedder"

  # prepare data for token2vec (modified import2vec)
  - source: github_search.data_engineering.prepare_module_corpus
    params:
      python_file_paths: ["{{python_files_path}}"]
    product: "output/module_corpus.csv"

  # train token2vec model
  - source: github_search.token2vec.train_token2vec
    upstream:
      - prepare_module_corpus
    params:
      n_iterations: 10000
      n_positive_imports: 32
      embedding_dim: "{{word2vec.dimension}}" 
    product:
      model_path: "output/import2vec_module_vectors{{word2vec.dimension}}.bin"

  # prepare paper dataset with imports extracted per-project
  - source: github_search.data_engineering.prepare_paperswithcode_with_imports_df
    upstream:
      - prepare_module_corpus
    params:
      python_file_paths: ["{{python_files_path}}"]
    product: "output/papers_with_imports.csv"

  # prepare python dependency graph records
  - source: github_search.data_engineering.prepare_dependency_records
    name:
      dependency_graph.prepare_records
    params:
      sample_files_per_repo: 1000
      add_repo_col: True
      use_basename: False
      python_file_path: "{{python_files_path}}"
      excluded_prefix: "venv"
    product: "output/dependency_records.feather"

  # additional information for dependency records
  - source: github_search.data_engineering.postprocess_dependency_records
    name:
      dependency_graph.postprocess_records
    upstream:
      - dependency_graph.prepare_records
      - prepare_paperswithcode_with_imports_df
    params:
      use_additional_records: False
      description_mode: False
    product: "output/processed_dependency_records.feather"

  #
  #
  # GRAPHS
  #
  #

  # extract python function df
  # f has columns
  # ['repo_name', 'path', 'function_name', 'function_code']
  - source: github_search.data_engineering.prepare_function_code_df
    params:
      python_file_path: "{{python_files_path}}"
      max_depth: 10
    product:
      "output/python_functions.feather"

  # train FastText model on Python files
  - source: github_search.data_engineering.train_python_token_fasttext
    params:
      python_file_path: "{{python_files_path}}"
      dim: "{{fasttext.dimension}}" 
      epoch: "{{fasttext.epochs}}"
      n_cores: 16 
    product:
      "output/python_files_fasttext_dim{{fasttext.dimension}}.bin"

  #
  - source: github_search.summarization.prepare_function_df_with_summarized_code
    params:
      transformer_model_name: "{{summarization.transformer_model_name}}"
    upstream: prepare_function_code_df
    product:
      "output/python_files_descriptions_{{summarization.transformer_model_name}}.feather"


  - source: github_search.graphs.prepare_graph.prepare_from_dependency_records
    name: graph.prepare_from_dependency_records
    upstream:
      - dependency_graph.prepare_records
    params:
      used_edges:
        - "repo-file"
    product: "output/dependency_records_igraph.pkl"

  - source: github_search.graphs.prepare_graph.prepare_from_function_code
    name: graph.prepare_from_function_code
    upstream:
      - prepare_function_code_df
    product: "output/function_code_igraph.pkl"

  # prepare graph list 
  - source: github_search.graphs.data_preparation.prepare_dataset_with_transformer
    name: gnn.prepare_dataset_with_transformer
    params:
      sentence_transformer_model_name: "{{gnn.sentence_transformer_model_name}}"
      batch_size: 128
      paperswithcode_path: "{{paperswithcode_path}}"
    upstream:
      - prepare_area_grouped_tasks
      - graph.prepare_from_function_code
    product: "output/graph_list.pkl"

  # prepare graph list 
  - source: github_search.graphs.data_preparation.prepare_dataset_with_word2vec
    name: gnn.prepare_dataset_with_word2vec
    params:
      batch_size: 128
      paperswithcode_path: "{{paperswithcode_path}}"
    upstream:
      - sentence_embeddings.prepare_w2v_model
      - prepare_area_grouped_tasks
      - graph.prepare_from_function_code
    product: "output/graph_list_w2v.pkl"

  - source: github_search.graphs.data_preparation.prepare_dataset_with_rnn
    name: gnn.prepare_dataset_with_rnn
    params:
      ulmfit_path: "output/models/learn_bigger_data_longer.pkl"
      paperswithcode_path: "{{paperswithcode_path}}"
    upstream:
      - prepare_area_grouped_tasks
      - dependency_graph.prepare_records
    product: "output/graph_list_rnn.h5"

  # split graph lists
  - source: github_search.graphs.data_preparation.prepare_dataset_split
    name: gnn.prepare_dataset_splits
    upstream:
      - gnn.prepare_dataset_with_rnn
      - prepare_repo_train_test_split
    product:
      train: "output/train_graph_list.pkl"
      test: "output/test_graph_list.pkl"
  - source: github_search.graphs.experiments.run_infomax
    name: gnn.train_infomax
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset_with_rnn
    params:
      hidden_channels: 128
      n_features: 128
    product:
      model_path: "output/models/infomax.pth"
      plot_path: "output/infomax_training_losses.png"

  - source: github_search.graphs.experiments.run_dependency_area_classification
    name: gnn.train_dep_graph
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset_with_rnn
    params:
      hidden_channels: 128
    product:
      model_path: "output/models/dep_graph_nn_area_training.pth"
      plot_path: "output/dep_graph_nn_area_training_losses.png"

  # train graph classification model using task area as labels
  - source: github_search.graphs.experiments.run_area_classification
    name: gnn.train_area_classification
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset_with_rnn
    params:
      hidden_channels: 64
    product:
      model_path: "output/models/graph_nn_area_training.pth"
      plot_path: "output/graph_nn_area_training_losses.png"

    # train graph multilabel classification model using tasks
  - source: github_search.graphs.experiments.run_multilabel_classification
    name: gnn.train_multilabel_task_classification
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset_with_rnn
    params:
      hidden_channels: 64
    product:
      model_path: "output/models/graph_nn_task_training.pth"
      plot_path: "output/graph_nn_task_training_losses.png"

  # train similarity model that uses MultipleNegativesRanking loss 
  - source: github_search.graphs.experiments.run_label_similarity_model
    name: gnn.train_label_similarity_model
    upstream:
      - prepare_area_grouped_tasks
      - prepare_repo_train_test_split
      - gnn.prepare_dataset_with_rnn
    params:
      similarity_model_params: "{{gnn.similarity_model_params}}"
      n_features: 128 
      ulmfit_path: "{{ulmfit_path}}" 
    product:
      model_path: "output/models/similarity_gnn.pth"
      plot_path: "output/graph_nn_similarity_training_losses.png"

  # prepare data for GraphSAGE model
  - source: github_search.graph_sage_data.make_graphsage_data
    name: gnn.make_graphsage_data
    upstream:
      - dependency_graph.prepare_records
    params:
      ulmfit_path: "{{ulmfit_path}}"
    product: "output/graphsage_input_embeddings.pth"

  # train GraphSAGE model
  - source: github_search.pytorch_geometric_training.run_gnn_experiment
    name: gnn.run_unsupervised_graph_sage
    upstream:
      - gnn.make_graphsage_data
      - train_python_token_fasttext
    params:
      params: "{{gnn.unsupervised_params}}"
      n_save_steps: 10000
      lr: 0.01
    product:
      plot_file: "output/loss_{{gnn.unsupervised_params.model_name}}_fasttext_dim{{fasttext.dimension}}_epochs{{gnn.unsupervised_params.epochs}}_dim{{gnn.unsupervised_params.hidden_channels}}_layers{{gnn.unsupervised_params.layers}}.png"
      gnn_token_embeddings: "output/{{gnn.unsupervised_params.model_name}}_embeddings_fasttext_dim{fasttext.dimension}}_epochs{{gnn.unsupervised_params.epochs}}_dim{{gnn.unsupervised_params.hidden_channels}}_layers{{gnn.unsupervised_params.layers}}.bin"
      model_path: "output/models/graphsage/{{gnn.unsupervised_params.model_name}}_model_{{gnn.unsupervised_params.epochs}}_dim{{gnn.unsupervised_params.hidden_channels}}_layers{{gnn.unsupervised_params.layers}}.pth"

  # neural bag of words
  - source: github_search.neural_bag_of_words.prepare_data.prepare_dependency_data_corpus
    name: nbow.prepare_dependency_data_corpus
    upstream:
      - dependency_graph.prepare_records
    product:
      raw_text: "output/dependency_records_corpus_raw.csv"
      text: "output/dependency_records_corpus.parquet"

  - source: github_search.neural_bag_of_words.prepare_data.prepare_nbow_dataset
    name: nbow.prepare_dataset
    upstream:
      - nbow.prepare_dependency_data_corpus
      - prepare_repo_train_test_split
    params:
      # columns that will be used in addition to 
      additional_columns: "{{nbow.additional_columns}}" 
    product:
      train: "output/nbow_data_train.parquet"
      test: "output/nbow_data_test.parquet"

  - source: github_search/neural_bag_of_words/nbow_training.py
    name: nbow.train-[[max_seq_length]]
    upstream:
      - nbow.prepare_dataset
      - prepare_repo_train_test_split
      - train_python_token_fasttext
    params:
      loss_function_name: "multiple_negatives_ranking_loss"
      neptune_config_path: "{{neptune_config_path}}"
      epochs: [1, 2, 5, 10, 20, 50, 100]
      batch_size: 128 
    grid:
      max_seq_length: ["median", "mean", 1000, 2000]
    product:
      model_dir: "output/models/nbow_[[max_seq_length]]"
      nb: "notebooks/output/NBOW_training_[[max_seq_length]].ipynb"

  - source: github_search/neural_bag_of_words/run_evaluation.py
    name: nbow.summarize_evaluation
    upstream: nbow.train-*
    product:
      best_model_dir: "output/models/best_nbow"
      metrics: "output/models/nbow_validation_metrics.yaml"
      nb: "output/notebooks/NBOW_Evaluation_Summary.ipynb"

  - source: github_search.neural_bag_of_words.sentencepiece.create_sentencepiece_model
    name: nbow.prepare_sentencepiece_model
    params:
      vocab_size: 50000
    upstream:
      nbow.prepare_dependency_data_corpus
    product: "output/sentencepiece/sp.model"

  # task embeddings
  # prepares data for task_embeddings_app streamlit app
  # depends on sentence_embeddings_main
  - source: github_search.embedding_utils.prepare_reduced_embeddings
    upstream:
      - train_abstract_readme_w2v
      - train_python_token_fasttext
    params:
      rnn_model_path: "output/sbert/sru2x256_epoch350"
      codebert_model_path: "output/sbert/codebert15"
    product: "output/reduced_features.pkl"

  # for each repo select files that are most similar
  # similarity is cosine similarity in provided model
  - source: github_search.python_code_analysis.select_repo_files
    upstream:
      - make_readmes 
    params:
      similar_col: "readme"
      bow_vectorizer_class: "CountVectorizer"
      files_per_repo: 10
      python_files_path: "{{python_files_path}}"
    product: "output/selected_python_files.feather"

  - source: github_search.python_code_analysis.extract_python_comments
    upstream:
      - select_repo_files
    params:
      line_neighbor_threshold: 2
    product:
      "output/selected_python_files_comments.feather"

  - source: github_search.python_code_analysis.extract_python_imports
    upstream:
      - select_repo_files
    product:
      "output/selected_python_files_imports.feather"

  - source: github_search.seq2seq_utils.prepare_path_docid_seq2seq_df
    upstream:
      - select_repo_files
      - make_readmes
    params:
      max_source_length: "{{transformer.max_source_length}}" 
      max_target_length: "{{transformer.max_target_length}}" 

    product:
      "output/path_docid_seq2seq.csv"

  - source: github_search.seq2seq_utils.prepare_seq2seq_dataset
    name: prepare_path_docid_seq2seq_dataset
    upstream:
      - prepare_path_docid_seq2seq_df
    params:
      base_model: "{{transformer.T5_model}}" 
      max_source_length: "{{transformer.max_source_length}}" 
      max_target_length: "{{transformer.max_target_length}}" 
    product:
      "output/seq2seq_hf_dataset"

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp parsing_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "Import = namedtuple(\"Import\", [\"module\", \"name\", \"alias\"])\n",
    "\n",
    "\n",
    "def _get_imports(file_content):\n",
    "    root = ast.parse(file_content)\n",
    "\n",
    "    for node in ast.iter_child_nodes(root):\n",
    "        if isinstance(node, ast.Import):\n",
    "            module = []\n",
    "        elif isinstance(node, ast.ImportFrom) and not node.module is None :  \n",
    "            module = node.module.split('.')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for n in node.names:\n",
    "            yield Import(module, n.name.split('.'), n.asname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import gzip\n",
      "from typing import Dict, Callable\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class CompressedKeyedVectors(object):\n",
      "\n",
      "    def __init__(self, vocab_path: str, embedding_path: str, to_lowercase: bool=True):\n",
      "        \"\"\"\n",
      "        Class from sdadas polish-nlp-resources\n",
      "        https://github.com/sdadas/polish-nlp-resources\n",
      "        I need to get it somewhere from where I can import it easily for using with custom BentoML model\n",
      "        \"\"\"\n",
      "        self.vocab_path: str = vocab_path\n",
      "        self.embedding_path: str = embedding_path\n",
      "        self.to_lower: bool = to_lowercase\n",
      "        self.vocab: Dict[str, int] = self.__load_vocab(vocab_path)\n",
      "        embedding = np.load(embedding_path)\n",
      "        self.codes: np.ndarray = embedding[embedding.files[0]]\n",
      "        self.codebook: np.ndarray = embedding[embedding.files[1]]\n",
      "        self.m = self.codes.shape[1]\n",
      "        self.k = int(self.codebook.shape[0] / self.m)\n",
      "        self.dim: int = self.codebook.shape[1]\n",
      "\n",
      "    def __load_vocab(self, vocab_path: str) -> Dict[str, int]:\n",
      "        open_func: Callable = gzip.open if vocab_path.endswith(\".gz\") else open\n",
      "        with open_func(vocab_path, \"rt\", encoding=\"utf-8\") as input_file:\n",
      "            return {line.strip():idx for idx, line in enumerate(input_file)}\n",
      "\n",
      "    def vocab_vector(self, word: str):\n",
      "        if word == \"<pad>\": return np.zeros(self.dim)\n",
      "        val: str = word.lower() if self.to_lower else word\n",
      "        index: int = self.vocab.get(val, self.vocab[\"<unk>\"])\n",
      "        codes = self.codes[index]\n",
      "        code_indices = np.array([idx * self.k + offset for idx, offset in enumerate(np.nditer(codes))])\n",
      "        return np.sum(self.codebook[code_indices], axis=0)\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        return self.vocab_vector(key)\n"
     ]
    }
   ],
   "source": [
    "example_file_content = 'import gzip\\nfrom typing import Dict, Callable\\nimport numpy as np\\n\\n\\nclass CompressedKeyedVectors(object):\\n\\n    def __init__(self, vocab_path: str, embedding_path: str, to_lowercase: bool=True):\\n        \"\"\"\\n        Class from sdadas polish-nlp-resources\\n        https://github.com/sdadas/polish-nlp-resources\\n        I need to get it somewhere from where I can import it easily for using with custom BentoML model\\n        \"\"\"\\n        self.vocab_path: str = vocab_path\\n        self.embedding_path: str = embedding_path\\n        self.to_lower: bool = to_lowercase\\n        self.vocab: Dict[str, int] = self.__load_vocab(vocab_path)\\n        embedding = np.load(embedding_path)\\n        self.codes: np.ndarray = embedding[embedding.files[0]]\\n        self.codebook: np.ndarray = embedding[embedding.files[1]]\\n        self.m = self.codes.shape[1]\\n        self.k = int(self.codebook.shape[0] / self.m)\\n        self.dim: int = self.codebook.shape[1]\\n\\n    def __load_vocab(self, vocab_path: str) -> Dict[str, int]:\\n        open_func: Callable = gzip.open if vocab_path.endswith(\".gz\") else open\\n        with open_func(vocab_path, \"rt\", encoding=\"utf-8\") as input_file:\\n            return {line.strip():idx for idx, line in enumerate(input_file)}\\n\\n    def vocab_vector(self, word: str):\\n        if word == \"<pad>\": return np.zeros(self.dim)\\n        val: str = word.lower() if self.to_lower else word\\n        index: int = self.vocab.get(val, self.vocab[\"<unk>\"])\\n        codes = self.codes[index]\\n        code_indices = np.array([idx * self.k + offset for idx, offset in enumerate(np.nditer(codes))])\\n        return np.sum(self.codebook[code_indices], axis=0)\\n\\n    def __getitem__(self, key):\\n        return self.vocab_vector(key)'\n",
    "print(example_file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(_get_imports('from . import tracking')) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Import(module=[], name=['gzip'], alias=None),\n",
       " Import(module=['typing'], name=['Dict'], alias=None),\n",
       " Import(module=['typing'], name=['Callable'], alias=None),\n",
       " Import(module=[], name=['numpy'], alias='np')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_get_imports(example_file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Import(module=['sklearn'], name=['linear_model'], alias=None),\n",
       " Import(module=['sklearn'], name=['model_selection'], alias=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_import_line = 'from sklearn import linear_model, model_selection'\n",
    "list(_get_imports(example_import_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_module_from_import(imp):\n",
    "    if imp.module == []:\n",
    "        return imp.name[0]\n",
    "    else:\n",
    "        return imp.module[0]\n",
    "\n",
    "\n",
    "def get_modules(file_content):\n",
    "    for imp in _get_imports(file_content):\n",
    "        yield get_module_from_import(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gzip', 'numpy', 'typing'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(get_modules(example_file_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

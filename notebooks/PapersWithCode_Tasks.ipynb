{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp paperswithcode_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# export\n",
    "\n",
    "\n",
    "def clean_task_name(task_name):\n",
    "    task_name = re.sub(r\"\\d+d\", \"\", task_name)\n",
    "    task_name = task_name.replace(\"-\", \" \")\n",
    "    return task_name.lower().strip()\n",
    "\n",
    "\n",
    "def get_paperswithcode_dfs(\n",
    "    paperswithcode_filename=\"data/links-between-papers-and-code.json.gz\",\n",
    "    papers_filename=\"data/papers-with-abstracts.json.gz\",\n",
    "):\n",
    "    paperswithcode_df = pd.read_json(paperswithcode_filename)\n",
    "    paperswithcode_df[\"repo\"] = paperswithcode_df[\"repo_url\"].str.replace(\n",
    "        \"https://github.com/\", \"\"\n",
    "    )\n",
    "\n",
    "    all_papers_df = pd.read_json(papers_filename)\n",
    "    return paperswithcode_df, all_papers_df\n",
    "\n",
    "\n",
    "def get_papers_with_repo_df(all_papers_df, paperswithcode_df, repo_names):\n",
    "    \"\"\"\n",
    "    add repo information to arxiv paper information\n",
    "    \"\"\"\n",
    "    paperswithcode_with_repo_df = paperswithcode_df[\n",
    "        paperswithcode_df[\"repo\"].isin(repo_names)\n",
    "    ]\n",
    "    paperswithcode_diff_columns = list(\n",
    "        paperswithcode_with_repo_df.columns.difference(all_papers_df.columns)\n",
    "    ) + [\"paper_url\"]\n",
    "    papers_with_repo_df = all_papers_df[\n",
    "        all_papers_df[\"paper_url\"].isin(paperswithcode_with_repo_df[\"paper_url\"])\n",
    "    ]\n",
    "\n",
    "    return papers_with_repo_df.merge(\n",
    "        paperswithcode_with_repo_df[paperswithcode_diff_columns], on=\"paper_url\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_papers_with_biggest_tasks(papers_with_repo_df, n_biggest_tasks):\n",
    "    \"\"\"\n",
    "    fetch papers which contain at least one task that is in n_biggest_tasks (by number of task occurrences)\n",
    "    \"\"\"\n",
    "    all_tasks = papers_with_repo_df.explode(\"tasks\")[\"tasks\"]\n",
    "    biggest_tasks = all_tasks.value_counts()[:n_biggest_tasks]\n",
    "\n",
    "    papers_with_repo_with_biggest_tasks_df = papers_with_repo_df[\n",
    "        papers_with_repo_df[\"tasks\"].apply(\n",
    "            lambda tasks: any(task in biggest_tasks.index for task in tasks)\n",
    "        )\n",
    "    ]\n",
    "    papers_with_repo_with_biggest_tasks_df[\n",
    "        \"most_common_task\"\n",
    "    ] = papers_with_repo_with_biggest_tasks_df[\"tasks\"].apply(\n",
    "        lambda tasks: biggest_tasks[\n",
    "            [t for t in tasks if t in biggest_tasks.index]\n",
    "        ].idxmax()\n",
    "        if len(biggest_tasks[[t for t in tasks if t in biggest_tasks.index]]) > 0\n",
    "        else None\n",
    "    )\n",
    "    return papers_with_repo_with_biggest_tasks_df\n",
    "\n",
    "\n",
    "def get_papers_with_biggest_tasks_df(n_biggest_tasks=None):\n",
    "    paperswithcode_df, all_papers_df = get_paperswithcode_dfs()\n",
    "    n_biggest_tasks = (\n",
    "        n_biggest_tasks if not n_biggest_tasks is None else len(paperswithcode_df)\n",
    "    )\n",
    "    papers_with_repo_df = get_papers_with_repo_df(\n",
    "        all_papers_df, paperswithcode_df, paperswithcode_df[\"repo\"]\n",
    "    )\n",
    "    return get_papers_with_biggest_tasks(\n",
    "        papers_with_repo_df, n_biggest_tasks=n_biggest_tasks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-15ed9967a212>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  paperswithcode_df['repo'] = paperswithcode_df['repo_url'].str.replace('https://github.com/', '')\n"
     ]
    }
   ],
   "source": [
    "paperswithcode_df = pd.read_json(\"data/links-between-papers-and-code.json.gz\")\n",
    "paperswithcode_df[\"repo\"] = paperswithcode_df[\"repo_url\"].str.replace(\n",
    "    \"https://github.com/\", \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_df = pd.read_json(\"data/papers-with-abstracts.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_files_df = pd.read_csv('data/python_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_task_counts(cleaned_tasks):\n",
    "    all_cleaned_tasks = cleaned_tasks.explode().dropna().apply(clean_task_name)\n",
    "    cleaned_tasks = all_cleaned_tasks.drop_duplicates()\n",
    "    return all_cleaned_tasks.value_counts()\n",
    "\n",
    "\n",
    "def get_papers_with_valid_tasks(all_papers_df, cleaned_tasks, min_task_occurrences):\n",
    "    task_counts = get_task_counts(cleaned_tasks)\n",
    "    valid_tasks = task_counts[task_counts >= min_task_occurrences].index\n",
    "    filtered_papers_tasks = cleaned_tasks.apply(\n",
    "        lambda ts: [t for t in ts if t in valid_tasks]\n",
    "    )\n",
    "    papers_with_tasks_df = all_papers_df[filtered_papers_tasks.apply(len) > 0]\n",
    "    papers_with_tasks_df[\"valid_tasks\"] = filtered_papers_tasks[\n",
    "        filtered_papers_tasks.apply(len) > 0\n",
    "    ]\n",
    "    return papers_with_tasks_df\n",
    "\n",
    "\n",
    "def add_least_common_task(\n",
    "    paperswithcode_with_tasks_df, cleaned_tasks, min_task_occurrences\n",
    "):\n",
    "    task_counts = get_task_counts(cleaned_tasks).sort_values()\n",
    "    task_counts = task_counts[task_counts > min_task_occurrences]\n",
    "    least_common_task = cleaned_tasks.apply(\n",
    "        lambda ts: task_counts.loc[[t for t in ts if t in task_counts.index]].index[0]\n",
    "        if any([t for t in ts if t in task_counts.index])\n",
    "        else None\n",
    "    )\n",
    "    paperswithcode_with_tasks_df[\"least_common_task\"] = least_common_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_paperswithcode_with_tasks_df(\n",
    "    paperswithcode_df, all_papers_df, min_task_occurrences=10\n",
    "):\n",
    "    all_papers_df[\"cleaned_tasks\"] = all_papers_df[\"tasks\"].apply(\n",
    "        lambda ts: [clean_task_name(t) for t in ts]\n",
    "    )\n",
    "    papers_with_valid_tasks_df = get_papers_with_valid_tasks(\n",
    "        all_papers_df, all_papers_df[\"cleaned_tasks\"], min_task_occurrences\n",
    "    )\n",
    "    paperswithcode_with_tasks_df = paperswithcode_df.merge(\n",
    "        papers_with_valid_tasks_df[[\"title\", \"valid_tasks\", \"abstract\"]],\n",
    "        left_on=\"paper_title\",\n",
    "        right_on=\"title\",\n",
    "    )\n",
    "    paperswithcode_with_tasks_df[\"tasks\"] = paperswithcode_with_tasks_df[\"valid_tasks\"]\n",
    "    paperswithcode_with_tasks_df = paperswithcode_with_tasks_df.groupby(\"repo\").apply(\n",
    "        lambda df: df.loc[df[\"tasks\"].apply(len).idxmax()]\n",
    "    )\n",
    "    add_least_common_task(\n",
    "        paperswithcode_with_tasks_df,\n",
    "        paperswithcode_with_tasks_df[\"valid_tasks\"],\n",
    "        min_task_occurrences,\n",
    "    )\n",
    "    paperswithcode_with_tasks_df.drop(\"valid_tasks\", axis=1, inplace=True)\n",
    "    all_valid_tasks = paperswithcode_with_tasks_df[\"least_common_task\"].unique()\n",
    "    paperswithcode_with_tasks_df[\"tasks\"] = paperswithcode_with_tasks_df[\"tasks\"].apply(\n",
    "        lambda ts: [t for t in ts if t in all_valid_tasks]\n",
    "    )\n",
    "    paperswithcode_with_tasks_df = paperswithcode_with_tasks_df.dropna(\n",
    "        axis=0, subset=[\"least_common_task\"]\n",
    "    )\n",
    "    return paperswithcode_with_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_area_grouped_tasks(paperswithcode_tasks_path=\"data/paperswithcode_tasks.csv\"):\n",
    "    area_grouped_tasks = pd.read_csv(\"data/paperswithcode_tasks.csv\").dropna()\n",
    "    area_grouped_tasks[\"task\"] = area_grouped_tasks[\"task\"].apply(clean_task_name)\n",
    "    area_counts = area_grouped_tasks[\"area\"].value_counts()\n",
    "    area_grouped_tasks = area_grouped_tasks[\n",
    "        area_grouped_tasks[\"area\"].isin(area_counts.index[area_counts > 1])\n",
    "    ]\n",
    "    return area_grouped_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113837,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers_df[\"tasks\"][all_papers_df[\"tasks\"].apply(len) > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-b34254f56b83>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_with_tasks_df['valid_tasks'] = filtered_papers_tasks[filtered_papers_tasks.apply(len) > 0]\n"
     ]
    }
   ],
   "source": [
    "paperswithcode_with_tasks_df = get_paperswithcode_with_tasks_df(\n",
    "    paperswithcode_df, all_papers_df, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_arxiv_id</th>\n",
       "      <th>paper_url_abs</th>\n",
       "      <th>paper_url_pdf</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>mentioned_in_paper</th>\n",
       "      <th>mentioned_in_github</th>\n",
       "      <th>framework</th>\n",
       "      <th>repo</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tasks</th>\n",
       "      <th>least_common_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://paperswithcode.com/paper/a-unifying-generative-model-for-graph</td>\n",
       "      <td>A Unifying Generative Model for Graph Learning Algorithms: Label Propagation, Graph Convolutions, and Combinations</td>\n",
       "      <td>2101.07730</td>\n",
       "      <td>https://arxiv.org/abs/2101.07730v2</td>\n",
       "      <td>https://arxiv.org/pdf/2101.07730v2.pdf</td>\n",
       "      <td>https://github.com/000Justin000/GaussianMRF</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>000Justin000/GaussianMRF</td>\n",
       "      <td>A Unifying Generative Model for Graph Learning Algorithms: Label Propagation, Graph Convolutions, and Combinations</td>\n",
       "      <td>Semi-supervised learning on graphs is a widely applicable problem in network science and machine learning. Two standard algorithms -- label propagation and graph neural networks -- both operate by...</td>\n",
       "      <td>[graph learning]</td>\n",
       "      <td>graph learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://paperswithcode.com/paper/graph-based-semi-supervised-active-learning</td>\n",
       "      <td>Graph-based Semi-Supervised &amp; Active Learning for Edge Flows</td>\n",
       "      <td>1905.07451</td>\n",
       "      <td>https://arxiv.org/abs/1905.07451v1</td>\n",
       "      <td>https://arxiv.org/pdf/1905.07451v1.pdf</td>\n",
       "      <td>https://github.com/000Justin000/ssl_edge</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>000Justin000/ssl_edge</td>\n",
       "      <td>Graph-based Semi-Supervised &amp; Active Learning for Edge Flows</td>\n",
       "      <td>We present a graph-based semi-supervised learning (SSL) method for learning edge flows defined on a graph. Specifically, given flow measurements on a subset of edges, we want to predict the flows ...</td>\n",
       "      <td>[active learning]</td>\n",
       "      <td>active learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://paperswithcode.com/paper/neural-ordinary-differential-equations</td>\n",
       "      <td>Neural Ordinary Differential Equations</td>\n",
       "      <td>1806.07366</td>\n",
       "      <td>https://arxiv.org/abs/1806.07366v5</td>\n",
       "      <td>https://arxiv.org/pdf/1806.07366v5.pdf</td>\n",
       "      <td>https://github.com/000Justin000/torchdiffeq</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>Neural Ordinary Differential Equations</td>\n",
       "      <td>We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The ...</td>\n",
       "      <td>[latent variable models, multivariate time series forecasting, multivariate time series imputation]</td>\n",
       "      <td>latent variable models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://paperswithcode.com/paper/one-shot-segmentation-in-clutter</td>\n",
       "      <td>One-Shot Segmentation in Clutter</td>\n",
       "      <td>1803.09597</td>\n",
       "      <td>http://arxiv.org/abs/1803.09597v2</td>\n",
       "      <td>http://arxiv.org/pdf/1803.09597v2.pdf</td>\n",
       "      <td>https://github.com/000c000l/oneShotLearningForSemanticSegmentation</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>tf</td>\n",
       "      <td>000c000l/oneShotLearningForSemanticSegmentation</td>\n",
       "      <td>One-Shot Segmentation in Clutter</td>\n",
       "      <td>We tackle the problem of one-shot segmentation: finding and segmenting a\\npreviously unseen object in a cluttered scene based on a single instruction\\nexample. We propose a novel dataset, which we...</td>\n",
       "      <td>[omniglot]</td>\n",
       "      <td>omniglot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://paperswithcode.com/paper/speaker-recognition-from-raw-waveform-with</td>\n",
       "      <td>Speaker Recognition from Raw Waveform with SincNet</td>\n",
       "      <td>1808.00158</td>\n",
       "      <td>https://arxiv.org/abs/1808.00158v3</td>\n",
       "      <td>https://arxiv.org/pdf/1808.00158v3.pdf</td>\n",
       "      <td>https://github.com/008karan/SincNet_demo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>008karan/SincNet_demo</td>\n",
       "      <td>Speaker Recognition from Raw Waveform with SincNet</td>\n",
       "      <td>Deep learning is progressively gaining popularity as a viable alternative to i-vectors for speaker recognition. Promising results have been recently obtained with Convolutional Neural Networks (CN...</td>\n",
       "      <td>[speaker identification, speaker recognition, speaker verification]</td>\n",
       "      <td>speaker identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43198</th>\n",
       "      <td>https://paperswithcode.com/paper/stargan-v2-diverse-image-synthesis-for</td>\n",
       "      <td>StarGAN v2: Diverse Image Synthesis for Multiple Domains</td>\n",
       "      <td>1912.01865</td>\n",
       "      <td>https://arxiv.org/abs/1912.01865v2</td>\n",
       "      <td>https://arxiv.org/pdf/1912.01865v2.pdf</td>\n",
       "      <td>https://github.com/zzz2010/starganv2_paddle</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>zzz2010/starganv2_paddle</td>\n",
       "      <td>StarGAN v2: Diverse Image Synthesis for Multiple Domains</td>\n",
       "      <td>A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over ...</td>\n",
       "      <td>[image generation, image to image translation]</td>\n",
       "      <td>image generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43199</th>\n",
       "      <td>https://paperswithcode.com/paper/explaining-image-classifiers-by</td>\n",
       "      <td>Explaining Image Classifiers by Counterfactual Generation</td>\n",
       "      <td>1807.08024</td>\n",
       "      <td>http://arxiv.org/abs/1807.08024v3</td>\n",
       "      <td>http://arxiv.org/pdf/1807.08024v3.pdf</td>\n",
       "      <td>https://github.com/zzzace2000/FIDO-saliency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>zzzace2000/FIDO-saliency</td>\n",
       "      <td>Explaining Image Classifiers by Counterfactual Generation</td>\n",
       "      <td>When an image classifier makes a prediction, which parts of the image are\\nrelevant and why? We can rephrase this question to ask: which parts of the\\nimage, if they were not seen by the classifie...</td>\n",
       "      <td>[image classification]</td>\n",
       "      <td>image classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43200</th>\n",
       "      <td>https://paperswithcode.com/paper/dynamic-measurement-scheduling-for-event</td>\n",
       "      <td>Dynamic Measurement Scheduling for Event Forecasting using Deep RL</td>\n",
       "      <td>1901.09699</td>\n",
       "      <td>https://arxiv.org/abs/1901.09699v3</td>\n",
       "      <td>https://arxiv.org/pdf/1901.09699v3.pdf</td>\n",
       "      <td>https://github.com/zzzace2000/autodiagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tf</td>\n",
       "      <td>zzzace2000/autodiagnosis</td>\n",
       "      <td>Dynamic Measurement Scheduling for Event Forecasting using Deep RL</td>\n",
       "      <td>Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement lea...</td>\n",
       "      <td>[mortality prediction]</td>\n",
       "      <td>mortality prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43201</th>\n",
       "      <td>https://paperswithcode.com/paper/dropout-feature-ranking-for-deep-learning</td>\n",
       "      <td>Dropout Feature Ranking for Deep Learning Models</td>\n",
       "      <td>1712.08645</td>\n",
       "      <td>http://arxiv.org/abs/1712.08645v2</td>\n",
       "      <td>http://arxiv.org/pdf/1712.08645v2.pdf</td>\n",
       "      <td>https://github.com/zzzace2000/dropout-feature-ranking</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>zzzace2000/dropout-feature-ranking</td>\n",
       "      <td>Dropout Feature Ranking for Deep Learning Models</td>\n",
       "      <td>Deep neural networks (DNNs) achieve state-of-the-art results in a variety of\\ndomains. Unfortunately, DNNs are notorious for their non-interpretability, and\\nthus limit their applicability in hypo...</td>\n",
       "      <td>[time series]</td>\n",
       "      <td>time series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43202</th>\n",
       "      <td>https://paperswithcode.com/paper/multitask-learning-and-benchmarking-with</td>\n",
       "      <td>Multitask learning and benchmarking with clinical time series data</td>\n",
       "      <td>1703.07771</td>\n",
       "      <td>https://arxiv.org/abs/1703.07771v3</td>\n",
       "      <td>https://arxiv.org/pdf/1703.07771v3.pdf</td>\n",
       "      <td>https://github.com/zzzace2000/mimic-preprocess</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>zzzace2000/mimic-preprocess</td>\n",
       "      <td>Multitask learning and benchmarking with clinical time series data</td>\n",
       "      <td>Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data availab...</td>\n",
       "      <td>[computational phenotyping, length of stay prediction, mortality prediction, time series]</td>\n",
       "      <td>computational phenotyping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43203 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          paper_url  \\\n",
       "0            https://paperswithcode.com/paper/a-unifying-generative-model-for-graph   \n",
       "1      https://paperswithcode.com/paper/graph-based-semi-supervised-active-learning   \n",
       "2           https://paperswithcode.com/paper/neural-ordinary-differential-equations   \n",
       "3                 https://paperswithcode.com/paper/one-shot-segmentation-in-clutter   \n",
       "4       https://paperswithcode.com/paper/speaker-recognition-from-raw-waveform-with   \n",
       "...                                                                             ...   \n",
       "43198       https://paperswithcode.com/paper/stargan-v2-diverse-image-synthesis-for   \n",
       "43199              https://paperswithcode.com/paper/explaining-image-classifiers-by   \n",
       "43200     https://paperswithcode.com/paper/dynamic-measurement-scheduling-for-event   \n",
       "43201    https://paperswithcode.com/paper/dropout-feature-ranking-for-deep-learning   \n",
       "43202     https://paperswithcode.com/paper/multitask-learning-and-benchmarking-with   \n",
       "\n",
       "                                                                                                              paper_title  \\\n",
       "0      A Unifying Generative Model for Graph Learning Algorithms: Label Propagation, Graph Convolutions, and Combinations   \n",
       "1                                                            Graph-based Semi-Supervised & Active Learning for Edge Flows   \n",
       "2                                                                                  Neural Ordinary Differential Equations   \n",
       "3                                                                                        One-Shot Segmentation in Clutter   \n",
       "4                                                                      Speaker Recognition from Raw Waveform with SincNet   \n",
       "...                                                                                                                   ...   \n",
       "43198                                                            StarGAN v2: Diverse Image Synthesis for Multiple Domains   \n",
       "43199                                                           Explaining Image Classifiers by Counterfactual Generation   \n",
       "43200                                                  Dynamic Measurement Scheduling for Event Forecasting using Deep RL   \n",
       "43201                                                                    Dropout Feature Ranking for Deep Learning Models   \n",
       "43202                                                  Multitask learning and benchmarking with clinical time series data   \n",
       "\n",
       "      paper_arxiv_id                       paper_url_abs  \\\n",
       "0         2101.07730  https://arxiv.org/abs/2101.07730v2   \n",
       "1         1905.07451  https://arxiv.org/abs/1905.07451v1   \n",
       "2         1806.07366  https://arxiv.org/abs/1806.07366v5   \n",
       "3         1803.09597   http://arxiv.org/abs/1803.09597v2   \n",
       "4         1808.00158  https://arxiv.org/abs/1808.00158v3   \n",
       "...              ...                                 ...   \n",
       "43198     1912.01865  https://arxiv.org/abs/1912.01865v2   \n",
       "43199     1807.08024   http://arxiv.org/abs/1807.08024v3   \n",
       "43200     1901.09699  https://arxiv.org/abs/1901.09699v3   \n",
       "43201     1712.08645   http://arxiv.org/abs/1712.08645v2   \n",
       "43202     1703.07771  https://arxiv.org/abs/1703.07771v3   \n",
       "\n",
       "                                paper_url_pdf  \\\n",
       "0      https://arxiv.org/pdf/2101.07730v2.pdf   \n",
       "1      https://arxiv.org/pdf/1905.07451v1.pdf   \n",
       "2      https://arxiv.org/pdf/1806.07366v5.pdf   \n",
       "3       http://arxiv.org/pdf/1803.09597v2.pdf   \n",
       "4      https://arxiv.org/pdf/1808.00158v3.pdf   \n",
       "...                                       ...   \n",
       "43198  https://arxiv.org/pdf/1912.01865v2.pdf   \n",
       "43199   http://arxiv.org/pdf/1807.08024v3.pdf   \n",
       "43200  https://arxiv.org/pdf/1901.09699v3.pdf   \n",
       "43201   http://arxiv.org/pdf/1712.08645v2.pdf   \n",
       "43202  https://arxiv.org/pdf/1703.07771v3.pdf   \n",
       "\n",
       "                                                                 repo_url  \\\n",
       "0                             https://github.com/000Justin000/GaussianMRF   \n",
       "1                                https://github.com/000Justin000/ssl_edge   \n",
       "2                             https://github.com/000Justin000/torchdiffeq   \n",
       "3      https://github.com/000c000l/oneShotLearningForSemanticSegmentation   \n",
       "4                                https://github.com/008karan/SincNet_demo   \n",
       "...                                                                   ...   \n",
       "43198                         https://github.com/zzz2010/starganv2_paddle   \n",
       "43199                         https://github.com/zzzace2000/FIDO-saliency   \n",
       "43200                         https://github.com/zzzace2000/autodiagnosis   \n",
       "43201               https://github.com/zzzace2000/dropout-feature-ranking   \n",
       "43202                      https://github.com/zzzace2000/mimic-preprocess   \n",
       "\n",
       "       mentioned_in_paper  mentioned_in_github framework  \\\n",
       "0                    True                False      none   \n",
       "1                    True                 True      none   \n",
       "2                   False                 True   pytorch   \n",
       "3                   False                 True        tf   \n",
       "4                   False                 True   pytorch   \n",
       "...                   ...                  ...       ...   \n",
       "43198               False                 True   pytorch   \n",
       "43199                True                 True   pytorch   \n",
       "43200                True                 True        tf   \n",
       "43201               False                 True   pytorch   \n",
       "43202               False                 True      none   \n",
       "\n",
       "                                                  repo  \\\n",
       "0                             000Justin000/GaussianMRF   \n",
       "1                                000Justin000/ssl_edge   \n",
       "2                             000Justin000/torchdiffeq   \n",
       "3      000c000l/oneShotLearningForSemanticSegmentation   \n",
       "4                                008karan/SincNet_demo   \n",
       "...                                                ...   \n",
       "43198                         zzz2010/starganv2_paddle   \n",
       "43199                         zzzace2000/FIDO-saliency   \n",
       "43200                         zzzace2000/autodiagnosis   \n",
       "43201               zzzace2000/dropout-feature-ranking   \n",
       "43202                      zzzace2000/mimic-preprocess   \n",
       "\n",
       "                                                                                                                    title  \\\n",
       "0      A Unifying Generative Model for Graph Learning Algorithms: Label Propagation, Graph Convolutions, and Combinations   \n",
       "1                                                            Graph-based Semi-Supervised & Active Learning for Edge Flows   \n",
       "2                                                                                  Neural Ordinary Differential Equations   \n",
       "3                                                                                        One-Shot Segmentation in Clutter   \n",
       "4                                                                      Speaker Recognition from Raw Waveform with SincNet   \n",
       "...                                                                                                                   ...   \n",
       "43198                                                            StarGAN v2: Diverse Image Synthesis for Multiple Domains   \n",
       "43199                                                           Explaining Image Classifiers by Counterfactual Generation   \n",
       "43200                                                  Dynamic Measurement Scheduling for Event Forecasting using Deep RL   \n",
       "43201                                                                    Dropout Feature Ranking for Deep Learning Models   \n",
       "43202                                                  Multitask learning and benchmarking with clinical time series data   \n",
       "\n",
       "                                                                                                                                                                                                      abstract  \\\n",
       "0      Semi-supervised learning on graphs is a widely applicable problem in network science and machine learning. Two standard algorithms -- label propagation and graph neural networks -- both operate by...   \n",
       "1      We present a graph-based semi-supervised learning (SSL) method for learning edge flows defined on a graph. Specifically, given flow measurements on a subset of edges, we want to predict the flows ...   \n",
       "2      We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The ...   \n",
       "3      We tackle the problem of one-shot segmentation: finding and segmenting a\\npreviously unseen object in a cluttered scene based on a single instruction\\nexample. We propose a novel dataset, which we...   \n",
       "4      Deep learning is progressively gaining popularity as a viable alternative to i-vectors for speaker recognition. Promising results have been recently obtained with Convolutional Neural Networks (CN...   \n",
       "...                                                                                                                                                                                                        ...   \n",
       "43198  A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over ...   \n",
       "43199  When an image classifier makes a prediction, which parts of the image are\\nrelevant and why? We can rephrase this question to ask: which parts of the\\nimage, if they were not seen by the classifie...   \n",
       "43200  Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement lea...   \n",
       "43201  Deep neural networks (DNNs) achieve state-of-the-art results in a variety of\\ndomains. Unfortunately, DNNs are notorious for their non-interpretability, and\\nthus limit their applicability in hypo...   \n",
       "43202  Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data availab...   \n",
       "\n",
       "                                                                                                     tasks  \\\n",
       "0                                                                                         [graph learning]   \n",
       "1                                                                                        [active learning]   \n",
       "2      [latent variable models, multivariate time series forecasting, multivariate time series imputation]   \n",
       "3                                                                                               [omniglot]   \n",
       "4                                      [speaker identification, speaker recognition, speaker verification]   \n",
       "...                                                                                                    ...   \n",
       "43198                                                       [image generation, image to image translation]   \n",
       "43199                                                                               [image classification]   \n",
       "43200                                                                               [mortality prediction]   \n",
       "43201                                                                                        [time series]   \n",
       "43202            [computational phenotyping, length of stay prediction, mortality prediction, time series]   \n",
       "\n",
       "               least_common_task  \n",
       "0                 graph learning  \n",
       "1                active learning  \n",
       "2         latent variable models  \n",
       "3                       omniglot  \n",
       "4         speaker identification  \n",
       "...                          ...  \n",
       "43198           image generation  \n",
       "43199       image classification  \n",
       "43200       mortality prediction  \n",
       "43201                time series  \n",
       "43202  computational phenotyping  \n",
       "\n",
       "[43203 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_tasks_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df[\"tasks\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_common_task_counts = paperswithcode_with_tasks_df[\n",
    "    \"least_common_task\"\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_least_common_tasks = least_common_task_counts[least_common_task_counts > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df[\n",
    "    paperswithcode_with_tasks_df[\"least_common_task\"].isin(\n",
    "        selected_least_common_tasks.index\n",
    "    )\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df.reset_index(drop=True).to_csv(\n",
    "    \"data/paperswithcode_with_tasks.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df[\"least_common_task\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df[\"tasks\"].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_tasks_df = all_papers_df[all_papers_df[\"tasks\"].apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_tasks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_df[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df[\"repo\"] = paperswithcode_df[\"repo_url\"].str.replace(\n",
    "    \"https://github.com/\", \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_repos = paperswithcode_df[\"repo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(paperswithcode_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(repo_names).intersection(paperswithcode_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df[\"repo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_df = get_papers_with_repo_df(\n",
    "    all_papers_df, paperswithcode_df, repo_names\n",
    ")\n",
    "papers_with_repo_df[\"tasks\"].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = papers_with_repo_df.explode(\"tasks\")[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks.value_counts()[all_tasks.value_counts() > 10]  # [:101].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df = get_papers_with_biggest_tasks(\n",
    "    papers_with_repo_df, None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[\n",
    "    papers_with_repo_with_biggest_tasks_df[\"tasks\"].apply(\n",
    "        lambda tasks: \"Hierarchical structure\" in tasks\n",
    "    )\n",
    "][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[\n",
    "    papers_with_repo_with_biggest_tasks_df[\"tasks\"].apply(len) > 1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting most common task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[\"most_common_task\"].value_counts()[:100].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_classes = {\n",
    "    \"Document Classification\": \"Text Classification\",\n",
    "    \"Abstractive Text Summarization\": \"Text Summarization\",\n",
    "    \"3D Human Pose Estimation\": \"Pose Estimation\",\n",
    "    \"Semantic Similarity\": \"Semantic Textual Similarity\",\n",
    "    \"Trajectory Prediction\": \"Autonomous Vehicles\",\n",
    "    \"Autonomous Driving\": \"Autonomous Vehicles\",\n",
    "    \"Feature Importance\": \"Feature Selection\",\n",
    "    \"Visual Tracking\": \"Object Tracking\",\n",
    "    \"Object Recognition\": \"Object Detection\",\n",
    "    \"Multi-Task Learning\": \"Transfer Learning\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_duplicated_classes = {\n",
    "    \"Adversarial Attack\": \"Adversarial Machine Learning\",\n",
    "    \"Adversarial Defense\": \"Adversarial Machine Learning\",\n",
    "    \"Voice Conversion\": \"Speech Generation\",\n",
    "    \"Lesion Segmentation\": \"Semantic Segmentation\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_classes = [\"Text-To-Sql\", \"Hiearchical structure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_task_counts = papers_with_repo_with_biggest_tasks_df[\n",
    "    \"most_common_task\"\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_task_counts[\n",
    "    most_common_task_counts > 10\n",
    "].sum()  # most_common_task_counts[:150].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_task_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[\"most_common_task\"].value_counts()[\n",
    "    :100\n",
    "].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting most matching task\n",
    "\n",
    "Matching is defined using similarity of embeddings of task name and article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = papers_with_repo_with_biggest_tasks_df.iloc[2][\"title\"]\n",
    "matched_texts = papers_with_repo_with_biggest_tasks_df.iloc[2][\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperswithcode\n",
    "\n",
    "client = paperswithcode.PapersWithCodeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(client.area_task_list(\"computer-vision\", page=2, items_per_page=1000).results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_with_biggest_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(client.task_get(\"trajectory-prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.task_paper_list(\"trajectory-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = papers_with_repo_with_biggest_tasks_df[\"paper_url\"].iloc[1].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(client.paper_get(paper_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = client.http.get(f\"/papers/{paper_id}/tasks/\")[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[paperswithcode.models.Task(**task) for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='adversarial' name='Adversarial'\n",
      "id='adversarial' name='Adversarial'\n",
      "adversarial : 15\n",
      "id='audio' name='Audio'\n",
      "id='audio' name='Audio'\n",
      "audio : 41\n",
      "id='computer-code' name='Computer Code'\n",
      "id='computer-code' name='Computer Code'\n",
      "computer-code : 40\n",
      "id='computer-vision' name='Computer Vision'\n",
      "id='computer-vision' name='Computer Vision'\n",
      "id='computer-vision' name='Computer Vision'\n",
      "computer-vision : 961\n",
      "id='graphs' name='Graphs'\n",
      "id='graphs' name='Graphs'\n",
      "graphs : 65\n",
      "id='knowledge-base' name='Knowledge Base'\n",
      "id='knowledge-base' name='Knowledge Base'\n",
      "knowledge-base : 24\n",
      "id='medical' name='Medical'\n",
      "id='medical' name='Medical'\n",
      "medical : 199\n",
      "id='methodology' name='Methodology'\n",
      "id='methodology' name='Methodology'\n",
      "methodology : 157\n",
      "id='miscellaneous' name='Miscellaneous'\n",
      "id='miscellaneous' name='Miscellaneous'\n",
      "miscellaneous : 143\n",
      "id='music' name='Music'\n",
      "id='music' name='Music'\n",
      "music : 17\n",
      "id='natural-language-processing' name='Natural Language Processing'\n",
      "id='natural-language-processing' name='Natural Language Processing'\n",
      "natural-language-processing : 458\n",
      "id='playing-games' name='Playing Games'\n",
      "id='playing-games' name='Playing Games'\n",
      "playing-games : 40\n",
      "id='reasoning' name='Reasoning'\n",
      "id='reasoning' name='Reasoning'\n",
      "reasoning : 21\n",
      "id='robots' name='Robots'\n",
      "id='robots' name='Robots'\n",
      "robots : 31\n",
      "id='speech' name='Speech'\n",
      "id='speech' name='Speech'\n",
      "speech : 51\n",
      "id='time-series' name='Time Series'\n",
      "id='time-series' name='Time Series'\n",
      "time-series : 68\n",
      "total tasks: 2331\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpClientError",
     "evalue": "HttpClientError(404: Not found.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpClientError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-377facd45003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea_task_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"computer-vision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_per_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tea_client/handler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHttpClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/paperswithcode/client.py\u001b[0m in \u001b[0;36marea_task_list\u001b[0;34m(self, area_id, page, items_per_page)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_per_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         return self.__page(\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/areas/{area_id}/tasks/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tea_client/http.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, headers, params, timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \"\"\"\n\u001b[0;32m--> 215\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tea_client/http.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, params, data, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERRORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpClientError\u001b[0m: HttpClientError(404: Not found.)"
     ]
    }
   ],
   "source": [
    "len(client.area_task_list(\"computer-vision\", page=3, items_per_page=1000).results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_tasks_df = pd.DataFrame(\n",
    "    {\"area\": area_grouped_tasks.keys(), \"task\": area_grouped_tasks.values()}\n",
    ").explode(\"task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'papers_with_repo_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-aae5bb8c1b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tasks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'papers_with_repo_df' is not defined"
     ]
    }
   ],
   "source": [
    "papers_with_repo_df[\"task\"] = papers_with_repo_df[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer-vision                961\n",
       "natural-language-processing    458\n",
       "medical                        199\n",
       "methodology                    157\n",
       "miscellaneous                  143\n",
       "time-series                     68\n",
       "graphs                          65\n",
       "speech                          51\n",
       "audio                           41\n",
       "computer-code                   40\n",
       "playing-games                   40\n",
       "robots                          31\n",
       "knowledge-base                  24\n",
       "reasoning                       21\n",
       "music                           17\n",
       "adversarial                     15\n",
       "Name: area, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tasks_df[\"area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'papers_with_repo_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8e1201cdfcf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers_task_exploded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'papers_with_repo_df' is not defined"
     ]
    }
   ],
   "source": [
    "papers_task_exploded_df = papers_with_repo_df.explode(\"task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_api_normalized = papers_task_exploded_df[\"task\"].str.lower().str.replace(\" \", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_api_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_without_area = task_api_normalized[\n",
    "    ~task_api_normalized.isin(area_tasks_df[\"task\"])\n",
    "].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tasks_df = pd.DataFrame({\"area\": \"miscellaneous\", \"task\": tasks_without_area})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df = pd.concat([area_tasks_df, other_tasks_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df.to_csv(\"data/paperswithcode_tasks.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_task_exploded_df[\"normalized_task\"] = task_api_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_area_df = papers_task_exploded_df.merge(\n",
    "    all_area_tasks_df, left_on=\"normalized_task\", right_on=\"task\", suffixes=[\"\", \"_\"]\n",
    ").drop(columns=[\"task_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_area_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_area_df[\"area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_area_df.groupby([\"area\", \"task\"]).agg(\"count\")[\"paper_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_counts = papers_area_df[\"area\"].value_counts()\n",
    "area_weights = area_counts.copy()\n",
    "area_weights = area_weights / area_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_tasks_df[area_tasks_df[\"area\"] == \"adversarial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_area_df[papers_area_df[\"area\"] == \"adversarial\"][\"task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks_df, test_tasks_df = model_selection.train_test_split(\n",
    "    all_area_tasks_df, test_size=0.2, stratify=all_area_tasks_df[\"area\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_train_df, papers_test_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlutil.maybe_pickler(\"/tmp/foo.pkl\") as writer:\n",
    "    writer.write_pickle_if_not_exists(lambda: papers_area_df.iloc[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://dfkiqyg0xf.execute-api.us-east-2.amazonaws.com/DEV2/storage/humtap-contributions/\"\n",
    "audio_contributions/audio/10_0FCBDDA0-953F-46A9-86B8-0AC8EAC89F03.opus|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp paperswithcode_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#export\n",
    "\n",
    "def clean_task_name(task_name):\n",
    "    task_name = re.sub(r\"\\d+d\", \"\", task_name)\n",
    "    task_name = task_name.replace(\"-\", \" \")\n",
    "    return task_name\n",
    "\n",
    "\n",
    "def get_paperswithcode_dfs(paperswithcode_filename='data/links-between-papers-and-code.json.gz', papers_filename='data/papers-with-abstracts.json.gz'):\n",
    "    paperswithcode_df = pd.read_json(paperswithcode_filename)\n",
    "    paperswithcode_df['repo'] = paperswithcode_df['repo_url'].str.replace('https://github.com/', '')\n",
    "\n",
    "    all_papers_df = pd.read_json(papers_filename)\n",
    "    return paperswithcode_df, all_papers_df\n",
    "\n",
    "\n",
    "def get_papers_with_repo_df(all_papers_df, paperswithcode_df, repo_names):\n",
    "    \"\"\"\n",
    "    add repo information to arxiv paper information\n",
    "    \"\"\"\n",
    "    paperswithcode_with_repo_df = paperswithcode_df[paperswithcode_df['repo'].isin(repo_names)]\n",
    "    paperswithcode_diff_columns = list(paperswithcode_with_repo_df.columns.difference(all_papers_df.columns)) + ['paper_url']\n",
    "    papers_with_repo_df = all_papers_df[all_papers_df['paper_url'].isin(paperswithcode_with_repo_df['paper_url'])]\n",
    "    \n",
    "    return papers_with_repo_df.merge(paperswithcode_with_repo_df[paperswithcode_diff_columns], on='paper_url')\n",
    "\n",
    "\n",
    "def get_papers_with_biggest_tasks(papers_with_repo_df, n_biggest_tasks):\n",
    "    \"\"\"\n",
    "    fetch papers which contain at least one task that is in n_biggest_tasks (by number of task occurrences)\n",
    "    \"\"\"\n",
    "    all_tasks = papers_with_repo_df.explode('tasks')['tasks'] \n",
    "    biggest_tasks = all_tasks.value_counts()[:n_biggest_tasks]\n",
    "    \n",
    "    papers_with_repo_with_biggest_tasks_df = papers_with_repo_df[papers_with_repo_df['tasks'].apply(lambda tasks: any(task in biggest_tasks.index for task in tasks))]\n",
    "    papers_with_repo_with_biggest_tasks_df['most_common_task'] = papers_with_repo_with_biggest_tasks_df['tasks'].apply(\n",
    "        lambda tasks: biggest_tasks[[t for t in tasks if t in biggest_tasks.index]].idxmax() if len(biggest_tasks[ [t for t in tasks if t in biggest_tasks.index]]) > 0 else None\n",
    "    )\n",
    "    return papers_with_repo_with_biggest_tasks_df \n",
    "\n",
    "\n",
    "def get_papers_with_biggest_tasks_df(n_biggest_tasks=None):\n",
    "    paperswithcode_df, all_papers_df = get_paperswithcode_dfs()\n",
    "    n_biggest_tasks = n_biggest_tasks if not n_biggest_tasks is None else len(paperswithcode_df) \n",
    "    papers_with_repo_df = get_papers_with_repo_df(all_papers_df, paperswithcode_df, paperswithcode_df['repo'])\n",
    "    return get_papers_with_biggest_tasks(papers_with_repo_df, n_biggest_tasks=n_biggest_tasks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['arxiv_id', 'title', 'abstract', 'url_abs', 'url_pdf', 'proceeding',\n",
      "       'authors', 'tasks', 'date', 'methods', 'framework',\n",
      "       'mentioned_in_github', 'mentioned_in_paper', 'paper_arxiv_id',\n",
      "       'paper_title', 'paper_url_abs', 'paper_url_pdf', 'repo', 'repo_url',\n",
      "       'paper_url'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tasks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tasks'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e07bc43fa44f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_papers_with_repos_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'huggingface/transformers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_biggest_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-a20840acafa8>\u001b[0m in \u001b[0;36mget_papers_with_repos_df\u001b[0;34m(repo_names, n_biggest_tasks)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpapers_with_repo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_papers_with_repo_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_papers_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaperswithcode_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_papers_with_biggest_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_biggest_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_biggest_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-a20840acafa8>\u001b[0m in \u001b[0;36mget_papers_with_biggest_tasks\u001b[0;34m(papers_with_repo_df, n_biggest_tasks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpapers_with_repo_with_biggest_tasks_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpapers_with_repo_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tasks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiggest_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     papers_with_repo_with_biggest_tasks_df['most_common_task'] = papers_with_repo_with_biggest_tasks_df['tasks'].apply(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbiggest_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiggest_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiggest_tasks\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiggest_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tasks'"
     ]
    }
   ],
   "source": [
    "get_papers_with_repos_df(['transformers'], top_biggest_tasks=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data/ipca*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ipca*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df = pd.read_json('data/links-between-papers-and-code.json.gz')\n",
    "paperswithcode_df['repo'] = paperswithcode_df['repo_url'].str.replace('https://github.com/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_df = pd.read_json('data/papers-with-abstracts.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df = pd.read_csv('data/python_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "      <th>sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport subprocess\\nimport shlex\\nimport re\\nimport os\\nimport argparse\\n\\nOKGREEN = '\\033[92m'\\nFAIL = '\\033[91m'\\nENDC = '\\033[0m'\\n\\nparser = argparse.ArgumentParser()\\np...</td>\n",
       "      <td>5364c4a113de40cb17e40b052fa012ad7daba2bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n# Author: Rico Sennrich\\n\\n\"\"\"Use operations learned with learn_bpe.py to encode a new text.\\nThe text will not be smaller, but use only a fixed vocabulary, with rare words...</td>\n",
       "      <td>261dd0ffc2e10ee32ea99ca80c99cde50a6e7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/concat-bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nparser = argparse.ArgumentParser()\\nparser.add_argument('vocab')\\nparser.add_argument('bpe')\\n\\n\\ndef build_vocab(bpe_pairs):\\n    vocab = set()\\n    for...</td>\n",
       "      <td>e6bc04ce4b16c0b91d00fc576ed5d261c711760f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/copy-model.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport argparse\\nimport subprocess\\nimport os\\nimport shutil\\nimport re\\nimport sys\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('model_dir')\\nparser.add_argu...</td>\n",
       "      <td>56014b1516fd6df606d500b5efc190efbe8d5203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/coverage.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nfrom collections import Counter\\nfrom itertools import starmap\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('filename')\\nparser.add_argumen...</td>\n",
       "      <td>f008bc3a194503c2e0c68ea2d5389db81905f6bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560178</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/convert_from_depre.py</td>\n",
       "      <td># --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Xinlei Chen\\n# ---------------------...</td>\n",
       "      <td>4ed7125b8568a0f9c60a3bf4670747ec4c497942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560179</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/demo.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Xinlei Chen...</td>\n",
       "      <td>2bd89335df588e010bbb22370274dcbd04bcb407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560180</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/reval.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# --------------------------------------------------------\\n# Fast R-CNN\\n# Copyright (c) 2015 Microsoft\\n# Licensed under The MIT License [see LICENSE for details]\\n# Wri...</td>\n",
       "      <td>612ae317fa5362b2b03b667cc2a8e17323d1a16f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560181</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/test_net.py</td>\n",
       "      <td># --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Zheqi he, Xinlei Chen, based on code...</td>\n",
       "      <td>c9cfc53f46f7f9d6c0885ffcad8727f86716ce22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560182</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Zheqi He, Xinlei Chen, based on code...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560183 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                owner repo_name                    file_path  \\\n",
       "0             trangvu   ape-npi                 run-tests.py   \n",
       "1             trangvu   ape-npi         scripts/apply_bpe.py   \n",
       "2             trangvu   ape-npi        scripts/concat-bpe.py   \n",
       "3             trangvu   ape-npi        scripts/copy-model.py   \n",
       "4             trangvu   ape-npi          scripts/coverage.py   \n",
       "...               ...       ...                          ...   \n",
       "560178  KyeongmoonKim        sb  tools/convert_from_depre.py   \n",
       "560179  KyeongmoonKim        sb                tools/demo.py   \n",
       "560180  KyeongmoonKim        sb               tools/reval.py   \n",
       "560181  KyeongmoonKim        sb            tools/test_net.py   \n",
       "560182  KyeongmoonKim        sb        tools/trainval_net.py   \n",
       "\n",
       "                                                                                                                                                                                                        content  \\\n",
       "0       #!/usr/bin/env python3\\nimport subprocess\\nimport shlex\\nimport re\\nimport os\\nimport argparse\\n\\nOKGREEN = '\\033[92m'\\nFAIL = '\\033[91m'\\nENDC = '\\033[0m'\\n\\nparser = argparse.ArgumentParser()\\np...   \n",
       "1       #!/usr/bin/env python3\\n# Author: Rico Sennrich\\n\\n\"\"\"Use operations learned with learn_bpe.py to encode a new text.\\nThe text will not be smaller, but use only a fixed vocabulary, with rare words...   \n",
       "2       #!/usr/bin/env python3\\n\\nimport argparse\\nparser = argparse.ArgumentParser()\\nparser.add_argument('vocab')\\nparser.add_argument('bpe')\\n\\n\\ndef build_vocab(bpe_pairs):\\n    vocab = set()\\n    for...   \n",
       "3       #!/usr/bin/env python3\\nimport argparse\\nimport subprocess\\nimport os\\nimport shutil\\nimport re\\nimport sys\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('model_dir')\\nparser.add_argu...   \n",
       "4       #!/usr/bin/env python3\\n\\nimport argparse\\nfrom collections import Counter\\nfrom itertools import starmap\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('filename')\\nparser.add_argumen...   \n",
       "...                                                                                                                                                                                                         ...   \n",
       "560178  # --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Xinlei Chen\\n# ---------------------...   \n",
       "560179  #!/usr/bin/env python\\n\\n# --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Xinlei Chen...   \n",
       "560180  #!/usr/bin/env python\\n\\n# --------------------------------------------------------\\n# Fast R-CNN\\n# Copyright (c) 2015 Microsoft\\n# Licensed under The MIT License [see LICENSE for details]\\n# Wri...   \n",
       "560181  # --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Zheqi he, Xinlei Chen, based on code...   \n",
       "560182  # --------------------------------------------------------\\n# Tensorflow Faster R-CNN\\n# Licensed under The MIT License [see LICENSE for details]\\n# Written by Zheqi He, Xinlei Chen, based on code...   \n",
       "\n",
       "                                             sha  \n",
       "0       5364c4a113de40cb17e40b052fa012ad7daba2bd  \n",
       "1       261dd0ffc2e10ee32ea99ca80c99cde50a6e7636  \n",
       "2       e6bc04ce4b16c0b91d00fc576ed5d261c711760f  \n",
       "3       56014b1516fd6df606d500b5efc190efbe8d5203  \n",
       "4       f008bc3a194503c2e0c68ea2d5389db81905f6bd  \n",
       "...                                          ...  \n",
       "560178  4ed7125b8568a0f9c60a3bf4670747ec4c497942  \n",
       "560179  2bd89335df588e010bbb22370274dcbd04bcb407  \n",
       "560180  612ae317fa5362b2b03b667cc2a8e17323d1a16f  \n",
       "560181  c9cfc53f46f7f9d6c0885ffcad8727f86716ce22  \n",
       "560182  e7b4dab93b6e543402f25f53696e918f3e3d0b5a  \n",
       "\n",
       "[560183 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df['repo_name_with_owner'] = python_files_df['owner']  + '/' + python_files_df['repo_name']\n",
    "repo_names = python_files_df['repo_name_with_owner'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199579 entries, 0 to 199578\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   paper_url   199579 non-null  object        \n",
      " 1   arxiv_id    159808 non-null  object        \n",
      " 2   title       199405 non-null  object        \n",
      " 3   abstract    199047 non-null  object        \n",
      " 4   url_abs     199579 non-null  object        \n",
      " 5   url_pdf     199405 non-null  object        \n",
      " 6   proceeding  58385 non-null   object        \n",
      " 7   authors     199579 non-null  object        \n",
      " 8   tasks       199579 non-null  object        \n",
      " 9   date        199405 non-null  datetime64[ns]\n",
      " 10  methods     199579 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(10)\n",
      "memory usage: 16.7+ MB\n"
     ]
    }
   ],
   "source": [
    "all_papers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df['repo'] = paperswithcode_df['repo_url'].str.replace('https://github.com/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_repos = paperswithcode_df['repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(paperswithcode_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(repo_names).intersection(paperswithcode_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        trangvu/ape-npi\n",
       "1                        xiezheng-cs/DTQ\n",
       "2                   Alaya-in-Matrix/MACE\n",
       "3                    harryhan618/LaneNet\n",
       "4        testingautomated-usi/selforacle\n",
       "                      ...               \n",
       "92580                 mabragor/cl-vknots\n",
       "92581              Expander/FlexibleSUSY\n",
       "92582             jiasenlu/HieCoAttenVQA\n",
       "92583            mgaido91/FBK-fairseq-ST\n",
       "92584          blessengeorge/compare_gan\n",
       "Name: repo, Length: 92585, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_df['repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trangvu/ape-npi', 'xiezheng-cs/DTQ', 'harryhan618/LaneNet', ...,\n",
       "       'laurinwagner/grouploss_plus', 'davydden/large-strain-matrix-free',\n",
       "       'KyeongmoonKim/sb'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_url', 'paper_title', 'paper_arxiv_id', 'paper_url_abs',\n",
       "       'paper_url_pdf', 'repo_url', 'mentioned_in_paper',\n",
       "       'mentioned_in_github', 'framework', 'repo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     1947\n",
       "2     1569\n",
       "0     1442\n",
       "3      826\n",
       "4      399\n",
       "5      194\n",
       "9       81\n",
       "6       52\n",
       "13      48\n",
       "7       28\n",
       "8       15\n",
       "11       4\n",
       "10       4\n",
       "Name: tasks, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_df = get_papers_with_repo_df(all_papers_df, paperswithcode_df, repo_names)\n",
    "papers_with_repo_df['tasks'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = papers_with_repo_df.explode('tasks')['tasks'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image Classification        469\n",
       "Object Detection            461\n",
       "Semantic Segmentation       449\n",
       "Machine Translation         258\n",
       "Language Modelling          246\n",
       "                           ... \n",
       "Tensor Networks              11\n",
       "Salient Object Detection     11\n",
       "Incremental Learning         11\n",
       "Relational Reasoning         11\n",
       "Text-To-Sql                  11\n",
       "Name: tasks, Length: 216, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks.value_counts()[all_tasks.value_counts() > 10]#[:101].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-927a2506dded>:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_with_repo_with_biggest_tasks_df['most_common_task'] = papers_with_repo_with_biggest_tasks_df['tasks'].apply(\n"
     ]
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df = get_papers_with_biggest_tasks(papers_with_repo_df, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926                                                               Learning to Abstract and Predict Human Actions\n",
       "1772                            Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval\n",
       "1966                                         Unsupervised Embedding of Hierarchical Structure in Euclidean Space\n",
       "2781                                             Benchmarking Deep Reinforcement Learning for Continuous Control\n",
       "2947                                                              PatchNet: A Tool for Deep Patch Classification\n",
       "2959                                                              Hyperbolic Graph Convolutional Neural Networks\n",
       "3056    Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\n",
       "3142                               A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\n",
       "3143                               A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\n",
       "3201                                                                   Discrete Autoencoders for Sequence Models\n",
       "3254                         A Hierarchical Decoding Model For Spoken Language Understanding From Unaligned Data\n",
       "3486                                          Leveraging Class Hierarchies with Metric-Guided Prototype Learning\n",
       "4442                                          Improving Quality of Hierarchical Clustering for Large Data Series\n",
       "4646                                               Poincaré Embeddings for Learning Hierarchical Representations\n",
       "4647                                               Poincaré Embeddings for Learning Hierarchical Representations\n",
       "4961                                                                  Of Non-Linearity and Commutativity in BERT\n",
       "5042                                                        CNN+CNN: Convolutional Decoders for Image Captioning\n",
       "5114                                       A Hierarchical Latent Structure for Variational Conversation Modeling\n",
       "6134                                                               Hierarchical Importance Weighted Autoencoders\n",
       "6488                     Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks\n",
       "6521                        Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[papers_with_repo_with_biggest_tasks_df['tasks'].apply(lambda tasks: 'Hierarchical structure' in tasks)]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>framework</th>\n",
       "      <th>mentioned_in_github</th>\n",
       "      <th>mentioned_in_paper</th>\n",
       "      <th>paper_arxiv_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url_abs</th>\n",
       "      <th>paper_url_pdf</th>\n",
       "      <th>repo</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>most_common_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://paperswithcode.com/paper/abcnn-attention-based-convolutional-neural</td>\n",
       "      <td>1512.05193</td>\n",
       "      <td>ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</td>\n",
       "      <td>How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with o...</td>\n",
       "      <td>http://arxiv.org/abs/1512.05193v4</td>\n",
       "      <td>http://arxiv.org/pdf/1512.05193v4.pdf</td>\n",
       "      <td>TACL 2016 1</td>\n",
       "      <td>[Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bo-Wen Zhou]</td>\n",
       "      <td>[Answer Selection, Natural Language Inference, Paraphrase Identification]</td>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1512.05193</td>\n",
       "      <td>ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</td>\n",
       "      <td>http://arxiv.org/abs/1512.05193v4</td>\n",
       "      <td>http://arxiv.org/pdf/1512.05193v4.pdf</td>\n",
       "      <td>shamalwinchurkar/question-classification</td>\n",
       "      <td>https://github.com/shamalwinchurkar/question-classification</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://paperswithcode.com/paper/unsupervised-representation-learning-by-1</td>\n",
       "      <td>1803.07728</td>\n",
       "      <td>Unsupervised Representation Learning by Predicting Image Rotations</td>\n",
       "      <td>Over the last years, deep convolutional neural networks (ConvNets) have\\ntransformed the field of computer vision thanks to their unparalleled capacity\\nto learn high level semantic image features...</td>\n",
       "      <td>http://arxiv.org/abs/1803.07728v1</td>\n",
       "      <td>http://arxiv.org/pdf/1803.07728v1.pdf</td>\n",
       "      <td>ICLR 2018 1</td>\n",
       "      <td>[Spyros Gidaris, Praveer Singh, Nikos Komodakis]</td>\n",
       "      <td>[Representation Learning, Unsupervised Representation Learning]</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1803.07728</td>\n",
       "      <td>Unsupervised Representation Learning by Predicting Image Rotations</td>\n",
       "      <td>http://arxiv.org/abs/1803.07728v1</td>\n",
       "      <td>http://arxiv.org/pdf/1803.07728v1.pdf</td>\n",
       "      <td>nab-126/resnet</td>\n",
       "      <td>https://github.com/nab-126/resnet</td>\n",
       "      <td>Representation Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://paperswithcode.com/paper/semantic-image-synthesis-via-adversarial</td>\n",
       "      <td>1707.06873</td>\n",
       "      <td>Semantic Image Synthesis via Adversarial Learning</td>\n",
       "      <td>In this paper, we propose a way of synthesizing realistic images directly\\nwith natural language description, which has many useful applications, e.g.\\nintelligent image manipulation. We attempt t...</td>\n",
       "      <td>http://arxiv.org/abs/1707.06873v1</td>\n",
       "      <td>http://arxiv.org/pdf/1707.06873v1.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Hao Dong, Simiao Yu, Chao Wu, Yike Guo]</td>\n",
       "      <td>[Image Generation, Image Manipulation]</td>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>...</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1707.06873</td>\n",
       "      <td>Semantic Image Synthesis via Adversarial Learning</td>\n",
       "      <td>http://arxiv.org/abs/1707.06873v1</td>\n",
       "      <td>http://arxiv.org/pdf/1707.06873v1.pdf</td>\n",
       "      <td>vtddggg/BilinearGAN_for_LBIE</td>\n",
       "      <td>https://github.com/vtddggg/BilinearGAN_for_LBIE</td>\n",
       "      <td>Image Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://paperswithcode.com/paper/ensembles-of-many-diverse-weak-defenses-can</td>\n",
       "      <td>2001.00308</td>\n",
       "      <td>ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense</td>\n",
       "      <td>There has been extensive research on developing defense techniques against adversarial attacks; however, they have been mainly designed for specific model families or application domains, therefor...</td>\n",
       "      <td>https://arxiv.org/abs/2001.00308v2</td>\n",
       "      <td>https://arxiv.org/pdf/2001.00308v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi]</td>\n",
       "      <td>[Adversarial Defense, Denoising]</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2001.00308</td>\n",
       "      <td>ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense</td>\n",
       "      <td>https://arxiv.org/abs/2001.00308v2</td>\n",
       "      <td>https://arxiv.org/pdf/2001.00308v2.pdf</td>\n",
       "      <td>softsys4ai/FlexiBO</td>\n",
       "      <td>https://github.com/softsys4ai/FlexiBO</td>\n",
       "      <td>Denoising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://paperswithcode.com/paper/3d-bounding-box-estimation-using-deep</td>\n",
       "      <td>1612.00496</td>\n",
       "      <td>3D Bounding Box Estimation Using Deep Learning and Geometry</td>\n",
       "      <td>We present a method for 3D object detection and pose estimation from a single\\nimage. In contrast to current techniques that only regress the 3D orientation\\nof an object, our method first regress...</td>\n",
       "      <td>http://arxiv.org/abs/1612.00496v2</td>\n",
       "      <td>http://arxiv.org/pdf/1612.00496v2.pdf</td>\n",
       "      <td>CVPR 2017 7</td>\n",
       "      <td>[Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana Kosecka]</td>\n",
       "      <td>[3D Object Detection, Object Detection, Pose Estimation, Semantic Segmentation, Viewpoint Estimation]</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>...</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1612.00496</td>\n",
       "      <td>3D Bounding Box Estimation Using Deep Learning and Geometry</td>\n",
       "      <td>http://arxiv.org/abs/1612.00496v2</td>\n",
       "      <td>http://arxiv.org/pdf/1612.00496v2.pdf</td>\n",
       "      <td>skhadem/3D-BoundingBox</td>\n",
       "      <td>https://github.com/skhadem/3D-BoundingBox</td>\n",
       "      <td>Object Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6578</th>\n",
       "      <td>https://paperswithcode.com/paper/robust-chinese-word-segmentation-with</td>\n",
       "      <td>1901.05816</td>\n",
       "      <td>Robust Chinese Word Segmentation with Contextualized Word Representations</td>\n",
       "      <td>In recent years, after the neural-network-based method was proposed, the\\naccuracy of the Chinese word segmentation task has made great progress.\\nHowever, when dealing with out-of-vocabulary word...</td>\n",
       "      <td>http://arxiv.org/abs/1901.05816v1</td>\n",
       "      <td>http://arxiv.org/pdf/1901.05816v1.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Yung-Sung Chuang]</td>\n",
       "      <td>[Chinese Word Segmentation, Language Modelling]</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>...</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1901.05816</td>\n",
       "      <td>Robust Chinese Word Segmentation with Contextualized Word Representations</td>\n",
       "      <td>http://arxiv.org/abs/1901.05816v1</td>\n",
       "      <td>http://arxiv.org/pdf/1901.05816v1.pdf</td>\n",
       "      <td>voidism/pywordseg</td>\n",
       "      <td>https://github.com/voidism/pywordseg</td>\n",
       "      <td>Language Modelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6593</th>\n",
       "      <td>https://paperswithcode.com/paper/recurrent-deep-embedding-networks-for</td>\n",
       "      <td>1805.12218</td>\n",
       "      <td>Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing</td>\n",
       "      <td>The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how pati...</td>\n",
       "      <td>https://arxiv.org/abs/1805.12218v2</td>\n",
       "      <td>https://arxiv.org/pdf/1805.12218v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Md. Rezaul Karim, Michael Cochez, Achille Zappa, Ratnesh Sahay, Oya Beyan, Dietrich-Rebholz Schuhmann, Stefan Decker]</td>\n",
       "      <td>[Feature Selection, Representation Learning]</td>\n",
       "      <td>2018-05-30</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1805.12218</td>\n",
       "      <td>Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing</td>\n",
       "      <td>https://arxiv.org/abs/1805.12218v2</td>\n",
       "      <td>https://arxiv.org/pdf/1805.12218v2.pdf</td>\n",
       "      <td>rezacsedu/Recurrent-Deep-Embedding-Networks</td>\n",
       "      <td>https://github.com/rezacsedu/Recurrent-Deep-Embedding-Networks</td>\n",
       "      <td>Representation Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>https://paperswithcode.com/paper/arabic-multi-dialect-segmentation-bi-lstm-crf</td>\n",
       "      <td>1708.05891</td>\n",
       "      <td>Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM</td>\n",
       "      <td>Arabic word segmentation is essential for a variety of NLP applications such\\nas machine translation and information retrieval. Segmentation entails breaking\\nwords into their constituent stems, a...</td>\n",
       "      <td>http://arxiv.org/abs/1708.05891v1</td>\n",
       "      <td>http://arxiv.org/pdf/1708.05891v1.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Mohamed Eldesouki, Younes Samih, Ahmed Abdelali, Mohammed Attia, Hamdy Mubarak, Kareem Darwish, Kallmeyer Laura]</td>\n",
       "      <td>[Domain Adaptation, Information Retrieval, Machine Translation]</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1708.05891</td>\n",
       "      <td>Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM</td>\n",
       "      <td>http://arxiv.org/abs/1708.05891v1</td>\n",
       "      <td>http://arxiv.org/pdf/1708.05891v1.pdf</td>\n",
       "      <td>qcri/dialectal_arabic_segmenter</td>\n",
       "      <td>https://github.com/qcri/dialectal_arabic_segmenter</td>\n",
       "      <td>Machine Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>https://paperswithcode.com/paper/learning-sparse-networks-using-targeted</td>\n",
       "      <td>1905.13678</td>\n",
       "      <td>Learning Sparse Networks Using Targeted Dropout</td>\n",
       "      <td>Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first ...</td>\n",
       "      <td>https://arxiv.org/abs/1905.13678v5</td>\n",
       "      <td>https://arxiv.org/pdf/1905.13678v5.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Aidan N. Gomez, Ivan Zhang, Siddhartha Rao Kamalakara, Divyam Madaan, Kevin Swersky, Yarin Gal, Geoffrey E. Hinton]</td>\n",
       "      <td>[Network Pruning, Neural Network Compression]</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1905.13678</td>\n",
       "      <td>Learning Sparse Networks Using Targeted Dropout</td>\n",
       "      <td>https://arxiv.org/abs/1905.13678v5</td>\n",
       "      <td>https://arxiv.org/pdf/1905.13678v5.pdf</td>\n",
       "      <td>for-ai/TD</td>\n",
       "      <td>https://github.com/for-ai/TD</td>\n",
       "      <td>Network Pruning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>https://paperswithcode.com/paper/distributed-optimization-of-multi-class-svms</td>\n",
       "      <td>1611.08480</td>\n",
       "      <td>Distributed Optimization of Multi-Class SVMs</td>\n",
       "      <td>Training of one-vs.-rest SVMs can be parallelized over the number of classes\\nin a straight forward way. Given enough computational resources, one-vs.-rest\\nSVMs can thus be trained on data involv...</td>\n",
       "      <td>http://arxiv.org/abs/1611.08480v2</td>\n",
       "      <td>http://arxiv.org/pdf/1611.08480v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Maximilian Alber, Julian Zimmert, Urun Dogan, Marius Kloft]</td>\n",
       "      <td>[Distributed Optimization, Text Classification]</td>\n",
       "      <td>2016-11-25</td>\n",
       "      <td>...</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1611.08480</td>\n",
       "      <td>Distributed Optimization of Multi-Class SVMs</td>\n",
       "      <td>http://arxiv.org/abs/1611.08480v2</td>\n",
       "      <td>http://arxiv.org/pdf/1611.08480v2.pdf</td>\n",
       "      <td>albermax/xcsvm</td>\n",
       "      <td>https://github.com/albermax/xcsvm</td>\n",
       "      <td>Text Classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3089 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           paper_url  \\\n",
       "3        https://paperswithcode.com/paper/abcnn-attention-based-convolutional-neural   \n",
       "4         https://paperswithcode.com/paper/unsupervised-representation-learning-by-1   \n",
       "5          https://paperswithcode.com/paper/semantic-image-synthesis-via-adversarial   \n",
       "10      https://paperswithcode.com/paper/ensembles-of-many-diverse-weak-defenses-can   \n",
       "11            https://paperswithcode.com/paper/3d-bounding-box-estimation-using-deep   \n",
       "...                                                                              ...   \n",
       "6578          https://paperswithcode.com/paper/robust-chinese-word-segmentation-with   \n",
       "6593          https://paperswithcode.com/paper/recurrent-deep-embedding-networks-for   \n",
       "6601  https://paperswithcode.com/paper/arabic-multi-dialect-segmentation-bi-lstm-crf   \n",
       "6602        https://paperswithcode.com/paper/learning-sparse-networks-using-targeted   \n",
       "6603   https://paperswithcode.com/paper/distributed-optimization-of-multi-class-svms   \n",
       "\n",
       "        arxiv_id  \\\n",
       "3     1512.05193   \n",
       "4     1803.07728   \n",
       "5     1707.06873   \n",
       "10    2001.00308   \n",
       "11    1612.00496   \n",
       "...          ...   \n",
       "6578  1901.05816   \n",
       "6593  1805.12218   \n",
       "6601  1708.05891   \n",
       "6602  1905.13678   \n",
       "6603  1611.08480   \n",
       "\n",
       "                                                                                             title  \\\n",
       "3                  ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs   \n",
       "4                               Unsupervised Representation Learning by Predicting Image Rotations   \n",
       "5                                                Semantic Image Synthesis via Adversarial Learning   \n",
       "10             ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense   \n",
       "11                                     3D Bounding Box Estimation Using Deep Learning and Geometry   \n",
       "...                                                                                            ...   \n",
       "6578                     Robust Chinese Word Segmentation with Contextualized Word Representations   \n",
       "6593  Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing   \n",
       "6601                                        Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM   \n",
       "6602                                               Learning Sparse Networks Using Targeted Dropout   \n",
       "6603                                                  Distributed Optimization of Multi-Class SVMs   \n",
       "\n",
       "                                                                                                                                                                                                     abstract  \\\n",
       "3     How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with o...   \n",
       "4     Over the last years, deep convolutional neural networks (ConvNets) have\\ntransformed the field of computer vision thanks to their unparalleled capacity\\nto learn high level semantic image features...   \n",
       "5     In this paper, we propose a way of synthesizing realistic images directly\\nwith natural language description, which has many useful applications, e.g.\\nintelligent image manipulation. We attempt t...   \n",
       "10    There has been extensive research on developing defense techniques against adversarial attacks; however, they have been mainly designed for specific model families or application domains, therefor...   \n",
       "11    We present a method for 3D object detection and pose estimation from a single\\nimage. In contrast to current techniques that only regress the 3D orientation\\nof an object, our method first regress...   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "6578  In recent years, after the neural-network-based method was proposed, the\\naccuracy of the Chinese word segmentation task has made great progress.\\nHowever, when dealing with out-of-vocabulary word...   \n",
       "6593  The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how pati...   \n",
       "6601  Arabic word segmentation is essential for a variety of NLP applications such\\nas machine translation and information retrieval. Segmentation entails breaking\\nwords into their constituent stems, a...   \n",
       "6602  Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first ...   \n",
       "6603  Training of one-vs.-rest SVMs can be parallelized over the number of classes\\nin a straight forward way. Given enough computational resources, one-vs.-rest\\nSVMs can thus be trained on data involv...   \n",
       "\n",
       "                                 url_abs  \\\n",
       "3      http://arxiv.org/abs/1512.05193v4   \n",
       "4      http://arxiv.org/abs/1803.07728v1   \n",
       "5      http://arxiv.org/abs/1707.06873v1   \n",
       "10    https://arxiv.org/abs/2001.00308v2   \n",
       "11     http://arxiv.org/abs/1612.00496v2   \n",
       "...                                  ...   \n",
       "6578   http://arxiv.org/abs/1901.05816v1   \n",
       "6593  https://arxiv.org/abs/1805.12218v2   \n",
       "6601   http://arxiv.org/abs/1708.05891v1   \n",
       "6602  https://arxiv.org/abs/1905.13678v5   \n",
       "6603   http://arxiv.org/abs/1611.08480v2   \n",
       "\n",
       "                                     url_pdf    proceeding  \\\n",
       "3      http://arxiv.org/pdf/1512.05193v4.pdf   TACL 2016 1   \n",
       "4      http://arxiv.org/pdf/1803.07728v1.pdf   ICLR 2018 1   \n",
       "5      http://arxiv.org/pdf/1707.06873v1.pdf  ICCV 2017 10   \n",
       "10    https://arxiv.org/pdf/2001.00308v2.pdf          None   \n",
       "11     http://arxiv.org/pdf/1612.00496v2.pdf   CVPR 2017 7   \n",
       "...                                      ...           ...   \n",
       "6578   http://arxiv.org/pdf/1901.05816v1.pdf          None   \n",
       "6593  https://arxiv.org/pdf/1805.12218v2.pdf          None   \n",
       "6601   http://arxiv.org/pdf/1708.05891v1.pdf          None   \n",
       "6602  https://arxiv.org/pdf/1905.13678v5.pdf          None   \n",
       "6603   http://arxiv.org/pdf/1611.08480v2.pdf          None   \n",
       "\n",
       "                                                                                                                     authors  \\\n",
       "3                                                                    [Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bo-Wen Zhou]   \n",
       "4                                                                           [Spyros Gidaris, Praveer Singh, Nikos Komodakis]   \n",
       "5                                                                                   [Hao Dong, Simiao Yu, Chao Wu, Yike Guo]   \n",
       "10                                                                    [Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi]   \n",
       "11                                                          [Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana Kosecka]   \n",
       "...                                                                                                                      ...   \n",
       "6578                                                                                                      [Yung-Sung Chuang]   \n",
       "6593  [Md. Rezaul Karim, Michael Cochez, Achille Zappa, Ratnesh Sahay, Oya Beyan, Dietrich-Rebholz Schuhmann, Stefan Decker]   \n",
       "6601       [Mohamed Eldesouki, Younes Samih, Ahmed Abdelali, Mohammed Attia, Hamdy Mubarak, Kareem Darwish, Kallmeyer Laura]   \n",
       "6602    [Aidan N. Gomez, Ivan Zhang, Siddhartha Rao Kamalakara, Divyam Madaan, Kevin Swersky, Yarin Gal, Geoffrey E. Hinton]   \n",
       "6603                                                            [Maximilian Alber, Julian Zimmert, Urun Dogan, Marius Kloft]   \n",
       "\n",
       "                                                                                                      tasks  \\\n",
       "3                                 [Answer Selection, Natural Language Inference, Paraphrase Identification]   \n",
       "4                                           [Representation Learning, Unsupervised Representation Learning]   \n",
       "5                                                                    [Image Generation, Image Manipulation]   \n",
       "10                                                                         [Adversarial Defense, Denoising]   \n",
       "11    [3D Object Detection, Object Detection, Pose Estimation, Semantic Segmentation, Viewpoint Estimation]   \n",
       "...                                                                                                     ...   \n",
       "6578                                                        [Chinese Word Segmentation, Language Modelling]   \n",
       "6593                                                           [Feature Selection, Representation Learning]   \n",
       "6601                                        [Domain Adaptation, Information Retrieval, Machine Translation]   \n",
       "6602                                                          [Network Pruning, Neural Network Compression]   \n",
       "6603                                                        [Distributed Optimization, Text Classification]   \n",
       "\n",
       "           date  ... framework mentioned_in_github  mentioned_in_paper  \\\n",
       "3    2015-12-16  ...        tf                True               False   \n",
       "4    2018-03-21  ...        tf                True               False   \n",
       "5    2017-07-21  ...   pytorch                True               False   \n",
       "10   2020-01-02  ...        tf                True               False   \n",
       "11   2016-12-01  ...   pytorch                True               False   \n",
       "...         ...  ...       ...                 ...                 ...   \n",
       "6578 2019-01-17  ...   pytorch                True               False   \n",
       "6593 2018-05-30  ...        tf                True               False   \n",
       "6601 2017-08-19  ...        tf                True               False   \n",
       "6602 2019-05-31  ...        tf               False                True   \n",
       "6603 2016-11-25  ...      none                True                True   \n",
       "\n",
       "      paper_arxiv_id  \\\n",
       "3         1512.05193   \n",
       "4         1803.07728   \n",
       "5         1707.06873   \n",
       "10        2001.00308   \n",
       "11        1612.00496   \n",
       "...              ...   \n",
       "6578      1901.05816   \n",
       "6593      1805.12218   \n",
       "6601      1708.05891   \n",
       "6602      1905.13678   \n",
       "6603      1611.08480   \n",
       "\n",
       "                                                                                       paper_title  \\\n",
       "3                  ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs   \n",
       "4                               Unsupervised Representation Learning by Predicting Image Rotations   \n",
       "5                                                Semantic Image Synthesis via Adversarial Learning   \n",
       "10             ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense   \n",
       "11                                     3D Bounding Box Estimation Using Deep Learning and Geometry   \n",
       "...                                                                                            ...   \n",
       "6578                     Robust Chinese Word Segmentation with Contextualized Word Representations   \n",
       "6593  Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing   \n",
       "6601                                        Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM   \n",
       "6602                                               Learning Sparse Networks Using Targeted Dropout   \n",
       "6603                                                  Distributed Optimization of Multi-Class SVMs   \n",
       "\n",
       "                           paper_url_abs  \\\n",
       "3      http://arxiv.org/abs/1512.05193v4   \n",
       "4      http://arxiv.org/abs/1803.07728v1   \n",
       "5      http://arxiv.org/abs/1707.06873v1   \n",
       "10    https://arxiv.org/abs/2001.00308v2   \n",
       "11     http://arxiv.org/abs/1612.00496v2   \n",
       "...                                  ...   \n",
       "6578   http://arxiv.org/abs/1901.05816v1   \n",
       "6593  https://arxiv.org/abs/1805.12218v2   \n",
       "6601   http://arxiv.org/abs/1708.05891v1   \n",
       "6602  https://arxiv.org/abs/1905.13678v5   \n",
       "6603   http://arxiv.org/abs/1611.08480v2   \n",
       "\n",
       "                               paper_url_pdf  \\\n",
       "3      http://arxiv.org/pdf/1512.05193v4.pdf   \n",
       "4      http://arxiv.org/pdf/1803.07728v1.pdf   \n",
       "5      http://arxiv.org/pdf/1707.06873v1.pdf   \n",
       "10    https://arxiv.org/pdf/2001.00308v2.pdf   \n",
       "11     http://arxiv.org/pdf/1612.00496v2.pdf   \n",
       "...                                      ...   \n",
       "6578   http://arxiv.org/pdf/1901.05816v1.pdf   \n",
       "6593  https://arxiv.org/pdf/1805.12218v2.pdf   \n",
       "6601   http://arxiv.org/pdf/1708.05891v1.pdf   \n",
       "6602  https://arxiv.org/pdf/1905.13678v5.pdf   \n",
       "6603   http://arxiv.org/pdf/1611.08480v2.pdf   \n",
       "\n",
       "                                             repo  \\\n",
       "3        shamalwinchurkar/question-classification   \n",
       "4                                  nab-126/resnet   \n",
       "5                    vtddggg/BilinearGAN_for_LBIE   \n",
       "10                             softsys4ai/FlexiBO   \n",
       "11                         skhadem/3D-BoundingBox   \n",
       "...                                           ...   \n",
       "6578                            voidism/pywordseg   \n",
       "6593  rezacsedu/Recurrent-Deep-Embedding-Networks   \n",
       "6601              qcri/dialectal_arabic_segmenter   \n",
       "6602                                    for-ai/TD   \n",
       "6603                               albermax/xcsvm   \n",
       "\n",
       "                                                            repo_url  \\\n",
       "3        https://github.com/shamalwinchurkar/question-classification   \n",
       "4                                  https://github.com/nab-126/resnet   \n",
       "5                    https://github.com/vtddggg/BilinearGAN_for_LBIE   \n",
       "10                             https://github.com/softsys4ai/FlexiBO   \n",
       "11                         https://github.com/skhadem/3D-BoundingBox   \n",
       "...                                                              ...   \n",
       "6578                            https://github.com/voidism/pywordseg   \n",
       "6593  https://github.com/rezacsedu/Recurrent-Deep-Embedding-Networks   \n",
       "6601              https://github.com/qcri/dialectal_arabic_segmenter   \n",
       "6602                                    https://github.com/for-ai/TD   \n",
       "6603                               https://github.com/albermax/xcsvm   \n",
       "\n",
       "                most_common_task  \n",
       "3     Natural Language Inference  \n",
       "4        Representation Learning  \n",
       "5               Image Generation  \n",
       "10                     Denoising  \n",
       "11              Object Detection  \n",
       "...                          ...  \n",
       "6578          Language Modelling  \n",
       "6593     Representation Learning  \n",
       "6601         Machine Translation  \n",
       "6602             Network Pruning  \n",
       "6603         Text Classification  \n",
       "\n",
       "[3089 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df[papers_with_repo_with_biggest_tasks_df['tasks'].apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4571, 21)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting most common task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4335"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df['most_common_task'].value_counts()[:100].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_classes = {\n",
    "    \"Document Classification\": \"Text Classification\",\n",
    "    \"Abstractive Text Summarization\": \"Text Summarization\",\n",
    "    \"3D Human Pose Estimation\": \"Pose Estimation\",\n",
    "    \"Semantic Similarity\": \"Semantic Textual Similarity\",\n",
    "    \"Trajectory Prediction\": \"Autonomous Vehicles\",\n",
    "    \"Autonomous Driving\": \"Autonomous Vehicles\",\n",
    "    \"Feature Importance\": \"Feature Selection\",\n",
    "    \"Visual Tracking\": \"Object Tracking\",\n",
    "    \"Object Recognition\": \"Object Detection\",\n",
    "    \"Multi-Task Learning\": \"Transfer Learning\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_duplicated_classes = {\n",
    "    \"Adversarial Attack\": \"Adversarial Machine Learning\",\n",
    "    \"Adversarial Defense\": \"Adversarial Machine Learning\",\n",
    "    \"Voice Conversion\": \"Speech Generation\",\n",
    "    \"Lesion Segmentation\": \"Semantic Segmentation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_classes = [\n",
    "    \"Text-To-Sql\",\n",
    "    \"Hiearchical structure\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_task_counts = papers_with_repo_with_biggest_tasks_df['most_common_task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4164"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_task_counts[most_common_task_counts > 10].sum()#most_common_task_counts[:150].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_task_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df['most_common_task'].value_counts()[:100].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting most matching task\n",
    "\n",
    "Matching is defined using similarity of embeddings of task name and article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = papers_with_repo_with_biggest_tasks_df.iloc[2]['title']\n",
    "matched_texts = papers_with_repo_with_biggest_tasks_df.iloc[2]['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Representation Learning', 'Unsupervised Representation Learning']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperswithcode\n",
    "\n",
    "client = paperswithcode.PapersWithCodeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(id='real-time-multi-object-tracking', name='Real-Time Multi-Object Tracking', description='Online and Real-time Multi-Object Tracking would achieve the real-time speed over 30 frames per second with online approach.'),\n",
       " Task(id='multi-animal-tracking-with-identification', name='Multi-Animal Tracking with identification', description='Tracking all animals in a video maintaining their identities after touches or occlusions.'),\n",
       " Task(id='hand-gesture-recognition', name='Hand Gesture Recognition', description=''),\n",
       " Task(id='visual-question-answering', name='Visual Question Answering', description='**Visual Question Answering** is a semantic task that aims to answer questions based on an image.\\r\\n\\r\\nImage Source: [visualqa.org](https://visualqa.org/)'),\n",
       " Task(id='image-compression-artifact-reduction', name='Image Compression Artifact Reduction', description=''),\n",
       " Task(id='video-frame-interpolation', name='Video Frame Interpolation', description='The goal of **Video Frame Interpolation** is to synthesize several frames in the middle of two adjacent frames of the original video. Video Frame Interpolation can be applied to generate slow motion video, increase video frame rate, and frame recovery in video streaming.\\n\\n\\n<span class=\"description-source\">Source: [Reducing the X-ray radiation exposure frequency in cardio-angiography via deep-learning based video interpolation ](https://arxiv.org/abs/2006.00781)</span>'),\n",
       " Task(id='lake-ice-detection', name='Lake Ice Monitoring', description=''),\n",
       " Task(id='visual-dialogue', name='Visual Dialog', description='Visual Dialog requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a follow-up question about the image, the task is to answer the question.'),\n",
       " Task(id='3d-canonical-hand-pose-estimation', name='3D Canonical Hand Pose Estimation', description='Image: [Lin et al](https://arxiv.org/pdf/2006.01320v1.pdf)'),\n",
       " Task(id='image-cropping', name='Image Cropping', description='**Image Cropping** is a common photo manipulation process, which improves the overall composition by removing unwanted regions. Image Cropping is widely used in photographic, film processing, graphic design, and printing businesses.\\n\\n\\n<span class=\"description-source\">Source: [Listwise View Ranking for Image Cropping ](https://arxiv.org/abs/1905.05352)</span>'),\n",
       " Task(id='future-prediction', name='Future prediction', description=''),\n",
       " Task(id='serial-style-transfer', name='Serial Style Transfer', description=''),\n",
       " Task(id='motion-compensation', name='Motion Compensation', description=''),\n",
       " Task(id='3d-human-dynamics', name='3D Human Dynamics', description='Image: [Zhang et al](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Predicting_3D_Human_Dynamics_From_Video_ICCV_2019_paper.pdf)'),\n",
       " Task(id='3d-part-segmentation', name='3D Part Segmentation', description='Segmenting 3D object parts\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [MeshCNN: A Network with an Edge](https://arxiv.org/pdf/1809.05910v2.pdf) )</span>'),\n",
       " Task(id='object-localization', name='Object Localization', description='**Object Localization** is the task of locating an instance of a particular object category in an image, typically by specifying a tightly cropped bounding box centered on the instance. An object proposal specifies a candidate bounding box, and an object proposal is said to be a correct localization if it sufficiently overlaps a human-labeled “ground-truth” bounding box for the given object. In the literature, the “Object Localization” task is to locate one instance of an object category, whereas “object detection” focuses on locating all instances of a category in a given image.\\n\\n\\n<span class=\"description-source\">Source: [Fast On-Line Kernel Density Estimation for Active Object Localization ](https://arxiv.org/abs/1611.05369)</span>'),\n",
       " Task(id='3d-point-cloud-classification', name='3D Point Cloud Classification', description='Image: [Qi et al](https://arxiv.org/pdf/1612.00593v2.pdf)'),\n",
       " Task(id='wildly-unsupervised-domain-adaptation', name='Wildly Unsupervised Domain Adaptation', description='Transferring knowledge from a noisy source domain to unlabeled target domain.'),\n",
       " Task(id='landslide-segmentation', name='Landslide segmentation', description=''),\n",
       " Task(id='frame-duplication-detection', name='Frame Duplication Detection', description=''),\n",
       " Task(id='structure-from-motion', name='Structure from Motion', description=''),\n",
       " Task(id='physical-attribute-prediction', name='Physical Attribute Prediction', description=''),\n",
       " Task(id='stereo-matching', name='Stereo Matching Hand', description=''),\n",
       " Task(id='medical-image-denoising', name='Medical Image Denoising', description=''),\n",
       " Task(id='smile-recognition', name='Smile Recognition', description='Smile recognition is the task of recognising a smiling face in a photo or video.'),\n",
       " Task(id='transparent-object-detection', name='Transparent Object Detection', description='Detecting transparent objects in 2D or 3D'),\n",
       " Task(id='3d-object-reconstruction', name='3D Object Reconstruction', description='Image: [Choy et al](https://arxiv.org/pdf/1604.00449v1.pdf)'),\n",
       " Task(id='pulmorary-vessel-segmentation', name='Pulmorary Vessel Segmentation', description=''),\n",
       " Task(id='jpeg-forgery-localization', name='Jpeg Forgery Localization', description=''),\n",
       " Task(id='video-denoising', name='Video Denoising', description=''),\n",
       " Task(id='visual-crowd-analysis', name='Visual Crowd Analysis', description=''),\n",
       " Task(id='video-generation', name='Video Generation', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [Logacheva et al.](https://paperswithcode.com/paper/deeplandscape-adversarial-modeling-of-1) )</span>'),\n",
       " Task(id='traffic-sign-detection', name='Traffic Sign Detection', description=''),\n",
       " Task(id='face-to-face-translation', name='Face to Face Translation', description='Given a video of a person speaking in a source language, generate a video of the same person speaking in a target language.'),\n",
       " Task(id='spectral-estimation-from-a-single-rgb-image', name='Spectral Estimation From A Single Rgb Image', description=''),\n",
       " Task(id='typeface-completion', name='Typeface Completion', description=''),\n",
       " Task(id='point-cloud-completion', name='Point Cloud Completion', description=''),\n",
       " Task(id='pornography-detection', name='Pornography Detection', description=''),\n",
       " Task(id='image-captioning', name='Image Captioning', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [Reflective Decoding Network for Image Captioning, ICCV\\'19](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf) )</span>'),\n",
       " Task(id='language-based-temporal-localization', name='Language-Based Temporal Localization', description=''),\n",
       " Task(id='blind-face-restoration', name='Blind Face Restoration', description=''),\n",
       " Task(id='object-detection-in-aerial-images', name='Object Detection In Aerial Images', description='Object Detection in Aerial Images is the task of detecting objects from aerial images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DOTA: A Large-Scale Dataset for Object Detection in Aerial Images](http://openaccess.thecvf.com/content_cvpr_2018/papers/Xia_DOTA_A_Large-Scale_CVPR_2018_paper.pdf) )</span>'),\n",
       " Task(id='rf-based-visual-tracking', name='RF-based Visual Tracking', description='From mID:\\r\\nhttps://doi.org/10.1109/DCOSS.2019.00028\\r\\n\\r\\n\"The key to offering personalised services in smart spaces is knowing where a particular person is with a high degree of accuracy. Visual tracking is one such solution, but concerns arise around the potential leakage of raw video information and many people are not comfortable accepting cameras in their homes or workplaces. We propose a human tracking and identification system (mID) based on millimeter wave radar which has a high tracking accuracy, without being visually compromising. Unlike competing techniques based on WiFi Channel State Information (CSI), it is capable of tracking and identifying multiple people simultaneously. Using a lowcost, commercial, off-the-shelf radar, we first obtain sparse point clouds and form temporally associated trajectories.\"\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [mID: Tracking and Identifying People with Millimeter Wave Radar](http://www.cs.ox.ac.uk/files/10889/%5BDCOSS19%5DmID.pdf) )</span>'),\n",
       " Task(id='vehicle-re-identification', name='Vehicle Re-Identification', description='Vehicle re-identification is the task of identifying the same vehicle across multiple cameras.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras](https://github.com/icarofua/siamese-two-stream) )</span>'),\n",
       " Task(id='group-activity-recognition', name='Group Activity Recognition', description='**Group Activity Recognition** is a subset of human activity recognition problem which focuses on the collective behavior of a group of people, resulted from the individual actions of the persons and their interactions. Collective activity recognition is a basic task for automatic human behavior analysis in many areas like surveillance or sports videos.\\n\\n\\n<span class=\"description-source\">Source: [A Multi-Stream Convolutional Neural Network Framework for Group Activity Recognition ](https://arxiv.org/abs/1812.10328)</span>'),\n",
       " Task(id='furniture-segmentation', name='furniture segmentation', description=''),\n",
       " Task(id='texture-classification', name='Texture Classification', description='**Texture Classification** is a fundamental issue in computer vision and image processing, playing a significant role in many applications such as medical image analysis, remote sensing, object recognition, document analysis, environment modeling, content-based image retrieval and many more.\\n\\n\\n<span class=\"description-source\">Source: [Improving Texture Categorization with Biologically Inspired Filtering ](https://arxiv.org/abs/1312.0072)</span>'),\n",
       " Task(id='occluded-face-detection', name='Occluded Face Detection', description=''),\n",
       " Task(id='detecting-shadows', name='Detecting Shadows', description=''),\n",
       " Task(id='3d-reconstruction', name='3D Reconstruction', description='Image: [Gwak et al](https://arxiv.org/pdf/1705.10904v2.pdf)'),\n",
       " Task(id='semantic-part-detection', name='Semantic Part Detection', description=''),\n",
       " Task(id='sports-analytics', name='Sports Analytics', description=''),\n",
       " Task(id='cloud-detection', name='Cloud Detection', description=''),\n",
       " Task(id='image-based-automatic-meter-reading', name='Image-based Automatic Meter Reading', description=''),\n",
       " Task(id='text-to-image', name='Text-To-Image', description=''),\n",
       " Task(id='viewpoint-estimation', name='Viewpoint Estimation', description=''),\n",
       " Task(id='texture-image-retrieval', name='Text-Image Retrieval', description='It include two tasks: (1) Image as Query and Text as Targets; (2) Text as Query and Image as Targets.'),\n",
       " Task(id='face-age-editing', name='Face Age Editing', description=''),\n",
       " Task(id='document-to-image-conversion', name='Document To Image Conversion', description=''),\n",
       " Task(id='single-image-blind-deblurring', name='Single-Image Blind Deblurring', description=''),\n",
       " Task(id='hand-gesture-recognition-1', name='Hand-Gesture Recognition', description=''),\n",
       " Task(id='multi-person-pose-estimation', name='Multi-Person Pose Estimation', description='Multi-person pose estimation is the task of estimating the pose of multiple people in one frame.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Human Pose Estimation with TensorFlow\\r\\n](https://github.com/eldar/pose-tensorflow) )</span>'),\n",
       " Task(id='markerless-motion-capture', name='Markerless Motion Capture', description=''),\n",
       " Task(id='saliency-prediction-1', name='Few-Shot Transfer Learning for Saliency Prediction', description='Saliency prediction aims to predict important locations in a visual scene. It is a per-pixel regression task with predicted values ranging from 0 to 1.\\r\\n\\r\\nBenefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models.'),\n",
       " Task(id='unseen-object-instance-segmentation', name='Unseen Object Instance Segmentation', description='Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image.\\r\\n\\r\\nImage Credit: [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers](https://arxiv.org/abs/2103.12340)'),\n",
       " Task(id='symmetry-detection', name='Symmetry Detection', description=''),\n",
       " Task(id='panoptic-segmentation', name='Panoptic Segmentation', description='Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Detectron2](https://github.com/facebookresearch/detectron2) )</span>'),\n",
       " Task(id='facial-attribute-classification', name='Facial Attribute Classification', description='Facial attribute classification is the task of classifying various attributes of a facial image - e.g. whether someone has a beard, is wearing a hat, and so on.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-task Learning of Cascaded CNN for Facial Attribute Classification\\r\\n](https://arxiv.org/pdf/1805.01290v1.pdf) )</span>'),\n",
       " Task(id='action-localization', name='Action Localization', description=''),\n",
       " Task(id='electron-microscopy', name='Electron Microscopy', description='Analysis of data from various types of electron microscopes.'),\n",
       " Task(id='pose-retrieval', name='Pose Retrieval', description='Retrieval of similar human poses from images or videos'),\n",
       " Task(id='speaker-specific-lip-to-speech-synthesis', name='Speaker-Specific Lip to Speech Synthesis', description='How accurately can we infer an individual’s speech style and content from his/her lip movements? [1]\\r\\n\\r\\nIn this task, the model is trained on a specific speaker, or a very limited set of speakers. \\r\\n\\r\\n[1] Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis, CVPR 2020.'),\n",
       " Task(id='face-identification', name='Face Identification', description='Face identification is the task of matching a given face image to one in an existing database of faces. It is the second part of face recognition (the first part being detection). It is a one-to-many mapping: you have to find an unknown person in a database to find who that person is.'),\n",
       " Task(id='video-instance-segmentation', name='Video Instance Segmentation', description='The goal of video instance segmentation is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain.\\r\\n\\r\\nTo facilitate research on this new task, a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks is built.'),\n",
       " Task(id='action-segmentation', name='Action Segmentation', description='**Action Segmentation** is a challenging problem in high-level video understanding. In its simplest form, Action Segmentation aims to segment a temporally untrimmed video by time and label each segmented part with one of pre-defined action labels. The results of Action Segmentation can be further used as input to various applications, such as video-to-text and action localization.\\n\\n\\n<span class=\"description-source\">Source: [TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation ](https://arxiv.org/abs/1705.07818)</span>'),\n",
       " Task(id='semi-supervised-instance-segmentation', name='Semi-Supervised Instance Segmentation', description=''),\n",
       " Task(id='trajectory-prediction', name='Trajectory Prediction', description='**Trajectory Prediction** is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs ](https://arxiv.org/abs/1912.01118)</span>'),\n",
       " Task(id='3d-multi-person-pose-estimation-root-relative', name='3D Multi-Person Pose Estimation (root-relative)', description='This task aims to solve root-relative 3D multi-person pose estimation (person-centric coordinate system). No ground truth human bounding box and human root joint coordinates are used during testing stage.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [RootNet](https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE) )</span>'),\n",
       " Task(id='finger-dorsal-image-spoof-detection', name='Finger Dorsal Image Spoof Detection', description=''),\n",
       " Task(id='unsupervised-object-segmentation', name='Unsupervised Object Segmentation', description=''),\n",
       " Task(id='rgb-t-tracking', name='Rgb-T Tracking', description=''),\n",
       " Task(id='video-description', name='Video Description', description='The goal of automatic **Video Description** is to tell a story about events happening in a video. While early Video Description methods produced captions for short clips that were manually segmented to contain a single event of interest, more recently dense video captioning has been proposed to both segment distinct events in time and describe them in a series of coherent sentences. This problem is a generalization of dense image region captioning and has many practical applications, such as generating textual summaries for the visually impaired, or detecting and describing important events in surveillance footage.\\n\\n\\n<span class=\"description-source\">Source: [Joint Event Detection and Description in Continuous Video Streams ](https://arxiv.org/abs/1802.10250)</span>'),\n",
       " Task(id='image-clustering', name='Image Clustering', description='Models that learn to label each image (i.e. cluster the dataset into its ground truth classes) without seeing the ground truth labels.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\"> Image credit: ImageNet clustering results of [SCAN: Learning to Classify Images without Labels (ECCV 2020)](https://arxiv.org/abs/2005.12320) </span>'),\n",
       " Task(id='dial-meter-reading', name='Dial Meter Reading', description=''),\n",
       " Task(id='colorization', name='Colorization', description='**Colorization** is the process of adding plausible color information to monochrome photographs or videos. Colorization is a highly undetermined problem, requiring mapping a real-valued luminance image to a three-dimensional color-valued one, that has not a unique solution.\\n\\n\\n<span class=\"description-source\">Source: [ChromaGAN: An Adversarial Approach for Picture Colorization ](https://arxiv.org/abs/1907.09837)</span>'),\n",
       " Task(id='junction-detection', name='Junction Detection', description=''),\n",
       " Task(id='hand-keypoint-localization', name='Hand Keypoint Localization', description=''),\n",
       " Task(id='3d-rotation-estimation', name='3D Rotation Estimation', description=''),\n",
       " Task(id='3d-absolute-human-pose-estimation', name='3D Absolute Human Pose Estimation', description='This task aims to solve absolute (camera-centric not root-relative) 3D human pose estimation. \\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [RootNet](https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE) )</span>'),\n",
       " Task(id='unsupervised-image-classification', name='Unsupervised Image Classification', description='Models that learn to label each image (i.e. cluster the dataset into its ground truth classes) without seeing the ground truth labels.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\"> Image credit: ImageNet clustering results of [SCAN: Learning to Classify Images without Labels (ECCV 2020)](https://arxiv.org/abs/2005.12320) </span>'),\n",
       " Task(id='fine-grained-visual-recognition', name='Fine-Grained Visual Recognition', description=''),\n",
       " Task(id='object-classification', name='Object Classification', description=''),\n",
       " Task(id='activeness-detection', name='Activeness Detection', description='Determining activeness via images'),\n",
       " Task(id='scene-text-recognition', name='Scene Text Recognition', description='See [Scene Text Detection](https://paperswithcode.com/task/scene-text-detection) for leaderboards in this task.'),\n",
       " Task(id='multi-view-learning', name='MULTI-VIEW LEARNING', description='**Multi-View Learning** is a machine learning framework where data are represented by multiple distinct feature groups, and each feature group is referred to as a particular view.\\n\\n\\n<span class=\"description-source\">Source: [Dissimilarity-based representation for radiomics applications ](https://arxiv.org/abs/1803.04460)</span>'),\n",
       " Task(id='image-retrieval', name='Image Retrieval', description='Image retrieval systems aim to find similar images to a query image among an image dataset.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DELF](https://github.com/tensorflow/models/tree/master/research/delf) )</span>'),\n",
       " Task(id='handwriting-generation', name='Handwriting generation', description='The inverse of handwriting recognition. From text generate and image of handwriting (offline) of trajectory of handwriting (online).'),\n",
       " Task(id='active-observation-completion', name='Active Observation Completion', description=''),\n",
       " Task(id='metamerism', name='Metamerism', description=''),\n",
       " Task(id='monocular-3d-object-detection', name='Monocular 3D Object Detection', description=''),\n",
       " Task(id='multi-object-tracking', name='Multi-Object Tracking', description=''),\n",
       " Task(id='temporal-localization', name='Temporal Localization', description=''),\n",
       " Task(id='mutual-gaze', name='Mutual Gaze', description='Detect if two people are looking at each other'),\n",
       " Task(id='shadow-detection-and-removal', name='Shadow Detection And Removal', description=''),\n",
       " Task(id='rf-based-gesture-recognition', name='RF-based Gesture Recognition', description='RF-based gesture sensing and recognition has increasingly attracted intense academic and industrial interest due to its various device-free applications in daily life, such as elder monitoring, mobile games. State-of-the-art approaches achieved accurate gesture sensing by using fine-grained RF signatures (such as CSI, Doppler effect) while could not achieve the same accuracy with coarse-grained RF signatures such as received signal strength (RSS).\\r\\n\\r\\nSee e.g.\\r\\n\\r\\nProject Soli in depth: How radar-detected gestures could set the Pixel 4 apart\\r\\nAn experimental Google project may finally be ready to make its way into the real world — and the implications could be enormous. https://www.computerworld.com/article/3402019/google-project-soli-pixel-4.html\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Accurate Human Gesture Sensing With\\r\\nCoarse-Grained RF Signatures](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8737967) )</span>'),\n",
       " Task(id='bokeh-effect-rendering', name='Bokeh Effect Rendering', description=''),\n",
       " Task(id='face-clustering', name='Face Clustering', description='Face Clustering in the videos'),\n",
       " Task(id='yield-mapping-in-apple-orchards', name='Yield Mapping In Apple Orchards', description=''),\n",
       " Task(id='pulmonary-arteryvein-classification', name='Pulmonary Artery–Vein Classification', description=''),\n",
       " Task(id='3d-face-reconstruction', name='3D Face Reconstruction', description='3D face reconstruction is the task of reconstructing a face from an image into a 3D form (or mesh).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [3DDFA_V2](https://github.com/cleardusk/3DDFA_V2) )</span>'),\n",
       " Task(id='content-based-image-retrieval', name='Content-Based Image Retrieval', description='**Content-Based Image Retrieval** is a well studied problem in computer vision, with retrieval problems generally divided into two groups: category-level retrieval and instance-level retrieval. Given a query image of the Sydney Harbour bridge, for instance, category-level retrieval aims to find any bridge in a given dataset of images, whilst instance-level retrieval must find the Sydney Harbour bridge to be considered a match.\\n\\n\\n<span class=\"description-source\">Source: [Camera Obscurer: Generative Art for Design Inspiration ](https://arxiv.org/abs/1903.02165)</span>'),\n",
       " Task(id='3d-surface-generation', name='3D Surface Generation', description='Image: [AtlasNet](https://arxiv.org/pdf/1802.05384v3.pdf)'),\n",
       " Task(id='fine-grained-image-classification', name='Fine-Grained Image Classification', description='The Fine-Grained Image Classification task focuses on differentiating between hard-to-distinguish object classes, such as species of birds, flowers, or animals; and identifying the makes or models of vehicles.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Looking for the Devil in the Details](https://arxiv.org/pdf/1903.06150v2.pdf) )</span>'),\n",
       " Task(id='multi-modal-image-segmentation', name='Multi-modal image segmentation', description=''),\n",
       " Task(id='material-classification', name='Material Classification', description=''),\n",
       " Task(id='multi-modal-subspace-clustering', name='Multi-modal Subspace Clustering', description=''),\n",
       " Task(id='window-detection', name='Window Detection', description=''),\n",
       " Task(id='hyperspectral-unmixing', name='Hyperspectral Unmixing', description='**Hyperspectral Unmixing** is a procedure that decomposes the measured pixel spectrum of hyperspectral data into a collection of constituent spectral signatures (or endmembers) and a set of corresponding fractional abundances. Hyperspectral Unmixing techniques have been widely used for a variety of applications, such as mineral mapping and land-cover change detection.\\n\\n\\n<span class=\"description-source\">Source: [An Augmented Linear Mixing Model to Address Spectral Variability for Hyperspectral Unmixing ](https://arxiv.org/abs/1810.12000)</span>'),\n",
       " Task(id='compressive-sensing', name='Compressive Sensing', description='**Compressive Sensing** is a new signal processing framework for efficiently acquiring and reconstructing a signal that have a sparse representation in a fixed linear basis.\\n\\n\\n<span class=\"description-source\">Source: [Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior ](https://arxiv.org/abs/1411.2405)</span>'),\n",
       " Task(id='explainable-artificial-intelligence', name='Explainable artificial intelligence', description='XAI refers to methods and techniques in the application of artificial intelligence (AI) such that the results of the solution can be understood by humans. It contrasts with the concept of the \"black box\" in machine learning where even its designers cannot explain why an AI arrived at a specific decision. XAI may be an implementation of the social right to explanation. XAI is relevant even if there is no legal right or regulatory requirement—for example, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. This way the aim of XAI is to explain what has been done, what is done right now, what will be done next and unveil the information the actions are based on. These characteristics make it possible (i) to confirm existing knowledge (ii) to challenge existing knowledge and (iii) to generate new assumptions.'),\n",
       " Task(id='indoor-monocular-depth-estimation', name='Indoor Monocular Depth Estimation', description=''),\n",
       " Task(id='self-supervised-anomaly-detection', name='Self-Supervised Anomaly Detection', description='Self-Supervision towards anomaly detection'),\n",
       " Task(id='zero-shot-object-detection', name='Zero-Shot Object Detection', description='Zero-shot object detection (ZSD) is the task of object detection where no visual training data is available for some of the target object classes.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts](https://github.com/salman-h-khan/ZSD_Release) )</span>'),\n",
       " Task(id='6d-pose-estimation', name='6D Pose Estimation using RGB', description='6D pose estimation is the task of detecting the 6D pose of an object, which include its location and orientation. This is an important task in robotics, where a robotic arm needs to know the location and orientation to detect and move objects in its vicinity successfully.  This allows the robot to operate safely and effectively alongside humans. The awareness of the position and orientation of objects in a scene is sometimes referred to as 6D, where the D stands for degrees of freedom pose.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Segmentation-driven 6D Object Pose Estimation](https://github.com/cvlab-epfl/segmentation-driven-pose) )</span>'),\n",
       " Task(id='image-classification', name='Image Classification', description='**Image Classification** is a fundamental task that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific label. Typically, Image Classification refers to images in which only one object appears and is analyzed. In contrast, object detection involves both classification and localization tasks, and is used to analyze more realistic cases in which multiple objects may exist in an image.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Metamorphic Testing for Object Detection Systems ](https://arxiv.org/abs/1912.12162)</span>'),\n",
       " Task(id='image-compression', name='Image Compression', description='**Image Compression** is an application of data compression for digital images to lower their storage and/or transmission requirements.\\n\\n\\n<span class=\"description-source\">Source: [Variable Rate Deep Image Compression With a Conditional Autoencoder ](https://arxiv.org/abs/1909.04802)</span>'),\n",
       " Task(id='3d-facial-expression-recognition', name='3D Facial Expression Recognition', description='3D facial expression recognition is the task of modelling facial expressions in 3D from an image or video.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Expression-Net](https://github.com/fengju514/Expression-Net) )</span>'),\n",
       " Task(id='rotated-mnist', name='Rotated MNIST', description=''),\n",
       " Task(id='facial-inpainting', name='Facial Inpainting', description='Facial inpainting (or face completion) is the task of generating plausible facial structures for missing pixels in a face image.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [SymmFCNet](https://github.com/csxmli2016/SymmFCNet) )</span>'),\n",
       " Task(id='facial-action-unit-detection', name='Facial Action Unit Detection', description='Facial action unit detection is the task of detecting action units from a video of a face - for example, lip tightening and cheek raising.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Self-supervised Representation Learning from Videos for Facial Action Unit Detection](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Self-Supervised_Representation_Learning_From_Videos_for_Facial_Action_Unit_Detection_CVPR_2019_paper.pdf) )</span>'),\n",
       " Task(id='weakly-supervised-object-detection', name='Weakly Supervised Object Detection', description='Weakly Supervised Object Detection (WSOD) is the task of training object detectors with only image tag supervisions.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Soft Proposal Networks for Weakly Supervised Object Localization](https://arxiv.org/pdf/1709.01829v1.pdf) )</span>'),\n",
       " Task(id='unsupervised-video-clustering', name='Unsupervised Video Clustering', description=''),\n",
       " Task(id='activity-prediction', name='Activity Prediction', description='Predict human activities in videos'),\n",
       " Task(id='sensor-modeling', name='Sensor Modeling', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving](https://arxiv.org/abs/1905.07290) )</span>'),\n",
       " Task(id='point-cloud-registration', name='Point Cloud Registration', description='**Point Cloud Registration** is a fundamental problem in 3D computer vision and photogrammetry. Given several sets of points in different coordinate systems, the aim of registration is to find the transformation that best aligns all of them into a common coordinate system. Point Cloud Registration plays a significant role in many vision applications such as 3D model reconstruction, cultural heritage management, landslide monitoring and solar energy analysis.\\n\\n\\n<span class=\"description-source\">Source: [Iterative Global Similarity Points : A robust coarse-to-fine integration solution for pairwise 3D point cloud registration ](https://arxiv.org/abs/1808.03899)</span>'),\n",
       " Task(id='activity-recognition', name='Activity Recognition', description='Human **Activity Recognition** is the problem of identifying events performed by humans given a video input. It is formulated as a binary (or multiclass) classification problem of outputting activity class labels. Activity Recognition is an important problem with many societal applications including smart surveillance, video search/retrieval, intelligent robots, and other monitoring systems.\\n\\n\\n<span class=\"description-source\">Source: [Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters ](https://arxiv.org/abs/1605.08140)</span>'),\n",
       " Task(id='document-layout-analysis', name='Document Layout Analysis', description='\"Document Layout Analysis is performed to determine physical structure of a document, that is, to determine document components. These document components can consist of single connected components-regions [...] of\\r\\npixels that are adjacent to form single regions [...] , or group\\r\\nof text lines. A text line is a group of characters, symbols,\\r\\nand words that are adjacent, “relatively close” to each other\\r\\nand through which a straight line can be drawn (usually with\\r\\nhorizontal or vertical orientation).\"  L. O\\'Gorman, \"The document spectrum for page layout analysis,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1162-1173, Nov. 1993.'),\n",
       " Task(id='de-aliasing', name='De-aliasing', description='De-aliasing is the problem of recovering the original high-frequency information that has been aliased during the acquisition of an image.'),\n",
       " Task(id='multiple-affordance-detection', name='Multiple Affordance Detection', description='Affordance detection is the task of detecting objects that are usable (or graspable) by a human.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [What can I do here? Leveraging Deep 3D saliency and geometry for fast and scalable multiple affordance detection](https://github.com/eduard626/deep-interaction-tensor) )</span>'),\n",
       " Task(id='semantic-image-matting', name='Semantic Image Matting', description=''),\n",
       " Task(id='damaged-building-detection', name='Damaged Building Detection', description=''),\n",
       " Task(id='rain-removal', name='Rain Removal', description=''),\n",
       " Task(id='salient-object-detection-1', name='Salient Object Detection', description=''),\n",
       " Task(id='camera-shot-boundary-detection', name='Camera shot boundary detection', description='The objective of camera shot boundary detection is to find the transitions between the camera shots in a video and classify the type of camera transition. This task is introduced in SoccerNet-v2, where 3 types of transitions are considered (abrupt, logo, smooth).'),\n",
       " Task(id='sign-language-recognition', name='Sign Language Recognition', description='Given a signed video input the task is to predict the (sequence of) sign(s) that are performed.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Word-level Deep Sign Language Recognition from Video:\\r\\nA New Large-scale Dataset and Methods Comparison](https://arxiv.org/pdf/1910.11006v1.pdf) )</span>'),\n",
       " Task(id='3d-shape-representation', name='3D Shape Representation', description='Image: [MeshNet](https://arxiv.org/pdf/1811.11424v1.pdf)'),\n",
       " Task(id='vnla', name='VNLA', description='Find objects in photorealistic environments by requesting and executing language subgoals.'),\n",
       " Task(id='real-time-semantic-segmentation', name='Real-Time Semantic Segmentation', description='Real-time semantic segmentation is the task of achieving computationally efficient semantic segmentation (while maintaining a base level of accuracy).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [TorchSeg](https://github.com/ycszen/TorchSeg) )</span>'),\n",
       " Task(id='action-spotting', name='Action Spotting', description=''),\n",
       " Task(id='handwritten-chinese-text-recognition', name='Handwritten Chinese Text Recognition', description='Handwritten Chinese text recognition is the task of interpreting handwritten Chinese input, e.g., from images of documents or scans.'),\n",
       " Task(id='person-re-identification', name='Person Re-Identification', description='Person re-identification is the task of associating images of the same person taken from different cameras or from the same camera in different occasions.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [PRID2011 dataset](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/) )</span>'),\n",
       " Task(id='image-to-image-translation', name='Image-to-Image Translation', description='Image-to-image translation is the task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Unpaired Image-to-Image Translation\\r\\nusing Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593v6.pdf) )</span>'),\n",
       " Task(id='multi-object-colocalization', name='Multi-object colocalization', description=''),\n",
       " Task(id='robust-face-alignment', name='Robust Face Alignment', description='Robust face alignment is the task of face alignment in unconstrained (non-artificial) conditions.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deep Alignment Network](https://github.com/MarekKowalski/DeepAlignmentNetwork) )</span>'),\n",
       " Task(id='micro-expression-spotting', name='Micro-Expression Spotting', description='Facial Micro-Expression Spotting is a challenging task in identifying onset, apex and/or offset over a short or long micro-expression sequence.'),\n",
       " Task(id='unbalanced-segmentation', name='Unbalanced Segmentation', description=''),\n",
       " Task(id='interactive-video-object-segmentation', name='Interactive Video Object Segmentation', description='The interactive scenario assumes the user gives iterative refinement inputs to the algorithm, in our case in the form of a scribble, to segment the objects of interest. Methods have to produce a segmentation mask for that object in all the frames of a video sequence taking into account all the user interactions.'),\n",
       " Task(id='lossy-compression-artifact-reduction', name='Lossy-Compression Artifact Reduction', description=''),\n",
       " Task(id='image-similarity-search', name='Image Similarity Search', description=''),\n",
       " Task(id='human-part-segmentation', name='Human Part Segmentation', description=''),\n",
       " Task(id='image-matting', name='Image Matting', description='**Image Matting** is the process of accurately estimating the foreground object in images and videos. It is a very important technique in image and video editing applications, particularly in film production for creating visual effects. In case of image segmentation, we segment the image into foreground and background by labeling the pixels. Image segmentation generates a binary image, in which a pixel either belongs to foreground or background. However, Image Matting is different from the image segmentation, wherein some pixels may belong to foreground as well as background, such pixels are called partial or mixed pixels. In order to fully separate the foreground from the background in an image, accurate estimation of the alpha values for partial or mixed pixels is necessary.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Automatic Trimap Generation for Image Matting ](https://arxiv.org/abs/1707.00333)</span>\\r\\n\\r\\n<span class=\"description-source\">Image Source: [Real-Time High-Resolution Background Matting](https://arxiv.org/pdf/2012.07810v1.pdf)</span>'),\n",
       " Task(id='learning-to-paint', name='Learning to Paint', description=''),\n",
       " Task(id='human-instance-segmentation', name='Human Instance Segmentation', description='Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image.\\r\\n\\r\\nImage Credit: [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers](https://arxiv.org/abs/2103.12340)'),\n",
       " Task(id='unsupervised-person-re-identification', name='Unsupervised Person Re-Identification', description=''),\n",
       " Task(id='panel-extraction', name='panel extraction', description=''),\n",
       " Task(id='3d-multi-person-pose-estimation-absolute', name='3D Multi-Person Pose Estimation (absolute)', description='This task aims to solve absolute 3D multi-person pose Estimation (camera-centric coordinates). No ground truth human bounding box and human root joint coordinates are used during testing stage.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [RootNet](https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE) )</span>'),\n",
       " Task(id='action-recognition-in-videos', name='Action Recognition', description='Please note some benchmarks may be located in the [Action Classification](https://paperswithcode.com/task/action-classification) or [Video Classification](https://paperswithcode.com/task/video-classification) tasks, e.g. Kinetics-400.'),\n",
       " Task(id='image-generation', name='Image Generation', description='Image generation (synthesis) is the task of generating new images from an existing dataset.\\r\\n\\r\\n- **Unconditional generation** refers to generating samples unconditionally from the dataset, i.e. $p(y)$\\r\\n- **Conditional image generation** (subtask) refers to generating samples conditionally from the dataset, based on a label, i.e. $p(y|x)$.\\r\\n\\r\\nIn this section, you can find state-of-the-art leaderboards for **unconditional generation**. For conditional  generation, and other types of image generations, refer to the subtasks.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [StyleGAN](https://github.com/NVlabs/stylegan) )</span>'),\n",
       " Task(id='lip-sync', name='Unconstrained Lip-synchronization', description='Given a video of an arbitrary person, and an arbitrary driving speech, the task is to generate a lip-synced video that matches the given speech. \\r\\n\\r\\nThis task requires the approach to not be constrained by identity, voice, or language.'),\n",
       " Task(id='cryogenic-electron-microscopy-cryo-em', name='Cryogenic Electron Microscopy (cryo-EM)', description='Analysis of images and videos from transmission electron microscopes, including single-particle cryogenic electron microscopy and cryogenic electron tomography (cryo-ET).\\r\\nhttps://en.wikipedia.org/wiki/Cryogenic_electron_microscopy'),\n",
       " Task(id='sar-image-despeckling', name='Sar Image Despeckling', description=''),\n",
       " Task(id='lane-detection', name='Lane Detection', description='Lane detection is the task of detecting lanes on a road from a camera.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [End-to-end Lane Detection\\r\\n](https://github.com/wvangansbeke/LaneDetection_End2End) )</span>'),\n",
       " Task(id='multi-target-domain-adaptation', name='Multi-target Domain Adaptation', description='The idea of Multi-target Domain Adaptation is to adapt a model from a single labelled source domain to multiple unlabelled target domains.'),\n",
       " Task(id='image-animation', name='Image Animation', description=''),\n",
       " Task(id='zero-shot-skeletal-action-recognition', name='Zero Shot Skeletal Action Recognition', description='Zero-Shot Learning for 3D skeletal action recognition'),\n",
       " Task(id='face-detection', name='Face Detection', description='Face detection is the task of detecting faces in a photo or video (and distinguishing them from other objects).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [insightface](https://github.com/deepinsight/insightface) )</span>'),\n",
       " Task(id='visual-object-tracking', name='Visual Object Tracking', description='**Visual Object Tracking** is an important research topic in computer vision, image understanding and pattern recognition. Given the initial state (centre location and scale) of a target in the first frame of a video sequence, the aim of Visual Object Tracking is to automatically obtain the states of the object in the subsequent video frames.\\n\\n\\n<span class=\"description-source\">Source: [Learning Adaptive Discriminative Correlation Filters via Temporal Consistency Preserving Spatial Feature Selection for Robust Visual Object Tracking ](https://arxiv.org/abs/1807.11348)</span>'),\n",
       " Task(id='vision-language-navigation', name='Vision-Language Navigation', description='Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Learning to Navigate Unseen Environments:\\r\\nBack Translation with Environmental Dropout](https://arxiv.org/pdf/1904.04195v1.pdf) )</span>'),\n",
       " Task(id='spatio-temporal-action-localization', name='Spatio-Temporal Action Localization', description=''),\n",
       " Task(id='3d-shape-modeling', name='3D Shape Modeling', description='Image: [Gkioxari et al](https://arxiv.org/pdf/1906.02739v2.pdf)'),\n",
       " Task(id='optical-flow-estimation', name='Optical Flow Estimation', description='**Optical Flow Estimation** is the problem of finding pixel-wise motions between consecutive images.\\n\\n\\n<span class=\"description-source\">Source: [Devon: Deformable Volume Network for Learning Optical Flow ](https://arxiv.org/abs/1802.07351)</span>'),\n",
       " Task(id='texture-synthesis', name='Texture Synthesis', description='The fundamental goal of example-based **Texture Synthesis** is to generate a texture, usually larger than the input, that faithfully captures all the visual characteristics of the exemplar, yet is neither identical to it, nor exhibits obvious unnatural looking artifacts.\\n\\n\\n<span class=\"description-source\">Source: [Non-Stationary Texture Synthesis by Adversarial Expansion ](https://arxiv.org/abs/1805.04487)</span>'),\n",
       " Task(id='semi-supervised-image-classification', name='Semi-Supervised Image Classification', description='Semi-supervised image classification leverages unlabelled data as well as labelled data to increase classification performance.\\r\\n\\r\\nYou may want to read some blog posts to get an overview before reading the papers and checking the leaderboards:\\r\\n\\r\\n- [An overview of proxy-label approaches for semi-supervised learning](https://ruder.io/semi-supervised/) - Sebastian Ruder\\r\\n- [Semi-Supervised Learning in Computer Vision](https://amitness.com/2020/07/semi-supervised-learning/) - Amit Chaudhary\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Self-Supervised Semi-Supervised Learning](https://arxiv.org/pdf/1905.03670v2.pdf) )</span>'),\n",
       " Task(id='object-recognition', name='Object Recognition', description='Object recognition is a computer vision technique for detecting + classifying objects in images or videos. Since this is a combined task of object detection plus image classification, the state-of-the-art tables are recorded for each component task [here](https://www.paperswithcode.com/task/object-detection) and  [here](https://www.paperswithcode.com/task/image-classification2).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Tensorflow Object Detection API\\r\\n](https://github.com/tensorflow/models/tree/master/research/object_detection) )</span>'),\n",
       " Task(id='image-relighting', name='Image Relighting', description='Image relighting involves changing the illumination settings of an image.'),\n",
       " Task(id='light-source-estimation', name='Outdoor Light Source Estimation', description=''),\n",
       " Task(id='saliency-prediction', name='Saliency Prediction', description='A saliency map is a model that predicts eye fixations on a visual scene.'),\n",
       " Task(id='object-tracking', name='Object Tracking', description='Object tracking is the task of taking an initial set of object detections, creating a unique ID for each of the initial detections, and then tracking each of the objects as they move around frames in a video, maintaining the ID assignment.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Towards-Realtime-MOT\\r\\n](https://github.com/Zhongdao/Towards-Realtime-MOT) )</span>'),\n",
       " Task(id='motion-estimation', name='Motion Estimation', description='**Motion Estimation** is used to determine the block-wise or pixel-wise motion vectors between two frames.\\n\\n\\n<span class=\"description-source\">Source: [MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement ](https://arxiv.org/abs/1810.08768)</span>'),\n",
       " Task(id='mental-workload-estimation', name='Mental Workload Estimation', description=''),\n",
       " Task(id='lightfield', name='Lightfield', description='Tasks related to the light-field imagery'),\n",
       " Task(id='generalized-zero-shot-skeletal-action', name='Generalized Zero Shot skeletal action recognition', description='Generalized Zero Shot Learning for 3d Skeletal Action Recognition'),\n",
       " Task(id='caricature', name='Caricature', description='**Caricature** is a pictorial representation or description that deliberately exaggerates a person’s distinctive features or peculiarities to create an easily identifiable visual likeness with a comic effect. This vivid art form contains the concepts of abstraction, simplification and exaggeration.\\n\\n\\n<span class=\"description-source\">Source: [Alive Caricature from 2D to 3D ](https://arxiv.org/abs/1803.06802)</span>'),\n",
       " Task(id='replay-grounding', name='Replay Grounding', description='Replay grounding is introduced in SoccerNet-v2 in the case of videos of soccer games. Given a replay shot of a soccer action, the objective is to retrieve when said action occurs within the whole live game.'),\n",
       " Task(id='stereo-matching-1', name='Stereo Matching', description='**Stereo Matching** is one of the core technologies in computer vision, which recovers 3D structures of real world from 2D images. It has been widely used in areas such as autonomous driving, augmented reality and robotics navigation. Given a pair of rectified stereo images, the goal of Stereo Matching is to compute the disparity for each pixel in the reference image, where disparity is defined as the horizontal displacement between a pair of corresponding pixels in the left and right images.\\n\\n\\n<span class=\"description-source\">Source: [Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching ](https://arxiv.org/abs/1909.03751)</span>'),\n",
       " Task(id='fake-image-detection', name='Fake Image Detection', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [FaceForensics++](https://github.com/ondyari/FaceForensics) )</span>'),\n",
       " Task(id='pose-tracking', name='Pose Tracking', description='**Pose Tracking** is the task of estimating multi-person human poses in videos and assigning unique instance IDs for each keypoint across frames. Accurate estimation of human keypoint-trajectories is useful for human action recognition, human interaction understanding, motion capture and animation.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking ](https://arxiv.org/abs/1905.02822)</span>'),\n",
       " Task(id='blind-image-deblurring', name='Blind Image Deblurring', description='**Blind Image Deblurring** is a classical problem in image processing and computer vision, which aims to recover a latent image from a blurred input.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Learning a Discriminative Prior for Blind Image Deblurring ](https://arxiv.org/abs/1803.03363)</span>'),\n",
       " Task(id='ensemble-learning', name='ensemble learning', description=''),\n",
       " Task(id='audio-visual-synchronization', name='Audio-Visual Synchronization', description=''),\n",
       " Task(id='generalized-zero-shot-learning-unseen', name='Generalized Zero-Shot Learning - Unseen', description='The average of the normalized top-1 prediction scores of unseen classes in the generalized zero-shot learning setting, where the label of a test sample is predicted among all (seen + unseen) classes.'),\n",
       " Task(id='automatic-post-editing', name='Automatic Post-Editing', description=''),\n",
       " Task(id='one-shot-visual-object-segmentation', name='Youtube-VOS', description=''),\n",
       " Task(id='facial-landmark-detection', name='Facial Landmark Detection', description='Facial landmark detection is the task of detecting key landmarks on the face and tracking them (being robust to rigid and non-rigid facial deformations due to head movements and facial expressions).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Style Aggregated Network for Facial Landmark Detection](https://arxiv.org/pdf/1803.04108v4.pdf) )</span>'),\n",
       " Task(id='abnormal-event-detection-in-video', name='Abnormal Event Detection In Video', description='**Abnormal Event Detection In Video** is a challenging task in computer vision, as the definition of what an abnormal event looks like depends very much on the context. For instance, a car driving by on the street is regarded as a normal event, but if the car enters a pedestrian area, this is regarded as an abnormal event. A person running on a sports court (normal event) versus running outside from a bank (abnormal event) is another example. Although what is considered abnormal depends on the context, we can generally agree that abnormal events should be unexpected events that occur less often than familiar (normal) events\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Unmasking the abnormal events in video ](https://arxiv.org/abs/1705.08182)</span>\\r\\n\\r\\nImage: [Ravanbakhsh et al](https://arxiv.org/pdf/1708.09644v1.pdf)'),\n",
       " Task(id='3d-semantic-instance-segmentation', name='3D Semantic Instance Segmentation', description='Image: [3D-SIS](https://github.com/Sekunde/3D-SIS)'),\n",
       " Task(id='concurrent-activity-recognition', name='Concurrent Activity Recognition', description=''),\n",
       " Task(id='multi-person-pose-estimation-and-tracking', name='Multi-Person Pose Estimation and Tracking', description='Joint multi-person pose estimation and tracking following the PoseTrack benchmark. \\r\\nhttps://posetrack.net/\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [PoseTrack](https://github.com/iqbalu/PoseTrack-CVPR2017) )</span>'),\n",
       " Task(id='video-object-segmentation', name='Video Object Segmentation', description='Video object segmentation is a binary labeling problem aiming to separate foreground object(s) from the background region of a video.\\r\\n\\r\\nFor leaderboards please refer to the different subtasks.'),\n",
       " Task(id='video-retrieval', name='Video Retrieval', description='The objective of video retrieval is as follows: given a text query and a pool of candidate videos, select the video which corresponds to the text query.  Typically, the videos are returned as a ranked list of candidates and scored via document retrieval metrics.'),\n",
       " Task(id='video-inpainting', name='Video Inpainting', description='The goal of **Video Inpainting** is to fill in missing regions of a given video sequence with contents that are both spatially and temporally coherent. Video Inpainting, also known as video completion, has many real-world applications such as undesired object removal and video restoration.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Deep Flow-Guided Video Inpainting ](https://arxiv.org/abs/1905.02884)</span>'),\n",
       " Task(id='interest-point-detection', name='Interest Point Detection', description=''),\n",
       " Task(id='medical-image-retrieval', name='Medical Image Retrieval', description=''),\n",
       " Task(id='intrinsic-image-decomposition', name='Intrinsic Image Decomposition', description='**Intrinsic Image Decomposition** is the process of separating an image into its formation components such as reflectance (albedo) and shading (illumination). Reflectance is the color of the object, invariant to camera viewpoint and illumination conditions, whereas shading, dependent on camera viewpoint and object geometry, consists of different illumination effects, such as shadows, shading and inter-reflections. Using intrinsic images, instead of the original images, can be beneficial for many computer vision algorithms. For instance, for shape-from-shading algorithms, the shading images contain important visual cues to recover geometry, while for segmentation and detection algorithms, reflectance images can be beneficial as they are independent of confounding illumination effects. Furthermore, intrinsic images are used in a wide range of computational photography applications, such as material recoloring, relighting, retexturing and stylization.\\n\\n\\n<span class=\"description-source\">Source: [CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition ](https://arxiv.org/abs/1712.01056)</span>'),\n",
       " Task(id='visual-localization', name='Visual Localization', description='**Visual Localization** is the problem of estimating the camera pose of a given image relative to a visual representation of a known scene.\\n\\n\\n<span class=\"description-source\">Source: [Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization ](https://arxiv.org/abs/1908.06387)</span>'),\n",
       " Task(id='multimodal-activity-recognition', name='Multimodal Activity Recognition', description=''),\n",
       " Task(id='3d-semantic-segmentation', name='3D Semantic Segmentation', description=''),\n",
       " Task(id='video-compression', name='Video Compression', description='**Video Compression** is a process of reducing the size of an image or video file by exploiting spatial and temporal redundancies within an image or video frame and across multiple video frames. The ultimate goal of a successful Video Compression system is to reduce data volume while retaining the perceptual quality of the decompressed data.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Adversarial Video Compression Guided by Soft Edge Detection ](https://arxiv.org/abs/1811.10673)</span>'),\n",
       " Task(id='shadow-detection', name='Shadow Detection', description=''),\n",
       " Task(id='action-recognition-in-still-images', name='Action Recognition In Still Images', description=''),\n",
       " Task(id='handwriting-recognition', name='Handwriting Recognition', description=''),\n",
       " Task(id='few-shot-camera-adaptive-color-constancy', name='Few-Shot Camera-Adaptive Color Constancy', description=''),\n",
       " Task(id='point-cloud-classification', name='Point Cloud Classification', description='Point Cloud Classification is a task involving the classification of unordered 3D point sets (point clouds).'),\n",
       " Task(id='face-recognition', name='Face Recognition', description='Facial recognition is the task of making a positive identification of a face in a photo or video image against a pre-existing database of faces. It begins with detection - distinguishing human faces from other objects in the image - and then works on identification of those detected faces.\\r\\n\\r\\nThe state of the art tables for this task are contained mainly in the consistent parts of the task : the face verification and face identification tasks.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Face Verification](https://shuftipro.com/face-verification) )</span>'),\n",
       " Task(id='drivable-area-detection', name='Drivable Area Detection', description=''),\n",
       " Task(id='open-world-object-detection', name='Open World Object Detection', description=\"Open World Object Detection is a computer vision problem where a model is tasked to: 1) identify objects that have not been introduced to it as `unknown', without explicit supervision to do so, and 2) incrementally learn these identified unknown categories without forgetting previously learned classes, when the corresponding labels are progressively received.\"),\n",
       " Task(id='self-supervised-image-classification', name='Self-Supervised Image Classification', description='This is the task of image classification using representations learnt with self-supervised learning. Self-supervised methods generally involve a pretext task that is solved to learn a good representation and a loss function to learn with. One example of a loss function is an autoencoder based loss where the goal is reconstruction of an image pixel-by-pixel. A more popular recent example is a contrastive loss, which measure the similarity of sample pairs in a representation space, and where there can be a varying target instead of a fixed target to reconstruct (as in the case of autoencoders).\\r\\n\\r\\nA common evaluation protocol is to train a linear classifier on top of (frozen) representations learnt by self-supervised methods. The leaderboards for the linear evaluation protocol can be found below. In practice, it is more common to fine-tune features on a downstream task. An alternative evaluation protocol therefore uses semi-supervised learning and finetunes on a % of the labels. The leaderboards for the finetuning protocol can be accessed [here](https://paperswithcode.com/task/semi-supervised-image-classification).\\r\\n\\r\\nYou may want to read some blog posts before reading the papers and checking the leaderboards:\\r\\n\\r\\n- [Contrastive Self-Supervised Learning](https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html) - Ankesh Anand\\r\\n- [The Illustrated Self-Supervised Learning](https://amitness.com/2020/02/illustrated-self-supervised-learning/) - Amit Chaudhary\\r\\n- [Self-supervised learning and computer vision](https://www.fast.ai/2020/01/13/self_supervised/) - Jeremy Howard\\r\\n- [Self-Supervised Representation Learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) - Lilian Weng\\r\\n\\r\\nThere is also Yann LeCun\\'s talk at AAAI-20 which you can watch [here](https://vimeo.com/390347111) (35:00+).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709v1.pdf) )</span>'),\n",
       " Task(id='text-spotting', name='Text Spotting', description=''),\n",
       " Task(id='low-light-image-enhancement', name='Low-Light Image Enhancement', description=''),\n",
       " Task(id='3d-object-retrieval', name='3D Object Retrieval', description='Source: [He et al](https://arxiv.org/pdf/1803.06189v1.pdf)'),\n",
       " Task(id='scanpath-prediction', name='Scanpath prediction', description='Learning to Predict Sequences of Human Fixations.'),\n",
       " Task(id='image-registration', name='Image Registration', description='**Image Registration** is a key component for multimodal image fusion, which generally refers to the process by which two or more image volumes and their corresponding features (acquired from different sensors, points of view, imaging modalities, etc.) are aligned into the same coordinate space. Medical images that are acquired from different imaging modalities use different imaging physics, which creates unique advantages and disadvantages. Relatively unique information about the imaged volume is provided by each modality. Image fusion through registration can integrate the complementary information from multimodal images to help achieve more accurate diagnosis and treatment.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Learning Deep Similarity Metric for 3D MR-TRUS Registration ](https://arxiv.org/abs/1806.04548)</span>'),\n",
       " Task(id='omniglot', name='Omniglot', description=''),\n",
       " Task(id='photo-geolocation-estimation', name='Photo geolocation estimation', description=''),\n",
       " Task(id='image-recognition', name='Image Recognition', description=''),\n",
       " Task(id='color-image-denoising', name='Color Image Denoising', description=''),\n",
       " Task(id='video-compressive-sensing', name='Video Compressive Sensing', description=''),\n",
       " Task(id='hybrid-positioning', name='Hybrid Positioning', description='Hybrid Positioning using CV and dead reckoning'),\n",
       " Task(id='3d-plane-detection', name='3D Plane Detection', description='Image: [Liu et al](https://arxiv.org/pdf/1812.04072v2.pdf)'),\n",
       " Task(id='self-supervised-action-recognition', name='Self-Supervised Action Recognition', description=''),\n",
       " Task(id='gaze-redirection', name='gaze redirection', description=''),\n",
       " Task(id='face-reenactment', name='Face Reenactment', description='**Face Reenactment** is an emerging conditional face synthesis task that aims at fulfilling two goals simultaneously: 1) transfer a source face shape to a target face; while 2) preserve the appearance and the identity of the target face.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [One-shot Face Reenactment ](https://arxiv.org/abs/1908.03251)</span>'),\n",
       " Task(id='detect-forged-images-and-videos', name='Detect Forged Images And Videos', description=''),\n",
       " Task(id='learning-with-noisy-labels', name='Learning with noisy labels', description='Learning with noisy labels'),\n",
       " Task(id='unet-quantization', name='UNET Quantization', description=''),\n",
       " Task(id='human-pose-forecasting', name='Human Pose Forecasting', description='Human pose forecasting is the task of detecting and predicting future human poses.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [EgoPose](https://github.com/Khrylx/EgoPose) )</span>'),\n",
       " Task(id='one-shot-3d-action-recognition', name='One-Shot 3D Action Recognition', description=''),\n",
       " Task(id='motion-segmentation', name='Motion Segmentation', description='**Motion Segmentation** is an essential task in many applications in Computer Vision and Robotics, such as surveillance, action recognition and scene understanding. The classic way to state the problem is the following: given a set of feature points that are tracked through a sequence of images, the goal is to cluster those trajectories according to the different motions they belong to. It is assumed that the scene contains multiple objects that are moving rigidly and independently in 3D-space.\\n\\n\\n<span class=\"description-source\">Source: [Robust Motion Segmentation from Pairwise Matches ](https://arxiv.org/abs/1905.09043)</span>'),\n",
       " Task(id='edge-detection', name='Edge Detection', description='**Edge Detection** is a fundamental image processing technique which involves computing an image gradient to quantify the magnitude and direction of edges in an image. Image gradients are used in various downstream tasks in computer vision such as line detection, feature detection, and image classification.\\n\\n\\n<span class=\"description-source\">Source: [Artistic Enhancement and Style Transfer of Image Edges using Directional Pseudo-coloring ](https://arxiv.org/abs/1906.07981)</span>'),\n",
       " Task(id='sensor-fusion', name='Sensor Fusion', description='**Sensor Fusion** is the broad category of combining various on-board sensors to produce better measurement estimates. These sensors are combined to compliment each other and overcome individual shortcomings.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements ](https://arxiv.org/abs/1809.07677)</span>'),\n",
       " Task(id='3d-object-recognition', name='3D Object Recognition', description='3D object recognition is the task of recognising objects from 3D data.\\r\\n\\r\\nNote that there are related tasks you can look at, such as [3D Object Detection](https://paperswithcode.com/task/3d-object-detection) which have more leaderboards.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Look Further to Recognize Better](https://arxiv.org/pdf/1907.12924v1.pdf) )</span>'),\n",
       " Task(id='skeleton-based-action-recognition', name='Skeleton Based Action Recognition', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [View Adaptive Neural Networks for High\\r\\nPerformance Skeleton-based Human Action\\r\\nRecognition](https://arxiv.org/pdf/1804.07453v3.pdf) )</span>'),\n",
       " Task(id='street-scene-parsing', name='Street Scene Parsing', description=''),\n",
       " Task(id='line-segment-detection', name='Line Segment Detection', description=''),\n",
       " Task(id='group-detection-in-crowds', name='Group Detection In Crowds', description=''),\n",
       " Task(id='im2spec', name='Im2Spec', description='Predicting spectra from images (and vice versa)'),\n",
       " Task(id='single-image-haze-removal', name='Single Image Haze Removal', description=''),\n",
       " Task(id='lake-detection', name='Lake Detection', description=''),\n",
       " Task(id='natural-image-orientation-angle-detection', name='Natural Image Orientation Angle Detection', description='Image orientation angle detection is a pretty challenging task for a machine because the machine has to learn the features of an image in such a way so that it can detect the arbitrary angle by which the image is rotated. Though there are some modern cameras with features involving inertial sensors that can correct image orientation in steps of 90 degrees, those features are seldom used. In this paper, we propose a method to detect the orientation angle of a digitally captured image where the image may have been captured by a camera at a tilted angle (between 0\\\\degree to 359\\\\degree).'),\n",
       " Task(id='semantic-correspondence', name='Semantic correspondence', description='The task of semantic correspondence aims to establish reliable visual correspondence between different instances of the same object category.'),\n",
       " Task(id='drone-navigation', name='Drone navigation', description='(Satellite -> Drone) Given one satellite-view image, the drone intends to find the most relevant place (drone-view images) that it has passed by. According to its flight history, the drone could be navigated back to the target place.'),\n",
       " Task(id='3d-object-tracking', name='3D Object Tracking', description=''),\n",
       " Task(id='deception-detection-in-videos', name='Deception Detection In Videos', description=''),\n",
       " Task(id='simultaneous-localization-and-mapping', name='Simultaneous Localization and Mapping', description='Simultaneous localization and mapping (SLAM) is the task of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent\\'s location within it.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [ORB-SLAM2](https://arxiv.org/pdf/1610.06475v2.pdf) )</span>'),\n",
       " Task(id='facial-beauty-prediction', name='Facial Beauty Prediction', description='Facial beauty prediction is the task of predicting the attractiveness of a face.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [SCUT-FBP5500: A Diverse Benchmark Dataset for Multi-Paradigm Facial Beauty Prediction](https://github.com/HCIILAB/SCUT-FBP5500-Database-Release) )</span>'),\n",
       " Task(id='logo-recognition', name='Logo Recognition', description=''),\n",
       " Task(id='image-declipping', name='Image Declipping', description=''),\n",
       " Task(id='3d-room-layouts-from-a-single-rgb-panorama', name='3D Room Layouts From A Single RGB Panorama', description='Image: [Zou et al](https://arxiv.org/pdf/1803.08999v1.pdf)'),\n",
       " Task(id='few-shot-object-detection', name='Few-Shot Object Detection', description='Target: To detect objects of novel categories with just a few training samples.'),\n",
       " Task(id='materials-imaging', name='Materials Imaging', description=''),\n",
       " Task(id='image-enhancement', name='Image Enhancement', description='**Image Enhancement** is basically improving the interpretability or perception of information in images for human viewers and providing ‘better’ input for other automated image processing techniques. The principal objective of Image Enhancement is to modify attributes of an image to make it more suitable for a given task and a specific observer.\\n\\n\\n<span class=\"description-source\">Source: [A Comprehensive Review of Image Enhancement Techniques ](https://arxiv.org/abs/1003.4053)</span>'),\n",
       " Task(id='face-swapping', name='Face Swapping', description='Face swapping refers to the task of swapping faces between images or in an video, while maintaining the rest of the body and environment context.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Swapped Face Detection using Deep Learning and Subjective Assessment](https://arxiv.org/pdf/1909.04217v1.pdf) )</span>'),\n",
       " Task(id='video-salient-object-detection', name='Video Salient Object Detection', description='Video salient object detection (VSOD) is significantly essential for understanding the underlying mechanism behind HVS during free-viewing in general and instrumental to a wide range of real-world applications, e.g., video segmentation, video captioning, video compression, autonomous driving, robotic interaction, weakly supervised attention. Besides its academic value and practical significance, VSOD presents great difficulties due to the challenges carried by video data (diverse motion patterns, occlusions, blur, large object deformations, etc.) and the inherent complexity of human visual attention behavior (i.e., selective attention allocation, attention shift) during dynamic scenes. Online benchmark: http://dpfan.net/davsod.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Shifting More Attention to Video Salient Object Detection, CVPR2019-Best Paper Finalist](https://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.pdf) )</span>'),\n",
       " Task(id='visual-recognition', name='Visual Recognition', description=''),\n",
       " Task(id='6d-pose-estimation-using-rgbd', name='6D Pose Estimation using RGBD', description='Image: [Zeng et al](https://arxiv.org/pdf/1609.09475v3.pdf)'),\n",
       " Task(id='fast-vehicle-detection', name='Fast Vehicle Detection', description='Fast vehicle detection is the task of detecting fast or speeding vehicles from video footage.'),\n",
       " Task(id='text-to-image-generation', name='Text-to-Image Generation', description='<span style=\"color:grey; opacity: 0.4\">( Image credit: [StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1710.10916v3.pdf) )</span>'),\n",
       " Task(id='event-based-vision', name='Event-based vision', description=''),\n",
       " Task(id='surgical-tool-detection', name='Surgical tool detection', description='Presence detection of various classes of surgical instruments in endoscopy videos.'),\n",
       " Task(id='gan-inversion', name='GAN inversion', description='Finding the latent code in the GAN latent space corresponding to a natural image.'),\n",
       " Task(id='image-to-gps-verification', name='Image-To-Gps Verification', description='The image-to-GPS verification task asks whether a given image is taken at a claimed GPS location.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network](https://arxiv.org/pdf/1811.07288v1.pdf) )</span>'),\n",
       " Task(id='video-story-qa', name='Video Story QA', description='MCQ about clips from movies/tvshows/etc'),\n",
       " Task(id='demosaicking', name='Demosaicking', description='Most modern digital cameras acquire color images by measuring only one color channel per pixel, red, green, or blue, according to a specific pattern called the Bayer pattern. **Demosaicking** is the processing step that reconstruct a full color image given these incomplete measurements.\\n\\n\\n<span class=\"description-source\">Source: [Revisiting Non Local Sparse Models for Image Restoration ](https://arxiv.org/abs/1912.02456)</span>'),\n",
       " Task(id='human-parsing', name='Human Parsing', description='Human parsing is the task of segmenting a human image into different fine-grained semantic parts such as head, torso, arms and legs.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-Human-Parsing (MHP)\\r\\n](https://github.com/ZhaoJ9014/Multi-Human-Parsing) )</span>'),\n",
       " Task(id='cross-view-person-re-identification', name='Cross-Modal  Person Re-Identification', description=''),\n",
       " Task(id='face-reconstruction', name='Face Reconstruction', description='Face reconstruction is the task of recovering the facial geometry of a face from an image.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: Microsoft [Deep3DFaceReconstruction](https://github.com/Microsoft/Deep3DFaceReconstruction) )</span>'),\n",
       " Task(id='thermal-infrared-object-tracking', name='Thermal Infrared Object Tracking', description=''),\n",
       " Task(id='object-detection', name='Object Detection', description='Object detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO, SSD and RetinaNet. Two-stage methods prioritize detection accuracy, and example models include Faster R-CNN, Mask R-CNN and Cascade R-CNN.\\r\\n\\r\\nThe most popular benchmark is the MSCOCO dataset. Models are typically evaluated according to a Mean Average Precision metric.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Detectron](https://github.com/facebookresearch/detectron) )</span>'),\n",
       " Task(id='hand-segmentation', name='Hand Segmentation', description=''),\n",
       " Task(id='head-pose-estimation', name='Head Pose Estimation', description='Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose\\r\\nEstimation from a Single Image](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_FSA-Net_Learning_Fine-Grained_Structure_Aggregation_for_Head_Pose_Estimation_From_CVPR_2019_paper.pdf) )</span>'),\n",
       " Task(id='video-text-retrieval', name='Video-Text Retrieval', description=\"Video-Text retrieval requires understanding of both video and language together. Therefore it's different to video retrieval task.\"),\n",
       " Task(id='computational-manga', name='computational manga', description=''),\n",
       " Task(id='3d-shape-recognition', name='3D Shape Recognition', description='Image: [Wei et al](https://arxiv.org/pdf/1908.10098v1.pdf)'),\n",
       " Task(id='weakly-supervised-action-localization', name='Weakly Supervised Action Localization', description='In this task, the training data consists of videos with a list of activities in them without any temporal boundary annotations. However, while testing, given a video, the algorithm should recognize the activities in the video and also provide the start and end time.'),\n",
       " Task(id='natural-language-moment-retrieval', name='Natural Language Moment Retrieval', description=''),\n",
       " Task(id='continuous-object-recognition', name='Continuous Object Recognition', description='Continuous object recognition is the task of performing object recognition on a data stream and learning continuously, trying to mitigate issues such as catastrophic forgetting.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [CORe50 dataset](https://vlomonaco.github.io/core50/) )</span>'),\n",
       " Task(id='autonomous-navigation', name='Autonomous Navigation', description='Autonomous navigation is the task of autonomously navigating a vehicle or robot to or around a location without human guidance.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Approximate LSTMs for Time-Constrained Inference:\\r\\nEnabling Fast Reaction in Self-Driving Cars](https://arxiv.org/pdf/1905.00689v2.pdf) )</span>'),\n",
       " Task(id='semi-supervised-anomaly-detection', name='Semi-supervised Anomaly Detection', description=''),\n",
       " Task(id='human-fmri-response-prediction', name='Human fMRI response prediction', description='The task is: Given a) the set of videos of everyday events and b) the corresponding brain responses recorded while human participants viewed those videos, use computational models to predict brain responses for videos.'),\n",
       " Task(id='monocular-3d-object-localization', name='Monocular 3D Object Localization', description=''),\n",
       " Task(id='visual-text-correction', name='Visual Text Correction', description=''),\n",
       " Task(id='autonomous-driving', name='Autonomous Driving', description='Autonomous driving is the task of driving a vehicle without human conduction. \\r\\n\\r\\nMany of the state-of-the-art results can be found at more general task pages such as [3D Object Detection](https://paperswithcode.com/task/3d-object-detection) and [Semantic Segmentation](https://paperswithcode.com/task/semantic-segmentation).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Exploring the Limitations of Behavior Cloning for Autonomous Driving](https://arxiv.org/pdf/1904.08980v1.pdf) )</span>'),\n",
       " Task(id='scene-aware-dialogue', name='Scene-Aware Dialogue', description=''),\n",
       " Task(id='image-matching', name='Image Matching', description=''),\n",
       " Task(id='multi-view-subspace-clustering', name='Multi-view Subspace Clustering', description=''),\n",
       " Task(id='scene-text-editing', name='Scene Text Editing', description=''),\n",
       " Task(id='cartoon-to-real-translation', name='Cartoon-To-Real Translation', description='Cartoon-to-real translation is the task of adapting cartoons to look like realistic photos.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Cartoon-to-real: An Approach to Translate Cartoon to Realistic Images using GAN](https://arxiv.org/pdf/1811.11796v3.pdf) )</span>'),\n",
       " Task(id='event-data-classification', name='Event data classification', description=''),\n",
       " Task(id='camouflaged-object-segmentation', name='Camouflaged Object Segmentation', description='Camouflaged object segmentation (COS) or Camouflaged object detection (COD), which was originally promoted by [T.-N. Le et al.](https://www.sciencedirect.com/science/article/abs/pii/S1077314219300608) (2017), aims to identify objects that conceal their texture into the surrounding environment. The high intrinsic similarities between the target object and the background make COS/COD far more challenging than the traditional object segmentation task. Also, refer to the online benchmarks on [CAMO dataset](https://sites.google.com/view/ltnghia/research/camo), [COD dataset](http://dpfan.net/Camouflage/), and [online demo](http://mc.nankai.edu.cn/cod).\\r\\n\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image source: [Anabranch Network for Camouflaged Object Segmentation](https://www.sciencedirect.com/science/article/abs/pii/S1077314219300608) )</span>'),\n",
       " Task(id='unsupervised-semantic-segmentation', name='Unsupervised Semantic Segmentation', description='Models that learn to segment each image (i.e. cluster the pixels into their ground truth classes) without seeing the ground truth labels.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [SegSort: Segmentation by Discriminative Sorting of Segments](http://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.pdf) )</span>'),\n",
       " Task(id='traffic-sign-recognition', name='Traffic Sign Recognition', description='Traffic sign recognition is the task of recognising traffic signs in an image or video.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Novel Deep Learning Model for Traffic Sign Detection Using Capsule\\r\\nNetworks ](https://arxiv.org/pdf/1805.04424v1.pdf) )</span>'),\n",
       " Task(id='generating-3d-point-clouds', name='Generating 3D Point Clouds', description=''),\n",
       " Task(id='multi-object-tracking-and-segmentation', name='Multi-Object Tracking and Segmentation', description=''),\n",
       " Task(id='factual-visual-question-answering', name='Factual Visual Question Answering', description=''),\n",
       " Task(id='iris-recognition', name='Iris Recognition', description=''),\n",
       " Task(id='robust-face-recognition', name='Robust Face Recognition', description='Robust face recognition is the task of performing recognition in an unconstrained environment, where there is variation of view-point, scale, pose, illumination and expression of the face images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [MeGlass dataset](https://github.com/cleardusk/MeGlass) )</span>'),\n",
       " Task(id='embodied-question-answering', name='Embodied Question Answering', description=''),\n",
       " Task(id='video-enhancement', name='Video Enhancement', description=''),\n",
       " Task(id='historical-color-image-dating', name='Historical Color Image Dating', description=''),\n",
       " Task(id='facial-makeup-transfer', name='Facial Makeup Transfer', description='Facial makeup transfer aims to translate the **makeup style** from a given *reference* makeup face image to another non-makeup one while *preserving face identity*.'),\n",
       " Task(id='single-image-based-hdr-reconstruction', name='Single-Image-Based Hdr Reconstruction', description=''),\n",
       " Task(id='observation-completion', name='Observation Completion', description=''),\n",
       " Task(id='face-parsing', name='Face Parsing', description='Classify pixels of a face image into different classes based on a given bounding box.'),\n",
       " Task(id='fine-grained-image-inpainting', name='Fine-Grained Image Inpainting', description=''),\n",
       " Task(id='object-proposal-generation', name='Object Proposal Generation', description='Object proposal generation is a preprocessing technique that has been widely used in current object detection pipelines to guide the search of objects and avoid exhaustive sliding window search across images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multiscale Combinatorial Grouping\\r\\nfor Image Segmentation and Object Proposal Generation](https://arxiv.org/pdf/1503.00848v4.pdf) )</span>'),\n",
       " Task(id='birds-eye-view-object-detection', name='Birds Eye View Object Detection', description='KITTI birds eye view detection task'),\n",
       " Task(id='visual-tracking', name='Visual Tracking', description='**Visual Tracking** is an essential and actively researched problem in the field of computer vision with various real-world applications such as robotic services, smart surveillance systems, autonomous driving, and human-computer interaction. It refers to the automatic estimation of the trajectory of an arbitrary target object, usually specified by a bounding box in the first frame, as it moves around in subsequent video frames.\\n\\n\\n<span class=\"description-source\">Source: [Learning Reinforced Attentional Representation for End-to-End Visual Tracking ](https://arxiv.org/abs/1908.10009)</span>'),\n",
       " Task(id='action-classification', name='Action Classification', description=''),\n",
       " Task(id='visual-social-relationship-recognition', name='Visual Social Relationship Recognition', description=''),\n",
       " Task(id='classify-3d-point-clouds', name='Classify 3D Point Clouds', description=''),\n",
       " Task(id='semi-supervised-sketch-based-image-retrieval', name='Semi-Supervised Sketch Based Image Retrieval', description='Whilst the number of photos can be easily scaled, each corresponding sketch still needs to be individually produced for fine-grained sketch-based image retrieval. The objective is to mitigate such an upper-bound on sketch data, and study whether unlabelled photos alone (of which they are many) can be cultivated for performance gain.'),\n",
       " Task(id='image-restoration', name='Image Restoration', description='**Image Restoration** is a family of inverse problems for obtaining a high quality image from a corrupted input image. Corruption may occur due to the image-capture process (e.g., noise, lens blur), post-processing (e.g., JPEG compression), or photography in non-ideal conditions (e.g., haze, motion blur).\\n\\n\\n<span class=\"description-source\">Source: [Blind Image Restoration without Prior Knowledge ](https://arxiv.org/abs/2003.01764)</span>'),\n",
       " Task(id='rgb-d-salient-object-detection', name='RGB-D Salient Object Detection', description='RGB-D Salient object detection (SOD) aims at distinguishing the most visually distinctive objects or regions in a scene from the given RGB and Depth data. It has a wide range of applications, including video/image segmentation, object recognition, visual tracking, foreground maps evaluation, image retrieval, content-aware image editing, information discovery, photosynthesis, and weakly\\r\\nsupervised semantic segmentation. Here, depth information plays an important complementary role in finding salient objects. Online benchmark: http://dpfan.net/d3netbenchmark.\\r\\n\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks, TNNLS20](https://ieeexplore.ieee.org/abstract/document/9107477) )</span>'),\n",
       " Task(id='3d-object-super-resolution', name='3D Object Super-Resolution', description='3D object super-resolution is the task of up-sampling 3D objects.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation](https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation) )</span>'),\n",
       " Task(id='referring-expression-segmentation', name='Referring Expression Segmentation', description='The task aims at labelling the pixels of an image or video that represent an object instance referred by a linguistic expression. In particular, the referring expression (RE) must allow the identification of an indivisual object in a discourse or scene (the referent). REs unambiguosly identify the target instace.'),\n",
       " Task(id='weakly-supervised-action-recognition', name='Weakly-Supervised Action Recognition', description='Action recognition with single-point annotations in time (there are no action start/stop time annotations)'),\n",
       " Task(id='face-anonymization', name='Face Anonymization', description=''),\n",
       " Task(id='color-image-compression-artifact-reduction', name='Color Image Compression Artifact Reduction', description=''),\n",
       " Task(id='sketch-based-image-retrieval', name='Sketch-Based Image Retrieval', description=''),\n",
       " Task(id='human-interaction-recognition', name='Human Interaction Recognition', description=''),\n",
       " Task(id='fine-grained-visual-categorization', name='Fine-Grained Visual Categorization', description=''),\n",
       " Task(id='animated-gif-generation', name='Animated GIF Generation', description=''),\n",
       " Task(id='scene-classification', name='Scene Classification', description='**Scene Classification** is a task in which scenes from photographs are categorically classified. Unlike object classification, which focuses on classifying prominent objects in the foreground, Scene Classification uses the layout of objects within the scene, in addition to the ambient context, for classification.\\n\\n\\n<span class=\"description-source\">Source: [Scene classification with Convolutional Neural Networks ](http://cs231n.stanford.edu/reports/2017/pdfs/102.pdf)</span>'),\n",
       " Task(id='whole-slide-images', name='whole slide images', description=''),\n",
       " Task(id='adversarial-attack-detection', name='Adversarial Attack Detection', description='The detection of adversarial attacks.'),\n",
       " Task(id='manufacturing-quality-control', name='Manufacturing Quality Control', description='AI for Quality control in manufacturing processes.'),\n",
       " Task(id='multiple-action-detection', name='Multiple Action Detection', description=''),\n",
       " Task(id='severity-prediction', name='severity prediction', description=''),\n",
       " Task(id='word-spotting-in-handwritten-documents', name='Word Spotting In Handwritten Documents', description=''),\n",
       " Task(id='multiple-object-forecasting', name='Multiple Object Forecasting', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multiple Object Forecasting](https://github.com/olly-styles/Multiple-Object-Forecasting) )</span>'),\n",
       " Task(id='3d-object-detection', name='3D Object Detection', description='2D object detection classifies the object category and estimates oriented 2D bounding boxes of physical objects from 3D sensor data.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [AVOD](https://github.com/kujason/avod) )</span>'),\n",
       " Task(id='video-style-transfer', name='Video Style Transfer', description=''),\n",
       " Task(id='prostate-zones-segmentation', name='Prostate Zones Segmentation', description=''),\n",
       " Task(id='scene-change-detection', name='Scene Change Detection', description=''),\n",
       " Task(id='unet-segmentation', name='UNET Segmentation', description=''),\n",
       " Task(id='transparent-object-depth-estimation', name='Transparent Object Depth Estimation', description='Estimating the 3D shape of transparent objects'),\n",
       " Task(id='mobile-periocular-recognition', name='Mobile Periocular Recognition', description='Periocular recognition is the task of recognising a person based on their eyes (periocular).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Heterogeneity Aware Deep Embedding for Mobile Periocular Recognition](https://arxiv.org/pdf/1811.00846v1.pdf) )</span>'),\n",
       " Task(id='image-super-resolution', name='Image Super-Resolution', description='Image super-resolution (SR) techniques reconstruct a higher-resolution image or sequence from the observed lower-resolution images. Usually the benchmarks are single-image super-resolution (SISR) tasks. <span style=\"color:grey; opacity: 0.6\">( Image credit: [BasicSR](https://github.com/xinntao/BasicSR) )</span>'),\n",
       " Task(id='physiological-computing', name='Physiological Computing', description=''),\n",
       " Task(id='surface-normals-estimation', name='Surface Normals Estimation', description=''),\n",
       " Task(id='egocentric-activity-recognition', name='Egocentric Activity Recognition', description=''),\n",
       " Task(id='disguised-face-verification', name='Disguised Face Verification', description=''),\n",
       " Task(id='anomaly-detection', name='Anomaly Detection', description='Anomaly Detection, Anomaly Segmentation, Novelty Detection, Out-of-Distribution Detection'),\n",
       " Task(id='multi-view-3d-shape-retrieval', name='Multi-View 3D Shape Retrieval', description=''),\n",
       " Task(id='video-restoration', name='Video Restoration', description=''),\n",
       " Task(id='action-anticipation', name='Action Anticipation', description=''),\n",
       " Task(id='depth-completion', name='Depth Completion', description='The **Depth Completion** task is a sub-problem of depth estimation. Instead of knowing nothing about the scene, the Depth Completion task has strong priors on scene depth. The goal of Depth Completion is to fill in the depth on pixels where there is no valid depth. The pixels where the input sparse depth map has a valid value should remain unchanged during the process.\\n\\n\\n<span class=\"description-source\">Source: [LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery ](https://arxiv.org/abs/1905.02744)</span>'),\n",
       " Task(id='hand-object-pose', name='hand-object pose', description='6D pose estimation of hand and object'),\n",
       " Task(id='polyp-segmentation', name='Polyp Segmentation', description='The goal of the project is to develop a computer-aided detection and diagnosis system for automatic polyp segmentation and detection.'),\n",
       " Task(id='one-shot-object-detection', name='One-Shot Object Detection', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [Siamese Mask R-CNN\\r\\n](https://github.com/bethgelab/siamese-mask-rcnn) )</span>'),\n",
       " Task(id='class-incremental-learning', name='class-incremental learning', description='Incremental learning of a sequence of tasks when the task-ID is not available at test time.'),\n",
       " Task(id='curved-text-detection', name='Curved Text Detection', description=''),\n",
       " Task(id='semi-supervised-semantic-segmentation', name='Semi-Supervised Semantic Segmentation', description=''),\n",
       " Task(id='sequential-image-classification', name='Sequential Image Classification', description='Sequential image classification is the task of classifying a sequence of images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [TensorFlow-101](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb) )</span>'),\n",
       " Task(id='deep-attention', name='Deep Attention', description=''),\n",
       " Task(id='face-verification', name='Face Verification', description='Face verification is the task of comparing a candidate face to another, and verifying whether it is a match. It is a one-to-one mapping: you have to check if this person is the correct one.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Pose-Robust Face Recognition via Deep Residual Equivariant Mapping](https://arxiv.org/pdf/1803.00839v1.pdf) )</span>'),\n",
       " Task(id='monocular-depth-estimation', name='Monocular Depth Estimation', description='The **Monocular Depth Estimation** is the task of estimating scene depth using a single image.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Defocus Deblurring Using Dual-Pixel Data ](https://arxiv.org/abs/2005.00305)</span>'),\n",
       " Task(id='salient-object-detection', name='RGB Salient Object Detection', description='RGB Salient object detection is a task-based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Attentive Feedback Network for Boundary-Aware Salient Object Detection](http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Attentive_Feedback_Network_for_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf) )</span>'),\n",
       " Task(id='one-shot-image-to-image-translation', name='One Shot Image to Image Translation', description='Image to Image translation where the source or target domains consist of a single sample only.'),\n",
       " Task(id='salt-and-pepper-noise-removal', name='Salt-And-Pepper Noise Removal', description='Salt-and-pepper noise is a form of noise sometimes seen on images. It is also known as impulse noise. This noise can be caused by sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [NAMF](https://arxiv.org/pdf/1910.07787v1.pdf) )</span>'),\n",
       " Task(id='self-driving-cars', name='Self-Driving Cars', description='Self-driving cars : the task of making a car that can drive itself without human guidance.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Learning a Driving Simulator](https://github.com/commaai/research) )</span>'),\n",
       " Task(id='semi-supervised-video-classification', name='Semi-Supervised Video Classification', description=''),\n",
       " Task(id='food-recognition', name='Food Recognition', description=''),\n",
       " Task(id='one-shot-segmentation', name='One-Shot Segmentation', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [One-Shot Learning for Semantic\\r\\nSegmentation](https://arxiv.org/pdf/1709.03410v1.pdf) )</span>'),\n",
       " Task(id='face-image-retrieval', name='Face Image Retrieval', description='Face image retrieval is the task of retrieving faces similar to a query, according to the given\\r\\ncriteria (e.g. identity) and rank them using their distances to the query.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [CP-mtML](http://openaccess.thecvf.com/content_cvpr_2016/papers/Bhattarai_CP-mtML_Coupled_Projection_CVPR_2016_paper.pdf) )</span>'),\n",
       " Task(id='point-cloud-super-resolution', name='Point Cloud Super Resolution', description='Point cloud super-resolution is a fundamental problem\\r\\nfor 3D reconstruction and 3D data understanding. It takes\\r\\na low-resolution (LR) point cloud as input and generates\\r\\na high-resolution (HR) point cloud with rich details'),\n",
       " Task(id='anomaly-detection-in-surveillance-videos', name='Anomaly Detection In Surveillance Videos', description=''),\n",
       " Task(id='weakly-supervised-3d-human-pose-estimation', name='Weakly-supervised 3D Human Pose Estimation', description='This task targets at 3D Human Pose Estimation with fewer 3D annotation.'),\n",
       " Task(id='satellite-image-classification', name='Satellite Image Classification', description=''),\n",
       " Task(id='pose-transfer', name='Pose Transfer', description=''),\n",
       " Task(id='real-time-instance-segmentation', name='Real-time Instance Segmentation', description='Similar to its parent task, instance segmentation, but with the goal of achieving real-time capabilities under a defined setting.\\r\\n\\r\\nImage Credit: [SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation](https://arxiv.org/pdf/2007.14772v1.pdf)'),\n",
       " Task(id='motion-retargeting', name='motion retargeting', description=''),\n",
       " Task(id='semi-supervised-person-re-identification', name='Semi-Supervised Person Re-Identification', description=''),\n",
       " Task(id='object-counting', name='Object Counting', description='The goal of **Object Counting** task is to count the number of object instances in a single image or video sequence. It has many real-world applications such as traffic flow monitoring, crowdedness estimation, and product counting.\\n\\n\\n<span class=\"description-source\">Source: [Learning to Count Objects with Few Exemplar Annotations ](https://arxiv.org/abs/1905.07898)</span>'),\n",
       " Task(id='few-shot-video-object-detection', name='Few-Shot Video Object Detection', description='Few-Shot Video Object Detection\\r\\n(FSVOD): given only a few support images of the target\\r\\nobject in an unseen class, detect all the objects belonging to\\r\\nthe same class in a given query video.'),\n",
       " Task(id='image-smoothing', name='image smoothing', description=''),\n",
       " Task(id='localization-in-video-forgery', name='Localization In Video Forgery', description=''),\n",
       " Task(id='video', name='Video', description=''),\n",
       " Task(id='eye-tracking', name='Gaze Prediction', description=''),\n",
       " Task(id='weakly-supervised-object-localization', name='Weakly-Supervised Object Localization', description=''),\n",
       " Task(id='pedestrian-attribute-recognition', name='Pedestrian Attribute Recognition', description='Pedestrian attribution recognition is the task of recognising pedestrian features - such as whether they are talking on a phone, whether they have a backpack, and so on.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis](https://arxiv.org/pdf/1709.09930v1.pdf) )</span>'),\n",
       " Task(id='point-cloud-generation', name='Point Cloud Generation', description=''),\n",
       " Task(id='sign-language-translation', name='Sign Language Translation', description='Given a video containing sign language, the task is to predict the translation into (written) spoken language.\\r\\n\\r\\nImage credit: [How2Sign](https://how2sign.github.io/)'),\n",
       " Task(id='fundus-to-angiography-generation', name='Fundus to Angiography Generation', description='Generating Retinal Fluorescein Angiography from Retinal Fundus Image using Generative Adversarial Networks.'),\n",
       " Task(id='jpeg-artifact-correction', name='JPEG Artifact Correction', description='Correction of visual artifacts caused by JPEG compression, these artifacts are usually grouped into three types: blocking, blurring, and ringing. They are caused by quantization and removal of high frequency DCT coefficients.'),\n",
       " Task(id='license-plate-detection', name='License Plate Detection', description=''),\n",
       " Task(id='face-transfer', name='Face Transfer', description='**Face Transfer** is a method for mapping face performances of one individual to facial animations of another one. It uses facial expressions and head poses from the video of a source actor to generate a video of a target character. Face Transfer is a special case of image-to-image translation tasks.\\n\\n\\n<span class=\"description-source\">Source: [Face Transfer with Generative Adversarial Network ](https://arxiv.org/abs/1710.06090)</span>'),\n",
       " Task(id='heterogeneous-face-recognition', name='Heterogeneous Face Recognition', description='Heterogeneous face recognition is the task of matching face images acquired from different sources (i.e., different sensors or different wavelengths) for identification or verification.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Pose Agnostic Cross-spectral Hallucination via Disentangling Independent Factors](https://arxiv.org/pdf/1909.04365v1.pdf) )</span>'),\n",
       " Task(id='sketch-recognition', name='Sketch Recognition', description=''),\n",
       " Task(id='3d-human-reconstruction', name='3D Human Reconstruction', description=''),\n",
       " Task(id='crowds', name='Crowds', description=''),\n",
       " Task(id='action-unit-detection', name='Action Unit Detection', description='Action unit detection is the task of detecting action units from a video - for example, types of facial action units (lip tightening, cheek raising) from a video of a face.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [AU R-CNN](https://arxiv.org/pdf/1812.05788v2.pdf) )</span>'),\n",
       " Task(id='story-visualization', name='Story Visualization', description=''),\n",
       " Task(id='trajectory-forecasting', name='Trajectory Forecasting', description='Trajectory forecasting is a sequential prediction task, where a forecasting model predicts future trajectories of all moving agents (humans, vehicles, etc.) in a scene, based on their past trajectories and/or the scene context.\\r\\n\\r\\n(Illustrative figure from [Social NCE: Contrastive Learning of Socially-aware Motion Representations](https://github.com/vita-epfl/social-nce))'),\n",
       " Task(id='scene-understanding', name='Scene Understanding', description=''),\n",
       " Task(id='multi-oriented-scene-text-detection', name='Multi-Oriented Scene Text Detection', description=''),\n",
       " Task(id='real-time-visual-tracking', name='Real-Time Visual Tracking', description=''),\n",
       " Task(id='displaced-people-recognition', name='Displaced People Recognition', description='Recognise displaced people from images.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level](https://arxiv.org/pdf/1905.02025v1.pdf) )</span>'),\n",
       " Task(id='document-binarization', name='Document Binarization', description=''),\n",
       " Task(id='video-prediction', name='Video Prediction', description='**Video Prediction** is the task of predicting future frames given past video frames.\\n\\n\\n<span class=\"description-source\">Source: [Photo-Realistic Video Prediction on Natural Videos of Largely Changing Frames ](https://arxiv.org/abs/2003.08635)</span>'),\n",
       " Task(id='user-constrained-thumbnail-generation', name='User Constrained Thumbnail Generation', description='Thumbnail generation is the task of generating image thumbnails from an input image.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [User Constrained Thumbnail Generation using Adaptive Convolutions](https://arxiv.org/pdf/1810.13054v3.pdf) )</span>'),\n",
       " Task(id='indoor-scene-reconstruction', name='Indoor Scene Reconstruction', description=''),\n",
       " Task(id='audio-visual-video-captioning', name='Audio-Visual Video Captioning', description=''),\n",
       " Task(id='license-plate-recognition', name='License Plate Recognition', description=''),\n",
       " Task(id='small-data', name='Small Data Image Classification', description='Supervised image classification with tens to hundreds of labeled training examples.'),\n",
       " Task(id='3d-point-cloud-matching', name='3D Point Cloud Matching', description='Image: [Gojic et al](https://openaccess.thecvf.com/content_CVPR_2019/papers/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.pdf)'),\n",
       " Task(id='unsupervised-facial-landmark-detection', name='Unsupervised Facial Landmark Detection', description='Facial landmark detection in the unsupervised setting popularized by [1].  The evaluation occurs in two stages:\\r\\n(1) Embeddings are first learned in an unsupervised manner (i.e. without labels);\\r\\n(2) A simple regressor is trained to regress landmarks from the unsupervised embedding.\\r\\n\\r\\n[1]  Thewlis, James, Hakan Bilen, and Andrea Vedaldi. \"Unsupervised learning of object landmarks by factorized spatial embeddings.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Unsupervised learning of object landmarks by factorized spatial embeddings](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/thewlis17unsupervised.pdf) )</span>'),\n",
       " Task(id='weakly-supervised-instance-segmentation', name='Weakly-supervised instance segmentation', description=''),\n",
       " Task(id='gan-image-forensics', name='GAN image forensics', description=''),\n",
       " Task(id='image-based-localization', name='Image-Based Localization', description=''),\n",
       " Task(id='3d-facial-landmark-localization', name='3D Facial Landmark Localization', description='Image: [Zhang et al](https://arxiv.org/pdf/1801.09242v1.pdf)'),\n",
       " Task(id='age-estimation', name='Age Estimation', description='Age Estimation is the task of estimating the age of a person from an image.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [BridgeNet](https://arxiv.org/pdf/1904.03358v1.pdf) )</span>'),\n",
       " Task(id='age-and-gender-classification', name='Age And Gender Classification', description='Age and gender classification is a dual-task of identifying the age and gender of a person from an image or video.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-Expert Gender Classification on Age Group by Integrating Deep Neural Networks](https://arxiv.org/pdf/1809.01990v2.pdf) )</span>'),\n",
       " Task(id='image-quality-estimation', name='Image Quality Estimation', description=''),\n",
       " Task(id='kiss-detection', name='Kiss Detection', description=''),\n",
       " Task(id='scene-segmentation', name='Scene Segmentation', description='Scene segmentation is the task of splitting a scene into its various object components.\\r\\n\\r\\nImage adapted from [Temporally coherent 4D reconstruction of complex dynamic scenes](https://paperswithcode.com/paper/temporally-coherent-4d-reconstruction-of2).'),\n",
       " Task(id='detecting-image-manipulation', name='Detecting Image Manipulation', description=''),\n",
       " Task(id='point-cloud-pre-training', name='Point Cloud Pre-training', description=''),\n",
       " Task(id='road-damage-detection', name='Road Damage Detection', description='Road damage detection is the task of detecting damage in roads.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Road Damage Detection And Classification In Smartphone Captured Images Using Mask R-CNN](https://arxiv.org/pdf/1811.04535v1.pdf) )</span>'),\n",
       " Task(id='universal-domain-adaptation', name='Universal Domain Adaptation', description=''),\n",
       " Task(id='hyperspectral-image-classification', name='Hyperspectral Image Classification', description='Hyperspectral image classification is the task of classifying a class label to every pixel in an image that was captured using (hyper)spectral sensors.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Shorten Spatial-spectral RNN with Parallel-GRU for Hyperspectral Image Classification](https://arxiv.org/pdf/1810.12563v1.pdf) )</span>'),\n",
       " Task(id='image-reconstruction', name='Image Reconstruction', description=''),\n",
       " Task(id='lip-sync-1', name='Constrained Lip-synchronization', description='This task deals with lip-syncing a video (or) an image to the desired target speech. Approaches in this task work only for a specific (limited set) of identities, languages, speech/voice. See also: Unconstrained lip-synchronization - https://paperswithcode.com/task/lip-sync'),\n",
       " Task(id='person-identification', name='Person Identification', description=''),\n",
       " Task(id='online-multi-object-tracking', name='Online Multi-Object Tracking', description='The goal of **Online Multi-Object Tracking** is to estimate the spatio-temporal trajectories of multiple objects in an online video stream (i.e., the video is provided frame-by-frame), which is a fundamental problem for numerous real-time applications, such as video surveillance, autonomous driving, and robot navigation.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [A Hybrid Data Association Framework for Robust Online Multi-Object Tracking ](https://arxiv.org/abs/1703.10764)</span>'),\n",
       " Task(id='video-background-subtraction', name='Video Background Subtraction', description=''),\n",
       " Task(id='retinal-oct-disease-classification', name='Retinal OCT Disease Classification', description='Classifying different Retinal degeneration from Optical Coherence Tomography Images (OCT).'),\n",
       " Task(id='hand-joint-reconstruction', name='Hand Joint Reconstruction', description=''),\n",
       " Task(id='keypoint-detection', name='Keypoint Detection', description='Keypoint detection involves simultaneously detecting people and localizing their keypoints. Keypoints are the same thing as interest points. They are spatial locations, or points in the image that define what is interesting or what stand out in the image. They are invariant to image rotation, shrinkage, translation, distortion, and so on.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [PifPaf: Composite Fields for Human Pose Estimation](https://github.com/vita-epfl/openpifpaf); \"Learning to surf\" by fotologic, license: CC-BY-2.0 )</span>'),\n",
       " Task(id='rf-based-pose-estimation', name='RF-based Pose Estimation', description='Detect human actions through walls and occlusions, and in poor lighting conditions. Taking radio frequency (RF) signals as input (e.g. Wifi), generating 3D human skeletons as an intermediate representation, and recognizing actions and interactions.\\r\\n\\r\\nSee e.g. RF-Pose from MIT for a good illustration of the approach\\r\\nhttp://rfpose.csail.mit.edu/\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Making the Invisible Visible](https://arxiv.org/pdf/1909.09300v1.pdf) )</span>'),\n",
       " Task(id='3d-feature-matching', name='3D Feature Matching', description='Image: [Choy et al](https://paperswithcode.com/paper/fully-convolutional-geometric-features)'),\n",
       " Task(id='facial-expression-recognition', name='Facial Expression Recognition', description='Facial expression recognition is the task of classifying the expressions on face images into various categories such as anger, fear, surprise, sadness, happiness and so on.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DeXpression](https://arxiv.org/pdf/1509.05371v2.pdf) )</span>'),\n",
       " Task(id='missing-markers-reconstruction', name='Missing Markers Reconstruction', description='Reconstructing missing markers in the motion caption 3d poses'),\n",
       " Task(id='3d-human-action-recognition', name='3D Action Recognition', description='Image: [Rahmani et al](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Rahmani_3D_Action_Recognition_CVPR_2016_paper.pdf)'),\n",
       " Task(id='face-anti-spoofing', name='Face Anti-Spoofing', description='Facial anti-spoofing is the task of preventing false facial verification by using a photo, video, mask or a different substitute for an authorized person’s face. Some examples of attacks:\\r\\n\\r\\n- **Print attack**: The attacker uses someone’s photo. The image is printed or displayed on a digital device.\\r\\n\\r\\n- **Replay/video attack**: A more sophisticated way to trick the system, which usually requires a looped video of a victim’s face. This approach ensures behaviour and facial movements to look more ‘natural’ compared to holding someone’s photo.\\r\\n\\r\\n- **3D mask attack**: During this type of attack, a mask is used as the tool of choice for spoofing. It’s an even more sophisticated attack than playing a face video. In addition to natural facial movements, it enables ways to deceive some extra layers of protection such as depth sensors.\\r\\n\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing](https://github.com/XgTu/GFA-CNN) )</span>'),\n",
       " Task(id='instance-segmentation', name='Instance Segmentation', description=\"Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image.\\r\\n\\r\\nImage Credit: [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers, CVPR'21](https://arxiv.org/abs/2103.12340)\"),\n",
       " Task(id='landmine', name='Landmine', description=''),\n",
       " Task(id='pose-estimation', name='Pose Estimation', description='Pose Estimation is a general problem in Computer Vision where we detect the position and orientation of an object.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch) )</span>'),\n",
       " Task(id='chinese-landscape-painting-generation', name='Chinese Landscape Painting Generation', description=''),\n",
       " Task(id='hand', name='Hand', description=''),\n",
       " Task(id='dehazing', name='Dehazing', description=''),\n",
       " Task(id='single-image-deraining', name='Single Image Deraining', description=''),\n",
       " Task(id='imagedocument-clustering', name='Image/Document Clustering', description=''),\n",
       " Task(id='image-inpainting', name='Image Inpainting', description='**Image Inpainting** is a task of reconstructing missing regions in an image. It is an important problem in computer vision and an essential functionality in many imaging and graphics applications, e.g. object removal, image restoration, manipulation, re-targeting, compositing, and image-based rendering.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling ](https://arxiv.org/abs/2005.11742)</span>'),\n",
       " Task(id='image-forensics', name='Image Forensics', description=''),\n",
       " Task(id='handwritten-word-generation', name='Handwritten Word Generation', description=''),\n",
       " Task(id='depth-estimation', name='Depth Estimation', description='**Depth Estimation** is a crucial step towards inferring scene geometry from 2D images. The goal in monocular Depth Estimation is to predict the depth value of each pixel, given only a single RGB image as input.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [DIODE: A Dense Indoor and Outdoor DEpth Dataset ](https://arxiv.org/abs/1908.00463)</span>'),\n",
       " Task(id='handwritten-digit-image-synthesis', name='Handwritten Digit Image Synthesis', description=''),\n",
       " Task(id='deblurring', name='Deblurring', description='<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deblurring Face Images using Uncertainty Guided\\r\\nMulti-Stream Semantic Networks](https://arxiv.org/pdf/1907.13106v1.pdf) )</span>'),\n",
       " Task(id='occlusion-estimation', name='Occlusion Estimation', description=''),\n",
       " Task(id='depth-image-upsampling', name='Depth Image Upsampling', description=''),\n",
       " Task(id='blood-cell-count', name='Blood Cell Count', description=''),\n",
       " Task(id='instance-search', name='Instance Search', description='Visual **Instance Search** is the task of retrieving from a database of images the ones that contain an instance of a visual query. It is typically much more challenging than finding images from the database that contain objects belonging to the same category as the object in the query. If the visual query is an image of a shoe, visual Instance Search does not try to find images of shoes, which might differ from the query in shape, color or size, but tries to find images of the exact same shoe as the one in the query image. Visual Instance Search challenges image representations as the features extracted from the images must enable such fine-grained recognition despite variations in viewpoints, scale, position, illumination, etc. Whereas holistic image representations, where each image is mapped to a single high-dimensional vector, are sufficient for coarse-grained similarity retrieval, local features are needed for instance retrieval.\\n\\n\\n<span class=\"description-source\">Source: [Dynamicity and Durability in Scalable Visual Instance Search ](https://arxiv.org/abs/1805.10942)</span>'),\n",
       " Task(id='weakly-supervised-temporal-action', name='Weakly-supervised Temporal Action Localization', description='Temporal Action Localization with weak supervision where only video-level labels are given for training'),\n",
       " Task(id='image-manipulation-detection', name='Image Manipulation Detection', description=''),\n",
       " Task(id='skills-assessment', name='Skills Assessment', description=''),\n",
       " Task(id='action-understanding', name='Action Understanding', description=''),\n",
       " Task(id='object-discovery', name='Object Discovery', description='**Object Discovery** is the task of identifying previously unseen objects.\\n\\n\\n<span class=\"description-source\">Source: [Unsupervised Object Discovery and Segmentation of RGBD-images ](https://arxiv.org/abs/1710.06929)</span>'),\n",
       " Task(id='relational-captioning', name='Relational Captioning', description=''),\n",
       " Task(id='skills-evaluation', name='Skills Evaluation', description=''),\n",
       " Task(id='animal-pose-estimation', name='Animal Pose Estimation', description='Animal pose estimation is the task of identifying the pose of an animal.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Using DeepLabCut for 3D markerless pose estimation across species and behaviors](http://www.mousemotorlab.org/s/NathMathis2019.pdf) )</span>'),\n",
       " Task(id='video-saliency-detection', name='Video Saliency Detection', description=''),\n",
       " Task(id='depiction-invariant-object-recognition', name='Depiction Invariant Object Recognition', description='Depiction invariant object recognition is the task of recognising objects irrespective of how they are visually depicted (line drawing, realistic shaded drawing, photograph etc.).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [SwiDeN](https://arxiv.org/pdf/1607.08764v1.pdf) )</span>'),\n",
       " Task(id='saliency-detection', name='Saliency Detection', description='**Saliency Detection** is a preprocessing step in computer vision which aims at finding salient objects in an image.\\n\\n\\n<span class=\"description-source\">Source: [An Unsupervised Game-Theoretic Approach to Saliency Detection ](https://arxiv.org/abs/1708.02476)</span>'),\n",
       " Task(id='image-manipulation', name='Image Manipulation', description=''),\n",
       " Task(id='predict-future-video-frames', name='Predict Future Video Frames', description=''),\n",
       " Task(id='gait-identification', name='Gait Identification', description=''),\n",
       " Task(id='intelligent-surveillance', name='Intelligent Surveillance', description=''),\n",
       " Task(id='autonomous-vehicles', name='Autonomous Vehicles', description='Autonomous vehicles is the task of making a vehicle that can guide itself without human conduction.\\r\\n\\r\\nMany of the state-of-the-art results can be found at more general task pages such as [3D Object Detection](https://paperswithcode.com/task/3d-object-detection) and [Semantic Segmentation](https://paperswithcode.com/task/semantic-segmentation).\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision](https://arxiv.org/abs/2007.13124) )</span>'),\n",
       " Task(id='weakly-supervised-segmentation', name='Weakly supervised segmentation', description=''),\n",
       " Task(id='self-supervised-learning', name='Self-Supervised Learning', description='**Self-Supervised Learning** is proposed for utilizing unlabeled data with the success of supervised learning. Producing a dataset with good labels is expensive, while unlabeled data is being generated all the time. The motivation of Self-Supervised Learning is to make use of the large amount of unlabeled data. The main idea of Self-Supervised Learning is to generate the labels from unlabeled data, according to the structure or characteristics of the data itself, and then train on this unsupervised data in a supervised manner. Self-Supervised Learning is wildly used in representation learning to make a model learn the latent features of the data. This technique is often employed in computer vision, video processing and robot control.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Self-supervised Point Set Local Descriptors for Point Cloud Registration ](https://arxiv.org/abs/2003.05199)</span>\\r\\n\\r\\nImage source: [LeCun](https://www.youtube.com/watch?v=7I0Qt7GALVk)'),\n",
       " Task(id='supervised-dimensionality-reduction', name='Supervised dimensionality reduction', description=''),\n",
       " Task(id='multiview-detection', name='Multiview Detection', description='Incorporating multiple camera views for detection in heavily occluded scenarios.'),\n",
       " Task(id='4d-spatio-temporal-semantic-segmentation', name='4D Spatio Temporal Semantic Segmentation', description='Image: [Choy et al](https://paperswithcode.com/paper/4d-spatio-temporal-convnets-minkowski)'),\n",
       " Task(id='visual-place-recognition', name='Visual Place Recognition', description='**Visual Place Recognition** is the task of matching a view of a place with a different view of the same place taken at a different time.\\r\\n\\r\\n<span class=\"description-source\">Source: [Visual place recognition using landmark distribution descriptors ](https://arxiv.org/abs/1608.04274)</span>\\r\\n\\r\\nImage credit: [Visual place recognition using landmark distribution descriptors](https://arxiv.org/pdf/1608.04274.pdf)'),\n",
       " Task(id='3d-object-detection-from-monocular-images', name='3D Object Detection From Monocular Images', description='This is the task of detecting 3D objects from monocular images (as opposed to LiDAR based counterparts). It is usually associated with autonomous driving based tasks.\\r\\n\\r\\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Orthographic Feature Transform for Monocular 3D Object Detection](https://arxiv.org/pdf/1811.08188v1.pdf) )</span>'),\n",
       " Task(id='artistic-style-classification', name='Artistic style classification', description='Classify the artistic style of an artwork image'),\n",
       " Task(id='cross-view-image-to-image-translation', name='Cross-View Image-to-Image Translation', description=''),\n",
       " Task(id='amodal-instance-segmentation', name='Amodal Instance Segmentation', description=''),\n",
       " Task(id='2d-object-detection', name='2D Object Detection', description='')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.area_task_list('computer-vision', items_per_page=1000).results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>framework</th>\n",
       "      <th>mentioned_in_github</th>\n",
       "      <th>mentioned_in_paper</th>\n",
       "      <th>paper_arxiv_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url_abs</th>\n",
       "      <th>paper_url_pdf</th>\n",
       "      <th>repo</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>most_common_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://paperswithcode.com/paper/a-reductions-approach-to-fair-classification</td>\n",
       "      <td>1803.02453</td>\n",
       "      <td>A Reductions Approach to Fair Classification</td>\n",
       "      <td>We present a systematic approach for achieving fairness in a binary\\nclassification setting. While we focus on two well-known quantitative\\ndefinitions of fairness, our approach encompasses many o...</td>\n",
       "      <td>http://arxiv.org/abs/1803.02453v3</td>\n",
       "      <td>http://arxiv.org/pdf/1803.02453v3.pdf</td>\n",
       "      <td>ICML 2018 7</td>\n",
       "      <td>[Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, Hanna Wallach]</td>\n",
       "      <td>[Fairness]</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>...</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1803.02453</td>\n",
       "      <td>A Reductions Approach to Fair Classification</td>\n",
       "      <td>http://arxiv.org/abs/1803.02453v3</td>\n",
       "      <td>http://arxiv.org/pdf/1803.02453v3.pdf</td>\n",
       "      <td>fairlearn/fairlearn</td>\n",
       "      <td>https://github.com/fairlearn/fairlearn</td>\n",
       "      <td>Fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://paperswithcode.com/paper/abcnn-attention-based-convolutional-neural</td>\n",
       "      <td>1512.05193</td>\n",
       "      <td>ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</td>\n",
       "      <td>How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with o...</td>\n",
       "      <td>http://arxiv.org/abs/1512.05193v4</td>\n",
       "      <td>http://arxiv.org/pdf/1512.05193v4.pdf</td>\n",
       "      <td>TACL 2016 1</td>\n",
       "      <td>[Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bo-Wen Zhou]</td>\n",
       "      <td>[Answer Selection, Natural Language Inference, Paraphrase Identification]</td>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1512.05193</td>\n",
       "      <td>ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</td>\n",
       "      <td>http://arxiv.org/abs/1512.05193v4</td>\n",
       "      <td>http://arxiv.org/pdf/1512.05193v4.pdf</td>\n",
       "      <td>shamalwinchurkar/question-classification</td>\n",
       "      <td>https://github.com/shamalwinchurkar/question-classification</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://paperswithcode.com/paper/unsupervised-representation-learning-by-1</td>\n",
       "      <td>1803.07728</td>\n",
       "      <td>Unsupervised Representation Learning by Predicting Image Rotations</td>\n",
       "      <td>Over the last years, deep convolutional neural networks (ConvNets) have\\ntransformed the field of computer vision thanks to their unparalleled capacity\\nto learn high level semantic image features...</td>\n",
       "      <td>http://arxiv.org/abs/1803.07728v1</td>\n",
       "      <td>http://arxiv.org/pdf/1803.07728v1.pdf</td>\n",
       "      <td>ICLR 2018 1</td>\n",
       "      <td>[Spyros Gidaris, Praveer Singh, Nikos Komodakis]</td>\n",
       "      <td>[Representation Learning, Unsupervised Representation Learning]</td>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1803.07728</td>\n",
       "      <td>Unsupervised Representation Learning by Predicting Image Rotations</td>\n",
       "      <td>http://arxiv.org/abs/1803.07728v1</td>\n",
       "      <td>http://arxiv.org/pdf/1803.07728v1.pdf</td>\n",
       "      <td>nab-126/resnet</td>\n",
       "      <td>https://github.com/nab-126/resnet</td>\n",
       "      <td>Representation Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://paperswithcode.com/paper/semantic-image-synthesis-via-adversarial</td>\n",
       "      <td>1707.06873</td>\n",
       "      <td>Semantic Image Synthesis via Adversarial Learning</td>\n",
       "      <td>In this paper, we propose a way of synthesizing realistic images directly\\nwith natural language description, which has many useful applications, e.g.\\nintelligent image manipulation. We attempt t...</td>\n",
       "      <td>http://arxiv.org/abs/1707.06873v1</td>\n",
       "      <td>http://arxiv.org/pdf/1707.06873v1.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Hao Dong, Simiao Yu, Chao Wu, Yike Guo]</td>\n",
       "      <td>[Image Generation, Image Manipulation]</td>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>...</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1707.06873</td>\n",
       "      <td>Semantic Image Synthesis via Adversarial Learning</td>\n",
       "      <td>http://arxiv.org/abs/1707.06873v1</td>\n",
       "      <td>http://arxiv.org/pdf/1707.06873v1.pdf</td>\n",
       "      <td>vtddggg/BilinearGAN_for_LBIE</td>\n",
       "      <td>https://github.com/vtddggg/BilinearGAN_for_LBIE</td>\n",
       "      <td>Image Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://paperswithcode.com/paper/ensembles-of-many-diverse-weak-defenses-can</td>\n",
       "      <td>2001.00308</td>\n",
       "      <td>ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense</td>\n",
       "      <td>There has been extensive research on developing defense techniques against adversarial attacks; however, they have been mainly designed for specific model families or application domains, therefor...</td>\n",
       "      <td>https://arxiv.org/abs/2001.00308v2</td>\n",
       "      <td>https://arxiv.org/pdf/2001.00308v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi]</td>\n",
       "      <td>[Adversarial Defense, Denoising]</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>...</td>\n",
       "      <td>tf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2001.00308</td>\n",
       "      <td>ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense</td>\n",
       "      <td>https://arxiv.org/abs/2001.00308v2</td>\n",
       "      <td>https://arxiv.org/pdf/2001.00308v2.pdf</td>\n",
       "      <td>softsys4ai/FlexiBO</td>\n",
       "      <td>https://github.com/softsys4ai/FlexiBO</td>\n",
       "      <td>Denoising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        paper_url  \\\n",
       "0   https://paperswithcode.com/paper/a-reductions-approach-to-fair-classification   \n",
       "3     https://paperswithcode.com/paper/abcnn-attention-based-convolutional-neural   \n",
       "4      https://paperswithcode.com/paper/unsupervised-representation-learning-by-1   \n",
       "5       https://paperswithcode.com/paper/semantic-image-synthesis-via-adversarial   \n",
       "10   https://paperswithcode.com/paper/ensembles-of-many-diverse-weak-defenses-can   \n",
       "\n",
       "      arxiv_id  \\\n",
       "0   1803.02453   \n",
       "3   1512.05193   \n",
       "4   1803.07728   \n",
       "5   1707.06873   \n",
       "10  2001.00308   \n",
       "\n",
       "                                                                                  title  \\\n",
       "0                                          A Reductions Approach to Fair Classification   \n",
       "3       ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs   \n",
       "4                    Unsupervised Representation Learning by Predicting Image Rotations   \n",
       "5                                     Semantic Image Synthesis via Adversarial Learning   \n",
       "10  ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense   \n",
       "\n",
       "                                                                                                                                                                                                   abstract  \\\n",
       "0   We present a systematic approach for achieving fairness in a binary\\nclassification setting. While we focus on two well-known quantitative\\ndefinitions of fairness, our approach encompasses many o...   \n",
       "3   How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with o...   \n",
       "4   Over the last years, deep convolutional neural networks (ConvNets) have\\ntransformed the field of computer vision thanks to their unparalleled capacity\\nto learn high level semantic image features...   \n",
       "5   In this paper, we propose a way of synthesizing realistic images directly\\nwith natural language description, which has many useful applications, e.g.\\nintelligent image manipulation. We attempt t...   \n",
       "10  There has been extensive research on developing defense techniques against adversarial attacks; however, they have been mainly designed for specific model families or application domains, therefor...   \n",
       "\n",
       "                               url_abs  \\\n",
       "0    http://arxiv.org/abs/1803.02453v3   \n",
       "3    http://arxiv.org/abs/1512.05193v4   \n",
       "4    http://arxiv.org/abs/1803.07728v1   \n",
       "5    http://arxiv.org/abs/1707.06873v1   \n",
       "10  https://arxiv.org/abs/2001.00308v2   \n",
       "\n",
       "                                   url_pdf    proceeding  \\\n",
       "0    http://arxiv.org/pdf/1803.02453v3.pdf   ICML 2018 7   \n",
       "3    http://arxiv.org/pdf/1512.05193v4.pdf   TACL 2016 1   \n",
       "4    http://arxiv.org/pdf/1803.07728v1.pdf   ICLR 2018 1   \n",
       "5    http://arxiv.org/pdf/1707.06873v1.pdf  ICCV 2017 10   \n",
       "10  https://arxiv.org/pdf/2001.00308v2.pdf          None   \n",
       "\n",
       "                                                                             authors  \\\n",
       "0   [Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, Hanna Wallach]   \n",
       "3                            [Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bo-Wen Zhou]   \n",
       "4                                   [Spyros Gidaris, Praveer Singh, Nikos Komodakis]   \n",
       "5                                           [Hao Dong, Simiao Yu, Chao Wu, Yike Guo]   \n",
       "10                            [Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi]   \n",
       "\n",
       "                                                                        tasks  \\\n",
       "0                                                                  [Fairness]   \n",
       "3   [Answer Selection, Natural Language Inference, Paraphrase Identification]   \n",
       "4             [Representation Learning, Unsupervised Representation Learning]   \n",
       "5                                      [Image Generation, Image Manipulation]   \n",
       "10                                           [Adversarial Defense, Denoising]   \n",
       "\n",
       "         date  ... framework mentioned_in_github  mentioned_in_paper  \\\n",
       "0  2018-03-06  ...      none                True               False   \n",
       "3  2015-12-16  ...        tf                True               False   \n",
       "4  2018-03-21  ...        tf                True               False   \n",
       "5  2017-07-21  ...   pytorch                True               False   \n",
       "10 2020-01-02  ...        tf                True               False   \n",
       "\n",
       "    paper_arxiv_id  \\\n",
       "0       1803.02453   \n",
       "3       1512.05193   \n",
       "4       1803.07728   \n",
       "5       1707.06873   \n",
       "10      2001.00308   \n",
       "\n",
       "                                                                            paper_title  \\\n",
       "0                                          A Reductions Approach to Fair Classification   \n",
       "3       ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs   \n",
       "4                    Unsupervised Representation Learning by Predicting Image Rotations   \n",
       "5                                     Semantic Image Synthesis via Adversarial Learning   \n",
       "10  ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense   \n",
       "\n",
       "                         paper_url_abs  \\\n",
       "0    http://arxiv.org/abs/1803.02453v3   \n",
       "3    http://arxiv.org/abs/1512.05193v4   \n",
       "4    http://arxiv.org/abs/1803.07728v1   \n",
       "5    http://arxiv.org/abs/1707.06873v1   \n",
       "10  https://arxiv.org/abs/2001.00308v2   \n",
       "\n",
       "                             paper_url_pdf  \\\n",
       "0    http://arxiv.org/pdf/1803.02453v3.pdf   \n",
       "3    http://arxiv.org/pdf/1512.05193v4.pdf   \n",
       "4    http://arxiv.org/pdf/1803.07728v1.pdf   \n",
       "5    http://arxiv.org/pdf/1707.06873v1.pdf   \n",
       "10  https://arxiv.org/pdf/2001.00308v2.pdf   \n",
       "\n",
       "                                        repo  \\\n",
       "0                        fairlearn/fairlearn   \n",
       "3   shamalwinchurkar/question-classification   \n",
       "4                             nab-126/resnet   \n",
       "5               vtddggg/BilinearGAN_for_LBIE   \n",
       "10                        softsys4ai/FlexiBO   \n",
       "\n",
       "                                                       repo_url  \\\n",
       "0                        https://github.com/fairlearn/fairlearn   \n",
       "3   https://github.com/shamalwinchurkar/question-classification   \n",
       "4                             https://github.com/nab-126/resnet   \n",
       "5               https://github.com/vtddggg/BilinearGAN_for_LBIE   \n",
       "10                        https://github.com/softsys4ai/FlexiBO   \n",
       "\n",
       "              most_common_task  \n",
       "0                     Fairness  \n",
       "3   Natural Language Inference  \n",
       "4      Representation Learning  \n",
       "5             Image Generation  \n",
       "10                   Denoising  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_with_biggest_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'trajectory-prediction',\n",
       " 'name': 'Trajectory Prediction',\n",
       " 'description': '**Trajectory Prediction** is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs ](https://arxiv.org/abs/1912.01118)</span>'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(client.task_get('trajectory-prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Papers(count=284, next_page=2, previous_page=None, results=[Paper(id='an-approach-to-vehicle-trajectory-prediction', arxiv_id='1802.08632', nips_id=None, url_abs='http://arxiv.org/abs/1802.08632v2', url_pdf='http://arxiv.org/pdf/1802.08632v2.pdf', title='An Approach to Vehicle Trajectory Prediction Using Automatically Generated Traffic Maps', abstract='Trajectory and intention prediction of traffic participants is an important\\ntask in automated driving and crucial for safe interaction with the\\nenvironment. In this paper, we present a new approach to vehicle trajectory\\nprediction based on automatically generated maps containing statistical\\ninformation about the behavior of traffic participants in a given area. These\\nmaps are generated based on trajectory observations using image processing and\\nmap matching techniques and contain all typical vehicle movements and\\nprobabilities in the considered area. Our prediction approach matches an\\nobserved trajectory to a behavior contained in the map and uses this\\ninformation to generate a prediction. We evaluated our approach on a dataset\\ncontaining over 14000 trajectories and found that it produces significantly\\nmore precise mid-term predictions compared to motion model-based prediction\\napproaches.', authors=['Jannik Quehl', 'Haohao Hu', 'Sascha Wirges', 'Martin Lauer'], published=datetime.date(2018, 2, 23), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='a-scalable-framework-for-trajectory', arxiv_id='1806.03582', nips_id=None, url_abs='http://arxiv.org/abs/1806.03582v3', url_pdf='http://arxiv.org/pdf/1806.03582v3.pdf', title='A Scalable Framework for Trajectory Prediction', abstract='Trajectory prediction (TP) is of great importance for a wide range of\\nlocation-based applications in intelligent transport systems such as\\nlocation-based advertising, route planning, traffic management, and early\\nwarning systems. In the last few years, the widespread use of GPS navigation\\nsystems and wireless communication technology enabled vehicles has resulted in\\nhuge volumes of trajectory data. The task of utilizing this data employing\\nspatio-temporal techniques for trajectory prediction in an efficient and\\naccurate manner is an ongoing research problem. Existing TP approaches are\\nlimited to short-term predictions. Moreover, they cannot handle a large volume\\nof trajectory data for long-term prediction. To address these limitations, we\\npropose a scalable clustering and Markov chain based hybrid framework, called\\nTraj-clusiVAT-based TP, for both short-term and long-term trajectory\\nprediction, which can handle a large number of overlapping trajectories in a\\ndense road network. Traj-clusiVAT can also determine the number of clusters,\\nwhich represent different movement behaviours in input trajectory data. In our\\nexperiments, we compare our proposed approach with a mixed Markov model\\n(MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method for\\nboth short- and long-term trajectory predictions. We performed our experiments\\non two real, vehicle trajectory datasets, including a large-scale trajectory\\ndataset consisting of 3.28 million trajectories obtained from 15,061 taxis in\\nSingapore over a period of one month. Experimental results on two real\\ntrajectory datasets show that our proposed approach outperforms the existing\\napproaches in terms of both short- and long-term prediction performances, based\\non prediction accuracy and distance error (in km).', authors=['Punit Rathore', 'Dheeraj Kumar', 'Sutharshan Rajasegarar', 'Marimuthu Palaniswami', 'James C. Bezdek'], published=datetime.date(2018, 6, 10), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='an-evaluation-of-trajectory-prediction', arxiv_id='1805.07663', nips_id=None, url_abs='http://arxiv.org/abs/1805.07663v6', url_pdf='http://arxiv.org/pdf/1805.07663v6.pdf', title='An Evaluation of Trajectory Prediction Approaches and Notes on the TrajNet Benchmark', abstract='In recent years, there is a shift from modeling the tracking problem based on\\nBayesian formulation towards using deep neural networks. Towards this end, in\\nthis paper the effectiveness of various deep neural networks for predicting\\nfuture pedestrian paths are evaluated. The analyzed deep networks solely rely,\\nlike in the traditional approaches, on observed tracklets without human-human\\ninteraction information. The evaluation is done on the publicly available\\nTrajNet benchmark dataset, which builds up a repository of considerable and\\npopular datasets for trajectory-based activity forecasting. We show that a\\nRecurrent-Encoder with a Dense layer stacked on top, referred to as\\nRED-predictor, is able to achieve sophisticated results compared to elaborated\\nmodels in such scenarios. Further, we investigate failure cases and give\\nexplanations for observed phenomena and give some recommendations for\\novercoming demonstrated shortcomings.', authors=['Stefan Becker', 'Ronny Hug', 'Wolfgang Hübner', 'Michael Arens'], published=datetime.date(2018, 5, 19), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='sophie-an-attentive-gan-for-predicting-paths', arxiv_id='1806.01482', nips_id=None, url_abs='http://arxiv.org/abs/1806.01482v2', url_pdf='http://arxiv.org/pdf/1806.01482v2.pdf', title='SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints', abstract='This paper addresses the problem of path prediction for multiple interacting\\nagents in a scene, which is a crucial step for many autonomous platforms such\\nas self-driving cars and social robots. We present \\\\textit{SoPhie}; an\\ninterpretable framework based on Generative Adversarial Network (GAN), which\\nleverages two sources of information, the path history of all the agents in a\\nscene, and the scene context information, using images of the scene. To predict\\na future path for an agent, both physical and social information must be\\nleveraged. Previous work has not been successful to jointly model physical and\\nsocial interactions. Our approach blends a social attention mechanism with a\\nphysical attention that helps the model to learn where to look in a large scene\\nand extract the most salient parts of the image relevant to the path. Whereas,\\nthe social attention component aggregates information across the different\\nagent interactions and extracts the most important trajectory information from\\nthe surrounding neighbors. SoPhie also takes advantage of GAN to generates more\\nrealistic samples and to capture the uncertain nature of the future paths by\\nmodeling its distribution. All these mechanisms enable our approach to predict\\nsocially and physically plausible paths for the agents and to achieve\\nstate-of-the-art performance on several different trajectory forecasting\\nbenchmarks.', authors=['Amir Sadeghian', 'Vineet Kosaraju', 'Ali Sadeghian', 'Noriaki Hirose', 'S. Hamid Rezatofighi', 'Silvio Savarese'], published=datetime.date(2018, 6, 5), conference='sophie-an-attentive-gan-for-predicting-paths-1', conference_url_abs='http://openaccess.thecvf.com/content_CVPR_2019/html/Sadeghian_SoPhie_An_Attentive_GAN_for_Predicting_Paths_Compliant_to_Social_CVPR_2019_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_CVPR_2019/papers/Sadeghian_SoPhie_An_Attentive_GAN_for_Predicting_Paths_Compliant_to_Social_CVPR_2019_paper.pdf', proceeding='cvpr-2019-6'), Paper(id='tree-memory-networks-for-modelling-long-term', arxiv_id='1703.04706', nips_id=None, url_abs='http://arxiv.org/abs/1703.04706v2', url_pdf='http://arxiv.org/pdf/1703.04706v2.pdf', title='Tree Memory Networks for Modelling Long-term Temporal Dependencies', abstract='In the domain of sequence modelling, Recurrent Neural Networks (RNN) have\\nbeen capable of achieving impressive results in a variety of application areas\\nincluding visual question answering, part-of-speech tagging and machine\\ntranslation. However this success in modelling short term dependencies has not\\nsuccessfully transitioned to application areas such as trajectory prediction,\\nwhich require capturing both short term and long term relationships. In this\\npaper, we propose a Tree Memory Network (TMN) for modelling long term and short\\nterm relationships in sequence-to-sequence mapping problems. The proposed\\nnetwork architecture is composed of an input module, controller and a memory\\nmodule. In contrast to related literature, which models the memory as a\\nsequence of historical states, we model the memory as a recursive tree\\nstructure. This structure more effectively captures temporal dependencies\\nacross both short term and long term sequences using its hierarchical\\nstructure. We demonstrate the effectiveness and flexibility of the proposed TMN\\nin two practical problems, aircraft trajectory modelling and pedestrian\\ntrajectory modelling in a surveillance setting, and in both cases we outperform\\nthe current state-of-the-art. Furthermore, we perform an in depth analysis on\\nthe evolution of the memory module content over time and provide visual\\nevidence on how the proposed TMN is able to map both long term and short term\\nrelationships efficiently via a hierarchical structure.', authors=['Tharindu Fernando', 'Simon Denman', 'Aaron McFadyen', 'Sridha Sridharan', 'Clinton Fookes'], published=datetime.date(2017, 3, 12), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='convolutional-social-pooling-for-vehicle', arxiv_id='1805.06771', nips_id=None, url_abs='http://arxiv.org/abs/1805.06771v1', url_pdf='http://arxiv.org/pdf/1805.06771v1.pdf', title='Convolutional Social Pooling for Vehicle Trajectory Prediction', abstract=\"Forecasting the motion of surrounding vehicles is a critical ability for an\\nautonomous vehicle deployed in complex traffic. Motion of all vehicles in a\\nscene is governed by the traffic context, i.e., the motion and relative spatial\\nconfiguration of neighboring vehicles. In this paper we propose an LSTM\\nencoder-decoder model that uses convolutional social pooling as an improvement\\nto social pooling layers for robustly learning interdependencies in vehicle\\nmotion. Additionally, our model outputs a multi-modal predictive distribution\\nover future trajectories based on maneuver classes. We evaluate our model using\\nthe publicly available NGSIM US-101 and I-80 datasets. Our results show\\nimprovement over the state of the art in terms of RMS values of prediction\\nerror and negative log-likelihoods of true future trajectories under the\\nmodel's predictive distribution. We also present a qualitative analysis of the\\nmodel's predicted distributions for various traffic scenarios.\", authors=['Nachiket Deo', 'Mohan M. Trivedi'], published=datetime.date(2018, 5, 15), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='multi-modal-trajectory-prediction-of', arxiv_id='1805.05499', nips_id=None, url_abs='http://arxiv.org/abs/1805.05499v1', url_pdf='http://arxiv.org/pdf/1805.05499v1.pdf', title='Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs', abstract='To safely and efficiently navigate through complex traffic scenarios,\\nautonomous vehicles need to have the ability to predict the future motion of\\nsurrounding vehicles. Multiple interacting agents, the multi-modal nature of\\ndriver behavior, and the inherent uncertainty involved in the task make motion\\nprediction of surrounding vehicles a challenging problem. In this paper, we\\npresent an LSTM model for interaction aware motion prediction of surrounding\\nvehicles on freeways. Our model assigns confidence values to maneuvers being\\nperformed by vehicles and outputs a multi-modal distribution over future motion\\nbased on them. We compare our approach with the prior art for vehicle motion\\nprediction on the publicly available NGSIM US-101 and I-80 datasets. Our\\nresults show an improvement in terms of RMS values of prediction error. We also\\npresent an ablative analysis of the components of our proposed model and\\nanalyze the predictions made by the model in complex traffic scenarios.', authors=['Nachiket Deo', 'Mohan M. Trivedi'], published=datetime.date(2018, 5, 15), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='sequence-to-sequence-prediction-of-vehicle', arxiv_id='1802.06338', nips_id=None, url_abs='http://arxiv.org/abs/1802.06338v3', url_pdf='http://arxiv.org/pdf/1802.06338v3.pdf', title='Sequence-to-Sequence Prediction of Vehicle Trajectory via LSTM Encoder-Decoder Architecture', abstract='In this paper, we propose a deep learning based vehicle trajectory prediction\\ntechnique which can generate the future trajectory sequence of surrounding\\nvehicles in real time. We employ the encoder-decoder architecture which\\nanalyzes the pattern underlying in the past trajectory using the long\\nshort-term memory (LSTM) based encoder and generates the future trajectory\\nsequence using the LSTM based decoder. This structure produces the $K$ most\\nlikely trajectory candidates over occupancy grid map by employing the beam\\nsearch technique which keeps the $K$ locally best candidates from the decoder\\noutput. The experiments conducted on highway traffic scenarios show that the\\nprediction accuracy of the proposed method is significantly higher than the\\nconventional trajectory prediction techniques.', authors=['Seong Hyeon Park', 'ByeongDo Kim', 'Chang Mook Kang', 'Chung Choo Chung', 'Jun Won Choi'], published=datetime.date(2018, 2, 18), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='social-gan-socially-acceptable-trajectories', arxiv_id='1803.10892', nips_id=None, url_abs='http://arxiv.org/abs/1803.10892v1', url_pdf='http://arxiv.org/pdf/1803.10892v1.pdf', title='Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks', abstract='Understanding human motion behavior is critical for autonomous moving\\nplatforms (like self-driving cars and social robots) if they are to navigate\\nhuman-centric environments. This is challenging because human motion is\\ninherently multimodal: given a history of human motion paths, there are many\\nsocially plausible ways that people could move in the future. We tackle this\\nproblem by combining tools from sequence prediction and generative adversarial\\nnetworks: a recurrent sequence-to-sequence model observes motion histories and\\npredicts future behavior, using a novel pooling mechanism to aggregate\\ninformation across people. We predict socially plausible futures by training\\nadversarially against a recurrent discriminator, and encourage diverse\\npredictions with a novel variety loss. Through experiments on several datasets\\nwe demonstrate that our approach outperforms prior work in terms of accuracy,\\nvariety, collision avoidance, and computational complexity.', authors=['Agrim Gupta', 'Justin Johnson', 'Li Fei-Fei', 'Silvio Savarese', 'Alexandre Alahi'], published=datetime.date(2018, 3, 29), conference='social-gan-socially-acceptable-trajectories-1', conference_url_abs='http://openaccess.thecvf.com/content_cvpr_2018/html/Gupta_Social_GAN_Socially_CVPR_2018_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_cvpr_2018/papers/Gupta_Social_GAN_Socially_CVPR_2018_paper.pdf', proceeding='cvpr-2018-6'), Paper(id='predicting-future-lane-changes-of-other', arxiv_id='1801.04340', nips_id=None, url_abs='https://arxiv.org/abs/1801.04340v4', url_pdf='https://arxiv.org/pdf/1801.04340v4.pdf', title='Predicting Future Lane Changes of Other Highway Vehicles using RNN-based Deep Models', abstract='In the event of sensor failure, autonomous vehicles need to safely execute emergency maneuvers while avoiding other vehicles on the road. To accomplish this, the sensor-failed vehicle must predict the future semantic behaviors of other drivers, such as lane changes, as well as their future trajectories given a recent window of past sensor observations. We address the first issue of semantic behavior prediction in this paper, which is a precursor to trajectory prediction, by introducing a framework that leverages the power of recurrent neural networks (RNNs) and graphical models. Our goal is to predict the future categorical driving intent, for lane changes, of neighboring vehicles up to three seconds into the future given as little as a one-second window of past LIDAR, GPS, inertial, and map data. We collect real-world data containing over 20 hours of highway driving using an autonomous Toyota vehicle. We propose a composite RNN model by adopting the methodology of Structural Recurrent Neural Networks (RNNs) to learn factor functions and take advantage of both the high-level structure of graphical models and the sequence modeling power of RNNs, which we expect to afford more transparent modeling and activity than opaque, single RNN models. To demonstrate our approach, we validate our model using authentic interstate highway driving to predict the future lane change maneuvers of other vehicles neighboring our autonomous vehicle. We find that our composite Structural RNN outperforms baselines by as much as 12% in balanced accuracy metrics.', authors=['Sajan Patel', 'Brent Griffin', 'Kristofer Kusano', 'Jason J. Corso'], published=datetime.date(2018, 1, 12), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='intentions-of-vulnerable-road-users-detection', arxiv_id='1803.03577', nips_id=None, url_abs='http://arxiv.org/abs/1803.03577v1', url_pdf='http://arxiv.org/pdf/1803.03577v1.pdf', title='Intentions of Vulnerable Road Users - Detection and Forecasting by Means of Machine Learning', abstract='Avoiding collisions with vulnerable road users (VRUs) using sensor-based\\nearly recognition of critical situations is one of the manifold opportunities\\nprovided by the current development in the field of intelligent vehicles. As\\nespecially pedestrians and cyclists are very agile and have a variety of\\nmovement options, modeling their behavior in traffic scenes is a challenging\\ntask. In this article we propose movement models based on machine learning\\nmethods, in particular artificial neural networks, in order to classify the\\ncurrent motion state and to predict the future trajectory of VRUs. Both model\\ntypes are also combined to enable the application of specifically trained\\nmotion predictors based on a continuously updated pseudo probabilistic state\\nclassification. Furthermore, the architecture is used to evaluate\\nmotion-specific physical models for starting and stopping and video-based\\npedestrian motion classification. A comprehensive dataset consisting of 1068\\npedestrian and 494 cyclist scenes acquired at an urban intersection is used for\\noptimization, training, and evaluation of the different models. The results\\nshow substantial higher classification rates and the ability to earlier\\nrecognize motion state changes with the machine learning approaches compared to\\ninteracting multiple model (IMM) Kalman Filtering. The trajectory prediction\\nquality is also improved for all kinds of test scenes, especially when starting\\nand stopping motions are included. Here, 37\\\\% and 41\\\\% lower position errors\\nwere achieved on average, respectively.', authors=['Michael Goldhammer', 'Sebastian Köhler', 'Stefan Zernetsch', 'Konrad Doll', 'Bernhard Sick', 'Klaus Dietmayer'], published=datetime.date(2018, 3, 9), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='tracking-by-prediction-a-deep-generative', arxiv_id='1803.03347', nips_id=None, url_abs='http://arxiv.org/abs/1803.03347v1', url_pdf='http://arxiv.org/pdf/1803.03347v1.pdf', title='Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking', abstract='Current multi-person localisation and tracking systems have an over reliance\\non the use of appearance models for target re-identification and almost no\\napproaches employ a complete deep learning solution for both objectives. We\\npresent a novel, complete deep learning framework for multi-person localisation\\nand tracking. In this context we first introduce a light weight sequential\\nGenerative Adversarial Network architecture for person localisation, which\\novercomes issues related to occlusions and noisy detections, typically found in\\na multi person environment. In the proposed tracking framework we build upon\\nrecent advances in pedestrian trajectory prediction approaches and propose a\\nnovel data association scheme based on predicted trajectories. This removes the\\nneed for computationally expensive person re-identification systems based on\\nappearance features and generates human like trajectories with minimal\\nfragmentation. The proposed method is evaluated on multiple public benchmarks\\nincluding both static and dynamic cameras and is capable of generating\\noutstanding performance, especially among other recently proposed deep neural\\nnetwork based approaches.', authors=['Tharindu Fernando', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes'], published=datetime.date(2018, 3, 9), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='a-machine-learning-approach-to-air-traffic', arxiv_id='1802.06588', nips_id=None, url_abs='http://arxiv.org/abs/1802.06588v2', url_pdf='http://arxiv.org/pdf/1802.06588v2.pdf', title='A Machine Learning Approach to Air Traffic Route Choice Modelling', abstract='Air Traffic Flow and Capacity Management (ATFCM) is one of the constituent\\nparts of Air Traffic Management (ATM). The goal of ATFCM is to make airport and\\nairspace capacity meet traffic demand and, when capacity opportunities are\\nexhausted, optimise traffic flows to meet the available capacity. One of the\\nkey enablers of ATFCM is the accurate estimation of future traffic demand. The\\navailable information (schedules, flight plans, etc.) and its associated level\\nof uncertainty differ across the different ATFCM planning phases, leading to\\nqualitative differences between the types of forecasting that are feasible at\\neach time horizon. While abundant research has been conducted on tactical\\ntrajectory prediction (i.e., during the day of operations), trajectory\\nprediction in the pre-tactical phase, when few or no flight plans are\\navailable, has received much less attention. As a consequence, the methods\\ncurrently in use for pre-tactical traffic forecast are still rather\\nrudimentary, often resulting in suboptimal ATFCM decision making. This paper\\nproposes a machine learning approach for the prediction of airlines route\\nchoices between two airports as a function of route characteristics, such as\\nflight efficiency, air navigation charges and expected level of congestion.\\nDifferent predictive models based on multinomial logistic regression and\\ndecision trees are formulated and calibrated with historical traffic data, and\\na critical evaluation of each model is conducted. We analyse the predictive\\npower of each model in terms of its ability to forecast traffic volumes at the\\nlevel of charging zones, proving significant potential to enhance pre-tactical\\ntraffic forecast. We conclude by discussing the limitations and room for\\nimprovement of the proposed approach, as well as the future developments\\nrequired to produce reliable traffic forecasts at a higher spatial and temporal\\nresolution.', authors=['Rodrigo Marcos', 'Oliva García-Cantú', 'Ricardo Herranz'], published=datetime.date(2018, 2, 19), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='an-lstm-network-for-highway-trajectory', arxiv_id='1801.07962', nips_id=None, url_abs='http://arxiv.org/abs/1801.07962v1', url_pdf='http://arxiv.org/pdf/1801.07962v1.pdf', title='An LSTM Network for Highway Trajectory Prediction', abstract=\"In order to drive safely and efficiently on public roads, autonomous vehicles\\nwill have to understand the intentions of surrounding vehicles, and adapt their\\nown behavior accordingly. If experienced human drivers are generally good at\\ninferring other vehicles' motion up to a few seconds in the future, most\\ncurrent Advanced Driving Assistance Systems (ADAS) are unable to perform such\\nmedium-term forecasts, and are usually limited to high-likelihood situations\\nsuch as emergency braking. In this article, we present a first step towards\\nconsistent trajectory prediction by introducing a long short-term memory (LSTM)\\nneural network, which is capable of accurately predicting future longitudinal\\nand lateral trajectories for vehicles on highway. Unlike previous work focusing\\non a low number of trajectories collected from a few drivers, our network was\\ntrained and validated on the NGSIM US-101 dataset, which contains a total of\\n800 hours of recorded trajectories in various traffic densities, representing\\nmore than 6000 individual drivers.\", authors=['Florent Altché', 'Arnaud de La Fortelle'], published=datetime.date(2018, 1, 24), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='how-would-surround-vehicles-move-a-unified', arxiv_id='1801.06523', nips_id=None, url_abs='http://arxiv.org/abs/1801.06523v1', url_pdf='http://arxiv.org/pdf/1801.06523v1.pdf', title='How would surround vehicles move? A Unified Framework for Maneuver Classification and Motion Prediction', abstract='Reliable prediction of surround vehicle motion is a critical requirement for\\npath planning for autonomous vehicles. In this paper we propose a unified\\nframework for surround vehicle maneuver classification and motion prediction\\nthat exploits multiple cues, namely, the estimated motion of vehicles, an\\nunderstanding of typical motion patterns of freeway traffic and inter-vehicle\\ninteraction. We report our results in terms of maneuver classification accuracy\\nand mean and median absolute error of predicted trajectories against the ground\\ntruth for real traffic data collected using vehicle mounted sensors on\\nfreeways. An ablative analysis is performed to analyze the relative importance\\nof each cue for trajectory prediction. Additionally, an analysis of execution\\ntime for the components of the framework is presented. Finally, we present\\nmultiple case studies analyzing the outputs of our model for complex traffic\\nscenarios', authors=['Nachiket Deo', 'Akshay Rangesh', 'Mohan M. Trivedi'], published=datetime.date(2018, 1, 19), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='long-term-on-board-prediction-of-people-in', arxiv_id='1711.09026', nips_id=None, url_abs='http://arxiv.org/abs/1711.09026v2', url_pdf='http://arxiv.org/pdf/1711.09026v2.pdf', title='Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty', abstract='Progress towards advanced systems for assisted and autonomous driving is\\nleveraging recent advances in recognition and segmentation methods. Yet, we are\\nstill facing challenges in bringing reliable driving to inner cities, as those\\nare composed of highly dynamic scenes observed from a moving platform at\\nconsiderable speeds. Anticipation becomes a key element in order to react\\ntimely and prevent accidents. In this paper we argue that it is necessary to\\npredict at least 1 second and we thus propose a new model that jointly predicts\\nego motion and people trajectories over such large time horizons. We pay\\nparticular attention to modeling the uncertainty of our estimates arising from\\nthe non-deterministic nature of natural traffic scenes. Our experimental\\nresults show that it is indeed possible to predict people trajectories at the\\ndesired time horizons and that our uncertainty estimates are informative of the\\nprediction error. We also show that both sequence modeling of trajectories as\\nwell as our novel method of long term odometry prediction are essential for\\nbest performance.', authors=['Apratim Bhattacharyya', 'Mario Fritz', 'Bernt Schiele'], published=datetime.date(2017, 11, 24), conference='long-term-on-board-prediction-of-people-in-1', conference_url_abs='http://openaccess.thecvf.com/content_cvpr_2018/html/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf', proceeding='cvpr-2018-6'), Paper(id='social-attention-modeling-attention-in-human', arxiv_id='1710.04689', nips_id=None, url_abs='http://arxiv.org/abs/1710.04689v2', url_pdf='http://arxiv.org/pdf/1710.04689v2.pdf', title='Social Attention: Modeling Attention in Human Crowds', abstract='Robots that navigate through human crowds need to be able to plan safe,\\nefficient, and human predictable trajectories. This is a particularly\\nchallenging problem as it requires the robot to predict future human\\ntrajectories within a crowd where everyone implicitly cooperates with each\\nother to avoid collisions. Previous approaches to human trajectory prediction\\nhave modeled the interactions between humans as a function of proximity.\\nHowever, that is not necessarily true as some people in our immediate vicinity\\nmoving in the same direction might not be as important as other people that are\\nfurther away, but that might collide with us in the future. In this work, we\\npropose Social Attention, a novel trajectory prediction model that captures the\\nrelative importance of each person when navigating in the crowd, irrespective\\nof their proximity. We demonstrate the performance of our method against a\\nstate-of-the-art approach on two publicly available crowd datasets and analyze\\nthe trained attention model to gain a better understanding of which surrounding\\nagents humans attend to, when navigating in a crowd.', authors=['Anirudh Vemula', 'Katharina Muelling', 'Jean Oh'], published=datetime.date(2017, 10, 12), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='3dof-pedestrian-trajectory-prediction-learned', arxiv_id='1710.00126', nips_id=None, url_abs='http://arxiv.org/abs/1710.00126v1', url_pdf='http://arxiv.org/pdf/1710.00126v1.pdf', title='3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data', abstract='This paper presents a novel 3DOF pedestrian trajectory prediction approach\\nfor autonomous mobile service robots. While most previously reported methods\\nare based on learning of 2D positions in monocular camera images, our approach\\nuses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D\\nposition plus 1D rotation within the world coordinate system). Our approach,\\nT-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using\\nlong-term data from real-world robot deployments and aims to learn\\ncontext-dependent (environment- and time-specific) human activities. Our\\napproach incorporates long-term temporal information (i.e. date and time) with\\nshort-term pose observations as input. A sequence-to-sequence LSTM\\nencoder-decoder is trained, which encodes observations into LSTM and then\\ndecodes as predictions. For deployment, it can perform on-the-fly prediction in\\nreal-time. Instead of using manually annotated data, we rely on a robust human\\ndetection, tracking and SLAM system, providing us with examples in a global\\ncoordinate system. We validate the approach using more than 15K pedestrian\\ntrajectories recorded in a care home environment over a period of three months.\\nThe experiment shows that the proposed T-Pose-LSTM model advances the\\nstate-of-the-art 2D-based method for human trajectory prediction in long-term\\nmobile robot deployments.', authors=['Li Sun', 'Zhi Yan', 'Sergi Molina Mellado', 'Marc Hanheide', 'Tom Duckett'], published=datetime.date(2017, 9, 30), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='probabilistic-vehicle-trajectory-prediction', arxiv_id='1704.07049', nips_id=None, url_abs='http://arxiv.org/abs/1704.07049v2', url_pdf='http://arxiv.org/pdf/1704.07049v2.pdf', title='Probabilistic Vehicle Trajectory Prediction over Occupancy Grid Map via Recurrent Neural Network', abstract=\"In this paper, we propose an efficient vehicle trajectory prediction\\nframework based on recurrent neural network. Basically, the characteristic of\\nthe vehicle's trajectory is different from that of regular moving objects since\\nit is affected by various latent factors including road structure, traffic\\nrules, and driver's intention. Previous state of the art approaches use\\nsophisticated vehicle behavior model describing these factors and derive the\\ncomplex trajectory prediction algorithm, which requires a system designer to\\nconduct intensive model optimization for practical use. Our approach is\\ndata-driven and simple to use in that it learns complex behavior of the\\nvehicles from the massive amount of trajectory data through deep neural network\\nmodel. The proposed trajectory prediction method employs the recurrent neural\\nnetwork called long short-term memory (LSTM) to analyze the temporal behavior\\nand predict the future coordinate of the surrounding vehicles. The proposed\\nscheme feeds the sequence of vehicles' coordinates obtained from sensor\\nmeasurements to the LSTM and produces the probabilistic information on the\\nfuture location of the vehicles over occupancy grid map. The experiments\\nconducted using the data collected from highway driving show that the proposed\\nmethod can produce reasonably good estimate of future trajectory.\", authors=['ByeoungDo Kim', 'Chang Mook Kang', 'Seung Hi Lee', 'Hyunmin Chae', 'Jaekyum Kim', 'Chung Choo Chung', 'Jun Won Choi'], published=datetime.date(2017, 4, 24), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='applying-deep-bidirectional-lstm-and-mixture', arxiv_id='1708.05824', nips_id=None, url_abs='http://arxiv.org/abs/1708.05824v1', url_pdf='http://arxiv.org/pdf/1708.05824v1.pdf', title='Applying Deep Bidirectional LSTM and Mixture Density Network for Basketball Trajectory Prediction', abstract='Data analytics helps basketball teams to create tactics. However, manual data\\ncollection and analytics are costly and ineffective. Therefore, we applied a\\ndeep bidirectional long short-term memory (BLSTM) and mixture density network\\n(MDN) approach. This model is not only capable of predicting a basketball\\ntrajectory based on real data, but it also can generate new trajectory samples.\\nIt is an excellent application to help coaches and players decide when and\\nwhere to shoot. Its structure is particularly suitable for dealing with time\\nseries problems. BLSTM receives forward and backward information at the same\\ntime, while stacking multiple BLSTMs further increases the learning ability of\\nthe model. Combined with BLSTMs, MDN is used to generate a multi-modal\\ndistribution of outputs. Thus, the proposed model can, in principle, represent\\narbitrary conditional probability distributions of output variables. We tested\\nour model with two experiments on three-pointer datasets from NBA SportVu data.\\nIn the hit-or-miss classification experiment, the proposed model outperformed\\nother models in terms of the convergence speed and accuracy. In the trajectory\\ngeneration experiment, eight model-generated trajectories at a given time\\nclosely matched real trajectories.', authors=['Yu Zhao', 'Rennong Yang', 'Guillaume Chevalier', 'Rajiv Shah', 'Rob Romijnders'], published=datetime.date(2017, 8, 19), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='cognitive-subscore-trajectory-prediction-in', arxiv_id='1706.08491', nips_id=None, url_abs='http://arxiv.org/abs/1706.08491v2', url_pdf='http://arxiv.org/pdf/1706.08491v2.pdf', title=\"Cognitive Subscore Trajectory Prediction in Alzheimer's Disease\", abstract=\"Accurate diagnosis of Alzheimer's Disease (AD) entails clinical evaluation of\\nmultiple cognition metrics and biomarkers. Metrics such as the Alzheimer's\\nDisease Assessment Scale - Cognitive test (ADAS-cog) comprise multiple\\nsubscores that quantify different aspects of a patient's cognitive state such\\nas learning, memory, and language production/comprehension. Although\\ncomputer-aided diagnostic techniques for classification of a patient's current\\ndisease state exist, they provide little insight into the relationship between\\nchanges in brain structure and different aspects of a patient's cognitive state\\nthat occur over time in AD. We have developed a Convolutional Neural Network\\narchitecture that can concurrently predict the trajectories of the 13 subscores\\ncomprised by a subject's ADAS-cog examination results from a current minimally\\npreprocessed structural MRI scan up to 36 months from image acquisition time\\nwithout resorting to manual feature extraction. Mean performance metrics are\\nwithin range of those of existing techniques that require manual feature\\nselection and are limited to predicting aggregate scores.\", authors=['Lev E. Givon', 'Laura J. Mariano', \"David O'Dowd\", 'John M. Irvine', 'Abraham R. Schneider'], published=datetime.date(2017, 6, 26), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='t-conv-a-convolutional-neural-network-for', arxiv_id='1611.07635', nips_id=None, url_abs='http://arxiv.org/abs/1611.07635v3', url_pdf='http://arxiv.org/pdf/1611.07635v3.pdf', title='T-CONV: A Convolutional Neural Network For Multi-scale Taxi Trajectory Prediction', abstract='Precise destination prediction of taxi trajectories can benefit many\\nintelligent location based services such as accurate ad for passengers.\\nTraditional prediction approaches, which treat trajectories as one-dimensional\\nsequences and process them in single scale, fail to capture the diverse\\ntwo-dimensional patterns of trajectories in different spatial scales. In this\\npaper, we propose T-CONV which models trajectories as two-dimensional images,\\nand adopts multi-layer convolutional neural networks to combine multi-scale\\ntrajectory patterns to achieve precise prediction. Furthermore, we conduct\\ngradient analysis to visualize the multi-scale spatial patterns captured by\\nT-CONV and extract the areas with distinct influence on the ultimate\\nprediction. Finally, we integrate multiple local enhancement convolutional\\nfields to explore these important areas deeply for better prediction.\\nComprehensive experiments based on real trajectory data show that T-CONV can\\nachieve higher accuracy than the state-of-the-art methods.', authors=['Jianming Lv', 'Qing Li', 'Xintong Wang'], published=datetime.date(2016, 11, 23), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='human-trajectory-prediction-using-spatially', arxiv_id='1705.09436', nips_id=None, url_abs='http://arxiv.org/abs/1705.09436v1', url_pdf='http://arxiv.org/pdf/1705.09436v1.pdf', title='Human Trajectory Prediction using Spatially aware Deep Attention Models', abstract='Trajectory Prediction of dynamic objects is a widely studied topic in the\\nfield of artificial intelligence. Thanks to a large number of applications like\\npredicting abnormal events, navigation system for the blind, etc. there have\\nbeen many approaches to attempt learning patterns of motion directly from data\\nusing a wide variety of techniques ranging from hand-crafted features to\\nsophisticated deep learning models for unsupervised feature learning. All these\\napproaches have been limited by problems like inefficient features in the case\\nof hand crafted features, large error propagation across the predicted\\ntrajectory and no information of static artefacts around the dynamic moving\\nobjects. We propose an end to end deep learning model to learn the motion\\npatterns of humans using different navigational modes directly from data using\\nthe much popular sequence to sequence model coupled with a soft attention\\nmechanism. We also propose a novel approach to model the static artefacts in a\\nscene and using these to predict the dynamic trajectories. The proposed method,\\ntested on trajectories of pedestrians, consistently outperforms previously\\nproposed state of the art approaches on a variety of large scale data sets. We\\nalso show how our architecture can be naturally extended to handle multiple\\nmodes of movement (say pedestrians, skaters, bikers and buses) simultaneously.', authors=['Daksh Varshneya', 'G. Srinivasaraghavan'], published=datetime.date(2017, 5, 26), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='context-aware-trajectory-prediction', arxiv_id='1705.02503', nips_id=None, url_abs='http://arxiv.org/abs/1705.02503v1', url_pdf='http://arxiv.org/pdf/1705.02503v1.pdf', title='Context-Aware Trajectory Prediction', abstract='Human motion and behaviour in crowded spaces is influenced by several\\nfactors, such as the dynamics of other moving agents in the scene, as well as\\nthe static elements that might be perceived as points of attraction or\\nobstacles. In this work, we present a new model for human trajectory prediction\\nwhich is able to take advantage of both human-human and human-space\\ninteractions. The future trajectory of humans, are generated by observing their\\npast positions and interactions with the surroundings. To this end, we propose\\na \"context-aware\" recurrent neural network LSTM model, which can learn and\\npredict human motion in crowded spaces such as a sidewalk, a museum or a\\nshopping mall. We evaluate our model on a public pedestrian datasets, and we\\ncontribute a new challenging dataset that collects videos of humans that\\nnavigate in a (real) crowded space such as a big museum. Results show that our\\napproach can predict human trajectories better when compared to previous\\nstate-of-the-art forecasting models.', authors=['Federico Bartoli', 'Giuseppe Lisanti', 'Lamberto Ballan', 'Alberto Del Bimbo'], published=datetime.date(2017, 5, 6), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='motion-prediction-under-multimodality-with', arxiv_id='1705.02082', nips_id=None, url_abs='http://arxiv.org/abs/1705.02082v1', url_pdf='http://arxiv.org/pdf/1705.02082v1.pdf', title='Motion Prediction Under Multimodality with Conditional Stochastic Networks', abstract='Given a visual history, multiple future outcomes for a video scene are\\nequally probable, in other words, the distribution of future outcomes has\\nmultiple modes. Multimodality is notoriously hard to handle by standard\\nregressors or classifiers: the former regress to the mean and the latter\\ndiscretize a continuous high dimensional output space. In this work, we present\\nstochastic neural network architectures that handle such multimodality through\\nstochasticity: future trajectories of objects, body joints or frames are\\nrepresented as deep, non-linear transformations of random (as opposed to\\ndeterministic) variables. Such random variables are sampled from simple\\nGaussian distributions whose means and variances are parametrized by the output\\nof convolutional encoders over the visual history. We introduce novel\\nconvolutional architectures for predicting future body joint trajectories that\\noutperform fully connected alternatives \\\\cite{DBLP:journals/corr/WalkerDGH16}.\\nWe introduce stochastic spatial transformers through optical flow warping for\\npredicting future frames, which outperform their deterministic equivalents\\n\\\\cite{DBLP:journals/corr/PatrauceanHC15}. Training stochastic networks involves\\nan intractable marginalization over stochastic variables. We compare various\\ntraining schemes that handle such marginalization through a) straightforward\\nsampling from the prior, b) conditional variational autoencoders\\n\\\\cite{NIPS2015_5775,DBLP:journals/corr/WalkerDGH16}, and, c) a proposed\\nK-best-sample loss that penalizes the best prediction under a fixed \"prediction\\nbudget\". We show experimental results on object trajectory prediction, human\\nbody joint trajectory prediction and video prediction under varying future\\nuncertainty, validating quantitatively and qualitatively our architectural\\nchoices and training schemes.', authors=['Katerina Fragkiadaki', 'Jonathan Huang', 'Alex Alemi', 'Sudheendra Vijayanarasimhan', 'Susanna Ricco', 'Rahul Sukthankar'], published=datetime.date(2017, 5, 5), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='desire-distant-future-prediction-in-dynamic', arxiv_id='1704.04394', nips_id=None, url_abs='http://arxiv.org/abs/1704.04394v1', url_pdf='http://arxiv.org/pdf/1704.04394v1.pdf', title='DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents', abstract='We introduce a Deep Stochastic IOC RNN Encoderdecoder framework, DESIRE, for\\nthe task of future predictions of multiple interacting agents in dynamic\\nscenes. DESIRE effectively predicts future locations of objects in multiple\\nscenes by 1) accounting for the multi-modal nature of the future prediction\\n(i.e., given the same context, future may vary), 2) foreseeing the potential\\nfuture outcomes and make a strategic prediction based on that, and 3) reasoning\\nnot only from the past motion history, but also from the scene context as well\\nas the interactions among the agents. DESIRE achieves these in a single\\nend-to-end trainable neural network model, while being computationally\\nefficient. The model first obtains a diverse set of hypothetical future\\nprediction samples employing a conditional variational autoencoder, which are\\nranked and refined by the following RNN scoring-regression module. Samples are\\nscored by accounting for accumulated future rewards, which enables better\\nlong-term strategic decisions similar to IOC frameworks. An RNN scene context\\nfusion module jointly captures past motion histories, the semantic scene\\ncontext and interactions among multiple agents. A feedback mechanism iterates\\nover the ranking and refinement to further boost the prediction accuracy. We\\nevaluate our model on two publicly available datasets: KITTI and Stanford Drone\\nDataset. Our experiments show that the proposed model significantly improves\\nthe prediction accuracy compared to other baseline methods.', authors=['Namhoon Lee', 'Wongun Choi', 'Paul Vernaza', 'Christopher B. Choy', 'Philip H. S. Torr', 'Manmohan Chandraker'], published=datetime.date(2017, 4, 14), conference='desire-distant-future-prediction-in-dynamic-1', conference_url_abs='http://openaccess.thecvf.com/content_cvpr_2017/html/Lee_DESIRE_Distant_Future_CVPR_2017_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_cvpr_2017/papers/Lee_DESIRE_Distant_Future_CVPR_2017_paper.pdf', proceeding='cvpr-2017-7'), Paper(id='soft-hardwired-attention-an-lstm-framework', arxiv_id='1702.05552', nips_id=None, url_abs='http://arxiv.org/abs/1702.05552v1', url_pdf='http://arxiv.org/pdf/1702.05552v1.pdf', title='Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection', abstract='As humans we possess an intuitive ability for navigation which we master\\nthrough years of practice; however existing approaches to model this trait for\\ndiverse tasks including monitoring pedestrian flow and detecting abnormal\\nevents have been limited by using a variety of hand-crafted features. Recent\\nresearch in the area of deep-learning has demonstrated the power of learning\\nfeatures directly from the data; and related research in recurrent neural\\nnetworks has shown exemplary results in sequence-to-sequence problems such as\\nneural machine translation and neural image caption generation. Motivated by\\nthese approaches, we propose a novel method to predict the future motion of a\\npedestrian given a short history of their, and their neighbours, past\\nbehaviour. The novelty of the proposed method is the combined attention model\\nwhich utilises both \"soft attention\" as well as \"hard-wired\" attention in order\\nto map the trajectory information from the local neighbourhood to the future\\npositions of the pedestrian of interest. We illustrate how a simple\\napproximation of attention weights (i.e hard-wired) can be merged together with\\nsoft attention weights in order to make our model applicable for challenging\\nreal world scenarios with hundreds of neighbours. The navigational capability\\nof the proposed method is tested on two challenging publicly available\\nsurveillance databases where our model outperforms the current-state-of-the-art\\nmethods. Additionally, we illustrate how the proposed architecture can be\\ndirectly applied for the task of abnormal event detection without handcrafting\\nthe features.', authors=['Tharindu Fernando', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes'], published=datetime.date(2017, 2, 18), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='employing-traditional-machine-learning', arxiv_id='1609.00203', nips_id=None, url_abs='http://arxiv.org/abs/1609.00203v1', url_pdf='http://arxiv.org/pdf/1609.00203v1.pdf', title='Employing traditional machine learning algorithms for big data streams analysis: the case of object trajectory prediction', abstract=\"In this paper, we model the trajectory of sea vessels and provide a service\\nthat predicts in near-real time the position of any given vessel in 4', 10',\\n20' and 40' time intervals. We explore the necessary tradeoffs between\\naccuracy, performance and resource utilization are explored given the large\\nvolume and update rates of input data. We start with building models based on\\nwell-established machine learning algorithms using static datasets and\\nmulti-scan training approaches and identify the best candidate to be used in\\nimplementing a single-pass predictive approach, under real-time constraints.\\nThe results are measured in terms of accuracy and performance and are compared\\nagainst the baseline kinematic equations. Results show that it is possible to\\nefficiently model the trajectory of multiple vessels using a single model,\\nwhich is trained and evaluated using an adequately large, static dataset, thus\\nachieving a significant gain in terms of resource usage while not compromising\\naccuracy.\", authors=['Angelos Valsamis', 'Konstantinos Tserpes', 'Dimitrios Zissis', 'Dimosthenis Anagnostopoulos', 'Theodora Varvarigou'], published=datetime.date(2016, 9, 1), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='knowledge-transfer-for-scene-specific-motion', arxiv_id='1603.06987', nips_id=None, url_abs='http://arxiv.org/abs/1603.06987v2', url_pdf='http://arxiv.org/pdf/1603.06987v2.pdf', title='Knowledge Transfer for Scene-specific Motion Prediction', abstract='When given a single frame of the video, humans can not only interpret the\\ncontent of the scene, but also they are able to forecast the near future. This\\nability is mostly driven by their rich prior knowledge about the visual world,\\nboth in terms of (i) the dynamics of moving agents, as well as (ii) the\\nsemantic of the scene. In this work we exploit the interplay between these two\\nkey elements to predict scene-specific motion patterns. First, we extract patch\\ndescriptors encoding the probability of moving to the adjacent patches, and the\\nprobability of being in that particular patch or changing behavior. Then, we\\nintroduce a Dynamic Bayesian Network which exploits this scene specific\\nknowledge for trajectory prediction. Experimental results demonstrate that our\\nmethod is able to accurately predict trajectories and transfer predictions to a\\nnovel scene characterized by similar elements.', authors=['Lamberto Ballan', 'Francesco Castaldo', 'Alexandre Alahi', 'Francesco Palmieri', 'Silvio Savarese'], published=datetime.date(2016, 3, 22), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='modeling-and-inferring-human-intents-and', arxiv_id='1606.07827', nips_id=None, url_abs='http://arxiv.org/abs/1606.07827v1', url_pdf='http://arxiv.org/pdf/1606.07827v1.pdf', title='Modeling and Inferring Human Intents and Latent Functional Objects for Trajectory Prediction', abstract='This paper is about detecting functional objects and inferring human\\nintentions in surveillance videos of public spaces. People in the videos are\\nexpected to intentionally take shortest paths toward functional objects subject\\nto obstacles, where people can satisfy certain needs (e.g., a vending machine\\ncan quench thirst), by following one of three possible intent behaviors: reach\\na single functional object and stop, or sequentially visit several functional\\nobjects, or initially start moving toward one goal but then change the intent\\nto move toward another. Since detecting functional objects in low-resolution\\nsurveillance videos is typically unreliable, we call them \"dark matter\"\\ncharacterized by the functionality to attract people. We formulate the\\nAgent-based Lagrangian Mechanics wherein human trajectories are\\nprobabilistically modeled as motions of agents in many layers of \"dark-energy\"\\nfields, where each agent can select a particular force field to affect its\\nmotions, and thus define the minimum-energy Dijkstra path toward the\\ncorresponding source \"dark matter\". For evaluation, we compiled and annotated a\\nnew dataset. The results demonstrate our effectiveness in predicting human\\nintent behaviors and trajectories, and localizing functional objects, as well\\nas discovering distinct functional classes of objects by clustering human\\nmotion behavior in the vicinity of functional objects.', authors=['Dan Xie', 'Tianmin Shu', 'Sinisa Todorovic', 'Song-Chun Zhu'], published=datetime.date(2016, 6, 24), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='tribeflow-mining-predicting-user-trajectories', arxiv_id='1511.01032', nips_id=None, url_abs='http://arxiv.org/abs/1511.01032v2', url_pdf='http://arxiv.org/pdf/1511.01032v2.pdf', title='TribeFlow: Mining & Predicting User Trajectories', abstract='Which song will Smith listen to next? Which restaurant will Alice go to\\ntomorrow? Which product will John click next? These applications have in common\\nthe prediction of user trajectories that are in a constant state of flux over a\\nhidden network (e.g. website links, geographic location). What users are doing\\nnow may be unrelated to what they will be doing in an hour from now. Mindful of\\nthese challenges we propose TribeFlow, a method designed to cope with the\\ncomplex challenges of learning personalized predictive models of\\nnon-stationary, transient, and time-heterogeneous user trajectories. TribeFlow\\nis a general method that can perform next product recommendation, next song\\nrecommendation, next location prediction, and general arbitrary-length user\\ntrajectory prediction without domain-specific knowledge. TribeFlow is more\\naccurate and up to 413x faster than top competitors.', authors=['Flavio Figueiredo', 'Bruno Ribeiro', 'Jussara Almeida', 'Christos Faloutsos'], published=datetime.date(2015, 11, 3), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='forecasting-social-navigation-in-crowded', arxiv_id='1601.00998', nips_id=None, url_abs='http://arxiv.org/abs/1601.00998v1', url_pdf='http://arxiv.org/pdf/1601.00998v1.pdf', title='Forecasting Social Navigation in Crowded Complex Scenes', abstract='When humans navigate a crowed space such as a university campus or the\\nsidewalks of a busy street, they follow common sense rules based on social\\netiquette. In this paper, we argue that in order to enable the design of new\\nalgorithms that can take fully advantage of these rules to better solve tasks\\nsuch as target tracking or trajectory forecasting, we need to have access to\\nbetter data in the first place. To that end, we contribute the very first large\\nscale dataset (to the best of our knowledge) that collects images and videos of\\nvarious types of targets (not just pedestrians, but also bikers, skateboarders,\\ncars, buses, golf carts) that navigate in a real-world outdoor environment such\\nas a university campus. We present an extensive evaluation where different\\nmethods for trajectory forecasting are evaluated and compared. Moreover, we\\npresent a new algorithm for trajectory prediction that exploits the complexity\\nof our new dataset and allows to: i) incorporate inter-class interactions into\\ntrajectory prediction models (e.g, pedestrian vs bike) as opposed to just\\nintra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree\\nto which the social forces are regulating an interaction. We call the latter\\n\"social sensitivity\"and it captures the sensitivity to which a target is\\nresponding to a certain interaction. An extensive experimental evaluation\\ndemonstrates the effectiveness of our novel approach.', authors=['Alexandre Robicquet', 'Alexandre Alahi', 'Amir Sadeghian', 'Bryan Anenberg', 'John Doherty', 'Eli Wu', 'Silvio Savarese'], published=datetime.date(2016, 1, 5), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='multiobjective-tactical-planning-under', arxiv_id='1309.4085', nips_id=None, url_abs='http://arxiv.org/abs/1309.4085v1', url_pdf='http://arxiv.org/pdf/1309.4085v1.pdf', title='Multiobjective Tactical Planning under Uncertainty for Air Traffic Flow and Capacity Management', abstract='We investigate a method to deal with congestion of sectors and delays in the\\ntactical phase of air traffic flow and capacity management. It relies on\\ntemporal objectives given for every point of the flight plans and shared among\\nthe controllers in order to create a collaborative environment. This would\\nenhance the transition from the network view of the flow management to the\\nlocal view of air traffic control. Uncertainty is modeled at the trajectory\\nlevel with temporal information on the boundary points of the crossed sectors\\nand then, we infer the probabilistic occupancy count. Therefore, we can model\\nthe accuracy of the trajectory prediction in the optimization process in order\\nto fix some safety margins. On the one hand, more accurate is our prediction;\\nmore efficient will be the proposed solutions, because of the tighter safety\\nmargins. On the other hand, when uncertainty is not negligible, the proposed\\nsolutions will be more robust to disruptions. Furthermore, a multiobjective\\nalgorithm is used to find the tradeoff between the delays and congestion, which\\nare antagonist in airspace with high traffic density. The flow management\\nposition can choose manually, or automatically with a preference-based\\nalgorithm, the adequate solution. This method is tested against two instances,\\none with 10 flights and 5 sectors and one with 300 flights and 16 sectors.', authors=['Gaétan Marceau', 'Pierre Savéant', 'Marc Schoenauer'], published=datetime.date(2013, 9, 16), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='learning-policies-for-contextual-submodular', arxiv_id='1305.2532', nips_id=None, url_abs='http://arxiv.org/abs/1305.2532v1', url_pdf='http://arxiv.org/pdf/1305.2532v1.pdf', title='Learning Policies for Contextual Submodular Prediction', abstract='Many prediction domains, such as ad placement, recommendation, trajectory\\nprediction, and document summarization, require predicting a set or list of\\noptions. Such lists are often evaluated using submodular reward functions that\\nmeasure both quality and diversity. We propose a simple, efficient, and\\nprovably near-optimal approach to optimizing such prediction problems based on\\nno-regret learning. Our method leverages a surprising result from online\\nsubmodular optimization: a single no-regret online learner can compete with an\\noptimal sequence of predictions. Compared to previous work, which either learn\\na sequence of classifiers or rely on stronger assumptions such as\\nrealizability, we ensure both data-efficiency as well as performance guarantees\\nin the fully agnostic setting. Experiments validate the efficiency and\\napplicability of the approach on a wide range of problems including manipulator\\ntrajectory optimization, news recommendation and document summarization.', authors=['Stephane Ross', 'Jiaji Zhou', 'Yisong Yue', 'Debadeepta Dey', 'J. Andrew Bagnell'], published=datetime.date(2013, 5, 11), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='moving-objects-analytics-survey-on-future', arxiv_id='1807.04639', nips_id=None, url_abs='http://arxiv.org/abs/1807.04639v1', url_pdf='http://arxiv.org/pdf/1807.04639v1.pdf', title='Moving Objects Analytics: Survey on Future Location & Trajectory Prediction Methods', abstract='The tremendous growth of positioning technologies and GPS enabled devices has\\nproduced huge volumes of tracking data during the recent years. This source of\\ninformation constitutes a rich input for data analytics processes, either\\noffline (e.g. cluster analysis, hot motion discovery) or online (e.g.\\nshort-term forecasting of forthcoming positions). This paper focuses on\\npredictive analytics for moving objects (could be pedestrians, cars, vessels,\\nplanes, animals, etc.) and surveys the state-of-the-art in the context of\\nfuture location and trajectory prediction. We provide an extensive review of\\nover 50 works, also proposing a novel taxonomy of predictive algorithms over\\nmoving objects. We also list the properties of several real datasets used in\\nthe past for validation purposes of those works and, motivated by this, we\\ndiscuss challenges that arise in the transition from conventional to Big Data\\napplications.\\n  CCS Concepts: Information systems > Spatial-temporal systems; Information\\nsystems > Data analytics; Information systems > Data mining; Computing\\nmethodologies > Machine learning Additional Key Words and Phrases: mobility\\ndata, moving object trajectories, trajectory prediction, future location\\nprediction.', authors=['Harris Georgiou', 'Sophia Karagiorgou', 'Yannis Kontoulis', 'Nikos Pelekis', 'Petros Petrou', 'David Scarlatti', 'Yannis Theodoridis'], published=datetime.date(2018, 7, 11), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='pedestrian-trajectory-prediction-with', arxiv_id='1807.08381', nips_id=None, url_abs='http://arxiv.org/abs/1807.08381v1', url_pdf='http://arxiv.org/pdf/1807.08381v1.pdf', title='Pedestrian Trajectory Prediction with Structured Memory Hierarchies', abstract='This paper presents a novel framework for human trajectory prediction based\\non multimodal data (video and radar). Motivated by recent neuroscience\\ndiscoveries, we propose incorporating a structured memory component in the\\nhuman trajectory prediction pipeline to capture historical information to\\nimprove performance. We introduce structured LSTM cells for modelling the\\nmemory content hierarchically, preserving the spatiotemporal structure of the\\ninformation and enabling us to capture both short-term and long-term context.\\nWe demonstrate how this architecture can be extended to integrate salient\\ninformation from multiple modalities to automatically store and retrieve\\nimportant information for decision making without any supervision. We evaluate\\nthe effectiveness of the proposed models on a novel multimodal dataset that we\\nintroduce, consisting of 40,000 pedestrian trajectories, acquired jointly from\\na radar system and a CCTV camera system installed in a public place. The\\nperformance is also evaluated on the publicly available New York Grand Central\\npedestrian database. In both settings, the proposed models demonstrate their\\ncapability to better anticipate future pedestrian motion compared to existing\\nstate of the art.', authors=['Tharindu Fernando', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes'], published=datetime.date(2018, 7, 22), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='naturalistic-driver-intention-and-path', arxiv_id='1807.09995', nips_id=None, url_abs='http://arxiv.org/abs/1807.09995v1', url_pdf='http://arxiv.org/pdf/1807.09995v1.pdf', title='Naturalistic Driver Intention and Path Prediction using Recurrent Neural Networks', abstract=\"Understanding the intentions of drivers at intersections is a critical\\ncomponent for autonomous vehicles. Urban intersections that do not have traffic\\nsignals are a common epicentre of highly variable vehicle movement and\\ninteractions. We present a method for predicting driver intent at urban\\nintersections through multi-modal trajectory prediction with uncertainty. Our\\nmethod is based on recurrent neural networks combined with a mixture density\\nnetwork output layer. To consolidate the multi-modal nature of the output\\nprobability distribution, we introduce a clustering algorithm that extracts the\\nset of possible paths that exist in the prediction output, and ranks them\\naccording to likelihood. To verify the method's performance and\\ngeneralizability, we present a real-world dataset that consists of over 23,000\\nvehicles traversing five different intersections, collected using a vehicle\\nmounted Lidar based tracking system. An array of metrics is used to demonstrate\\nthe performance of the model against several baselines.\", authors=['Alex Zyner', 'Stewart Worrall', 'Eduardo Nebot'], published=datetime.date(2018, 7, 26), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='a-learning-based-framework-for-two', arxiv_id='1808.00516', nips_id=None, url_abs='http://arxiv.org/abs/1808.00516v1', url_pdf='http://arxiv.org/pdf/1808.00516v1.pdf', title='A Learning-Based Framework for Two-Dimensional Vehicle Maneuver Prediction over V2V Networks', abstract=\"Situational awareness in vehicular networks could be substantially improved\\nutilizing reliable trajectory prediction methods. More precise situational\\nawareness, in turn, results in notably better performance of critical safety\\napplications, such as Forward Collision Warning (FCW), as well as comfort\\napplications like Cooperative Adaptive Cruise Control (CACC). Therefore,\\nvehicle trajectory prediction problem needs to be deeply investigated in order\\nto come up with an end to end framework with enough precision required by the\\nsafety applications' controllers. This problem has been tackled in the\\nliterature using different methods. However, machine learning, which is a\\npromising and emerging field with remarkable potential for time series\\nprediction, has not been explored enough for this purpose. In this paper, a\\ntwo-layer neural network-based system is developed which predicts the future\\nvalues of vehicle parameters, such as velocity, acceleration, and yaw rate, in\\nthe first layer and then predicts the two-dimensional, i.e. longitudinal and\\nlateral, trajectory points based on the first layer's outputs. The performance\\nof the proposed framework has been evaluated in realistic cut-in scenarios from\\nSafety Pilot Model Deployment (SPMD) dataset and the results show a noticeable\\nimprovement in the prediction accuracy in comparison with the kinematics model\\nwhich is the dominant employed model by the automotive industry. Both ideal and\\nnonideal communication circumstances have been investigated for our system\\nevaluation. For non-ideal case, an estimation step is included in the framework\\nbefore the parameter prediction block to handle the drawbacks of packet drops\\nor sensor failures and reconstruct the time series of vehicle parameters at a\\ndesirable frequency.\", authors=['Hossein Nourkhiz Mahjoub', 'Amin Tahmasbi-Sarvestani', 'Hadi Kazemi', 'Yaser P. Fallah'], published=datetime.date(2018, 8, 1), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='scene-lstm-a-model-for-human-trajectory', arxiv_id='1808.04018', nips_id=None, url_abs='http://arxiv.org/abs/1808.04018v2', url_pdf='http://arxiv.org/pdf/1808.04018v2.pdf', title='Scene-LSTM: A Model for Human Trajectory Prediction', abstract=\"We develop a human movement trajectory prediction system that incorporates\\nthe scene information (Scene-LSTM) as well as human movement trajectories\\n(Pedestrian movement LSTM) in the prediction process within static crowded\\nscenes. We superimpose a two-level grid structure (scene is divided into grid\\ncells each modeled by a scene-LSTM, which are further divided into smaller\\nsub-grids for finer spatial granularity) and explore common human trajectories\\noccurring in the grid cell (e.g., making a right or left turn onto sidewalks\\ncoming out of an alley; or standing still at bus/train stops). Two coupled LSTM\\nnetworks, Pedestrian movement LSTMs (one per target) and the corresponding\\nScene-LSTMs (one per grid-cell) are trained simultaneously to predict the next\\nmovements. We show that such common path information greatly influences\\nprediction of future movement. We further design a scene data filter that holds\\nimportant non-linear movement information. The scene data filter allows us to\\nselect the relevant parts of the information from the grid cell's memory\\nrelative to a target's state. We evaluate and compare two versions of our\\nmethod with the Linear and several existing LSTM-based methods on five crowded\\nvideo sequences from the UCY [1] and ETH [2] datasets. The results show that\\nour method reduces the location displacement errors compared to related methods\\nand specifically about 80% reduction compared to social interaction methods.\", authors=['Huynh Manh', 'Gita Alaghband'], published=datetime.date(2018, 8, 12), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='convolutional-neural-network-for-trajectory', arxiv_id='1809.00696', nips_id=None, url_abs='http://arxiv.org/abs/1809.00696v2', url_pdf='http://arxiv.org/pdf/1809.00696v2.pdf', title='Convolutional Neural Network for Trajectory Prediction', abstract='Predicting trajectories of pedestrians is quintessential for autonomous\\nrobots which share the same environment with humans. In order to effectively\\nand safely interact with humans, trajectory prediction needs to be both precise\\nand computationally efficient. In this work, we propose a convolutional neural\\nnetwork (CNN) based human trajectory prediction approach. Unlike more recent\\nLSTM-based moles which attend sequentially to each frame, our model supports\\nincreased parallelism and effective temporal representation. The proposed\\ncompact CNN model is faster than the current approaches yet still yields\\ncompetitive results.', authors=['Nishant Nikhil', 'Brendan Tran Morris'], published=datetime.date(2018, 9, 3), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='socially-aware-kalman-neural-networks-for', arxiv_id='1809.05408', nips_id=None, url_abs='https://arxiv.org/abs/1809.05408v4', url_pdf='https://arxiv.org/pdf/1809.05408v4.pdf', title='Socially Aware Kalman Neural Networks for Trajectory Prediction', abstract='Trajectory prediction is a critical technique in the navigation of robots and autonomous vehicles. However, the complex traffic and dynamic uncertainties yield challenges in the effectiveness and robustness in modeling. We purpose a data-driven approach socially aware Kalman neural networks (SAKNN) where the interaction layer and the Kalman layer are embedded in the architecture, resulting in a class of architectures with huge potential to directly learn from high variance sensor input and robustly generate low variance outcomes. The evaluation of our approach on NGSIM dataset demonstrates that SAKNN performs state-of-the-art on prediction effectiveness in a relatively long-term horizon and significantly improves the signal-to-noise ratio of the predicted signal.', authors=['Ce Ju', 'Zheng Wang', 'Xiaoyu Zhang'], published=datetime.date(2018, 9, 14), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='classify-predict-detect-anticipate-and', arxiv_id='1809.08875', nips_id=None, url_abs='http://arxiv.org/abs/1809.08875v3', url_pdf='http://arxiv.org/pdf/1809.08875v3.pdf', title='A Probabilistic Semi-Supervised Approach to Multi-Task Human Activity Modeling', abstract='Human behavior is a continuous stochastic spatio-temporal process which is\\ngoverned by semantic actions and affordances as well as latent factors.\\nTherefore, video-based human activity modeling is concerned with a number of\\ntasks such as inferring current and future semantic labels, predicting future\\ncontinuous observations as well as imagining possible future label and feature\\nsequences. In this paper we present a semi-supervised probabilistic deep latent\\nvariable model that can represent both discrete labels and continuous\\nobservations as well as latent dynamics over time. This allows the model to\\nsolve several tasks at once without explicit fine-tuning. We focus here on the\\ntasks of action classification, detection, prediction and anticipation as well\\nas motion prediction and synthesis based on 3D human activity data recorded\\nwith Kinect. We further extend the model to capture hierarchical label\\nstructure and to model the dependencies between multiple entities, such as a\\nhuman and objects. Our experiments demonstrate that our principled approach to\\nhuman activity modeling can be used to detect current and anticipate future\\nsemantic labels and to predict and synthesize future label and feature\\nsequences. When comparing our model to state-of-the-art approaches, which are\\nspecifically designed for e.g. action classification, we find that our\\nprobabilistic formulation outperforms or is comparable to these task specific\\nmodels.', authors=['Judith Bütepage', 'Hedvig Kjellström', 'Danica Kragic'], published=datetime.date(2018, 9, 24), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='modeling-multimodal-dynamic-spatiotemporal', arxiv_id='1810.05993', nips_id=None, url_abs='https://arxiv.org/abs/1810.05993v3', url_pdf='https://arxiv.org/pdf/1810.05993v3.pdf', title='The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs', abstract='Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.', authors=['Boris Ivanovic', 'Marco Pavone'], published=datetime.date(2018, 10, 14), conference='the-trajectron-probabilistic-multi-agent', conference_url_abs='http://openaccess.thecvf.com/content_ICCV_2019/html/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf', proceeding='iccv-2019-10'), Paper(id='multiple-interactions-made-easy-mime-large', arxiv_id='1810.07121', nips_id=None, url_abs='http://arxiv.org/abs/1810.07121v1', url_pdf='http://arxiv.org/pdf/1810.07121v1.pdf', title='Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data for Imitation', abstract='In recent years, we have seen an emergence of data-driven approaches in\\nrobotics. However, most existing efforts and datasets are either in simulation\\nor focus on a single task in isolation such as grasping, pushing or poking. In\\norder to make progress and capture the space of manipulation, we would need to\\ncollect a large-scale dataset of diverse tasks such as pouring, opening\\nbottles, stacking objects etc. But how does one collect such a dataset? In this\\npaper, we present the largest available robotic-demonstration dataset (MIME)\\nthat contains 8260 human-robot demonstrations over 20 different robotic tasks\\n(https://sites.google.com/view/mimedataset). These tasks range from the simple\\ntask of pushing objects to the difficult task of stacking household objects.\\nOur dataset consists of videos of human demonstrations and kinesthetic\\ntrajectories of robot demonstrations. We also propose to use this dataset for\\nthe task of mapping 3rd person video features to robot trajectories.\\nFurthermore, we present two different approaches using this dataset and\\nevaluate the predicted robot trajectories against ground-truth trajectories. We\\nhope our dataset inspires research in multiple areas including visual\\nimitation, trajectory prediction, and multi-task robotic learning.', authors=['Pratyusha Sharma', 'Lekha Mohan', 'Lerrel Pinto', 'Abhinav Gupta'], published=datetime.date(2018, 10, 16), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='integrating-kinematics-and-environment', arxiv_id='1810.07225', nips_id=None, url_abs='http://arxiv.org/abs/1810.07225v1', url_pdf='http://arxiv.org/pdf/1810.07225v1.pdf', title='Integrating kinematics and environment context into deep inverse reinforcement learning for predicting off-road vehicle trajectories', abstract=\"Predicting the motion of a mobile agent from a third-person perspective is an\\nimportant component for many robotics applications, such as autonomous\\nnavigation and tracking. With accurate motion prediction of other agents,\\nrobots can plan for more intelligent behaviors to achieve specified objectives,\\ninstead of acting in a purely reactive way. Previous work addresses motion\\nprediction by either only filtering kinematics, or using hand-designed and\\nlearned representations of the environment. Instead of separating kinematic and\\nenvironmental context, we propose a novel approach to integrate both into an\\ninverse reinforcement learning (IRL) framework for trajectory prediction.\\nInstead of exponentially increasing the state-space complexity with kinematics,\\nwe propose a two-stage neural network architecture that considers motion and\\nenvironment together to recover the reward function. The first-stage network\\nlearns feature representations of the environment using low-level LiDAR\\nstatistics and the second-stage network combines those learned features with\\nkinematics data. We collected over 30 km of off-road driving data and validated\\nexperimentally that our method can effectively extract useful environmental and\\nkinematic features. We generate accurate predictions of the distribution of\\nfuture trajectories of the vehicle, encoding complex behaviors such as\\nmulti-modal distributions at road intersections, and even show different\\npredictions at the same intersection depending on the vehicle's speed.\", authors=['Yanfu Zhang', 'Wenshan Wang', 'Rogerio Bonatti', 'Daniel Maturana', 'Sebastian Scherer'], published=datetime.date(2018, 10, 16), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='trafficpredict-trajectory-prediction-for', arxiv_id='1811.02146', nips_id=None, url_abs='http://arxiv.org/abs/1811.02146v5', url_pdf='http://arxiv.org/pdf/1811.02146v5.pdf', title='TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents', abstract=\"To safely and efficiently navigate in complex urban traffic, autonomous\\nvehicles must make responsible predictions in relation to surrounding\\ntraffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and\\ncritical task is to explore the movement patterns of different traffic-agents\\nand predict their future trajectories accurately to help the autonomous vehicle\\nmake reasonable navigation decision. To solve this problem, we propose a long\\nshort-term memory-based (LSTM-based) realtime traffic prediction algorithm,\\nTrafficPredict. Our approach uses an instance layer to learn instances'\\nmovements and interactions and has a category layer to learn the similarities\\nof instances belonging to the same type to refine the prediction. In order to\\nevaluate its performance, we collected trajectory datasets in a large city\\nconsisting of varying conditions and traffic densities. The dataset includes\\nmany challenging scenarios where vehicles, bicycles, and pedestrians move among\\none another. We evaluate the performance of TrafficPredict on our new dataset\\nand highlight its higher accuracy for trajectory prediction by comparing with\\nprior prediction methods.\", authors=['Yuexin Ma', 'Xinge Zhu', 'Sibo Zhang', 'Ruigang Yang', 'Wenping Wang', 'Dinesh Manocha'], published=datetime.date(2018, 11, 6), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='joint-monocular-3d-vehicle-detection-and', arxiv_id='1811.10742', nips_id=None, url_abs='https://arxiv.org/abs/1811.10742v3', url_pdf='https://arxiv.org/pdf/1811.10742v3.pdf', title='Joint Monocular 3D Vehicle Detection and Tracking', abstract='Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.', authors=['Hou-Ning Hu', 'Qi-Zhi Cai', 'Dequan Wang', 'Ji Lin', 'Min Sun', 'Philipp Krähenbühl', 'Trevor Darrell', 'Fisher Yu'], published=datetime.date(2018, 11, 26), conference='joint-monocular-3d-vehicle-detection-and-1', conference_url_abs='http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.html', conference_url_pdf='http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.pdf', proceeding='iccv-2019-10'), Paper(id='attention-based-recurrent-neural-network-for', arxiv_id='1812.07151', nips_id=None, url_abs='https://arxiv.org/abs/1812.07151v2', url_pdf='https://arxiv.org/pdf/1812.07151v2.pdf', title='Attention-based Recurrent Neural Network for Urban Vehicle Trajectory Prediction', abstract='With the increasing deployment of diverse positioning devices and location-based services, a huge amount of spatial and temporal information has been collected and accumulated as trajectory data. Among many applications, trajectory-based location prediction is gaining increasing attention because of its potential to improve the performance of many applications in multiple domains. This research focuses on trajectory sequence prediction methods using trajectory data obtained from the vehicles in urban traffic network. As Recurrent Neural Network(RNN) model is previously proposed, we propose an improved method of Attention-based Recurrent Neural Network model(ARNN) for urban vehicle trajectory prediction. We introduce attention mechanism into urban vehicle trajectory prediction to explain the impact of network-level traffic state information. The model is evaluated using the Bluetooth data of private vehicles collected in Brisbane, Australia with 5 metrics which are widely used in the sequence modeling. The proposed ARNN model shows significant performance improvement compared to the existing RNN models considering not only the cells to be visited but also the alignment of the cells in sequence.', authors=['Seongjin Choi', 'Jiwon Kim', 'Hwasoo Yeo'], published=datetime.date(2018, 12, 18), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='gd-gan-generative-adversarial-networks-for', arxiv_id='1812.07667', nips_id=None, url_abs='http://arxiv.org/abs/1812.07667v1', url_pdf='http://arxiv.org/pdf/1812.07667v1.pdf', title='GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds', abstract=\"This paper presents a novel deep learning framework for human trajectory\\nprediction and detecting social group membership in crowds. We introduce a\\ngenerative adversarial pipeline which preserves the spatio-temporal structure\\nof the pedestrian's neighbourhood, enabling us to extract relevant attributes\\ndescribing their social identity. We formulate the group detection task as an\\nunsupervised learning problem, obviating the need for supervised learning of\\ngroup memberships via hand labeled databases, allowing us to directly employ\\nthe proposed framework in different surveillance settings. We evaluate the\\nproposed trajectory prediction and group detection frameworks on multiple\\npublic benchmarks, and for both tasks the proposed method demonstrates its\\ncapability to better anticipate human sociological behaviour compared to the\\nexisting state-of-the-art methods.\", authors=['Tharindu Fernando', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes'], published=datetime.date(2018, 12, 18), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding=None), Paper(id='encoding-crowd-interaction-with-deep-neural', arxiv_id=None, nips_id=None, url_abs='http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.html', url_pdf='http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf', title='Encoding Crowd Interaction With Deep Neural Network for Pedestrian Trajectory Prediction', abstract='Pedestrian trajectory prediction is a challenging task because of the complex nature of humans. In this paper, we tackle the problem within a deep learning framework by considering motion information of each pedestrian and its interaction with the crowd. Specifically, motivated by the residual learning in deep learning, we propose to predict displacement between neighboring frames for each pedestrian sequentially. To predict such displacement, we design a crowd interaction deep neural network (CIDNN) which considers the different importance of different pedestrians for the displacement prediction of a target pedestrian. Specifically, we use an LSTM to model motion information for all pedestrians and use a multi-layer perceptron to map the location of each pedestrian to a high dimensional feature space where the inner product between features is used as a measurement for the spatial affinity between two pedestrians. Then we weight the motion features of all pedestrians based on their spatial affinity to the target pedestrian for location displacement prediction. Extensive experiments on publicly available datasets validate the effectiveness of our method for trajectory prediction.', authors=['Yanyu Xu', 'Zhixin Piao', 'Shenghua Gao'], published=datetime.date(2018, 6, 1), conference=None, conference_url_abs=None, conference_url_pdf=None, proceeding='cvpr-2018-6')])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.task_paper_list('trajectory-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = papers_with_repo_with_biggest_tasks_df['paper_url'].iloc[1].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcnn-attention-based-convolutional-neural'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'abcnn-attention-based-convolutional-neural',\n",
       " 'arxiv_id': '1512.05193',\n",
       " 'nips_id': None,\n",
       " 'url_abs': 'http://arxiv.org/abs/1512.05193v4',\n",
       " 'url_pdf': 'http://arxiv.org/pdf/1512.05193v4.pdf',\n",
       " 'title': 'ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs',\n",
       " 'abstract': \"How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with one individual task by fine-tuning a\\nspecific system; (ii) models each sentence's representation separately, rarely\\nconsidering the impact of the other sentence; or (iii) relies fully on manually\\ndesigned, task-specific linguistic features. This work presents a general\\nAttention Based Convolutional Neural Network (ABCNN) for modeling a pair of\\nsentences. We make three contributions. (i) ABCNN can be applied to a wide\\nvariety of tasks that require modeling of sentence pairs. (ii) We propose three\\nattention schemes that integrate mutual influence between sentences into CNN;\\nthus, the representation of each sentence takes into consideration its\\ncounterpart. These interdependent sentence pair representations are more\\npowerful than isolated sentence representations. (iii) ABCNN achieves\\nstate-of-the-art performance on AS, PI and TE tasks.\",\n",
       " 'authors': ['Wenpeng Yin', 'Hinrich Schütze', 'Bing Xiang', 'Bowen Zhou'],\n",
       " 'published': datetime.date(2015, 12, 16),\n",
       " 'conference': 'abcnn-attention-based-convolutional-neural-1',\n",
       " 'conference_url_abs': 'https://www.aclweb.org/anthology/Q16-1019/',\n",
       " 'conference_url_pdf': 'https://www.aclweb.org/anthology/Q16-1019',\n",
       " 'proceeding': 'tacl-2016-1'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(client.paper_get(paper_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcnn-attention-based-convolutional-neural'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = client.http.get(f\"/papers/{paper_id}/tasks/\")['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(id='natural-language-inference', name='Natural Language Inference', description='Natural language inference is the task of determining whether a \"hypothesis\" is \\r\\ntrue (entailment), false (contradiction), or undetermined (neutral) given a \"premise\".\\r\\n\\r\\nExample:\\r\\n\\r\\n| Premise | Label | Hypothesis |\\r\\n| --- | ---| --- |\\r\\n| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\\r\\n| An older and younger man smiling. | neutral  | Two men are smiling and laughing at the cats playing on the floor. |\\r\\n| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |'),\n",
       " Task(id='paraphrase-identification', name='Paraphrase Identification', description='The goal of **Paraphrase Identification** is to determine whether a pair of sentences have the same meaning.\\r\\n\\r\\n\\r\\n<span class=\"description-source\">Source: [Adversarial Examples with Difficult Common Words for Paraphrase Identification ](https://arxiv.org/abs/1909.02560)</span>\\r\\n\\r\\nImage source: [On Paraphrase Identification Corpora ](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1000_Paper.pdf)'),\n",
       " Task(id='answer-selection', name='Answer Selection', description='**Answer Selection** is the task of identifying the correct answer to a question from a pool of candidate answers. This task can be formulated as a classification or a ranking problem.\\n\\n\\n<span class=\"description-source\">Source: [Learning Analogy-Preserving Sentence Embeddings for Answer Selection ](https://arxiv.org/abs/1910.05315)</span>')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paperswithcode.models.Task(**task) for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial : 9\n",
      "audio : 30\n",
      "computer-code : 35\n",
      "computer-vision : 500\n",
      "graphs : 52\n",
      "knowledge-base : 22\n",
      "medical : 183\n",
      "methodology : 138\n",
      "miscellaneous : 124\n",
      "music : 17\n",
      "natural-language-processing : 347\n",
      "playing-games : 39\n",
      "reasoning : 15\n",
      "robots : 27\n",
      "speech : 51\n",
      "time-series : 52\n",
      "total tasks: 1641\n"
     ]
    }
   ],
   "source": [
    "import paperswithcode\n",
    "\n",
    "client = paperswithcode.PapersWithCodeClient()\n",
    "areas = client.area_list().results\n",
    "s = 0\n",
    "\n",
    "area_grouped_tasks = {}\n",
    "\n",
    "for a in areas:\n",
    "    area_tasks = [t.id for t in client.area_task_list(a.id, items_per_page=1000).results]\n",
    "    area_grouped_tasks[a.id] = area_tasks\n",
    "    n_tasks_per_area = len(area_tasks)\n",
    "    print(a.id, ':', n_tasks_per_area)\n",
    "    s += n_tasks_per_area\n",
    "print('total tasks:', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_tasks_df = pd.DataFrame({'area': area_grouped_tasks.keys(), 'task': area_grouped_tasks.values()}).explode('task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_df['task'] = papers_with_repo_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_task_exploded_df = papers_with_repo_df.explode('task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_api_normalized = papers_task_exploded_df['task'].str.lower().str.replace(' ', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         fairness\n",
       "1                information-plane\n",
       "2                information-plane\n",
       "3                 answer-selection\n",
       "3       natural-language-inference\n",
       "                   ...            \n",
       "6604                misinformation\n",
       "6605              voice-conversion\n",
       "6606              voice-conversion\n",
       "6607                style-transfer\n",
       "6608                style-transfer\n",
       "Name: task, Length: 13636, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_api_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_without_area = task_api_normalized[~task_api_normalized.isin(area_tasks_df['task'])].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tasks_df = pd.DataFrame({\"area\": \"miscellaneous\", \"task\": tasks_without_area})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df = pd.concat([area_tasks_df, other_tasks_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>inference-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>data-poisoning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          area                 task\n",
       "0  adversarial     adversarial-text\n",
       "0  adversarial   adversarial-attack\n",
       "0  adversarial  adversarial-defense\n",
       "0  adversarial     inference-attack\n",
       "0  adversarial       data-poisoning"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_area_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_tasks_df.to_csv('data/paperswithcode_tasks.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_task_exploded_df['normalized_task'] = task_api_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_area_df = papers_task_exploded_df.merge(all_area_tasks_df, left_on='normalized_task', right_on='task', suffixes=['', '_']).drop(columns=['task_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>inference-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>data-poisoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>sleep-quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>left-atrium-segmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>drug–drug-interaction-extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>video-question-answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>text-based-person-retrieval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1879 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              area                              task\n",
       "0      adversarial                  adversarial-text\n",
       "0      adversarial                adversarial-attack\n",
       "0      adversarial               adversarial-defense\n",
       "0      adversarial                  inference-attack\n",
       "0      adversarial                    data-poisoning\n",
       "..             ...                               ...\n",
       "238  miscellaneous                     sleep-quality\n",
       "239  miscellaneous          left-atrium-segmentation\n",
       "240  miscellaneous  drug–drug-interaction-extraction\n",
       "241  miscellaneous          video-question-answering\n",
       "242  miscellaneous       text-based-person-retrieval\n",
       "\n",
       "[1879 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_area_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_url', 'arxiv_id', 'title', 'abstract', 'url_abs', 'url_pdf',\n",
       "       'proceeding', 'authors', 'tasks', 'date', 'methods', 'framework',\n",
       "       'mentioned_in_github', 'mentioned_in_paper', 'paper_arxiv_id',\n",
       "       'paper_title', 'paper_url_abs', 'paper_url_pdf', 'repo', 'repo_url',\n",
       "       'task', 'normalized_task', 'area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_area_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miscellaneous                  4123\n",
       "computer-vision                3346\n",
       "natural-language-processing    2433\n",
       "methodology                    2113\n",
       "medical                         606\n",
       "graphs                          260\n",
       "time-series                     255\n",
       "speech                          248\n",
       "playing-games                   198\n",
       "music                           177\n",
       "reasoning                       131\n",
       "knowledge-base                  118\n",
       "adversarial                      68\n",
       "robots                           55\n",
       "audio                            42\n",
       "computer-code                    37\n",
       "Name: area, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_area_df['area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "area         task                    \n",
       "adversarial  Adversarial Attack          32\n",
       "             Adversarial Defense         21\n",
       "             Adversarial Text             4\n",
       "             Data Poisoning               4\n",
       "             Inference Attack             7\n",
       "                                         ..\n",
       "time-series  Time Series Forecasting     14\n",
       "             Time Series Prediction       5\n",
       "             Time-to-Event Prediction     1\n",
       "             Traffic Prediction           9\n",
       "             Trajectory Prediction       13\n",
       "Name: paper_url, Length: 1021, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_area_df.groupby(['area', 'task']).agg('count')['paper_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_counts = papers_area_df['area'].value_counts()\n",
    "area_weights = area_counts.copy()\n",
    "area_weights = area_weights / area_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>adversarial-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>inference-attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>data-poisoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>website-fingerprinting-attacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>provable-adversarial-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>website-fingerprinting-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adversarial</td>\n",
       "      <td>real-world-adversarial-attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          area                            task\n",
       "0  adversarial                adversarial-text\n",
       "0  adversarial              adversarial-attack\n",
       "0  adversarial             adversarial-defense\n",
       "0  adversarial                inference-attack\n",
       "0  adversarial                  data-poisoning\n",
       "0  adversarial  website-fingerprinting-attacks\n",
       "0  adversarial    provable-adversarial-defense\n",
       "0  adversarial  website-fingerprinting-defense\n",
       "0  adversarial   real-world-adversarial-attack"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tasks_df[area_tasks_df['area'] == 'adversarial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2121     Adversarial Defense\n",
       "2122     Adversarial Defense\n",
       "2123     Adversarial Defense\n",
       "2124     Adversarial Defense\n",
       "2125     Adversarial Defense\n",
       "                ...         \n",
       "8535          Data Poisoning\n",
       "10453       Adversarial Text\n",
       "10454       Adversarial Text\n",
       "10455       Adversarial Text\n",
       "10456       Adversarial Text\n",
       "Name: task, Length: 68, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_area_df[papers_area_df['area'] == 'adversarial']['task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks_df, test_tasks_df = model_selection.train_test_split(all_area_tasks_df, test_size=0.2, stratify=all_area_tasks_df['area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>graphs</td>\n",
       "      <td>graph-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>computer-vision</td>\n",
       "      <td>rf-based-gesture-recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>handwritten-digit-recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>methodology</td>\n",
       "      <td>entity-embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>nuclei-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>image-retrieval-with-multi-modal-query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>robots</td>\n",
       "      <td>robot-task-planning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>knowledge-base</td>\n",
       "      <td>relational-pattern-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>scene-parsing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>graphs</td>\n",
       "      <td>connectivity-estimation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                area                                    task\n",
       "4             graphs                    graph-classification\n",
       "3    computer-vision            rf-based-gesture-recognition\n",
       "124    miscellaneous           handwritten-digit-recognition\n",
       "7        methodology                       entity-embeddings\n",
       "6            medical                   nuclei-classification\n",
       "..               ...                                     ...\n",
       "175    miscellaneous  image-retrieval-with-multi-modal-query\n",
       "13            robots                     robot-task-planning\n",
       "5     knowledge-base             relational-pattern-learning\n",
       "134    miscellaneous                           scene-parsing\n",
       "4             graphs                 connectivity-estimation\n",
       "\n",
       "[376 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_train_df, papers_test_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2842, 23)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>mentioned_in_paper</th>\n",
       "      <th>paper_arxiv_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url_abs</th>\n",
       "      <th>paper_url_pdf</th>\n",
       "      <th>repo</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>task</th>\n",
       "      <th>normalized_task</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12733</th>\n",
       "      <td>https://paperswithcode.com/paper/unpaired-image-to-image-translation-using</td>\n",
       "      <td>1703.10593</td>\n",
       "      <td>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</td>\n",
       "      <td>Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs....</td>\n",
       "      <td>https://arxiv.org/abs/1703.10593v7</td>\n",
       "      <td>https://arxiv.org/pdf/1703.10593v7.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros]</td>\n",
       "      <td>[Image-to-Image Translation, Multimodal Unsupervised Image-To-Image Translation, Style Transfer, Unsupervised Image-To-Image Translation]</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1703.10593</td>\n",
       "      <td>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</td>\n",
       "      <td>https://arxiv.org/abs/1703.10593v7</td>\n",
       "      <td>https://arxiv.org/pdf/1703.10593v7.pdf</td>\n",
       "      <td>danieldritter/deep_learning_models</td>\n",
       "      <td>https://github.com/danieldritter/deep_learning_models</td>\n",
       "      <td>Unsupervised Image-To-Image Translation</td>\n",
       "      <td>unsupervised-image-to-image-translation</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>https://paperswithcode.com/paper/network-embedding-as-matrix-factorization</td>\n",
       "      <td>1710.02971</td>\n",
       "      <td>Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</td>\n",
       "      <td>Since the invention of word2vec, the skip-gram model has significantly\\nadvanced the research of network embedding, such as the recent emergence of the\\nDeepWalk, LINE, PTE, and node2vec approache...</td>\n",
       "      <td>http://arxiv.org/abs/1710.02971v4</td>\n",
       "      <td>http://arxiv.org/pdf/1710.02971v4.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang]</td>\n",
       "      <td>[Network Embedding, Representation Learning]</td>\n",
       "      <td>2017-10-09</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1710.02971</td>\n",
       "      <td>Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</td>\n",
       "      <td>http://arxiv.org/abs/1710.02971v4</td>\n",
       "      <td>http://arxiv.org/pdf/1710.02971v4.pdf</td>\n",
       "      <td>fahrbach/icml-2020-faster-graph-embeddings</td>\n",
       "      <td>https://github.com/fahrbach/icml-2020-faster-graph-embeddings</td>\n",
       "      <td>Representation Learning</td>\n",
       "      <td>representation-learning</td>\n",
       "      <td>methodology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>https://paperswithcode.com/paper/deep-semi-supervised-anomaly-detection</td>\n",
       "      <td>1906.02694</td>\n",
       "      <td>Deep Semi-Supervised Anomaly Detection</td>\n",
       "      <td>Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning pr...</td>\n",
       "      <td>https://arxiv.org/abs/1906.02694v2</td>\n",
       "      <td>https://arxiv.org/pdf/1906.02694v2.pdf</td>\n",
       "      <td>ICLR 2020 1</td>\n",
       "      <td>[Lukas Ruff, Robert A. Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, Klaus-Robert Müller, Marius Kloft]</td>\n",
       "      <td>[Anomaly Detection, Outlier Detection]</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1906.02694</td>\n",
       "      <td>Deep Semi-Supervised Anomaly Detection</td>\n",
       "      <td>https://arxiv.org/abs/1906.02694v2</td>\n",
       "      <td>https://arxiv.org/pdf/1906.02694v2.pdf</td>\n",
       "      <td>kevinwss/Deep-SAD-Baseline</td>\n",
       "      <td>https://github.com/kevinwss/Deep-SAD-Baseline</td>\n",
       "      <td>Outlier Detection</td>\n",
       "      <td>outlier-detection</td>\n",
       "      <td>methodology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>https://paperswithcode.com/paper/exact-hard-monotonic-attention-for-character</td>\n",
       "      <td>1905.06319</td>\n",
       "      <td>Exact Hard Monotonic Attention for Character-Level Transduction</td>\n",
       "      <td>Many common character-level, string-to-string transduction tasks, e.g. graphemeto-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequ...</td>\n",
       "      <td>https://arxiv.org/abs/1905.06319v2</td>\n",
       "      <td>https://arxiv.org/pdf/1905.06319v2.pdf</td>\n",
       "      <td>ACL 2019 7</td>\n",
       "      <td>[Shijie Wu, Ryan Cotterell]</td>\n",
       "      <td>[Morphological Inflection]</td>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1905.06319</td>\n",
       "      <td>Exact Hard Monotonic Attention for Character-Level Transduction</td>\n",
       "      <td>https://arxiv.org/abs/1905.06319v2</td>\n",
       "      <td>https://arxiv.org/pdf/1905.06319v2.pdf</td>\n",
       "      <td>AssafSinger94/sigmorphon-2020-inflection</td>\n",
       "      <td>https://github.com/AssafSinger94/sigmorphon-2020-inflection</td>\n",
       "      <td>Morphological Inflection</td>\n",
       "      <td>morphological-inflection</td>\n",
       "      <td>natural-language-processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7882</th>\n",
       "      <td>https://paperswithcode.com/paper/monte-carlo-syntax-marginals-for-exploring</td>\n",
       "      <td>1804.06004</td>\n",
       "      <td>Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses</td>\n",
       "      <td>Dependency parsing research, which has made significant gains in recent\\nyears, typically focuses on improving the accuracy of single-tree predictions.\\nHowever, ambiguity is inherent to natural l...</td>\n",
       "      <td>http://arxiv.org/abs/1804.06004v1</td>\n",
       "      <td>http://arxiv.org/pdf/1804.06004v1.pdf</td>\n",
       "      <td>NAACL 2018 6</td>\n",
       "      <td>[Katherine A. Keith, Su Lin Blodgett, Brendan O'Connor]</td>\n",
       "      <td>[Dependency Parsing]</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1804.06004</td>\n",
       "      <td>Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses</td>\n",
       "      <td>http://arxiv.org/abs/1804.06004v1</td>\n",
       "      <td>http://arxiv.org/pdf/1804.06004v1.pdf</td>\n",
       "      <td>slanglab/transition_sampler</td>\n",
       "      <td>https://github.com/slanglab/transition_sampler</td>\n",
       "      <td>Dependency Parsing</td>\n",
       "      <td>dependency-parsing</td>\n",
       "      <td>natural-language-processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>https://paperswithcode.com/paper/adversarial-robustness-from-self-supervised</td>\n",
       "      <td>2003.12862</td>\n",
       "      <td>Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</td>\n",
       "      <td>Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We intr...</td>\n",
       "      <td>https://arxiv.org/abs/2003.12862v1</td>\n",
       "      <td>https://arxiv.org/pdf/2003.12862v1.pdf</td>\n",
       "      <td>CVPR 2020 6</td>\n",
       "      <td>[Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>2003.12862</td>\n",
       "      <td>Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</td>\n",
       "      <td>https://arxiv.org/abs/2003.12862v1</td>\n",
       "      <td>https://arxiv.org/pdf/2003.12862v1.pdf</td>\n",
       "      <td>TAMU-VITA/Adv-SS-Pretraining</td>\n",
       "      <td>https://github.com/TAMU-VITA/Adv-SS-Pretraining</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6407</th>\n",
       "      <td>https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization</td>\n",
       "      <td>1710.09412</td>\n",
       "      <td>mixup: Beyond Empirical Risk Minimization</td>\n",
       "      <td>Large deep neural networks are powerful, but exhibit undesirable behaviors\\nsuch as memorization and sensitivity to adversarial examples. In this work, we\\npropose mixup, a simple learning princip...</td>\n",
       "      <td>http://arxiv.org/abs/1710.09412v2</td>\n",
       "      <td>http://arxiv.org/pdf/1710.09412v2.pdf</td>\n",
       "      <td>ICLR 2018 1</td>\n",
       "      <td>[Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz]</td>\n",
       "      <td>[Domain Generalization, Image Classification, Semi-Supervised Image Classification]</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1710.09412</td>\n",
       "      <td>mixup: Beyond Empirical Risk Minimization</td>\n",
       "      <td>http://arxiv.org/abs/1710.09412v2</td>\n",
       "      <td>http://arxiv.org/pdf/1710.09412v2.pdf</td>\n",
       "      <td>rwightman/pytorch-image-models</td>\n",
       "      <td>https://github.com/rwightman/pytorch-image-models</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>https://paperswithcode.com/paper/uncertainty-aware-self-ensembling-model-for</td>\n",
       "      <td>1907.07034</td>\n",
       "      <td>Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation</td>\n",
       "      <td>Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In t...</td>\n",
       "      <td>https://arxiv.org/abs/1907.07034v1</td>\n",
       "      <td>https://arxiv.org/pdf/1907.07034v1.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, Pheng-Ann Heng]</td>\n",
       "      <td>[Left Atrium Segmentation, Medical Image Segmentation, Semantic Segmentation]</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1907.07034</td>\n",
       "      <td>Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation</td>\n",
       "      <td>https://arxiv.org/abs/1907.07034v1</td>\n",
       "      <td>https://arxiv.org/pdf/1907.07034v1.pdf</td>\n",
       "      <td>Luoxd1996/DTC</td>\n",
       "      <td>https://github.com/Luoxd1996/DTC</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>semantic-segmentation</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>https://paperswithcode.com/paper/rethinking-the-smaller-norm-less-informative</td>\n",
       "      <td>1802.00124</td>\n",
       "      <td>Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</td>\n",
       "      <td>Model pruning has become a useful technique that improves the computational\\nefficiency of deep learning, making it possible to deploy solutions in\\nresource-limited scenarios. A widely-used pract...</td>\n",
       "      <td>http://arxiv.org/abs/1802.00124v2</td>\n",
       "      <td>http://arxiv.org/pdf/1802.00124v2.pdf</td>\n",
       "      <td>ICLR 2018 1</td>\n",
       "      <td>[Jianbo Ye, Xin Lu, Zhe Lin, James Z. Wang]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1802.00124</td>\n",
       "      <td>Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</td>\n",
       "      <td>http://arxiv.org/abs/1802.00124v2</td>\n",
       "      <td>http://arxiv.org/pdf/1802.00124v2.pdf</td>\n",
       "      <td>jack-willturner/batchnorm-pruning</td>\n",
       "      <td>https://github.com/jack-willturner/batchnorm-pruning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>https://paperswithcode.com/paper/metric-learn-metric-learning-algorithms-in</td>\n",
       "      <td>1908.04710</td>\n",
       "      <td>metric-learn: Metric Learning Algorithms in Python</td>\n",
       "      <td>metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface co...</td>\n",
       "      <td>https://arxiv.org/abs/1908.04710v3</td>\n",
       "      <td>https://arxiv.org/pdf/1908.04710v3.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet]</td>\n",
       "      <td>[Metric Learning, Model Selection]</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1908.04710</td>\n",
       "      <td>metric-learn: Metric Learning Algorithms in Python</td>\n",
       "      <td>https://arxiv.org/abs/1908.04710v3</td>\n",
       "      <td>https://arxiv.org/pdf/1908.04710v3.pdf</td>\n",
       "      <td>all-umass/metric_learn</td>\n",
       "      <td>https://github.com/all-umass/metric_learn</td>\n",
       "      <td>Model Selection</td>\n",
       "      <td>model-selection</td>\n",
       "      <td>methodology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11368 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           paper_url  \\\n",
       "12733     https://paperswithcode.com/paper/unpaired-image-to-image-translation-using   \n",
       "240       https://paperswithcode.com/paper/network-embedding-as-matrix-factorization   \n",
       "10245        https://paperswithcode.com/paper/deep-semi-supervised-anomaly-detection   \n",
       "8193   https://paperswithcode.com/paper/exact-hard-monotonic-attention-for-character   \n",
       "7882     https://paperswithcode.com/paper/monte-carlo-syntax-marginals-for-exploring   \n",
       "...                                                                              ...   \n",
       "1867    https://paperswithcode.com/paper/adversarial-robustness-from-self-supervised   \n",
       "6407       https://paperswithcode.com/paper/mixup-beyond-empirical-risk-minimization   \n",
       "3230    https://paperswithcode.com/paper/uncertainty-aware-self-ensembling-model-for   \n",
       "1052   https://paperswithcode.com/paper/rethinking-the-smaller-norm-less-informative   \n",
       "4518     https://paperswithcode.com/paper/metric-learn-metric-learning-algorithms-in   \n",
       "\n",
       "         arxiv_id  \\\n",
       "12733  1703.10593   \n",
       "240    1710.02971   \n",
       "10245  1906.02694   \n",
       "8193   1905.06319   \n",
       "7882   1804.06004   \n",
       "...           ...   \n",
       "1867   2003.12862   \n",
       "6407   1710.09412   \n",
       "3230   1907.07034   \n",
       "1052   1802.00124   \n",
       "4518   1908.04710   \n",
       "\n",
       "                                                                                                  title  \\\n",
       "12733                   Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks   \n",
       "240               Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec   \n",
       "10245                                                            Deep Semi-Supervised Anomaly Detection   \n",
       "8193                                    Exact Hard Monotonic Attention for Character-Level Transduction   \n",
       "7882                             Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses   \n",
       "...                                                                                                 ...   \n",
       "1867                           Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning   \n",
       "6407                                                          mixup: Beyond Empirical Risk Minimization   \n",
       "3230            Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation   \n",
       "1052   Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers   \n",
       "4518                                                 metric-learn: Metric Learning Algorithms in Python   \n",
       "\n",
       "                                                                                                                                                                                                      abstract  \\\n",
       "12733  Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs....   \n",
       "240    Since the invention of word2vec, the skip-gram model has significantly\\nadvanced the research of network embedding, such as the recent emergence of the\\nDeepWalk, LINE, PTE, and node2vec approache...   \n",
       "10245  Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning pr...   \n",
       "8193   Many common character-level, string-to-string transduction tasks, e.g. graphemeto-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequ...   \n",
       "7882   Dependency parsing research, which has made significant gains in recent\\nyears, typically focuses on improving the accuracy of single-tree predictions.\\nHowever, ambiguity is inherent to natural l...   \n",
       "...                                                                                                                                                                                                        ...   \n",
       "1867   Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We intr...   \n",
       "6407   Large deep neural networks are powerful, but exhibit undesirable behaviors\\nsuch as memorization and sensitivity to adversarial examples. In this work, we\\npropose mixup, a simple learning princip...   \n",
       "3230   Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In t...   \n",
       "1052   Model pruning has become a useful technique that improves the computational\\nefficiency of deep learning, making it possible to deploy solutions in\\nresource-limited scenarios. A widely-used pract...   \n",
       "4518   metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface co...   \n",
       "\n",
       "                                  url_abs  \\\n",
       "12733  https://arxiv.org/abs/1703.10593v7   \n",
       "240     http://arxiv.org/abs/1710.02971v4   \n",
       "10245  https://arxiv.org/abs/1906.02694v2   \n",
       "8193   https://arxiv.org/abs/1905.06319v2   \n",
       "7882    http://arxiv.org/abs/1804.06004v1   \n",
       "...                                   ...   \n",
       "1867   https://arxiv.org/abs/2003.12862v1   \n",
       "6407    http://arxiv.org/abs/1710.09412v2   \n",
       "3230   https://arxiv.org/abs/1907.07034v1   \n",
       "1052    http://arxiv.org/abs/1802.00124v2   \n",
       "4518   https://arxiv.org/abs/1908.04710v3   \n",
       "\n",
       "                                      url_pdf    proceeding  \\\n",
       "12733  https://arxiv.org/pdf/1703.10593v7.pdf  ICCV 2017 10   \n",
       "240     http://arxiv.org/pdf/1710.02971v4.pdf          None   \n",
       "10245  https://arxiv.org/pdf/1906.02694v2.pdf   ICLR 2020 1   \n",
       "8193   https://arxiv.org/pdf/1905.06319v2.pdf    ACL 2019 7   \n",
       "7882    http://arxiv.org/pdf/1804.06004v1.pdf  NAACL 2018 6   \n",
       "...                                       ...           ...   \n",
       "1867   https://arxiv.org/pdf/2003.12862v1.pdf   CVPR 2020 6   \n",
       "6407    http://arxiv.org/pdf/1710.09412v2.pdf   ICLR 2018 1   \n",
       "3230   https://arxiv.org/pdf/1907.07034v1.pdf          None   \n",
       "1052    http://arxiv.org/pdf/1802.00124v2.pdf   ICLR 2018 1   \n",
       "4518   https://arxiv.org/pdf/1908.04710v3.pdf          None   \n",
       "\n",
       "                                                                                                                        authors  \\\n",
       "12733                                                               [Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros]   \n",
       "240                                                        [Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang]   \n",
       "10245  [Lukas Ruff, Robert A. Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, Klaus-Robert Müller, Marius Kloft]   \n",
       "8193                                                                                                [Shijie Wu, Ryan Cotterell]   \n",
       "7882                                                                    [Katherine A. Keith, Su Lin Blodgett, Brendan O'Connor]   \n",
       "...                                                                                                                         ...   \n",
       "1867                                              [Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang]   \n",
       "6407                                                          [Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz]   \n",
       "3230                                                         [Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, Pheng-Ann Heng]   \n",
       "1052                                                                                [Jianbo Ye, Xin Lu, Zhe Lin, James Z. Wang]   \n",
       "4518                                             [William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet]   \n",
       "\n",
       "                                                                                                                                           tasks  \\\n",
       "12733  [Image-to-Image Translation, Multimodal Unsupervised Image-To-Image Translation, Style Transfer, Unsupervised Image-To-Image Translation]   \n",
       "240                                                                                                 [Network Embedding, Representation Learning]   \n",
       "10245                                                                                                     [Anomaly Detection, Outlier Detection]   \n",
       "8193                                                                                                                  [Morphological Inflection]   \n",
       "7882                                                                                                                        [Dependency Parsing]   \n",
       "...                                                                                                                                          ...   \n",
       "1867                                                                                                                                          []   \n",
       "6407                                                         [Domain Generalization, Image Classification, Semi-Supervised Image Classification]   \n",
       "3230                                                               [Left Atrium Segmentation, Medical Image Segmentation, Semantic Segmentation]   \n",
       "1052                                                                                                                                          []   \n",
       "4518                                                                                                          [Metric Learning, Model Selection]   \n",
       "\n",
       "            date  ... mentioned_in_paper paper_arxiv_id  \\\n",
       "12733 2017-03-30  ...              False     1703.10593   \n",
       "240   2017-10-09  ...              False     1710.02971   \n",
       "10245 2019-06-06  ...              False     1906.02694   \n",
       "8193  2019-05-15  ...              False     1905.06319   \n",
       "7882  2018-04-17  ...               True     1804.06004   \n",
       "...          ...  ...                ...            ...   \n",
       "1867  2020-03-28  ...               True     2003.12862   \n",
       "6407  2017-10-25  ...              False     1710.09412   \n",
       "3230  2019-07-16  ...              False     1907.07034   \n",
       "1052  2018-02-01  ...              False     1802.00124   \n",
       "4518  2019-08-13  ...              False     1908.04710   \n",
       "\n",
       "                                                                                            paper_title  \\\n",
       "12733                   Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks   \n",
       "240               Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec   \n",
       "10245                                                            Deep Semi-Supervised Anomaly Detection   \n",
       "8193                                    Exact Hard Monotonic Attention for Character-Level Transduction   \n",
       "7882                             Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses   \n",
       "...                                                                                                 ...   \n",
       "1867                           Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning   \n",
       "6407                                                          mixup: Beyond Empirical Risk Minimization   \n",
       "3230            Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation   \n",
       "1052   Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers   \n",
       "4518                                                 metric-learn: Metric Learning Algorithms in Python   \n",
       "\n",
       "                            paper_url_abs  \\\n",
       "12733  https://arxiv.org/abs/1703.10593v7   \n",
       "240     http://arxiv.org/abs/1710.02971v4   \n",
       "10245  https://arxiv.org/abs/1906.02694v2   \n",
       "8193   https://arxiv.org/abs/1905.06319v2   \n",
       "7882    http://arxiv.org/abs/1804.06004v1   \n",
       "...                                   ...   \n",
       "1867   https://arxiv.org/abs/2003.12862v1   \n",
       "6407    http://arxiv.org/abs/1710.09412v2   \n",
       "3230   https://arxiv.org/abs/1907.07034v1   \n",
       "1052    http://arxiv.org/abs/1802.00124v2   \n",
       "4518   https://arxiv.org/abs/1908.04710v3   \n",
       "\n",
       "                                paper_url_pdf  \\\n",
       "12733  https://arxiv.org/pdf/1703.10593v7.pdf   \n",
       "240     http://arxiv.org/pdf/1710.02971v4.pdf   \n",
       "10245  https://arxiv.org/pdf/1906.02694v2.pdf   \n",
       "8193   https://arxiv.org/pdf/1905.06319v2.pdf   \n",
       "7882    http://arxiv.org/pdf/1804.06004v1.pdf   \n",
       "...                                       ...   \n",
       "1867   https://arxiv.org/pdf/2003.12862v1.pdf   \n",
       "6407    http://arxiv.org/pdf/1710.09412v2.pdf   \n",
       "3230   https://arxiv.org/pdf/1907.07034v1.pdf   \n",
       "1052    http://arxiv.org/pdf/1802.00124v2.pdf   \n",
       "4518   https://arxiv.org/pdf/1908.04710v3.pdf   \n",
       "\n",
       "                                             repo  \\\n",
       "12733          danieldritter/deep_learning_models   \n",
       "240    fahrbach/icml-2020-faster-graph-embeddings   \n",
       "10245                  kevinwss/Deep-SAD-Baseline   \n",
       "8193     AssafSinger94/sigmorphon-2020-inflection   \n",
       "7882                  slanglab/transition_sampler   \n",
       "...                                           ...   \n",
       "1867                 TAMU-VITA/Adv-SS-Pretraining   \n",
       "6407               rwightman/pytorch-image-models   \n",
       "3230                                Luoxd1996/DTC   \n",
       "1052            jack-willturner/batchnorm-pruning   \n",
       "4518                       all-umass/metric_learn   \n",
       "\n",
       "                                                            repo_url  \\\n",
       "12733          https://github.com/danieldritter/deep_learning_models   \n",
       "240    https://github.com/fahrbach/icml-2020-faster-graph-embeddings   \n",
       "10245                  https://github.com/kevinwss/Deep-SAD-Baseline   \n",
       "8193     https://github.com/AssafSinger94/sigmorphon-2020-inflection   \n",
       "7882                  https://github.com/slanglab/transition_sampler   \n",
       "...                                                              ...   \n",
       "1867                 https://github.com/TAMU-VITA/Adv-SS-Pretraining   \n",
       "6407               https://github.com/rwightman/pytorch-image-models   \n",
       "3230                                https://github.com/Luoxd1996/DTC   \n",
       "1052            https://github.com/jack-willturner/batchnorm-pruning   \n",
       "4518                       https://github.com/all-umass/metric_learn   \n",
       "\n",
       "                                          task  \\\n",
       "12733  Unsupervised Image-To-Image Translation   \n",
       "240                    Representation Learning   \n",
       "10245                        Outlier Detection   \n",
       "8193                  Morphological Inflection   \n",
       "7882                        Dependency Parsing   \n",
       "...                                        ...   \n",
       "1867                                       NaN   \n",
       "6407                      Image Classification   \n",
       "3230                     Semantic Segmentation   \n",
       "1052                                       NaN   \n",
       "4518                           Model Selection   \n",
       "\n",
       "                               normalized_task                         area  \n",
       "12733  unsupervised-image-to-image-translation                miscellaneous  \n",
       "240                    representation-learning                  methodology  \n",
       "10245                        outlier-detection                  methodology  \n",
       "8193                  morphological-inflection  natural-language-processing  \n",
       "7882                        dependency-parsing  natural-language-processing  \n",
       "...                                        ...                          ...  \n",
       "1867                                       NaN                miscellaneous  \n",
       "6407                      image-classification              computer-vision  \n",
       "3230                     semantic-segmentation                miscellaneous  \n",
       "1052                                       NaN                miscellaneous  \n",
       "4518                           model-selection                  methodology  \n",
       "\n",
       "[11368 rows x 23 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:/tmp/foo.pkl already exists\n"
     ]
    }
   ],
   "source": [
    "with mlutil.maybe_pickler(\"/tmp/foo.pkl\") as writer:\n",
    "    writer.write_pickle_if_not_exists(lambda: papers_area_df.iloc[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://dfkiqyg0xf.execute-api.us-east-2.amazonaws.com/DEV2/storage/humtap-contributions/\"\n",
    "audio_contributions/audio/10_0FCBDDA0-953F-46A9-86B8-0AC8EAC89F03.opus|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

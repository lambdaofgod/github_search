{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp python_function_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import astunparse\n",
    "\n",
    "import multiprocessing.pool\n",
    "import tqdm\n",
    "\n",
    "from github_search import python_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 8.08 s, total: 19.2 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "python_files_df = pd.read_feather(\"data/all_crawled_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df = pd.read_csv(\"data/paperswithcode_with_tasks.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#!/usr/bin/env python3\\nimport subprocess\\nimp...</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#!/usr/bin/env python3\\n# Author: Rico Sennric...</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\npar...</td>\n",
       "      <td>scripts/concat-bpe.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#!/usr/bin/env python3\\nimport argparse\\nimpor...</td>\n",
       "      <td>scripts/copy-model.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nfro...</td>\n",
       "      <td>scripts/coverage.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                   path  \\\n",
       "0  #!/usr/bin/env python3\\nimport subprocess\\nimp...           run-tests.py   \n",
       "1  #!/usr/bin/env python3\\n# Author: Rico Sennric...   scripts/apply_bpe.py   \n",
       "2  #!/usr/bin/env python3\\n\\nimport argparse\\npar...  scripts/concat-bpe.py   \n",
       "3  #!/usr/bin/env python3\\nimport argparse\\nimpor...  scripts/copy-model.py   \n",
       "4  #!/usr/bin/env python3\\n\\nimport argparse\\nfro...    scripts/coverage.py   \n",
       "\n",
       "         repo_name  \n",
       "0  trangvu/ape-npi  \n",
       "1  trangvu/ape-npi  \n",
       "2  trangvu/ape-npi  \n",
       "3  trangvu/ape-npi  \n",
       "4  trangvu/ape-npi  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_repos_python_files_df = python_files_df[python_files_df['repo_name'].isin(paperswithcode_with_tasks_df['repo'].iloc[:100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_repos_python_files_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_repo_similarities(\n",
    "    paperswithcode_df: pd.DataFrame,\n",
    "    repo_grouped_contents: dict,\n",
    "    similar_col: str,\n",
    "    bow_vectorizer_class: str,\n",
    "):\n",
    "    bow_vectorizer = getattr(feature_extraction.text, bow_vectorizer_class)()\n",
    "\n",
    "    bow_vectorizer.fit(paperswithcode_df[similar_col])\n",
    "    return {\n",
    "        repo: metrics.pairwise.cosine_similarity(\n",
    "            bow_vectorizer.transform([abstract]),\n",
    "            bow_vectorizer.transform(repo_grouped_contents[repo]),\n",
    "        )[0]\n",
    "        for (repo, abstract) in paperswithcode_df[[\"repo\", similar_col]].itertuples(\n",
    "            index=False\n",
    "        )\n",
    "        if len(repo_grouped_contents[repo]) > 0\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.53it/s]\n"
     ]
    }
   ],
   "source": [
    "repo_grouped_contents = {\n",
    "    repo: python_files_df[python_files_df[\"repo_name\"] == repo][\"content\"]\n",
    "    for repo in tqdm.tqdm(paperswithcode_with_tasks_df[\"repo\"].iloc[:100])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 ms, sys: 0 ns, total: 3.6 ms\n",
      "Wall time: 3.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "d = dict(tuple(selected_repos_python_files_df.groupby(\"repo_name\")[['content']]))\n",
    "for (k, v) in d.items():\n",
    "    d[k] = v.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import io\\nimport os\\nfrom os import path\\nimport re\\nfrom setuptools import setup, find_packages\\n# To use consisten encodings\\nfrom codecs import open\\n\\n# Function from: https://github.com/pytorch/vision/blob/master/setup.py\\n\\n\\ndef read(*names, **kwargs):\\n    with io.open(\\n        os.path.join(os.path.dirname(__file__), *names),\\n        encoding=kwargs.get(\"encoding\", \"utf8\")\\n    ) as fp:\\n        return fp.read()\\n\\n# Function from: https://github.com/pytorch/vision/blob/master/setup.py\\n\\n\\ndef find_version(*file_paths):\\n    version_file = read(*file_paths)\\n    version_match = re.search(r\"^__version__ = [\\'\\\\\"]([^\\'\\\\\"]*)[\\'\\\\\"]\",\\n                              version_file, re.M)\\n    if version_match:\\n        return version_match.group(1)\\n    raise RuntimeError(\"Unable to find version string.\")\\n\\nhere = path.abspath(path.dirname(__file__))\\n\\n# Get the long description from the README file\\nwith open(path.join(here, \\'README.md\\'), encoding=\\'utf-8\\') as readme_file:\\n    long_description = readme_file.read()\\n\\nVERSION = find_version(\\'face_alignment\\', \\'__init__.py\\')\\n\\nrequirements = [\\n    \\'torch\\',\\n    \\'numpy\\',\\n    \\'scipy>=0.17\\',\\n    \\'scikit-image\\',\\n    \\'opencv-python\\',\\n    \\'tqdm\\',\\n    \\'numba\\',\\n    \\'enum34;python_version<\"3.4\"\\'\\n]\\n\\nsetup(\\n    name=\\'face_alignment\\',\\n    version=VERSION,\\n\\n    description=\"Detector 2D or 3D face landmarks from Python\",\\n    long_description=long_description,\\n    long_description_content_type=\"text/markdown\",\\n\\n    # Author details\\n    author=\"Adrian Bulat\",\\n    author_email=\"adrian@adrianbulat.com\",\\n    url=\"https://github.com/1adrianb/face-alignment\",\\n\\n    # Package info\\n    packages=find_packages(exclude=(\\'test\\',)),\\n\\n    python_requires=\\'>=3\\',\\n    install_requires=requirements,\\n    license=\\'BSD\\',\\n    zip_safe=True,\\n\\n    classifiers=[\\n        \\'Development Status :: 5 - Production/Stable\\',\\n        \\'Operating System :: OS Independent\\',\\n        \\'License :: OSI Approved :: BSD License\\',\\n        \\'Natural Language :: English\\',\\n\\n        # Supported python versions\\n        \\'Programming Language :: Python :: 3\\',\\n        \\'Programming Language :: Python :: 3.3\\',\\n        \\'Programming Language :: Python :: 3.4\\',\\n        \\'Programming Language :: Python :: 3.5\\',\\n        \\'Programming Language :: Python :: 3.6\\',\\n        \\'Programming Language :: Python :: 3.7\\',\\n        \\'Programming Language :: Python :: 3.8\\',\\n    ],\\n)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = python_files_df[\"content\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#!/usr/bin/env python3\\n\\nimport itertools\\nimport argparse\\nimport re\\nimport os\\nimport sys\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('log_files', nargs='+')\\nparser.add_argument('--dev-prefix')\\nparser.add_argument('--score', choices=('ter', 'bleu', 'wer'))\\nparser.add_argument('--task-name')\\nparser.add_argument('--time', action='store_true')\\nparser.add_argument('--params', action='store_true')\\n\\nparser.add_argument('--ter', action='store_true')\\nparser.add_argument('--bleu', action='store_true')\\nparser.add_argument('--wer', action='store_true')\\nparser.add_argument('--cer', action='store_true')\\nparser.add_argument('--bleu1', action='store_true')\\nparser.add_argument('--loss', action='store_true')\\n\\n\\ndef print_scores(log_file, time=False, label=None):\\n    with open(log_file) as log_file:\\n        scores = {}\\n        times = {}\\n        current_step = 0\\n        max_step = 0\\n        starting_time = None\\n        param_count = None\\n\\n        def read_time(line):\\n            if not time:\\n                return None\\n            m = re.match('../.. ..:..:..', line)\\n            if m:\\n                return dateutil.parser.parse(m.group(0))\\n\\n        for line in log_file:\\n            if starting_time is None:\\n                starting_time = read_time(line)\\n            if param_count is None:\\n                m = re.search('number of parameters: (.*)', line)\\n                if m:\\n                    param_count = m.group(1)\\n\\n            m = re.search('step (\\\\d+)', line)\\n            if m:\\n                current_step = int(m.group(1))\\n                times.setdefault(current_step, read_time(line)) \\n                max_step = max(max_step, current_step)\\n                continue\\n\\n            if args.task_name is not None:\\n                if not re.search(args.task_name, line):\\n                    continue\\n            if args.dev_prefix is not None:\\n                if not re.search(args.task_name, line):\\n                    continue\\n\\n            m = re.findall('(loss|bleu|score|ter|wer|cer|bleu1|penalty|ratio)=(\\\\d+.\\\\d+)', line)\\n            if m:\\n                scores_ = {k: float(v) for k, v in m}\\n                scores.setdefault(current_step, scores_)\\n\\n        def key(d):\\n            score = d.get(args.score.lower())\\n            if score is None:\\n                score = d.get('score')\\n\\n            if args.score in ('ter', 'wer', 'cer', 'loss'):\\n                score = -score\\n            return score\\n\\n        step, best = max(scores.items(), key=lambda p: key(p[1]))\\n\\n        if 'score' in best:\\n            missing_key = next(k for k in ['bleu', 'ter', 'wer', 'cer', 'bleu1', 'loss'] if k not in best)\\n            best[missing_key] = best.pop('score')\\n\\n        keys = [args.score, 'bleu', 'ter', 'wer', 'cer', 'bleu1', 'loss', 'penalty', 'ratio']\\n        best = sorted(best.items(), key=lambda p: keys.index(p[0]))\\n\\n        def pretty_time(seconds):\\n            seconds = int(seconds)\\n            s = []\\n            days, seconds = divmod(seconds, 3600 * 24)\\n            if days > 0:\\n                s.append('{}d'.format(days))\\n            hours, seconds = divmod(seconds, 3600)\\n            if hours > 0:\\n                s.append('{}h'.format(hours))\\n            minutes, seconds = divmod(seconds, 60)\\n            if minutes > 0 and days == 0:\\n                s.append('{}min'.format(minutes))\\n            if days == 0 and hours == 0 and minutes < 5 and seconds > 0:\\n                s.append('{}s'.format(seconds))\\n            return ''.join(s)\\n\\n        if time:\\n            total_time = (times[max_step] - starting_time).total_seconds()\\n            train_time = (times[step] - starting_time).total_seconds()\\n            time_string = ' time={}/{}'.format(pretty_time(train_time), pretty_time(total_time))\\n        else:\\n            time_string = ''\\n\\n        if label is None:\\n            label = ''\\n        if args.params and param_count is not None:\\n            param_string = ' params={}'.format(param_count)\\n        else:\\n            param_string = ''\\n\\n        print(label + ' '.join(itertools.starmap('{}={:.2f}'.format, best)),\\n              'step={}/{}'.format(step, max_step) + param_string + time_string)\\n\\nif __name__ == '__main__':\\n    args = parser.parse_args()\\n    args.log_files = [os.path.join(log_file, 'log.txt') if os.path.isdir(log_file) else log_file\\n                      for log_file in args.log_files]\\n\\n    if args.ter:\\n        args.score = 'ter'\\n    elif args.wer:\\n        args.score = 'wer'\\n    elif args.cer:\\n        args.score = 'cer'\\n    elif args.bleu1:\\n        args.score = 'bleu1'\\n    elif args.loss:\\n        args.score = 'loss'\\n    elif args.bleu or args.score is None:\\n        args.score = 'bleu'\\n\\n    if args.time:\\n        import dateutil.parser\\n\\n    labels = None\\n    if not labels:\\n        filenames = [os.path.basename(log_file) for log_file in args.log_files]\\n        if len(set(filenames)) == len(filenames):\\n            labels = filenames\\n    if not labels:\\n        dirnames = [os.path.basename(os.path.dirname(log_file)) for log_file in args.log_files]\\n        if len(set(dirnames)) == len(dirnames):\\n            labels = dirnames\\n    if not labels:\\n        labels = ['model_{}'.format(i) for i in range(1, len(args.log_files) + 1)]\\n\\n    label_len = max(map(len, labels))\\n    format_string = '{{:<{}}}'.format(label_len + 2)\\n\\n    for label, log_file in zip(labels, args.log_files):\\n        try:\\n            if len(args.log_files) == 1:\\n                label = None\\n            else:\\n                label = format_string.format(label + ':')\\n\\n            print_scores(log_file, time=args.time, label=label)\\n        except:\\n            pass\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[\"content\"].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3169680, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ast = ast.parse(example_file)  # .body[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def select_class_names(lines):\n",
    "    return [\n",
    "        line.strip()\n",
    "        for line in lines\n",
    "        if line.lstrip().startswith(\"class \") and line.rstrip().endswith(\":\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def select_function_names(lines):\n",
    "    return [line.strip() for line in lines if line.lstrip().startswith(\"def \")]\n",
    "\n",
    "\n",
    "def select_lines(text, use_function_names=True, use_class_names=False):\n",
    "    lines = text.split(\"\\n\")\n",
    "    selected_lines = []\n",
    "    if use_class_names:\n",
    "        selected_lines = selected_lines + select_class_names(lines)\n",
    "    if use_function_names:\n",
    "        selected_lines = selected_lines + select_function_names(lines)\n",
    "\n",
    "    return selected_lines\n",
    "\n",
    "\n",
    "def select_functions(file_contents):\n",
    "    return [\n",
    "        elem\n",
    "        for elem in ast.parse(file_contents).body\n",
    "        if elem.__class__ is ast.FunctionDef\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_def = ast.parse(\"def foo(x): return x + 1\").body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def foo(x):\n",
      "    return (x + 1)\n"
     ]
    }
   ],
   "source": [
    "print(astunparse.unparse(fn_def).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = python_files_df[\"content\"].iloc[8]\n",
    "fn_defs = select_functions(example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_ast.FunctionDef at 0x7eff0b17c6a0>,\n",
       " <_ast.FunctionDef at 0x7eff0b105ac0>,\n",
       " <_ast.FunctionDef at 0x7eff0b105e80>,\n",
       " <_ast.FunctionDef at 0x7eff0b114550>,\n",
       " <_ast.FunctionDef at 0x7eff0b118340>,\n",
       " <_ast.FunctionDef at 0x7eff0b11dcd0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndef create_parser():\\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='learn BPE-based word segmentation')\\n    parser.add_argument('--input', '-i', type=argparse.FileType('r'), default=sys.stdin, metavar='PATH', help='Input text (default: standard input).')\\n    parser.add_argument('--output', '-o', type=argparse.FileType('w'), default=sys.stdout, metavar='PATH', help='Output file for BPE codes (default: standard output)')\\n    parser.add_argument('--symbols', '-s', type=int, default=10000, help='Create this many new symbols (each representing a character n-gram) (default: %(default)s))')\\n    parser.add_argument('--verbose', '-v', action='store_true', help='verbose mode.')\\n    parser.add_argument('--min-freq', '-f', type=int, default=2, help='minimum word frequency')\\n    return parser\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astunparse.unparse(fn_defs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndef prune_stats(stats, big_stats, threshold):\\n    'Prune statistics dict for efficiency of max()\\\\n\\\\n    The frequency of a symbol pair never increases, so pruning is generally safe\\\\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\\\\n    big_stats keeps full statistics for when we need to access pruned items\\\\n    '\\n    for (item, freq) in list(stats.items()):\\n        if (freq < threshold):\\n            del stats[item]\\n            if (freq < 0):\\n                big_stats[item] += freq\\n            else:\\n                big_stats[item] = freq\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astunparse.unparse(fn_defs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def prune_stats(stats, big_stats, threshold):\\n    'Prune statistics dict for efficiency of max()\\\\n\\\\n    The frequency of a symbol pair never increases, so pruning is generally safe\\\\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\\\\n    big_stats keeps full statistics for when we need to access pruned items\\\\n    '\\n    for (item, freq) in list(stats.items()):\\n        if (freq < threshold):\\n            del stats[item]\\n            if (freq < 0):\\n                big_stats[item] += freq\\n            else:\\n                big_stats[item] = freq\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import astunparse\n",
    "\n",
    "astunparse.unparse(fn_defs[-1]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import io\n",
    "import tokenize\n",
    "import keyword\n",
    "import re\n",
    "\n",
    "\n",
    "PYTHON_KEYWORDS = set(keyword.kwlist)\n",
    "\n",
    "\n",
    "def tokenize_snakecase(identifier):\n",
    "    return identifier.split(\"_\")\n",
    "\n",
    "\n",
    "def tokenize_camelcase(identifier):\n",
    "    matches = re.finditer(\n",
    "        \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\", identifier\n",
    "    )\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "def tokenize_python(identifier, lowercase=False):\n",
    "    if \"_\" in identifier:\n",
    "        tokens = tokenize_snakecase(identifier)\n",
    "    else:\n",
    "        tokens = tokenize_camelcase(identifier)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "\n",
    "def get_file_variable_token_set(file_text, min_token_length=2, lowercase=True):\n",
    "    token_infos = list(tokenize.generate_tokens(io.StringIO(file_text).readline))\n",
    "    raw_tokens = [t.string for t in token_infos if t.type == 1]\n",
    "    all_tokens = (tokenize_python(t, lowercase) for t in raw_tokens)\n",
    "    all_tokens = [\n",
    "        token\n",
    "        for tokens in all_tokens\n",
    "        for token in tokens\n",
    "        if len(token) > min_token_length and not token in PYTHON_KEYWORDS\n",
    "    ]\n",
    "    return set(all_tokens)\n",
    "\n",
    "\n",
    "def maybe_get_file_variable_token_string(file_text, min_token_length=2):\n",
    "    try:\n",
    "        tokens = get_file_variable_token_set(file_text)\n",
    "    except:\n",
    "        return None\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow/models                                8094\n",
       "huggingface/transformers                         1242\n",
       "vincentzhang/faster-rcnn-fcn                       90\n",
       "mobvoi/wenet                                       83\n",
       "cfedermann/Appraise                                59\n",
       "trangvu/ape-npi                                    50\n",
       "testingautomated-usi/selforacle                    43\n",
       "repo_name                                          35\n",
       "bfelbo/deepmoji                                    34\n",
       "claudiogreco/coling18-gte                          27\n",
       "facebookresearch/fastText                          24\n",
       "ruotianluo/DiscCaptioning                          23\n",
       "SJTU-lqiu/QA4IE                                    21\n",
       "kchengiva/DecoupleGCN-DropGraph                    20\n",
       "mravanelli/theano-kaldi-rnn                        16\n",
       "harryhan618/LaneNet                                16\n",
       "xiezheng-cs/DTQ                                    16\n",
       "feifengwhu/question_attention                      16\n",
       "yanggeng1995/GAN-TTS                               14\n",
       "sunalbert/Sequential-patch-based-segmentation      14\n",
       "OIPLab-DUT/ATSA                                    11\n",
       "AvrilCheng/LidarStereoNet                          10\n",
       "Nyn-ynu/MCD                                         8\n",
       "Xharlie/Eye-movement-similarity-clustering          7\n",
       "arunbalajeev/query-video-summary                    5\n",
       "pedrormjunior/ssvm-results                          4\n",
       "bprabhakar/text-to-image                            4\n",
       "nitika-verma/FeaStNet                               3\n",
       "rktamplayo/HCSC                                     3\n",
       "imatge-upc/hate-speech-detection                    2\n",
       "aliosia/DeepPrivInf2017                             2\n",
       "yamada-kd/nepal                                     2\n",
       "BehradToghi/kNN_SWin                                1\n",
       "francescociompi/stain-normalization-isbi-2017       1\n",
       "Name: repo_name, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.iloc[:10000][\"repo_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selected_lines = select_lines(\n",
    "    \" \".join(\n",
    "        python_files_df[python_files_df[\"repo_name\"] == \"transformers\"][\n",
    "            \"content\"\n",
    "        ].dropna()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_functions(code_ast, depth):\n",
    "    if depth == 0:\n",
    "        return None\n",
    "    if type(code_ast) is ast.FunctionDef:\n",
    "        return (code_ast.name, astunparse.unparse(code_ast).strip())\n",
    "    elif type(code_ast) is ast.ClassDef:\n",
    "        return (code_ast.name, astunparse.unparse(code_ast).strip())\n",
    "    elif hasattr(code_ast, \"body\"):\n",
    "        maybe_functions = [get_functions(item, depth) for item in code_ast.body]\n",
    "        return [fn for fn in maybe_functions if not fn is None]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_function_tuples(code, max_depth):\n",
    "    try:\n",
    "        code_ast = ast.parse(code)\n",
    "        return [\n",
    "            fn_data for fn_data in get_functions(code_ast, max_depth) if type(fn_data) is tuple\n",
    "        ]\n",
    "    except (SyntaxError, multiprocessing.pool.MaybeEncodingError, RecursionError) as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_function_df(filename, file_contents):\n",
    "    return pd.DataFrame(\n",
    "        [(filename,) + tp for tp in get_function_tuples(file_contents)],\n",
    "        columns=[\"file_path\", \"function_name\", \"function_code\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'path', 'repo_name'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import tqdm\n",
    "\n",
    "def get_function_data_df(files_df, max_depth=10):\n",
    "    tqdm.auto.tqdm.pandas(desc=\"making python functions df\")\n",
    "    return pd.DataFrame(\n",
    "        list(\n",
    "            files_df.progress_apply(\n",
    "                lambda row: [\n",
    "                    (row[\"repo_name\"], row[\"path\"]) + tp\n",
    "                    for tp in get_function_tuples(row[\"content\"], max_depth)\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            .explode()\n",
    "            .dropna()\n",
    "            .values\n",
    "        ),\n",
    "        columns=[\"repo_name\", \"path\", \"function_name\", \"function_code\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(content='import os\\nimport re\\nimport setuptools\\nfrom pathlib import Path\\n\\np = Path(__file__)\\n\\nsetup_requires = [\\n    \\'numpy\\',\\n    \\'pytest-runner\\'\\n]\\n\\ninstall_requires = [\\n]\\ntest_require = [\\n    \\'pytest-cov\\',\\n    \\'pytest-html\\',\\n    \\'pytest\\'\\n]\\n\\nsetuptools.setup(\\n    name=\"tfdbonas\",\\n    version=\\'0.1.0\\',\\n    python_requires=\\'>3.5\\',\\n    author=\"Koji Ono\",\\n    author_email=\"kbu94982@gmail.com\",\\n    description=\"Tensorflow Deep Bayes Optimization for Neural Network Architecture Search (tfDBONAS)\",\\n    url=\\'https://github.com/0h-n0/tfdbonas\\',\\n    long_description=(p.parent / \\'README.md\\').open(encoding=\\'utf-8\\').read(),\\n    long_description_content_type=\\'text/markdown\\',\\n    packages=setuptools.find_packages(),\\n    install_requires=install_requires,\\n    setup_requires=setup_requires,\\n    tests_require=test_require,\\n    extras_require={\\n        \\'docs\\': [\\n            \\'sphinx >= 1.4\\',\\n            \\'sphinx_rtd_theme\\']},\\n    classifiers=[\\n        \\'Programming Language :: Python :: 3.6\\',\\n        \\'Programming Language :: Python :: 3.7\\',\\n    ],\\n)', path='setup.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # x* = 0\\n    # f(x*) = 0\\n    o = 20 + np.e\\n    o += -20 * np.exp(-0.2*(trial.x ** 2 / 2 + trial.y ** 2 / 2))\\n    o += -np.exp(np.cos(2 * math.pi * trial.x**2) / 2 + np.cos(2 * math.pi * trial.y**2) / 2)\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-30, 30, 0.5))\\n    searcher.register_trial('y', np.arange(-30, 30, 0.5))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_ackley_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # (x*, y*) = [1, 1]\\n    # f(x*, y*) = 0\\n    o = 100*(trial.y - trial.x**2)**2 + (trial.x - 1)**2\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-2.048, 2.048, 0.02))\\n    searcher.register_trial('y', np.arange(-2.048, 2.048, 0.02))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_rosenbrock_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport warnings\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    return -(trial.x**2 + trial.y**2)\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-10, 10, 0.1))\\n    searcher.register_trial('y', np.arange(-10, 10, 0.1))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn('results = {}'.format(searcher.result))\\n    warnings.warn('best_trial {}'.format(searcher.best_trial))\\n    warnings.warn('best_value {}'.format(searcher.best_value))\", path='examples/dngo_2d_sphere_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 4:\\n            if layer[2] is None and layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                xx = [x1, x2, xx[2], xx[3]]\\n            elif layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                xx = [x1, x2, x3, xx[3]]\\n            else:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                x4 = layer[3](xx[3])\\n                xx = [x1, x2, x3, x4]\\n        elif len(layer) == 3:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2], xx[3]]\\n        elif len(layer) == 2:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2]]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_trains = np.split(x_train, 4, axis=1)\\n    x_tests = np.split(x_test, 4, axis=1)\\n    inputs = [keras.layers.Input(shape=(7, 28), name=f'input{i}')\\n              for i in range(1, 5)]\\n    out = create_model(network, inputs)\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=inputs, outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit(x_trains, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_tests, y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m3 = create_modal()\\n    m4 = create_modal()\\n    m = m1 + m2 + m3 + m4\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 24\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_four_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network):\\n    model = keras.Sequential()\\n    for layer in network:\\n        model.add(layer[0])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    model = create_model(network)\\n\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    #mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\nif __name__ == \\'__main__\\':\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(28, 28)),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation=\\'softmax\\'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    num_nodes = 10\\n    num_layer_type = 3\\n    searcher = Searcher()\\n    searcher.register_trial(\\'graph\\', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f\\'tfdbonas.deep_surrogate_models:GCNSurrogateModel\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print((\\'best_trial\\', searcher.best_trial))\\n    print((\\'best_value\\', searcher.best_value))', path='examples/mnist_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_model(h1, h2, h3, h4):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(h1, activation=\\'relu\\'),\\n        keras.layers.Dense(h2, activation=\\'relu\\'),\\n        keras.layers.Dense(h3, activation=\\'relu\\'),\\n        keras.layers.Dense(h4, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_model(trial.h1, trial.h2, trial.h3, trial.h4)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01) #trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128) #trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\nif __name__ == \\'__main__\\':\\n    searcher = Searcher()\\n    searcher.register_trial(\\'h1\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h2\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h3\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h4\\', [16, 32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=4,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))', path='examples/mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 2:\\n            x1 = layer[0](xx[0])\\n            x2 = layer[1](xx[1])\\n            xx = [x1, x2]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n                print((xx.shape))\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_train_1, x_train_2 = np.split(x_train, 2, axis=1)\\n    x_test_1, x_test_2 = np.split(x_test, 2, axis=1)\\n    left_input = keras.layers.Input(shape=(14, 28), name='left_input')\\n    right_input = keras.layers.Input(shape=(14, 28), name='right_input')\\n    out = create_model(network, [left_input, right_input])\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=[left_input, right_input], outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit([x_train_1, x_train_2], y_train, epochs=1, batch_size=128)\\n    out = model.evaluate([x_test_1, x_test_2], y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m = m1 + m2\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 12\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_two_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\n# ref: https://github.com/optuna/optuna/blob/master/examples/quadratic_simple.py\\nimport optuna\\nimport numpy as np\\n\\ndef objective(trial):\\n    x = trial.suggest_categorical('x', np.arange(-10, 10, 0.1))\\n    y = trial.suggest_categorical('y', np.arange(-10, 10, 0.1))\\n    return x**2 + y**2\\n\\n\\nif __name__ == '__main__':\\n    # Let us minimize the objective function above.\\n    print('Running 10 trials...')\\n    study = optuna.create_study()\\n    study.optimize(objective, n_trials=10)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can continue the optimization as follows.\\n    print('Running 20 additional trials...')\\n    study.optimize(objective, n_trials=20)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can specify the timeout instead of a number of trials.\\n    print('Running additional trials in 2 seconds...')\\n    study.optimize(objective, timeout=2.0)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\", path='examples/optuna_2d_shapre_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from tfdbonas.acquistion_functions import (_expected_improvement,\\n                                           AcquisitonFunction,\\n                                           AcquisitonFunctionType)\\n\\nimport numpy as np\\nimport pytest\\n\\n\\ndef test__expected_improvement():\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n    assert eis.size == 9\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_with_invalid_shape():\\n    mean = np.arange(0, 1, 0.1).reshape(2, 5)\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_invalid_type():\\n    mean = [0.1*i for i in range(10)]\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__AcquisitonFunctionType():\\n    assert AcquisitonFunctionType.EI == 1\\n\\ndef test__AcquisitonFunction():\\n    f = AcquisitonFunction(AcquisitonFunctionType.EI)\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = f(mean, sigma, min_val)\\n    assert eis.size == 9', path='tests/test_acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pytest\\n\\nimport tensorflow as tf\\n\\nfrom tfdbonas.deep_surrogate_models import SimpleNetwork\\nfrom tfdbonas.trial import Trial\\n\\n@pytest.mark.skipif(True, reason=\"remove kgcn\")\\ndef test_get_kgcn_gcn_class():\\n    import tensorflow as tf\\n    gcn_class = get_kgcn_gcn_class()\\n    g = gcn_class(32)\\n    features = tf.placeholder(tf.float32, shape=(1, 25, 10))\\n    adj = [tf.sparse_placeholder(tf.float32, shape=(25, 25)), ]\\n    o = g(features, [adj,])\\n    print((g.bases.shape))\\n    assert (1, 32) == o.shape\\n    assert (1, 64) == g.bases.shape\\n\\n\\n@pytest.mark.skipif(not tf.test.is_gpu_available(), reason=\"No GPU\")\\ndef test_simple_network():\\n    s = SimpleNetwork(1, 32)\\n    t = Trial()\\n    setattr(t, \\'hidden1\\', 16)\\n    setattr(t, \\'hidden2\\', 32)\\n    setattr(t, \\'lr\\', 0.01)\\n    setattr(t, \\'batchsize\\', 64)\\n    s.train([t,], [0.2,])', path='tests/test_deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import os\\nimport unittest\\nimport importlib\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom tfdbonas.optimizer import DNGO\\nfrom tfdbonas.trial import TrialGenerator\\n\\n\\n@pytest.fixture(\\n    params=[\\n        {'n_trials': 10, 'n_remains': 990},\\n        {'n_trials': 500, 'n_remains': 500},\\n        {'n_trials': 1, 'n_remains': 999},\\n        {'n_trials': 1000, 'n_remains': 0},\\n    ]\\n)\\ndef correct_tirals_test_case(request):\\n    return request.param\\n\\ndef test_dngo_random_search(correct_tirals_test_case):\\n    data = correct_tirals_test_case\\n    def objective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n    t = TrialGenerator()\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    algo = DNGO(t)\\n    assert len(t) == 1000\\n    assert data['n_remains'] == len(algo._random_search(objective, data['n_trials']))\\n    assert data['n_trials'] == len(algo._searched_trial_indices)\\n    assert data['n_trials'] == len(algo.results)\\n\\n\\nclass TestDNGO(unittest.TestCase):\\n    @staticmethod\\n    def objective(trial) -> float:\\n        return trial.hidden1 * trial.hidden2 * trial.lr * trial.batchsize\\n\\n    def setUp(self):\\n        params = [\\n            ['hidden1', [16, 32, 64, 128]],\\n            ['hidden2', [16, 32, 64, 128]],\\n            ['lr', [0.1 * i for i in range(10)]],\\n            ['batchsize', [64 * i for i in range(10)]],\\n        ]\\n        self.trial_generator = TrialGenerator()\\n        for inputs in params:\\n            self.trial_generator.register(inputs[0], inputs[1])\\n\\n    def test_dngo_bayes_search(self):\\n        algo = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        model_kwargs = dict(input_dim=4)\\n        algo._random_search(TestDNGO.objective, n_random)\\n        algo._bayes_search(TestDNGO.objective, n_bayes, path, model_kwargs)\\n\\n    def test__calc_marginal_log_likelihood(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        phi = np.random.rand(10, 10)\\n        y_values = np.random.rand(10)\\n        optimizer._calc_marginal_log_likelihood(theta, phi, y_values, 10, 10)\\n\\n    def test__predict(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_predict{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        remained_trial_indices = [4, 5, 6]\\n        optimizer.k_inv = np.random.rand(64, 64)\\n        optimizer.mat = np.random.rand(64, 64)\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        mean, var = optimizer._predict(theta,\\n                                       remained_trial_indices,\\n                                       deep_surrogate_model)\\n\\n\\n    def test__train_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n\\n    def load_class(self, path):\\n        splited_path = path.split(':')\\n        assert len(splited_path) == 2, f'invalid input {splited_path}'\\n        module, class_name = splited_path\\n        module = importlib.import_module(module)\\n        return getattr(module, class_name)\\n\\n    def test__predict_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        trained_bases = optimizer._predict_deep_surrogate_model(searched_trial_indices, deep_surrogate_model)\\n\\n    def test__calc_acq_values_ai(self):\\n        optimizer = DNGO(self.trial_generator)\\n        mean = np.random.rand(10)\\n        var = np.random.rand(10)\\n        results = {1: 1*i for i in range(1)}\\n        optimizer._calc_acq_value(mean, var, results)\", path='tests/test_optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import pytest\\n\\nfrom tfdbonas.searcher import Searcher\\n\\n\\ndef test_searcher():\\n    def ovjective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 100\\n\\n@pytest.mark.skip\\ndef test_searcher_with_integrated_test():\\n    def objective(trial):\\n        return trial.lr\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 1000\\n    s.search(objective, 100, n_random_trials=10)\", path='tests/test_searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from tfdbonas.trial import Trial, TrialGenerator\\n\\nimport pytest\\n\\n\\nclass Params:\\n    def __init__(self, param):\\n        self.param = param\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Params):\\n            return NotImplemented\\n        return self.param == other.param\\n\\n    def __str__(self):\\n        return 'Params'\\n\\n    def __repr__(self):\\n        return 'Params'\\n\\n#### Test for Trial ####\\n\\n@pytest.fixture(\\n        params=[\\n            [['hello', 1], [1, '{hello: 1}']],\\n            [['hello', 'str'], ['str', '{hello: str}']],\\n            [['hello', 0.0], [0.0, '{hello: 0.0}']],\\n            [['object', Params(1)], [Params(1), '{object: Params}']],\\n        ]\\n)\\ndef setattr_one_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\n@pytest.fixture(\\n        params=[\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n        ]\\n)\\ndef setattr_multiple_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_trial_instantiation(setattr_one_registered_samples_for_trial):\\n    input = setattr_one_registered_samples_for_trial[0]\\n    expected = setattr_one_registered_samples_for_trial[1]\\n    t = Trial()\\n    setattr(t, *input)\\n    assert getattr(t, input[0]) == expected[0]\\n    assert str(t) == expected[1]\\n\\ndef test_multiple_trial_instantiation(setattr_multiple_registered_samples_for_trial):\\n    t = Trial()\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        setattr(t, *input)\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        expected = samples[1]\\n        assert getattr(t, input[0]) == expected\\n\\n\\n#### Test for TrialGenerator ####\\n\\ndef test_trialgenerator_register():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n\\n    assert len(t) == 1000\\n    assert t[0]._elements == {'ints': 0,\\n                              'strs': 'str0',\\n                              'floats': 0.0}\\n    tt = Trial()\\n    tt.ints = 0\\n    tt.strs = 'str0'\\n    tt.floats = 0.0\\n\\n    assert t[0] == tt\\n\\n    assert t[998]._elements == {'ints': 8,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n    assert t[999]._elements == {'ints': 9,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n\\n@pytest.mark.xfail\\ndef test_trialgenerator_register_IndexError():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    print((t[1000]))\", path='tests/test_trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from pathlib import Path\\n\\nimport numpy as np\\nimport pytest\\n\\nfrom tfdbonas.utils import load_class, is_float\\n\\n\\ndef test_load_class():\\n    path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n    c = load_class(path)\\n    c()\\n\\n@pytest.fixture(\\n    params=[[0.1, True],\\n            [np.float16(0.1), True],\\n            [np.float32(0.1), True],\\n            [np.float64(0.1), True],\\n            [1, False],\\n            [np.int8(1), False],\\n            [np.int16(1), False],\\n            [np.int32(1), False],\\n            [np.int64(1), False],\\n    ]\\n)\\ndef float_types(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_is_float(float_types):\\n    input, expected = float_types\\n    assert is_float(input) == expected\", path='tests/test_utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport pytest\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_simple_network_model(hidden_size=128, activation=\\'relu\\'):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(hidden_size, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_simple_network_model(trial.hidden_size)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n@pytest.mark.heavy\\ndef test_simple_network():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.xfail\\ndef test_simple_network_with_categorical_features():\\n    # (False, reason=\"categorical feature is not supported yet. [idea] I shoud apply embedding layers for categorical inputs.\")\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n\\n    searcher.register_trial(\\'activation\\', [None, \\'relu\\', \\'tanh\\'])\\n    #\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\ndef create_cnn_model(trial):\\n    model = keras.Sequential([\\n        keras.layers.Conv2D(trial.cnn_h1, trial.cnn_k1,\\n                            (trial.cnn_s1, trial.cnn_s1),\\n                            activation=\\'relu\\', input_shape=(28, 28, 1)),\\n        keras.layers.MaxPooling2D(trial.pool_k1),\\n        keras.layers.Conv2D(trial.cnn_h2, trial.cnn_k2,\\n                            (trial.cnn_s2, trial.cnn_s2),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k2),\\n        keras.layers.Conv2D(trial.cnn_h3, trial.cnn_k3,\\n                            (trial.cnn_s3, trial.cnn_s3),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k3),\\n        keras.layers.Flatten(),\\n        keras.layers.Dense(trial.fc1, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\'),\\n    ])\\n    return model\\n\\ndef cnn_objectve(trial: Trial):\\n    try:\\n        model = create_cnn_model(trial)\\n        mnist = tf.keras.datasets.mnist\\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n        x_train = x_train.reshape((60000, 28, 28, 1))\\n        x_test = x_test.reshape((10000, 28, 28, 1))\\n        x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n        opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n        model.compile(optimizer=opt,\\n                      loss=\\'sparse_categorical_crossentropy\\',\\n                      metrics=[\\'accuracy\\'])\\n        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n        model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n        out = model.evaluate(x_test,  y_test, verbose=2)\\n        accuracy = out[1]\\n        return accuracy\\n    except Exception:\\n        # Graph construction is failed.\\n        return 0.0\\n\\n@pytest.mark.skipif(True, reason=\\'heavy conputational cost.\\')\\ndef test_cnn_model_with_many_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 516678750\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.skipif(True, reason=\\'memory error (32GB).\\')\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 73728\\n    # var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n    # MemoryError: Unable to allocate array with shape (73718, 73718) and data type float64\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.heavy\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1])\\n    searcher.register_trial(\\'cnn_s2\\', [1])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))', path='tests/integrated_test/test_mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.stats\\n\\nfrom .utils import is_float\\n\\ndef _expected_improvement(mean: np.array, sigma: np.array, min_val: np.array):\\n    assert isinstance(mean, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert isinstance(sigma, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert is_float(min_val), f\\'instance type error, {type(mean)}\\'\\n    assert len(mean.shape) == 1, f\\'Invalid shape error, {mean.shape}\\'\\n    assert len(sigma.shape) == 1, f\\'Invalid shape error, {sigma.shape}\\'\\n    assert mean.size == sigma.size, f\\'Invalid shape error, {sigma.size} != {sigma.size}\\'\\n    assert min_val.size == 1, f\\'Invalid shape error, {min_val.size}\\'\\n\\n    dist = scipy.stats.norm(loc=0.0, scale=1.0)\\n    gamma = (min_val - mean) / sigma\\n    pdf = dist.pdf(x=gamma)\\n    cdf = scipy.stats.norm.cdf(x=gamma, loc=0., scale=1.)\\n    ei = (min_val - mean) * cdf + (sigma * pdf)\\n    return ei\\n\\n\\nclass AcquisitonFunctionType(Flag):\\n    EI = auto()\\n\\n\\nclass AcquisitonFunction:\\n    def __init__(self, aftype: AcquisitonFunctionType = AcquisitonFunctionType.EI):\\n        if AcquisitonFunctionType.EI == aftype:\\n            self.af_func = _expected_improvement\\n        else:\\n            raise NotImplementedError(\"EI is only supported\")\\n\\n    def __call__(self, *args, **kwargs):\\n        return self.af_func(*args, **kwargs)', path='tfdbonas/acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import uuid\\nimport typing\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.layers as L\\n\\nfrom .trial import Trial\\nfrom .layers import GraphConvolution, GraphGather\\n\\n\\nclass BaseSurrogateModel:\\n    def __init__(self,\\n                 n_train_epochs: int = 100,\\n                 save_path: str = 'network-{uuid.uuid1()}.ckpt'):\\n        self.n_train_epochs = n_train_epochs\\n        self.save_path = save_path\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        raise NotImplementedError\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        raise NotImplementedError\\n\\n\\nclass GCNSurrogateModel(BaseSurrogateModel):\\n    def __init__(self,\\n                 num_nodes: int = 32,\\n                 input_channels: int = 3,\\n                 output_channels: int = 1,\\n                 hidden_channels: int = 64,\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/gcnnet-{uuid.uuid1()}.ckpt'):\\n        super(GCNSurrogateModel, self).__init__(n_train_epochs, save_path)\\n        self.gcn1 = GraphConvolution(16, activation='tanh')\\n        self.gcn2 = GraphConvolution(32, activation='tanh')\\n        self.gcn3 = GraphConvolution(64, activation='tanh')\\n        self.gather = GraphGather()\\n        self.l1 = L.Dense(hidden_channels)\\n        self.l2 = L.Dense(output_channels)\\n        self.bases = None\\n        self.tf_config = tf.compat.v1.ConfigProto(log_device_placement=False,\\n                                                  gpu_options=tf.compat.v1.GPUOptions(\\n                                                      allow_growth=True,\\n                                                  ))\\n        self._build_graph(input_channels, num_nodes, output_channels)\\n        self.vars_to_train = tf.compat.v1.trainable_variables()\\n        self.saver = tf.compat.v1.train.Saver(self.vars_to_train)\\n\\n    def last_layer(self, inputs):\\n        features, adj = inputs\\n        x = self.gcn1([features, adj])\\n        x = self.gcn2([x, adj])\\n        x = self.gcn3([x, adj])\\n        x = self.gather(x)\\n        x = self.l1(x)\\n        self.bases = x\\n        return x\\n\\n    def __call__(self, inputs, adj):\\n        x = self.last_layer([inputs, adj])\\n        x = self.l2(x)\\n        return x\\n\\n    def _build_graph(self, input_channels, num_nodes, output_channles):\\n        self.graph = tf.compat.v1.get_default_graph()\\n        with self.graph.as_default():\\n            self.y_plh = tf.compat.v1.placeholder(tf.float32,\\n                                              shape=[None, output_channles],\\n                                              name='ytrain')\\n            self.x_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, input_channels], name='x')\\n            self.x_adj_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, num_nodes], name='x_adj')\\n            out = self(self.x_plh, self.x_adj_plh)\\n            self.mse_loss = tf.reduce_mean(tf.square(self.y_plh - out))\\n            self.train_loss = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(self.mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        # currently only support batch_size == 1\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                sess.run(tf.compat.v1.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for trial, y in zip(xtrain, ytrain):\\n                        _, (adj, features) = trial.graph\\n                        features = np.expand_dims(features, axis=0) # add batch dimmension\\n                        adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y})\\n\\n\\n                for trial, y in zip(xtrain, ytrain):\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with self.graph.as_default():\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                self.saver.restore(sess, self.save_path)\\n                for trial in xeval:\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n        return bases\\n\\n\\nclass SimpleNetwork(BaseSurrogateModel):\\n    def __init__(self,\\n                 input_dim: int = 4,\\n                 output_dim: int = 1,\\n                 hidden_dim: int = 64,\\n                 activation='tanh',\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/simplenetwork-{uuid.uuid1()}.ckpt'):\\n        super(SimpleNetwork, self).__init__(n_train_epochs, save_path)\\n        self.first_layer = tf.keras.models.Sequential([\\n            L.Dense(16, activation),\\n            L.Dense(32, activation),\\n            L.Dense(hidden_dim, activation)])\\n        self.last_layer = L.Dense(output_dim)\\n        self.tf_config = tf.ConfigProto(log_device_placement=False,\\n                                        gpu_options=tf.GPUOptions(\\n                                            allow_growth=True,\\n                                        ))\\n        self.input_dim = input_dim\\n        self.output_dim = output_dim\\n        self.n_train_epochs = n_train_epochs\\n        self._build_graph(input_dim, output_dim)\\n        self.saver = tf.compat.v1.train.Saver()\\n\\n    def __call__(self, x):\\n        self.bases = self.first_layer(x)\\n        return self.last_layer(self.bases)\\n\\n    def _build_graph(self, xdim: int, ydim: int):\\n        if True:\\n            self.y_plh_train = tf.placeholder(tf.float32, shape=[None, ydim], name='ytrain')\\n            self.x_plh_train = tf.placeholder(tf.float32, shape=[None, xdim], name='xtrain')\\n            out = self(self.x_plh_train)\\n            mse_loss = tf.reduce_mean(tf.square(self.y_plh_train - out))\\n            self.train_loss = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.Session(config=self.tf_config) as sess:\\n                sess.run(tf.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for x, y in zip(xtrain, ytrain):\\n                        x = x.to_numpy().reshape(1, self.input_dim)\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh_train: x, self.y_plh_train: y})\\n                for x, y in zip(xtrain, ytrain):\\n                    x = x.to_numpy().reshape(1, self.input_dim)\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with tf.Session(config=self.tf_config) as sess:\\n            self.saver.restore(sess, self.save_path)\\n            for x in xeval:\\n                x = x.to_numpy().reshape(1, self.input_dim)\\n                y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n            bases = np.concatenate(bases)\\n        return bases\", path='tfdbonas/deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='# -*- coding: utf-8 -*-\\n#\\n# Copyright 2018-2020 Data61, CSIRO\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras import backend as K\\nfrom tensorflow.keras import activations, initializers, constraints, regularizers\\nfrom tensorflow.keras.layers import Input, Layer, Lambda, Dropout, Reshape\\n\\n\\nclass GraphConvolution(Layer):\\n\\n    \"\"\"\\n    Graph Convolution (GCN) Keras layer.\\n    The implementation is based on the keras-gcn github repo https://github.com/tkipf/keras-gcn.\\n\\n    Original paper: Semi-Supervised Classification with Graph Convolutional Networks. Thomas N. Kipf, Max Welling,\\n    International Conference on Learning Representations (ICLR), 2017 https://github.com/tkipf/gcn\\n\\n    Notes:\\n      - The inputs are tensors with a batch dimension of 1:\\n        Keras requires this batch dimension, and for full-batch methods\\n        we only have a single \"batch\".\\n\\n      - There are three inputs required, the node features, the output\\n        indices (the nodes that are to be selected in the final layer)\\n        and the normalized graph Laplacian matrix\\n\\n      - This class assumes that the normalized Laplacian matrix is passed as\\n        input to the Keras methods.\\n\\n      - The output indices are used when ``final_layer=True`` and the returned outputs\\n        are the final-layer features for the nodes indexed by output indices.\\n\\n      - If ``final_layer=False`` all the node features are output in the same ordering as\\n        given by the adjacency matrix.\\n\\n    Args:\\n        units (int): dimensionality of output feature vectors\\n        activation (str or func): nonlinear activation applied to layer\\'s output to obtain output features\\n        use_bias (bool): toggles an optional bias\\n        final_layer (bool): If False the layer returns output for all nodes,\\n                            if True it returns the subset specified by the indices passed to it.\\n        kernel_initializer (str or func, optional): The initialiser to use for the weights.\\n        kernel_regularizer (str or func, optional): The regulariser to use for the weights.\\n        kernel_constraint (str or func, optional): The constraint to use for the weights.\\n        bias_initializer (str or func, optional): The initialiser to use for the bias.\\n        bias_regularizer (str or func, optional): The regulariser to use for the bias.\\n        bias_constraint (str or func, optional): The constraint to use for the bias.\\n    \"\"\"\\n\\n    def __init__(\\n            self,\\n            units,\\n            activation=None,\\n            use_bias=True,\\n            final_layer=False,\\n            input_dim=None,\\n            kernel_initializer=\"glorot_uniform\",\\n            kernel_regularizer=None,\\n            kernel_constraint=None,\\n            bias_initializer=\"zeros\",\\n            bias_regularizer=None,\\n            bias_constraint=None,\\n            **kwargs\\n    ):\\n        if \"input_shape\" not in kwargs and input_dim is not None:\\n            kwargs[\"input_shape\"] = (input_dim,)\\n\\n        self.units = units\\n        self.activation = activations.get(activation)\\n        self.use_bias = use_bias\\n        self.final_layer = final_layer\\n\\n        self.kernel_initializer = initializers.get(kernel_initializer)\\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\\n        self.kernel_constraint = constraints.get(kernel_constraint)\\n        self.bias_initializer = initializers.get(bias_initializer)\\n        self.bias_regularizer = regularizers.get(bias_regularizer)\\n        self.bias_constraint = constraints.get(bias_constraint)\\n\\n        super().__init__(**kwargs)\\n\\n    def get_config(self):\\n        \"\"\"\\n        Gets class configuration for Keras serialization.\\n        Used by keras model serialization.\\n\\n        Returns:\\n            A dictionary that contains the config of the layer\\n        \"\"\"\\n\\n        config = {\\n            \"units\": self.units,\\n            \"use_bias\": self.use_bias,\\n            \"final_layer\": self.final_layer,\\n            \"activation\": activations.serialize(self.activation),\\n            \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\\n            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\\n            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\\n            \"bias_initializer\": initializers.serialize(self.bias_initializer),\\n            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\\n            \"bias_constraint\": constraints.serialize(self.bias_constraint),\\n        }\\n\\n        base_config = super().get_config()\\n        return {**base_config, **config}\\n\\n    def compute_output_shape(self, input_shapes):\\n        \"\"\"\\n        Computes the output shape of the layer.\\n        Assumes the following inputs:\\n\\n        Args:\\n            input_shapes (tuple of ints)\\n                Shape tuples can include None for free dimensions, instead of an integer.\\n\\n        Returns:\\n            An input shape tuple.\\n        \"\"\"\\n        feature_shape, out_shape, *As_shapes = input_shapes\\n\\n        batch_dim = feature_shape[0]\\n        if self.final_layer:\\n            out_dim = out_shape[1]\\n        else:\\n            out_dim = feature_shape[1]\\n\\n        return batch_dim, out_dim, self.units\\n\\n    def build(self, input_shapes):\\n        \"\"\"\\n        Builds the layer\\n\\n        Args:\\n            input_shapes (list of int): shapes of the layer\\'s inputs (node features and adjacency matrix)\\n\\n        \"\"\"\\n        feat_shape = input_shapes[0]\\n        input_dim = int(feat_shape[-1])\\n\\n        self.kernel = self.add_weight(\\n            shape=(input_dim, self.units),\\n            initializer=self.kernel_initializer,\\n            name=\"kernel\",\\n            regularizer=self.kernel_regularizer,\\n            constraint=self.kernel_constraint,\\n        )\\n\\n        if self.use_bias:\\n            self.bias = self.add_weight(\\n                shape=(self.units,),\\n                initializer=self.bias_initializer,\\n                name=\"bias\",\\n                regularizer=self.bias_regularizer,\\n                constraint=self.bias_constraint,\\n            )\\n        else:\\n            self.bias = None\\n        self.built = True\\n\\n\\n    def call(self, inputs):\\n        \"\"\"\\n        Applies the layer.\\n\\n        Args:\\n            inputs (list): a list of 3 input tensors that includes\\n                node features (size 1 x N x F),\\n                output indices (size 1 x M)\\n                graph adjacency matrix (size N x N),\\n                where N is the number of nodes in the graph, and\\n                F is the dimensionality of node features.\\n\\n        Returns:\\n            Keras Tensor that represents the output of the layer.\\n        \"\"\"\\n        features, *As = inputs\\n        batch_dim, n_nodes, _ = K.int_shape(features)\\n        if batch_dim != 1:\\n            raise ValueError(\\n                \"Currently full-batch methods only support a batch dimension of one\"\\n            )\\n\\n        # Remove singleton batch dimension\\n        features = K.squeeze(features, 0)\\n\\n        # Calculate the layer operation of GCN\\n        A = As[0]\\n        h_graph = K.dot(A, features)\\n        output = K.dot(h_graph, self.kernel)\\n\\n        # Add optional bias & apply activation\\n        if self.bias is not None:\\n            output += self.bias\\n        output = self.activation(output)\\n\\n        return output\\n\\n\\nclass GraphGather(Layer):\\n    # copy from https://github.com/clinfo/kGCN/blob/master/kgcn/layers.py\\n    def __init__(self, **kwargs):\\n        super(GraphGather, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):  # input: batch_size x node_num x #inputs\\n        super(GraphGather, self).build(input_shape)\\n\\n    def call(self, inputs, **kwargs):\\n        return tf.reduce_sum(inputs, axis=1)\\n\\n    def compute_output_shape(self, input_shape):\\n        return input_shape[0], input_shape[2]', path='tfdbonas/layers.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import math\\nimport typing\\nimport random\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.optimize\\nimport scipy.stats\\n\\nfrom .trial import Trial, TrialGenerator\\nfrom .utils import State, load_class\\nfrom .acquistion_functions import AcquisitonFunction, AcquisitonFunctionType\\n\\n\\nclass OptimizerType(Flag):\\n    DNGO = auto()\\n\\n\\nclass DNGO:\\n    def __init__(self, trial_generator, acq_func_type=AcquisitonFunctionType.EI):\\n        self._trials_indices = list(range(len(trial_generator)))\\n        self.trial_generator = trial_generator\\n        self._state = State.NotInitialized\\n        self._searched_trial_indices: typing.List[int] = []\\n        self.results: typing.Dict[int, float] = {}\\n        self._deep_surrogate_model_restore_path = \\'/tmp/model.ckpt\\'\\n        self.acq_func = AcquisitonFunction(acq_func_type)\\n\\n    def run(self, objective: typing.Callable[[Trial], float],\\n            n_trials: int, **kwargs):\\n        n_random_trials = kwargs[\\'n_random_trials\\']\\n        deep_surrogate_model = kwargs[\\'deep_surrogate_model\\']\\n        model_kwargs = kwargs[\\'model_kwargs\\']\\n        _ = self._random_search(objective, n_random_trials)\\n        results = self._bayes_search(objective,\\n                                     n_trials - n_random_trials,\\n                                     deep_surrogate_model,\\n                                     model_kwargs)\\n        return results\\n\\n    def _random_search(self,\\n                       objective: typing.Callable[[Trial], float],\\n                       n_trials: int) -> typing.List[int]:\\n        assert len(self._trials_indices) >= n_trials, (f\\'len(self._trials_indices) >= n_trials:\\'\\n                                                       f\\' {len(self._trials_indices)} >= {n_trials}\\')\\n        trial_indices = random.sample(self._trials_indices, n_trials)\\n        for i in trial_indices:\\n            self.results[i] = objective(self.trial_generator[i])\\n            self._trials_indices.remove(i)\\n        self._searched_trial_indices += trial_indices\\n        self._state = State.Initialized\\n        return self._trials_indices # return remained trials\\n\\n    def _bayes_search(self,\\n                      objective: typing.Callable[[Trial], float],\\n                      n_trials: int,\\n                      deep_surrogate_model_path: str,\\n                      model_kwargs: typing.Dict) -> typing.List[int]:\\n        deep_surrogate_model_class = load_class(deep_surrogate_model_path)\\n        deep_surrogate_model = deep_surrogate_model_class(**model_kwargs)\\n        assert self._state == State.Initialized, (\\'not initialied: please call \\'\\n                                                  \\'self.random_search() before calling bayes_search.\\')\\n        assert len(self._searched_trial_indices) != 0, \\'Before searching, you have to run random search.\\'\\n        for _ in range(n_trials):\\n            trained_bases = self._train_deep_surrogate_model(\\n                self._searched_trial_indices,\\n                self.results,\\n                deep_surrogate_model)\\n            n_samples = len(self._searched_trial_indices)\\n            n_features = self.trial_generator.n_features\\n            params = self._update_mll_params(trained_bases,\\n                                             self._searched_trial_indices,\\n                                             self.results,\\n                                             n_samples,\\n                                             n_features)\\n            mean, var = self._predict(params, self._trials_indices, deep_surrogate_model)\\n            acq_values = self._calc_acq_value(mean, var, self.results)\\n            next_sample_index = self._trials_indices[np.argmax(acq_values)]\\n            self._searched_trial_indices.append(next_sample_index)\\n            self._trials_indices.remove(next_sample_index)\\n            self.results[next_sample_index] = objective(self.trial_generator[next_sample_index])\\n        return self.results\\n\\n    def _train_deep_surrogate_model(self,\\n                                    searched_trial_indices: typing.List[int],\\n                                    results: typing.Dict[int, float],\\n                                    deep_surrogate_model,\\n                                    n_training_epochs: int = 100):\\n        assert isinstance(n_training_epochs, int), f\\'invalid input type: type(n_training_epochs) {type(n_training_epochs)}\\'\\n        assert len(searched_trial_indices) == len(results), (\\'invalid inputs, searched_trial_indices[{searched_trial_indices}] \\'\\n                                                             \\'and results[{results}] must be the same length.\\')\\n        searched_trials = [self.trial_generator[i] for i in searched_trial_indices]\\n        trained_bases = deep_surrogate_model.train(searched_trials, results, n_training_epochs)\\n        return trained_bases\\n\\n    def _predict_deep_surrogate_model(self,\\n                                      non_searched_trial_indices: typing.List[int],\\n                                      deep_surrogate_model):\\n        non_searched_trials = [self.trial_generator[i] for i in non_searched_trial_indices]\\n        predicted_bases = deep_surrogate_model.predict(non_searched_trials)\\n        return predicted_bases\\n\\n    def _predict(self, params, remained_trial_indicees, deep_surrogate_model):\\n        _, beta = np.exp(params)\\n        predicted_bases = self._predict_deep_surrogate_model(remained_trial_indicees,\\n                                                             deep_surrogate_model)\\n        mean = np.matmul(predicted_bases, self.mat)\\n        var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n        return mean, var\\n\\n    def _calc_acq_value(self, mean, var, results):\\n        # TODO: current version is just for EI.\\n        min_val = np.float32(np.min(list(results.values())))\\n        return self.acq_func(mean, var, min_val)\\n\\n    def _update_mll_params(self, bases, searched_trial_indices,\\n                           results, n_samples, n_features):\\n\\n        y_values = np.array([results[i] for i in searched_trial_indices])\\n        params = scipy.optimize.fmin(self._calc_marginal_log_likelihood,\\n                                     np.random.rand(2),\\n                                     args=(bases, y_values, n_samples, n_features))\\n        return params\\n\\n    def _calc_marginal_log_likelihood(self,\\n                                      theta,\\n                                      phi,\\n                                      y_values,\\n                                      n_samples,\\n                                      n_features):\\n        # TODO: input type check\\n        assert theta.size == 2, f\"invalid input: theta => {theta}\"\\n        assert len(theta.shape) == 1, f\"invalid input: theta => {theta}\"\\n        assert y_values.size == n_samples, f\"invalid input: y_values.size => {y_values.size}\"\\n        alpha, beta = np.exp(theta)\\n\\n        # calculate K matrix\\n        identity = np.eye(phi.shape[1])\\n        phi_t = phi.transpose(1, 0)\\n        k_mat = beta * np.matmul(phi_t, phi) + alpha * identity\\n\\n        # calculate mat\\n        k_inv = np.linalg.inv(k_mat)\\n        mat = beta * np.matmul(k_inv, phi_t)\\n        mat = np.matmul(mat, y_values)\\n\\n        self.mat = np.float32(mat)\\n        self.k_inv = np.float32(k_inv)\\n        mll = n_features / 2. * np.log(alpha)\\n        mll += n_samples / 2. * np.log(beta)\\n        mll -= n_samples / 2. * np.log(2 * math.pi)\\n        mll -= beta / 2. * np.linalg.norm(y_values - np.matmul(phi, mat))\\n        mll -= alpha / 2. * mat.dot(mat)\\n        mll -= 0.5 * np.log(np.linalg.det(k_mat))\\n        return -mll', path='tfdbonas/optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pathlib\\nimport typing\\n\\nfrom .trial import (Trial,\\n                    TrialGenerator)\\nfrom .optimizer import (OptimizerType,\\n                        DNGO)\\nfrom .utils import State\\n\\n\\n\\nclass Searcher:\\n    def __init__(self, search_algorithm=OptimizerType.DNGO):\\n        self.trial_generator = TrialGenerator()\\n        self.search_algorithm = search_algorithm\\n        self._state = State.NotInitialized\\n\\n    def register_trial(self, name: str, trial: list):\\n        self.trial_generator.register(name, trial)\\n\\n    def search(self,\\n               objective: typing.Callable[[Trial], float],\\n               n_trials: int, **kwargs):\\n\\n        if OptimizerType.DNGO == self.search_algorithm:\\n            Optimizer = DNGO\\n            if not \\'deep_surrogate_model\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'deep_surrogate_model(str)\\' in \\'kwargs\\' as search options\")\\n            if not \\'n_random_trials\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n            if not \\'model_kwargs\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n\\n        else:\\n            raise NotImplementedError(\"supported optimizer: DNGO\")\\n        optimizer = Optimizer(self.trial_generator)\\n        self.result = optimizer.run(objective, n_trials, **kwargs)\\n        max_value_idx = max(self.result, key=lambda k: self.result[k])\\n        self.best_trial = self.trial_generator[max_value_idx]\\n        self.best_value = self.result[max_value_idx]\\n        return self\\n\\n    def __len__(self):\\n        return len(self.trial_generator)', path='tfdbonas/searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import typing\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\n\\n\\nclass Params(Flag):\\n    NN = auto()\\n\\n\\nclass Trial:\\n    \\'\\'\\' this class is only accessed by TrialGenerator.\\n    \\'\\'\\'\\n    def __init__(self):\\n        self._elements = {}\\n        del self._elements[\\'_elements\\'] # remove self setattr\\n\\n    def __setattr__(self, name: str, value):\\n        super.__setattr__(self, name, value)\\n        self._elements[name] = value\\n\\n    def __eq__(self, other: dict or \\'Trial\\'):\\n        if isinstance(other, Trial):\\n            return self._elements == other._elements\\n        elif isinstance(other, dict):\\n            return self._elements == other\\n        else:\\n            return NotImplemented\\n    def __len__(self):\\n        \\'\\'\\' return number of elements\\n        \\'\\'\\'\\n        return len(self._elements)\\n\\n    def __str__(self):\\n        o = \"{\"\\n        for k, v in list(self._elements.items()):\\n            if k == \\'_elements\\':\\n                continue\\n            o += f\"{k}: {v}, \"\\n        o = o[:-2] # remove the last comma.\\n        o += \"}\"\\n        return o\\n\\n    def to_dict(self):\\n        return self._elements\\n\\n    def to_numpy(self):\\n        \\'\\'\\'if elements are one values, return\\n        \\'\\'\\'\\n        return np.array([v for k, v in list(self._elements.items())])\\n\\n\\nclass TrialGenerator:\\n    def __init__(self):\\n        self._registered: typing.Dict[str, list] = {}\\n        self._registered_length: typing.Dict[str, int] = {}\\n        self.trial = Trial()\\n        self._len = 1\\n        self._n_features = 0\\n\\n    def register(self, name: str, trials: typing.List) -> None:\\n        assert len(trials) != 0, \"can\\'t accept empty trials.\"\\n        if name in list(self._registered.keys()):\\n            if self._registered[name] == trials:\\n                return\\n            self._len //= len(self._registered[name])\\n            # trials are updated\\n        self._registered[name] = trials\\n        self._registered_length[name] = len(trials)\\n        setattr(self.trial, name, None)\\n        self._len *= len(trials)\\n        self._n_features += 1\\n\\n    @property\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.getter\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.setter\\n    def n_features(self, value):\\n        raise NotImplementedError\\n\\n    def __getitem__(self, index: int) -> Trial:\\n        if index >= self._len:\\n            raise IndexError(f\"len(self) => {self._len}, your index is invalid[{index}].\")\\n        indices: typing.Dict[str, int] = {}\\n        trial = Trial()\\n        for idx, (k, n) in enumerate(self._registered_length.items()):\\n            if (idx + 1) == len(self._registered):\\n                # final key\\n                indices[k] = index % n\\n            else:\\n                remain = index % n\\n                indices[k] = remain\\n                index //= n\\n        for k, n in list(indices.items()):\\n            setattr(trial, k, self._registered[k][n])\\n        return trial\\n\\n    def __len__(self):\\n        if len(self._registered) == 0:\\n            return 0\\n        return self._len\\n\\n    def __str__(self):\\n        o = \"\"\\n        for k, i in list(self._registered.items()):\\n            o += f\"{k} : {i}\\\\n\"\\n        return o', path='tfdbonas/trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from enum import Flag, auto\\nimport importlib\\n\\nfrom .trial import Trial\\n\\nimport numpy as np\\n\\n\\nclass Result:\\n    def __init__(self, n_remenbers: int = 10):\\n        self.trials = []\\n        self.score = []\\n        self.max_score: float = 0.0\\n\\n    def push(self, score: float, trial: Trial):\\n        pass\\n\\n    def top(self) -> float:\\n        return 0\\n\\n\\nclass State(Flag):\\n    Initialized = auto()\\n    NotInitialized = auto()\\n\\ndef is_float(x) -> bool:\\n    if isinstance(x, float):\\n        return True\\n    if isinstance(x, np.float16):\\n        return True\\n    if isinstance(x, np.float32):\\n        return True\\n    if isinstance(x, np.float64):\\n        return True\\n    return False\\n\\ndef load_class(path: str):\\n    ''' load a class from path(: str).\\n    ```\\n    path = 'hoge.hoge.hoge:HogeClass'\\n    HogeClass = load_class(path)\\n    c = HogeClass()\\n    ```\\n    '''\\n    splited_path = path.split(':')\\n    assert len(splited_path) == 2, f'invalid input {splited_path}'\\n    module, class_name = splited_path\\n    module = importlib.import_module(module)\\n    return getattr(module, class_name)\", path='tfdbonas/utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from .searcher import Searcher\\nfrom .trial import Trial', path='tfdbonas/__init__.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import os\\nimport re\\nimport setuptools\\nfrom pathlib import Path\\n\\np = Path(__file__)\\n\\nsetup_requires = [\\n    \\'numpy\\',\\n    \\'pytest-runner\\'\\n]\\n\\ninstall_requires = [\\n]\\ntest_require = [\\n    \\'pytest-cov\\',\\n    \\'pytest-html\\',\\n    \\'pytest\\'\\n]\\n\\nsetuptools.setup(\\n    name=\"tfdbonas\",\\n    version=\\'0.1.0\\',\\n    python_requires=\\'>3.5\\',\\n    author=\"Koji Ono\",\\n    author_email=\"kbu94982@gmail.com\",\\n    description=\"Tensorflow Deep Bayes Optimization for Neural Network Architecture Search (tfDBONAS)\",\\n    url=\\'https://github.com/0h-n0/tfdbonas\\',\\n    long_description=(p.parent / \\'README.md\\').open(encoding=\\'utf-8\\').read(),\\n    long_description_content_type=\\'text/markdown\\',\\n    packages=setuptools.find_packages(),\\n    install_requires=install_requires,\\n    setup_requires=setup_requires,\\n    tests_require=test_require,\\n    extras_require={\\n        \\'docs\\': [\\n            \\'sphinx >= 1.4\\',\\n            \\'sphinx_rtd_theme\\']},\\n    classifiers=[\\n        \\'Programming Language :: Python :: 3.6\\',\\n        \\'Programming Language :: Python :: 3.7\\',\\n    ],\\n)', path='setup.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # x* = 0\\n    # f(x*) = 0\\n    o = 20 + np.e\\n    o += -20 * np.exp(-0.2*(trial.x ** 2 / 2 + trial.y ** 2 / 2))\\n    o += -np.exp(np.cos(2 * math.pi * trial.x**2) / 2 + np.cos(2 * math.pi * trial.y**2) / 2)\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-30, 30, 0.5))\\n    searcher.register_trial('y', np.arange(-30, 30, 0.5))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_ackley_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # (x*, y*) = [1, 1]\\n    # f(x*, y*) = 0\\n    o = 100*(trial.y - trial.x**2)**2 + (trial.x - 1)**2\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-2.048, 2.048, 0.02))\\n    searcher.register_trial('y', np.arange(-2.048, 2.048, 0.02))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_rosenbrock_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport warnings\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    return -(trial.x**2 + trial.y**2)\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-10, 10, 0.1))\\n    searcher.register_trial('y', np.arange(-10, 10, 0.1))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn('results = {}'.format(searcher.result))\\n    warnings.warn('best_trial {}'.format(searcher.best_trial))\\n    warnings.warn('best_value {}'.format(searcher.best_value))\", path='examples/dngo_2d_sphere_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 4:\\n            if layer[2] is None and layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                xx = [x1, x2, xx[2], xx[3]]\\n            elif layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                xx = [x1, x2, x3, xx[3]]\\n            else:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                x4 = layer[3](xx[3])\\n                xx = [x1, x2, x3, x4]\\n        elif len(layer) == 3:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2], xx[3]]\\n        elif len(layer) == 2:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2]]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_trains = np.split(x_train, 4, axis=1)\\n    x_tests = np.split(x_test, 4, axis=1)\\n    inputs = [keras.layers.Input(shape=(7, 28), name=f'input{i}')\\n              for i in range(1, 5)]\\n    out = create_model(network, inputs)\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=inputs, outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit(x_trains, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_tests, y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m3 = create_modal()\\n    m4 = create_modal()\\n    m = m1 + m2 + m3 + m4\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 24\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_four_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network):\\n    model = keras.Sequential()\\n    for layer in network:\\n        model.add(layer[0])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    model = create_model(network)\\n\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    #mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\nif __name__ == \\'__main__\\':\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(28, 28)),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation=\\'softmax\\'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    num_nodes = 10\\n    num_layer_type = 3\\n    searcher = Searcher()\\n    searcher.register_trial(\\'graph\\', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f\\'tfdbonas.deep_surrogate_models:GCNSurrogateModel\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print((\\'best_trial\\', searcher.best_trial))\\n    print((\\'best_value\\', searcher.best_value))', path='examples/mnist_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_model(h1, h2, h3, h4):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(h1, activation=\\'relu\\'),\\n        keras.layers.Dense(h2, activation=\\'relu\\'),\\n        keras.layers.Dense(h3, activation=\\'relu\\'),\\n        keras.layers.Dense(h4, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_model(trial.h1, trial.h2, trial.h3, trial.h4)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01) #trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128) #trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\nif __name__ == \\'__main__\\':\\n    searcher = Searcher()\\n    searcher.register_trial(\\'h1\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h2\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h3\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h4\\', [16, 32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=4,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))', path='examples/mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 2:\\n            x1 = layer[0](xx[0])\\n            x2 = layer[1](xx[1])\\n            xx = [x1, x2]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n                print((xx.shape))\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_train_1, x_train_2 = np.split(x_train, 2, axis=1)\\n    x_test_1, x_test_2 = np.split(x_test, 2, axis=1)\\n    left_input = keras.layers.Input(shape=(14, 28), name='left_input')\\n    right_input = keras.layers.Input(shape=(14, 28), name='right_input')\\n    out = create_model(network, [left_input, right_input])\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=[left_input, right_input], outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit([x_train_1, x_train_2], y_train, epochs=1, batch_size=128)\\n    out = model.evaluate([x_test_1, x_test_2], y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m = m1 + m2\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 12\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_two_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\n# ref: https://github.com/optuna/optuna/blob/master/examples/quadratic_simple.py\\nimport optuna\\nimport numpy as np\\n\\ndef objective(trial):\\n    x = trial.suggest_categorical('x', np.arange(-10, 10, 0.1))\\n    y = trial.suggest_categorical('y', np.arange(-10, 10, 0.1))\\n    return x**2 + y**2\\n\\n\\nif __name__ == '__main__':\\n    # Let us minimize the objective function above.\\n    print('Running 10 trials...')\\n    study = optuna.create_study()\\n    study.optimize(objective, n_trials=10)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can continue the optimization as follows.\\n    print('Running 20 additional trials...')\\n    study.optimize(objective, n_trials=20)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can specify the timeout instead of a number of trials.\\n    print('Running additional trials in 2 seconds...')\\n    study.optimize(objective, timeout=2.0)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\", path='examples/optuna_2d_shapre_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from tfdbonas.acquistion_functions import (_expected_improvement,\\n                                           AcquisitonFunction,\\n                                           AcquisitonFunctionType)\\n\\nimport numpy as np\\nimport pytest\\n\\n\\ndef test__expected_improvement():\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n    assert eis.size == 9\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_with_invalid_shape():\\n    mean = np.arange(0, 1, 0.1).reshape(2, 5)\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_invalid_type():\\n    mean = [0.1*i for i in range(10)]\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__AcquisitonFunctionType():\\n    assert AcquisitonFunctionType.EI == 1\\n\\ndef test__AcquisitonFunction():\\n    f = AcquisitonFunction(AcquisitonFunctionType.EI)\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = f(mean, sigma, min_val)\\n    assert eis.size == 9', path='tests/test_acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pytest\\n\\nimport tensorflow as tf\\n\\nfrom tfdbonas.deep_surrogate_models import SimpleNetwork\\nfrom tfdbonas.trial import Trial\\n\\n@pytest.mark.skipif(True, reason=\"remove kgcn\")\\ndef test_get_kgcn_gcn_class():\\n    import tensorflow as tf\\n    gcn_class = get_kgcn_gcn_class()\\n    g = gcn_class(32)\\n    features = tf.placeholder(tf.float32, shape=(1, 25, 10))\\n    adj = [tf.sparse_placeholder(tf.float32, shape=(25, 25)), ]\\n    o = g(features, [adj,])\\n    print((g.bases.shape))\\n    assert (1, 32) == o.shape\\n    assert (1, 64) == g.bases.shape\\n\\n\\n@pytest.mark.skipif(not tf.test.is_gpu_available(), reason=\"No GPU\")\\ndef test_simple_network():\\n    s = SimpleNetwork(1, 32)\\n    t = Trial()\\n    setattr(t, \\'hidden1\\', 16)\\n    setattr(t, \\'hidden2\\', 32)\\n    setattr(t, \\'lr\\', 0.01)\\n    setattr(t, \\'batchsize\\', 64)\\n    s.train([t,], [0.2,])', path='tests/test_deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import os\\nimport unittest\\nimport importlib\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom tfdbonas.optimizer import DNGO\\nfrom tfdbonas.trial import TrialGenerator\\n\\n\\n@pytest.fixture(\\n    params=[\\n        {'n_trials': 10, 'n_remains': 990},\\n        {'n_trials': 500, 'n_remains': 500},\\n        {'n_trials': 1, 'n_remains': 999},\\n        {'n_trials': 1000, 'n_remains': 0},\\n    ]\\n)\\ndef correct_tirals_test_case(request):\\n    return request.param\\n\\ndef test_dngo_random_search(correct_tirals_test_case):\\n    data = correct_tirals_test_case\\n    def objective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n    t = TrialGenerator()\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    algo = DNGO(t)\\n    assert len(t) == 1000\\n    assert data['n_remains'] == len(algo._random_search(objective, data['n_trials']))\\n    assert data['n_trials'] == len(algo._searched_trial_indices)\\n    assert data['n_trials'] == len(algo.results)\\n\\n\\nclass TestDNGO(unittest.TestCase):\\n    @staticmethod\\n    def objective(trial) -> float:\\n        return trial.hidden1 * trial.hidden2 * trial.lr * trial.batchsize\\n\\n    def setUp(self):\\n        params = [\\n            ['hidden1', [16, 32, 64, 128]],\\n            ['hidden2', [16, 32, 64, 128]],\\n            ['lr', [0.1 * i for i in range(10)]],\\n            ['batchsize', [64 * i for i in range(10)]],\\n        ]\\n        self.trial_generator = TrialGenerator()\\n        for inputs in params:\\n            self.trial_generator.register(inputs[0], inputs[1])\\n\\n    def test_dngo_bayes_search(self):\\n        algo = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        model_kwargs = dict(input_dim=4)\\n        algo._random_search(TestDNGO.objective, n_random)\\n        algo._bayes_search(TestDNGO.objective, n_bayes, path, model_kwargs)\\n\\n    def test__calc_marginal_log_likelihood(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        phi = np.random.rand(10, 10)\\n        y_values = np.random.rand(10)\\n        optimizer._calc_marginal_log_likelihood(theta, phi, y_values, 10, 10)\\n\\n    def test__predict(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_predict{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        remained_trial_indices = [4, 5, 6]\\n        optimizer.k_inv = np.random.rand(64, 64)\\n        optimizer.mat = np.random.rand(64, 64)\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        mean, var = optimizer._predict(theta,\\n                                       remained_trial_indices,\\n                                       deep_surrogate_model)\\n\\n\\n    def test__train_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n\\n    def load_class(self, path):\\n        splited_path = path.split(':')\\n        assert len(splited_path) == 2, f'invalid input {splited_path}'\\n        module, class_name = splited_path\\n        module = importlib.import_module(module)\\n        return getattr(module, class_name)\\n\\n    def test__predict_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        trained_bases = optimizer._predict_deep_surrogate_model(searched_trial_indices, deep_surrogate_model)\\n\\n    def test__calc_acq_values_ai(self):\\n        optimizer = DNGO(self.trial_generator)\\n        mean = np.random.rand(10)\\n        var = np.random.rand(10)\\n        results = {1: 1*i for i in range(1)}\\n        optimizer._calc_acq_value(mean, var, results)\", path='tests/test_optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import pytest\\n\\nfrom tfdbonas.searcher import Searcher\\n\\n\\ndef test_searcher():\\n    def ovjective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 100\\n\\n@pytest.mark.skip\\ndef test_searcher_with_integrated_test():\\n    def objective(trial):\\n        return trial.lr\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 1000\\n    s.search(objective, 100, n_random_trials=10)\", path='tests/test_searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from tfdbonas.trial import Trial, TrialGenerator\\n\\nimport pytest\\n\\n\\nclass Params:\\n    def __init__(self, param):\\n        self.param = param\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Params):\\n            return NotImplemented\\n        return self.param == other.param\\n\\n    def __str__(self):\\n        return 'Params'\\n\\n    def __repr__(self):\\n        return 'Params'\\n\\n#### Test for Trial ####\\n\\n@pytest.fixture(\\n        params=[\\n            [['hello', 1], [1, '{hello: 1}']],\\n            [['hello', 'str'], ['str', '{hello: str}']],\\n            [['hello', 0.0], [0.0, '{hello: 0.0}']],\\n            [['object', Params(1)], [Params(1), '{object: Params}']],\\n        ]\\n)\\ndef setattr_one_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\n@pytest.fixture(\\n        params=[\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n        ]\\n)\\ndef setattr_multiple_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_trial_instantiation(setattr_one_registered_samples_for_trial):\\n    input = setattr_one_registered_samples_for_trial[0]\\n    expected = setattr_one_registered_samples_for_trial[1]\\n    t = Trial()\\n    setattr(t, *input)\\n    assert getattr(t, input[0]) == expected[0]\\n    assert str(t) == expected[1]\\n\\ndef test_multiple_trial_instantiation(setattr_multiple_registered_samples_for_trial):\\n    t = Trial()\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        setattr(t, *input)\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        expected = samples[1]\\n        assert getattr(t, input[0]) == expected\\n\\n\\n#### Test for TrialGenerator ####\\n\\ndef test_trialgenerator_register():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n\\n    assert len(t) == 1000\\n    assert t[0]._elements == {'ints': 0,\\n                              'strs': 'str0',\\n                              'floats': 0.0}\\n    tt = Trial()\\n    tt.ints = 0\\n    tt.strs = 'str0'\\n    tt.floats = 0.0\\n\\n    assert t[0] == tt\\n\\n    assert t[998]._elements == {'ints': 8,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n    assert t[999]._elements == {'ints': 9,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n\\n@pytest.mark.xfail\\ndef test_trialgenerator_register_IndexError():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    print((t[1000]))\", path='tests/test_trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from pathlib import Path\\n\\nimport numpy as np\\nimport pytest\\n\\nfrom tfdbonas.utils import load_class, is_float\\n\\n\\ndef test_load_class():\\n    path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n    c = load_class(path)\\n    c()\\n\\n@pytest.fixture(\\n    params=[[0.1, True],\\n            [np.float16(0.1), True],\\n            [np.float32(0.1), True],\\n            [np.float64(0.1), True],\\n            [1, False],\\n            [np.int8(1), False],\\n            [np.int16(1), False],\\n            [np.int32(1), False],\\n            [np.int64(1), False],\\n    ]\\n)\\ndef float_types(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_is_float(float_types):\\n    input, expected = float_types\\n    assert is_float(input) == expected\", path='tests/test_utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport pytest\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_simple_network_model(hidden_size=128, activation=\\'relu\\'):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(hidden_size, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_simple_network_model(trial.hidden_size)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n@pytest.mark.heavy\\ndef test_simple_network():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.xfail\\ndef test_simple_network_with_categorical_features():\\n    # (False, reason=\"categorical feature is not supported yet. [idea] I shoud apply embedding layers for categorical inputs.\")\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n\\n    searcher.register_trial(\\'activation\\', [None, \\'relu\\', \\'tanh\\'])\\n    #\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\ndef create_cnn_model(trial):\\n    model = keras.Sequential([\\n        keras.layers.Conv2D(trial.cnn_h1, trial.cnn_k1,\\n                            (trial.cnn_s1, trial.cnn_s1),\\n                            activation=\\'relu\\', input_shape=(28, 28, 1)),\\n        keras.layers.MaxPooling2D(trial.pool_k1),\\n        keras.layers.Conv2D(trial.cnn_h2, trial.cnn_k2,\\n                            (trial.cnn_s2, trial.cnn_s2),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k2),\\n        keras.layers.Conv2D(trial.cnn_h3, trial.cnn_k3,\\n                            (trial.cnn_s3, trial.cnn_s3),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k3),\\n        keras.layers.Flatten(),\\n        keras.layers.Dense(trial.fc1, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\'),\\n    ])\\n    return model\\n\\ndef cnn_objectve(trial: Trial):\\n    try:\\n        model = create_cnn_model(trial)\\n        mnist = tf.keras.datasets.mnist\\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n        x_train = x_train.reshape((60000, 28, 28, 1))\\n        x_test = x_test.reshape((10000, 28, 28, 1))\\n        x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n        opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n        model.compile(optimizer=opt,\\n                      loss=\\'sparse_categorical_crossentropy\\',\\n                      metrics=[\\'accuracy\\'])\\n        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n        model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n        out = model.evaluate(x_test,  y_test, verbose=2)\\n        accuracy = out[1]\\n        return accuracy\\n    except Exception:\\n        # Graph construction is failed.\\n        return 0.0\\n\\n@pytest.mark.skipif(True, reason=\\'heavy conputational cost.\\')\\ndef test_cnn_model_with_many_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 516678750\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.skipif(True, reason=\\'memory error (32GB).\\')\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 73728\\n    # var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n    # MemoryError: Unable to allocate array with shape (73718, 73718) and data type float64\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.heavy\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1])\\n    searcher.register_trial(\\'cnn_s2\\', [1])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))', path='tests/integrated_test/test_mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.stats\\n\\nfrom .utils import is_float\\n\\ndef _expected_improvement(mean: np.array, sigma: np.array, min_val: np.array):\\n    assert isinstance(mean, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert isinstance(sigma, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert is_float(min_val), f\\'instance type error, {type(mean)}\\'\\n    assert len(mean.shape) == 1, f\\'Invalid shape error, {mean.shape}\\'\\n    assert len(sigma.shape) == 1, f\\'Invalid shape error, {sigma.shape}\\'\\n    assert mean.size == sigma.size, f\\'Invalid shape error, {sigma.size} != {sigma.size}\\'\\n    assert min_val.size == 1, f\\'Invalid shape error, {min_val.size}\\'\\n\\n    dist = scipy.stats.norm(loc=0.0, scale=1.0)\\n    gamma = (min_val - mean) / sigma\\n    pdf = dist.pdf(x=gamma)\\n    cdf = scipy.stats.norm.cdf(x=gamma, loc=0., scale=1.)\\n    ei = (min_val - mean) * cdf + (sigma * pdf)\\n    return ei\\n\\n\\nclass AcquisitonFunctionType(Flag):\\n    EI = auto()\\n\\n\\nclass AcquisitonFunction:\\n    def __init__(self, aftype: AcquisitonFunctionType = AcquisitonFunctionType.EI):\\n        if AcquisitonFunctionType.EI == aftype:\\n            self.af_func = _expected_improvement\\n        else:\\n            raise NotImplementedError(\"EI is only supported\")\\n\\n    def __call__(self, *args, **kwargs):\\n        return self.af_func(*args, **kwargs)', path='tfdbonas/acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import uuid\\nimport typing\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.layers as L\\n\\nfrom .trial import Trial\\nfrom .layers import GraphConvolution, GraphGather\\n\\n\\nclass BaseSurrogateModel:\\n    def __init__(self,\\n                 n_train_epochs: int = 100,\\n                 save_path: str = 'network-{uuid.uuid1()}.ckpt'):\\n        self.n_train_epochs = n_train_epochs\\n        self.save_path = save_path\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        raise NotImplementedError\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        raise NotImplementedError\\n\\n\\nclass GCNSurrogateModel(BaseSurrogateModel):\\n    def __init__(self,\\n                 num_nodes: int = 32,\\n                 input_channels: int = 3,\\n                 output_channels: int = 1,\\n                 hidden_channels: int = 64,\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/gcnnet-{uuid.uuid1()}.ckpt'):\\n        super(GCNSurrogateModel, self).__init__(n_train_epochs, save_path)\\n        self.gcn1 = GraphConvolution(16, activation='tanh')\\n        self.gcn2 = GraphConvolution(32, activation='tanh')\\n        self.gcn3 = GraphConvolution(64, activation='tanh')\\n        self.gather = GraphGather()\\n        self.l1 = L.Dense(hidden_channels)\\n        self.l2 = L.Dense(output_channels)\\n        self.bases = None\\n        self.tf_config = tf.compat.v1.ConfigProto(log_device_placement=False,\\n                                                  gpu_options=tf.compat.v1.GPUOptions(\\n                                                      allow_growth=True,\\n                                                  ))\\n        self._build_graph(input_channels, num_nodes, output_channels)\\n        self.vars_to_train = tf.compat.v1.trainable_variables()\\n        self.saver = tf.compat.v1.train.Saver(self.vars_to_train)\\n\\n    def last_layer(self, inputs):\\n        features, adj = inputs\\n        x = self.gcn1([features, adj])\\n        x = self.gcn2([x, adj])\\n        x = self.gcn3([x, adj])\\n        x = self.gather(x)\\n        x = self.l1(x)\\n        self.bases = x\\n        return x\\n\\n    def __call__(self, inputs, adj):\\n        x = self.last_layer([inputs, adj])\\n        x = self.l2(x)\\n        return x\\n\\n    def _build_graph(self, input_channels, num_nodes, output_channles):\\n        self.graph = tf.compat.v1.get_default_graph()\\n        with self.graph.as_default():\\n            self.y_plh = tf.compat.v1.placeholder(tf.float32,\\n                                              shape=[None, output_channles],\\n                                              name='ytrain')\\n            self.x_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, input_channels], name='x')\\n            self.x_adj_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, num_nodes], name='x_adj')\\n            out = self(self.x_plh, self.x_adj_plh)\\n            self.mse_loss = tf.reduce_mean(tf.square(self.y_plh - out))\\n            self.train_loss = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(self.mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        # currently only support batch_size == 1\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                sess.run(tf.compat.v1.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for trial, y in zip(xtrain, ytrain):\\n                        _, (adj, features) = trial.graph\\n                        features = np.expand_dims(features, axis=0) # add batch dimmension\\n                        adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y})\\n\\n\\n                for trial, y in zip(xtrain, ytrain):\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with self.graph.as_default():\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                self.saver.restore(sess, self.save_path)\\n                for trial in xeval:\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n        return bases\\n\\n\\nclass SimpleNetwork(BaseSurrogateModel):\\n    def __init__(self,\\n                 input_dim: int = 4,\\n                 output_dim: int = 1,\\n                 hidden_dim: int = 64,\\n                 activation='tanh',\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/simplenetwork-{uuid.uuid1()}.ckpt'):\\n        super(SimpleNetwork, self).__init__(n_train_epochs, save_path)\\n        self.first_layer = tf.keras.models.Sequential([\\n            L.Dense(16, activation),\\n            L.Dense(32, activation),\\n            L.Dense(hidden_dim, activation)])\\n        self.last_layer = L.Dense(output_dim)\\n        self.tf_config = tf.ConfigProto(log_device_placement=False,\\n                                        gpu_options=tf.GPUOptions(\\n                                            allow_growth=True,\\n                                        ))\\n        self.input_dim = input_dim\\n        self.output_dim = output_dim\\n        self.n_train_epochs = n_train_epochs\\n        self._build_graph(input_dim, output_dim)\\n        self.saver = tf.compat.v1.train.Saver()\\n\\n    def __call__(self, x):\\n        self.bases = self.first_layer(x)\\n        return self.last_layer(self.bases)\\n\\n    def _build_graph(self, xdim: int, ydim: int):\\n        if True:\\n            self.y_plh_train = tf.placeholder(tf.float32, shape=[None, ydim], name='ytrain')\\n            self.x_plh_train = tf.placeholder(tf.float32, shape=[None, xdim], name='xtrain')\\n            out = self(self.x_plh_train)\\n            mse_loss = tf.reduce_mean(tf.square(self.y_plh_train - out))\\n            self.train_loss = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.Session(config=self.tf_config) as sess:\\n                sess.run(tf.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for x, y in zip(xtrain, ytrain):\\n                        x = x.to_numpy().reshape(1, self.input_dim)\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh_train: x, self.y_plh_train: y})\\n                for x, y in zip(xtrain, ytrain):\\n                    x = x.to_numpy().reshape(1, self.input_dim)\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with tf.Session(config=self.tf_config) as sess:\\n            self.saver.restore(sess, self.save_path)\\n            for x in xeval:\\n                x = x.to_numpy().reshape(1, self.input_dim)\\n                y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n            bases = np.concatenate(bases)\\n        return bases\", path='tfdbonas/deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='# -*- coding: utf-8 -*-\\n#\\n# Copyright 2018-2020 Data61, CSIRO\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras import backend as K\\nfrom tensorflow.keras import activations, initializers, constraints, regularizers\\nfrom tensorflow.keras.layers import Input, Layer, Lambda, Dropout, Reshape\\n\\n\\nclass GraphConvolution(Layer):\\n\\n    \"\"\"\\n    Graph Convolution (GCN) Keras layer.\\n    The implementation is based on the keras-gcn github repo https://github.com/tkipf/keras-gcn.\\n\\n    Original paper: Semi-Supervised Classification with Graph Convolutional Networks. Thomas N. Kipf, Max Welling,\\n    International Conference on Learning Representations (ICLR), 2017 https://github.com/tkipf/gcn\\n\\n    Notes:\\n      - The inputs are tensors with a batch dimension of 1:\\n        Keras requires this batch dimension, and for full-batch methods\\n        we only have a single \"batch\".\\n\\n      - There are three inputs required, the node features, the output\\n        indices (the nodes that are to be selected in the final layer)\\n        and the normalized graph Laplacian matrix\\n\\n      - This class assumes that the normalized Laplacian matrix is passed as\\n        input to the Keras methods.\\n\\n      - The output indices are used when ``final_layer=True`` and the returned outputs\\n        are the final-layer features for the nodes indexed by output indices.\\n\\n      - If ``final_layer=False`` all the node features are output in the same ordering as\\n        given by the adjacency matrix.\\n\\n    Args:\\n        units (int): dimensionality of output feature vectors\\n        activation (str or func): nonlinear activation applied to layer\\'s output to obtain output features\\n        use_bias (bool): toggles an optional bias\\n        final_layer (bool): If False the layer returns output for all nodes,\\n                            if True it returns the subset specified by the indices passed to it.\\n        kernel_initializer (str or func, optional): The initialiser to use for the weights.\\n        kernel_regularizer (str or func, optional): The regulariser to use for the weights.\\n        kernel_constraint (str or func, optional): The constraint to use for the weights.\\n        bias_initializer (str or func, optional): The initialiser to use for the bias.\\n        bias_regularizer (str or func, optional): The regulariser to use for the bias.\\n        bias_constraint (str or func, optional): The constraint to use for the bias.\\n    \"\"\"\\n\\n    def __init__(\\n            self,\\n            units,\\n            activation=None,\\n            use_bias=True,\\n            final_layer=False,\\n            input_dim=None,\\n            kernel_initializer=\"glorot_uniform\",\\n            kernel_regularizer=None,\\n            kernel_constraint=None,\\n            bias_initializer=\"zeros\",\\n            bias_regularizer=None,\\n            bias_constraint=None,\\n            **kwargs\\n    ):\\n        if \"input_shape\" not in kwargs and input_dim is not None:\\n            kwargs[\"input_shape\"] = (input_dim,)\\n\\n        self.units = units\\n        self.activation = activations.get(activation)\\n        self.use_bias = use_bias\\n        self.final_layer = final_layer\\n\\n        self.kernel_initializer = initializers.get(kernel_initializer)\\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\\n        self.kernel_constraint = constraints.get(kernel_constraint)\\n        self.bias_initializer = initializers.get(bias_initializer)\\n        self.bias_regularizer = regularizers.get(bias_regularizer)\\n        self.bias_constraint = constraints.get(bias_constraint)\\n\\n        super().__init__(**kwargs)\\n\\n    def get_config(self):\\n        \"\"\"\\n        Gets class configuration for Keras serialization.\\n        Used by keras model serialization.\\n\\n        Returns:\\n            A dictionary that contains the config of the layer\\n        \"\"\"\\n\\n        config = {\\n            \"units\": self.units,\\n            \"use_bias\": self.use_bias,\\n            \"final_layer\": self.final_layer,\\n            \"activation\": activations.serialize(self.activation),\\n            \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\\n            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\\n            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\\n            \"bias_initializer\": initializers.serialize(self.bias_initializer),\\n            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\\n            \"bias_constraint\": constraints.serialize(self.bias_constraint),\\n        }\\n\\n        base_config = super().get_config()\\n        return {**base_config, **config}\\n\\n    def compute_output_shape(self, input_shapes):\\n        \"\"\"\\n        Computes the output shape of the layer.\\n        Assumes the following inputs:\\n\\n        Args:\\n            input_shapes (tuple of ints)\\n                Shape tuples can include None for free dimensions, instead of an integer.\\n\\n        Returns:\\n            An input shape tuple.\\n        \"\"\"\\n        feature_shape, out_shape, *As_shapes = input_shapes\\n\\n        batch_dim = feature_shape[0]\\n        if self.final_layer:\\n            out_dim = out_shape[1]\\n        else:\\n            out_dim = feature_shape[1]\\n\\n        return batch_dim, out_dim, self.units\\n\\n    def build(self, input_shapes):\\n        \"\"\"\\n        Builds the layer\\n\\n        Args:\\n            input_shapes (list of int): shapes of the layer\\'s inputs (node features and adjacency matrix)\\n\\n        \"\"\"\\n        feat_shape = input_shapes[0]\\n        input_dim = int(feat_shape[-1])\\n\\n        self.kernel = self.add_weight(\\n            shape=(input_dim, self.units),\\n            initializer=self.kernel_initializer,\\n            name=\"kernel\",\\n            regularizer=self.kernel_regularizer,\\n            constraint=self.kernel_constraint,\\n        )\\n\\n        if self.use_bias:\\n            self.bias = self.add_weight(\\n                shape=(self.units,),\\n                initializer=self.bias_initializer,\\n                name=\"bias\",\\n                regularizer=self.bias_regularizer,\\n                constraint=self.bias_constraint,\\n            )\\n        else:\\n            self.bias = None\\n        self.built = True\\n\\n\\n    def call(self, inputs):\\n        \"\"\"\\n        Applies the layer.\\n\\n        Args:\\n            inputs (list): a list of 3 input tensors that includes\\n                node features (size 1 x N x F),\\n                output indices (size 1 x M)\\n                graph adjacency matrix (size N x N),\\n                where N is the number of nodes in the graph, and\\n                F is the dimensionality of node features.\\n\\n        Returns:\\n            Keras Tensor that represents the output of the layer.\\n        \"\"\"\\n        features, *As = inputs\\n        batch_dim, n_nodes, _ = K.int_shape(features)\\n        if batch_dim != 1:\\n            raise ValueError(\\n                \"Currently full-batch methods only support a batch dimension of one\"\\n            )\\n\\n        # Remove singleton batch dimension\\n        features = K.squeeze(features, 0)\\n\\n        # Calculate the layer operation of GCN\\n        A = As[0]\\n        h_graph = K.dot(A, features)\\n        output = K.dot(h_graph, self.kernel)\\n\\n        # Add optional bias & apply activation\\n        if self.bias is not None:\\n            output += self.bias\\n        output = self.activation(output)\\n\\n        return output\\n\\n\\nclass GraphGather(Layer):\\n    # copy from https://github.com/clinfo/kGCN/blob/master/kgcn/layers.py\\n    def __init__(self, **kwargs):\\n        super(GraphGather, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):  # input: batch_size x node_num x #inputs\\n        super(GraphGather, self).build(input_shape)\\n\\n    def call(self, inputs, **kwargs):\\n        return tf.reduce_sum(inputs, axis=1)\\n\\n    def compute_output_shape(self, input_shape):\\n        return input_shape[0], input_shape[2]', path='tfdbonas/layers.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import math\\nimport typing\\nimport random\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.optimize\\nimport scipy.stats\\n\\nfrom .trial import Trial, TrialGenerator\\nfrom .utils import State, load_class\\nfrom .acquistion_functions import AcquisitonFunction, AcquisitonFunctionType\\n\\n\\nclass OptimizerType(Flag):\\n    DNGO = auto()\\n\\n\\nclass DNGO:\\n    def __init__(self, trial_generator, acq_func_type=AcquisitonFunctionType.EI):\\n        self._trials_indices = list(range(len(trial_generator)))\\n        self.trial_generator = trial_generator\\n        self._state = State.NotInitialized\\n        self._searched_trial_indices: typing.List[int] = []\\n        self.results: typing.Dict[int, float] = {}\\n        self._deep_surrogate_model_restore_path = \\'/tmp/model.ckpt\\'\\n        self.acq_func = AcquisitonFunction(acq_func_type)\\n\\n    def run(self, objective: typing.Callable[[Trial], float],\\n            n_trials: int, **kwargs):\\n        n_random_trials = kwargs[\\'n_random_trials\\']\\n        deep_surrogate_model = kwargs[\\'deep_surrogate_model\\']\\n        model_kwargs = kwargs[\\'model_kwargs\\']\\n        _ = self._random_search(objective, n_random_trials)\\n        results = self._bayes_search(objective,\\n                                     n_trials - n_random_trials,\\n                                     deep_surrogate_model,\\n                                     model_kwargs)\\n        return results\\n\\n    def _random_search(self,\\n                       objective: typing.Callable[[Trial], float],\\n                       n_trials: int) -> typing.List[int]:\\n        assert len(self._trials_indices) >= n_trials, (f\\'len(self._trials_indices) >= n_trials:\\'\\n                                                       f\\' {len(self._trials_indices)} >= {n_trials}\\')\\n        trial_indices = random.sample(self._trials_indices, n_trials)\\n        for i in trial_indices:\\n            self.results[i] = objective(self.trial_generator[i])\\n            self._trials_indices.remove(i)\\n        self._searched_trial_indices += trial_indices\\n        self._state = State.Initialized\\n        return self._trials_indices # return remained trials\\n\\n    def _bayes_search(self,\\n                      objective: typing.Callable[[Trial], float],\\n                      n_trials: int,\\n                      deep_surrogate_model_path: str,\\n                      model_kwargs: typing.Dict) -> typing.List[int]:\\n        deep_surrogate_model_class = load_class(deep_surrogate_model_path)\\n        deep_surrogate_model = deep_surrogate_model_class(**model_kwargs)\\n        assert self._state == State.Initialized, (\\'not initialied: please call \\'\\n                                                  \\'self.random_search() before calling bayes_search.\\')\\n        assert len(self._searched_trial_indices) != 0, \\'Before searching, you have to run random search.\\'\\n        for _ in range(n_trials):\\n            trained_bases = self._train_deep_surrogate_model(\\n                self._searched_trial_indices,\\n                self.results,\\n                deep_surrogate_model)\\n            n_samples = len(self._searched_trial_indices)\\n            n_features = self.trial_generator.n_features\\n            params = self._update_mll_params(trained_bases,\\n                                             self._searched_trial_indices,\\n                                             self.results,\\n                                             n_samples,\\n                                             n_features)\\n            mean, var = self._predict(params, self._trials_indices, deep_surrogate_model)\\n            acq_values = self._calc_acq_value(mean, var, self.results)\\n            next_sample_index = self._trials_indices[np.argmax(acq_values)]\\n            self._searched_trial_indices.append(next_sample_index)\\n            self._trials_indices.remove(next_sample_index)\\n            self.results[next_sample_index] = objective(self.trial_generator[next_sample_index])\\n        return self.results\\n\\n    def _train_deep_surrogate_model(self,\\n                                    searched_trial_indices: typing.List[int],\\n                                    results: typing.Dict[int, float],\\n                                    deep_surrogate_model,\\n                                    n_training_epochs: int = 100):\\n        assert isinstance(n_training_epochs, int), f\\'invalid input type: type(n_training_epochs) {type(n_training_epochs)}\\'\\n        assert len(searched_trial_indices) == len(results), (\\'invalid inputs, searched_trial_indices[{searched_trial_indices}] \\'\\n                                                             \\'and results[{results}] must be the same length.\\')\\n        searched_trials = [self.trial_generator[i] for i in searched_trial_indices]\\n        trained_bases = deep_surrogate_model.train(searched_trials, results, n_training_epochs)\\n        return trained_bases\\n\\n    def _predict_deep_surrogate_model(self,\\n                                      non_searched_trial_indices: typing.List[int],\\n                                      deep_surrogate_model):\\n        non_searched_trials = [self.trial_generator[i] for i in non_searched_trial_indices]\\n        predicted_bases = deep_surrogate_model.predict(non_searched_trials)\\n        return predicted_bases\\n\\n    def _predict(self, params, remained_trial_indicees, deep_surrogate_model):\\n        _, beta = np.exp(params)\\n        predicted_bases = self._predict_deep_surrogate_model(remained_trial_indicees,\\n                                                             deep_surrogate_model)\\n        mean = np.matmul(predicted_bases, self.mat)\\n        var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n        return mean, var\\n\\n    def _calc_acq_value(self, mean, var, results):\\n        # TODO: current version is just for EI.\\n        min_val = np.float32(np.min(list(results.values())))\\n        return self.acq_func(mean, var, min_val)\\n\\n    def _update_mll_params(self, bases, searched_trial_indices,\\n                           results, n_samples, n_features):\\n\\n        y_values = np.array([results[i] for i in searched_trial_indices])\\n        params = scipy.optimize.fmin(self._calc_marginal_log_likelihood,\\n                                     np.random.rand(2),\\n                                     args=(bases, y_values, n_samples, n_features))\\n        return params\\n\\n    def _calc_marginal_log_likelihood(self,\\n                                      theta,\\n                                      phi,\\n                                      y_values,\\n                                      n_samples,\\n                                      n_features):\\n        # TODO: input type check\\n        assert theta.size == 2, f\"invalid input: theta => {theta}\"\\n        assert len(theta.shape) == 1, f\"invalid input: theta => {theta}\"\\n        assert y_values.size == n_samples, f\"invalid input: y_values.size => {y_values.size}\"\\n        alpha, beta = np.exp(theta)\\n\\n        # calculate K matrix\\n        identity = np.eye(phi.shape[1])\\n        phi_t = phi.transpose(1, 0)\\n        k_mat = beta * np.matmul(phi_t, phi) + alpha * identity\\n\\n        # calculate mat\\n        k_inv = np.linalg.inv(k_mat)\\n        mat = beta * np.matmul(k_inv, phi_t)\\n        mat = np.matmul(mat, y_values)\\n\\n        self.mat = np.float32(mat)\\n        self.k_inv = np.float32(k_inv)\\n        mll = n_features / 2. * np.log(alpha)\\n        mll += n_samples / 2. * np.log(beta)\\n        mll -= n_samples / 2. * np.log(2 * math.pi)\\n        mll -= beta / 2. * np.linalg.norm(y_values - np.matmul(phi, mat))\\n        mll -= alpha / 2. * mat.dot(mat)\\n        mll -= 0.5 * np.log(np.linalg.det(k_mat))\\n        return -mll', path='tfdbonas/optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pathlib\\nimport typing\\n\\nfrom .trial import (Trial,\\n                    TrialGenerator)\\nfrom .optimizer import (OptimizerType,\\n                        DNGO)\\nfrom .utils import State\\n\\n\\n\\nclass Searcher:\\n    def __init__(self, search_algorithm=OptimizerType.DNGO):\\n        self.trial_generator = TrialGenerator()\\n        self.search_algorithm = search_algorithm\\n        self._state = State.NotInitialized\\n\\n    def register_trial(self, name: str, trial: list):\\n        self.trial_generator.register(name, trial)\\n\\n    def search(self,\\n               objective: typing.Callable[[Trial], float],\\n               n_trials: int, **kwargs):\\n\\n        if OptimizerType.DNGO == self.search_algorithm:\\n            Optimizer = DNGO\\n            if not \\'deep_surrogate_model\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'deep_surrogate_model(str)\\' in \\'kwargs\\' as search options\")\\n            if not \\'n_random_trials\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n            if not \\'model_kwargs\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n\\n        else:\\n            raise NotImplementedError(\"supported optimizer: DNGO\")\\n        optimizer = Optimizer(self.trial_generator)\\n        self.result = optimizer.run(objective, n_trials, **kwargs)\\n        max_value_idx = max(self.result, key=lambda k: self.result[k])\\n        self.best_trial = self.trial_generator[max_value_idx]\\n        self.best_value = self.result[max_value_idx]\\n        return self\\n\\n    def __len__(self):\\n        return len(self.trial_generator)', path='tfdbonas/searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import typing\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\n\\n\\nclass Params(Flag):\\n    NN = auto()\\n\\n\\nclass Trial:\\n    \\'\\'\\' this class is only accessed by TrialGenerator.\\n    \\'\\'\\'\\n    def __init__(self):\\n        self._elements = {}\\n        del self._elements[\\'_elements\\'] # remove self setattr\\n\\n    def __setattr__(self, name: str, value):\\n        super.__setattr__(self, name, value)\\n        self._elements[name] = value\\n\\n    def __eq__(self, other: dict or \\'Trial\\'):\\n        if isinstance(other, Trial):\\n            return self._elements == other._elements\\n        elif isinstance(other, dict):\\n            return self._elements == other\\n        else:\\n            return NotImplemented\\n    def __len__(self):\\n        \\'\\'\\' return number of elements\\n        \\'\\'\\'\\n        return len(self._elements)\\n\\n    def __str__(self):\\n        o = \"{\"\\n        for k, v in list(self._elements.items()):\\n            if k == \\'_elements\\':\\n                continue\\n            o += f\"{k}: {v}, \"\\n        o = o[:-2] # remove the last comma.\\n        o += \"}\"\\n        return o\\n\\n    def to_dict(self):\\n        return self._elements\\n\\n    def to_numpy(self):\\n        \\'\\'\\'if elements are one values, return\\n        \\'\\'\\'\\n        return np.array([v for k, v in list(self._elements.items())])\\n\\n\\nclass TrialGenerator:\\n    def __init__(self):\\n        self._registered: typing.Dict[str, list] = {}\\n        self._registered_length: typing.Dict[str, int] = {}\\n        self.trial = Trial()\\n        self._len = 1\\n        self._n_features = 0\\n\\n    def register(self, name: str, trials: typing.List) -> None:\\n        assert len(trials) != 0, \"can\\'t accept empty trials.\"\\n        if name in list(self._registered.keys()):\\n            if self._registered[name] == trials:\\n                return\\n            self._len //= len(self._registered[name])\\n            # trials are updated\\n        self._registered[name] = trials\\n        self._registered_length[name] = len(trials)\\n        setattr(self.trial, name, None)\\n        self._len *= len(trials)\\n        self._n_features += 1\\n\\n    @property\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.getter\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.setter\\n    def n_features(self, value):\\n        raise NotImplementedError\\n\\n    def __getitem__(self, index: int) -> Trial:\\n        if index >= self._len:\\n            raise IndexError(f\"len(self) => {self._len}, your index is invalid[{index}].\")\\n        indices: typing.Dict[str, int] = {}\\n        trial = Trial()\\n        for idx, (k, n) in enumerate(self._registered_length.items()):\\n            if (idx + 1) == len(self._registered):\\n                # final key\\n                indices[k] = index % n\\n            else:\\n                remain = index % n\\n                indices[k] = remain\\n                index //= n\\n        for k, n in list(indices.items()):\\n            setattr(trial, k, self._registered[k][n])\\n        return trial\\n\\n    def __len__(self):\\n        if len(self._registered) == 0:\\n            return 0\\n        return self._len\\n\\n    def __str__(self):\\n        o = \"\"\\n        for k, i in list(self._registered.items()):\\n            o += f\"{k} : {i}\\\\n\"\\n        return o', path='tfdbonas/trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from enum import Flag, auto\\nimport importlib\\n\\nfrom .trial import Trial\\n\\nimport numpy as np\\n\\n\\nclass Result:\\n    def __init__(self, n_remenbers: int = 10):\\n        self.trials = []\\n        self.score = []\\n        self.max_score: float = 0.0\\n\\n    def push(self, score: float, trial: Trial):\\n        pass\\n\\n    def top(self) -> float:\\n        return 0\\n\\n\\nclass State(Flag):\\n    Initialized = auto()\\n    NotInitialized = auto()\\n\\ndef is_float(x) -> bool:\\n    if isinstance(x, float):\\n        return True\\n    if isinstance(x, np.float16):\\n        return True\\n    if isinstance(x, np.float32):\\n        return True\\n    if isinstance(x, np.float64):\\n        return True\\n    return False\\n\\ndef load_class(path: str):\\n    ''' load a class from path(: str).\\n    ```\\n    path = 'hoge.hoge.hoge:HogeClass'\\n    HogeClass = load_class(path)\\n    c = HogeClass()\\n    ```\\n    '''\\n    splited_path = path.split(':')\\n    assert len(splited_path) == 2, f'invalid input {splited_path}'\\n    module, class_name = splited_path\\n    module = importlib.import_module(module)\\n    return getattr(module, class_name)\", path='tfdbonas/utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from .searcher import Searcher\\nfrom .trial import Trial', path='tfdbonas/__init__.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import os\\nimport re\\nimport setuptools\\nfrom pathlib import Path\\n\\np = Path(__file__)\\n\\nsetup_requires = [\\n    \\'numpy\\',\\n    \\'pytest-runner\\'\\n]\\n\\ninstall_requires = [\\n]\\ntest_require = [\\n    \\'pytest-cov\\',\\n    \\'pytest-html\\',\\n    \\'pytest\\'\\n]\\n\\nsetuptools.setup(\\n    name=\"tfdbonas\",\\n    version=\\'0.1.0\\',\\n    python_requires=\\'>3.5\\',\\n    author=\"Koji Ono\",\\n    author_email=\"kbu94982@gmail.com\",\\n    description=\"Tensorflow Deep Bayes Optimization for Neural Network Architecture Search (tfDBONAS)\",\\n    url=\\'https://github.com/0h-n0/tfdbonas\\',\\n    long_description=(p.parent / \\'README.md\\').open(encoding=\\'utf-8\\').read(),\\n    long_description_content_type=\\'text/markdown\\',\\n    packages=setuptools.find_packages(),\\n    install_requires=install_requires,\\n    setup_requires=setup_requires,\\n    tests_require=test_require,\\n    extras_require={\\n        \\'docs\\': [\\n            \\'sphinx >= 1.4\\',\\n            \\'sphinx_rtd_theme\\']},\\n    classifiers=[\\n        \\'Programming Language :: Python :: 3.6\\',\\n        \\'Programming Language :: Python :: 3.7\\',\\n    ],\\n)', path='setup.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # x* = 0\\n    # f(x*) = 0\\n    o = 20 + np.e\\n    o += -20 * np.exp(-0.2*(trial.x ** 2 / 2 + trial.y ** 2 / 2))\\n    o += -np.exp(np.cos(2 * math.pi * trial.x**2) / 2 + np.cos(2 * math.pi * trial.y**2) / 2)\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-30, 30, 0.5))\\n    searcher.register_trial('y', np.arange(-30, 30, 0.5))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_ackley_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport math\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    # (x*, y*) = [1, 1]\\n    # f(x*, y*) = 0\\n    o = 100*(trial.y - trial.x**2)**2 + (trial.x - 1)**2\\n    return -o\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-2.048, 2.048, 0.02))\\n    searcher.register_trial('y', np.arange(-2.048, 2.048, 0.02))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    print(('results = {}'.format(searcher.result)))\\n    print(('best_trial {}'.format(searcher.best_trial)))\\n    print(('best_value {}'.format(searcher.best_value)))\", path='examples/dngo_2d_rosenbrock_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport warnings\\n\\nfrom tfdbonas import Searcher, Trial\\nimport numpy as np\\n\\n\\ndef objectve(trial: Trial):\\n    return -(trial.x**2 + trial.y**2)\\n\\n\\nif __name__ == '__main__':\\n    searcher = Searcher()\\n    searcher.register_trial('x', np.arange(-10, 10, 0.1))\\n    searcher.register_trial('y', np.arange(-10, 10, 0.1))\\n    model_kwargs = dict(\\n        input_dim=2, # coresponding to the number of register_trial\\n        n_train_epochs=200,\\n    )\\n    n_trials = 20\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model='tfdbonas.deep_surrogate_models:SimpleNetwork',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn('results = {}'.format(searcher.result))\\n    warnings.warn('best_trial {}'.format(searcher.best_trial))\\n    warnings.warn('best_value {}'.format(searcher.best_value))\", path='examples/dngo_2d_sphere_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 4:\\n            if layer[2] is None and layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                xx = [x1, x2, xx[2], xx[3]]\\n            elif layer[3] is None:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                xx = [x1, x2, x3, xx[3]]\\n            else:\\n                x1 = layer[0](xx[0])\\n                x2 = layer[1](xx[1])\\n                x3 = layer[2](xx[2])\\n                x4 = layer[3](xx[3])\\n                xx = [x1, x2, x3, x4]\\n        elif len(layer) == 3:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2], xx[3]]\\n        elif len(layer) == 2:\\n            x1 = keras.layers.concatenate([xx[0], xx[1]], axis=1)\\n            xx = [x1, xx[2]]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_trains = np.split(x_train, 4, axis=1)\\n    x_tests = np.split(x_test, 4, axis=1)\\n    inputs = [keras.layers.Input(shape=(7, 28), name=f'input{i}')\\n              for i in range(1, 5)]\\n    out = create_model(network, inputs)\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=inputs, outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit(x_trains, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_tests, y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m3 = create_modal()\\n    m4 = create_modal()\\n    m = m1 + m2 + m3 + m4\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 24\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_four_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network):\\n    model = keras.Sequential()\\n    for layer in network:\\n        model.add(layer[0])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    model = create_model(network)\\n\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    #mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\nif __name__ == \\'__main__\\':\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(28, 28)),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=16),\\n                                       dict(units=32),\\n                                       dict(units=64),\\n                                       dict(units=128),\\n                                       dict(units=512),\\n                                       dict(units=1028),\\n    ])\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation=\\'softmax\\'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    num_nodes = 10\\n    num_layer_type = 3\\n    searcher = Searcher()\\n    searcher.register_trial(\\'graph\\', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f\\'tfdbonas.deep_surrogate_models:GCNSurrogateModel\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print((\\'best_trial\\', searcher.best_trial))\\n    print((\\'best_value\\', searcher.best_value))', path='examples/mnist_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_model(h1, h2, h3, h4):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(h1, activation=\\'relu\\'),\\n        keras.layers.Dense(h2, activation=\\'relu\\'),\\n        keras.layers.Dense(h3, activation=\\'relu\\'),\\n        keras.layers.Dense(h4, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_model(trial.h1, trial.h2, trial.h3, trial.h4)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01) #trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=128) #trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\nif __name__ == \\'__main__\\':\\n    searcher = Searcher()\\n    searcher.register_trial(\\'h1\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h2\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h3\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'h4\\', [16, 32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    # searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=4,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))', path='examples/mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\nimport uuid\\nimport typing\\nfrom pathlib import Path\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\n#from tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\nfrom tnng import Generator, MultiHeadLinkedListLayer\\nimport tfcg\\n\\n\\ndef create_model(network, inputs):\\n    xx = inputs\\n    for layer in network:\\n        if len(layer) == 2:\\n            x1 = layer[0](xx[0])\\n            x2 = layer[1](xx[1])\\n            xx = [x1, x2]\\n        elif len(layer) == 1:\\n            if layer[0] == 'concat':\\n                xx = keras.layers.concatenate(xx, axis=1)\\n                print((xx.shape))\\n            else:\\n                xx = layer[0](xx)\\n    return xx\\n\\n\\ndef objectve(trial: Trial):\\n    network, (_, _) = trial.graph\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    x_train_1, x_train_2 = np.split(x_train, 2, axis=1)\\n    x_test_1, x_test_2 = np.split(x_test, 2, axis=1)\\n    left_input = keras.layers.Input(shape=(14, 28), name='left_input')\\n    right_input = keras.layers.Input(shape=(14, 28), name='right_input')\\n    out = create_model(network, [left_input, right_input])\\n    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\\n    model = keras.Model(inputs=[left_input, right_input], outputs=out)\\n    model.compile(optimizer=opt,\\n                  loss='sparse_categorical_crossentropy',\\n                  metrics=['accuracy'])\\n    model.fit([x_train_1, x_train_2], y_train, epochs=1, batch_size=128)\\n    out = model.evaluate([x_test_1, x_test_2], y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n\\ndef create_modal():\\n    m = MultiHeadLinkedListLayer()\\n    # graph created\\n    dense_kwargs = [dict(units=32), dict(units=128), dict(units=512)]\\n    m.append_lazy(keras.layers.Flatten, [dict(input_shape=(14, 28)),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    m.append_lazy(keras.layers.Dense, dense_kwargs)\\n    m.append_lazy(keras.layers.ReLU, [dict(),])\\n    return m\\n\\nif __name__ == '__main__':\\n    m1 = create_modal()\\n    m2 = create_modal()\\n    m = m1 + m2\\n    m.append_lazy(keras.layers.Dense, [dict(units=10, activation='softmax'),])\\n    g = Generator(m, dump_nn_graph=True)\\n    g.draw_graph('/home/ono/Dropbox/test.png')\\n    num_nodes = 12\\n    num_layer_type = 4\\n    searcher = Searcher()\\n    searcher.register_trial('graph', g)\\n    n_trials = 30\\n    model_kwargs = dict(\\n        num_nodes=num_nodes,\\n        input_channels=num_layer_type,\\n        n_train_epochs=400,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=f'tfdbonas.deep_surrogate_models:GCNSurrogateModel',\\n                        n_random_trials=10,\\n                        model_kwargs=model_kwargs)\\n    print((searcher.result))\\n    print(('best_trial', searcher.best_trial))\\n    print(('best_value', searcher.best_value))\", path='examples/mnist_two_modal_neural_architecture_search.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"#!/usr/bin/env python\\n# ref: https://github.com/optuna/optuna/blob/master/examples/quadratic_simple.py\\nimport optuna\\nimport numpy as np\\n\\ndef objective(trial):\\n    x = trial.suggest_categorical('x', np.arange(-10, 10, 0.1))\\n    y = trial.suggest_categorical('y', np.arange(-10, 10, 0.1))\\n    return x**2 + y**2\\n\\n\\nif __name__ == '__main__':\\n    # Let us minimize the objective function above.\\n    print('Running 10 trials...')\\n    study = optuna.create_study()\\n    study.optimize(objective, n_trials=10)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can continue the optimization as follows.\\n    print('Running 20 additional trials...')\\n    study.optimize(objective, n_trials=20)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\\n\\n    # We can specify the timeout instead of a number of trials.\\n    print('Running additional trials in 2 seconds...')\\n    study.optimize(objective, timeout=2.0)\\n    print(('Best value: {} (params: {})\\\\n'.format(study.best_value, study.best_params)))\", path='examples/optuna_2d_shapre_function.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from tfdbonas.acquistion_functions import (_expected_improvement,\\n                                           AcquisitonFunction,\\n                                           AcquisitonFunctionType)\\n\\nimport numpy as np\\nimport pytest\\n\\n\\ndef test__expected_improvement():\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n    assert eis.size == 9\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_with_invalid_shape():\\n    mean = np.arange(0, 1, 0.1).reshape(2, 5)\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__expected_improvement_invalid_type():\\n    mean = [0.1*i for i in range(10)]\\n    sigma = np.arange(0, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = _expected_improvement(mean, sigma, min_val)\\n\\n@pytest.mark.xfail\\ndef test__AcquisitonFunctionType():\\n    assert AcquisitonFunctionType.EI == 1\\n\\ndef test__AcquisitonFunction():\\n    f = AcquisitonFunction(AcquisitonFunctionType.EI)\\n    mean = np.arange(0.1, 1, 0.1)\\n    sigma = np.arange(0.1, 1, 0.1)\\n    min_val = np.float64(0.1)\\n    eis = f(mean, sigma, min_val)\\n    assert eis.size == 9', path='tests/test_acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pytest\\n\\nimport tensorflow as tf\\n\\nfrom tfdbonas.deep_surrogate_models import SimpleNetwork\\nfrom tfdbonas.trial import Trial\\n\\n@pytest.mark.skipif(True, reason=\"remove kgcn\")\\ndef test_get_kgcn_gcn_class():\\n    import tensorflow as tf\\n    gcn_class = get_kgcn_gcn_class()\\n    g = gcn_class(32)\\n    features = tf.placeholder(tf.float32, shape=(1, 25, 10))\\n    adj = [tf.sparse_placeholder(tf.float32, shape=(25, 25)), ]\\n    o = g(features, [adj,])\\n    print((g.bases.shape))\\n    assert (1, 32) == o.shape\\n    assert (1, 64) == g.bases.shape\\n\\n\\n@pytest.mark.skipif(not tf.test.is_gpu_available(), reason=\"No GPU\")\\ndef test_simple_network():\\n    s = SimpleNetwork(1, 32)\\n    t = Trial()\\n    setattr(t, \\'hidden1\\', 16)\\n    setattr(t, \\'hidden2\\', 32)\\n    setattr(t, \\'lr\\', 0.01)\\n    setattr(t, \\'batchsize\\', 64)\\n    s.train([t,], [0.2,])', path='tests/test_deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import os\\nimport unittest\\nimport importlib\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom tfdbonas.optimizer import DNGO\\nfrom tfdbonas.trial import TrialGenerator\\n\\n\\n@pytest.fixture(\\n    params=[\\n        {'n_trials': 10, 'n_remains': 990},\\n        {'n_trials': 500, 'n_remains': 500},\\n        {'n_trials': 1, 'n_remains': 999},\\n        {'n_trials': 1000, 'n_remains': 0},\\n    ]\\n)\\ndef correct_tirals_test_case(request):\\n    return request.param\\n\\ndef test_dngo_random_search(correct_tirals_test_case):\\n    data = correct_tirals_test_case\\n    def objective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n    t = TrialGenerator()\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    algo = DNGO(t)\\n    assert len(t) == 1000\\n    assert data['n_remains'] == len(algo._random_search(objective, data['n_trials']))\\n    assert data['n_trials'] == len(algo._searched_trial_indices)\\n    assert data['n_trials'] == len(algo.results)\\n\\n\\nclass TestDNGO(unittest.TestCase):\\n    @staticmethod\\n    def objective(trial) -> float:\\n        return trial.hidden1 * trial.hidden2 * trial.lr * trial.batchsize\\n\\n    def setUp(self):\\n        params = [\\n            ['hidden1', [16, 32, 64, 128]],\\n            ['hidden2', [16, 32, 64, 128]],\\n            ['lr', [0.1 * i for i in range(10)]],\\n            ['batchsize', [64 * i for i in range(10)]],\\n        ]\\n        self.trial_generator = TrialGenerator()\\n        for inputs in params:\\n            self.trial_generator.register(inputs[0], inputs[1])\\n\\n    def test_dngo_bayes_search(self):\\n        algo = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        model_kwargs = dict(input_dim=4)\\n        algo._random_search(TestDNGO.objective, n_random)\\n        algo._bayes_search(TestDNGO.objective, n_bayes, path, model_kwargs)\\n\\n    def test__calc_marginal_log_likelihood(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        phi = np.random.rand(10, 10)\\n        y_values = np.random.rand(10)\\n        optimizer._calc_marginal_log_likelihood(theta, phi, y_values, 10, 10)\\n\\n    def test__predict(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_predict{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        remained_trial_indices = [4, 5, 6]\\n        optimizer.k_inv = np.random.rand(64, 64)\\n        optimizer.mat = np.random.rand(64, 64)\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        mean, var = optimizer._predict(theta,\\n                                       remained_trial_indices,\\n                                       deep_surrogate_model)\\n\\n\\n    def test__train_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n\\n    def load_class(self, path):\\n        splited_path = path.split(':')\\n        assert len(splited_path) == 2, f'invalid input {splited_path}'\\n        module, class_name = splited_path\\n        module = importlib.import_module(module)\\n        return getattr(module, class_name)\\n\\n    def test__predict_deep_surrogate_model(self):\\n        optimizer = DNGO(self.trial_generator)\\n        n_random = 10\\n        n_bayes = 10\\n        optimizer._deep_surrogate_model_restore_path = f'/tmp/test_model_{os.getpid()}.ckpt'\\n        path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n        theta = np.random.rand(2)\\n        searched_trial_indices = [1, 2, 3]\\n        deep_surrogate_model = self.load_class(path)()\\n        results = {str(i): i for i in range(3)}\\n        n_epochs = 1\\n        trained_bases = optimizer._train_deep_surrogate_model(searched_trial_indices,\\n                                                              results,\\n                                                              deep_surrogate_model,\\n                                                              n_epochs)\\n        trained_bases = optimizer._predict_deep_surrogate_model(searched_trial_indices, deep_surrogate_model)\\n\\n    def test__calc_acq_values_ai(self):\\n        optimizer = DNGO(self.trial_generator)\\n        mean = np.random.rand(10)\\n        var = np.random.rand(10)\\n        results = {1: 1*i for i in range(1)}\\n        optimizer._calc_acq_value(mean, var, results)\", path='tests/test_optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import pytest\\n\\nfrom tfdbonas.searcher import Searcher\\n\\n\\ndef test_searcher():\\n    def ovjective(trial):\\n        return trial.lr * trial.bs\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 100\\n\\n@pytest.mark.skip\\ndef test_searcher_with_integrated_test():\\n    def objective(trial):\\n        return trial.lr\\n\\n    params = [\\n        ['lr', [0.1 * i for i in range(10)]],\\n        ['bs', [64 * i for i in range(10)]],\\n        ['network', [64 * i for i in range(10)]],\\n    ]\\n\\n    s = Searcher()\\n\\n    for p in params:\\n        s.register_trial(p[0], p[1])\\n    assert len(s) == 1000\\n    s.search(objective, 100, n_random_trials=10)\", path='tests/test_searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from tfdbonas.trial import Trial, TrialGenerator\\n\\nimport pytest\\n\\n\\nclass Params:\\n    def __init__(self, param):\\n        self.param = param\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Params):\\n            return NotImplemented\\n        return self.param == other.param\\n\\n    def __str__(self):\\n        return 'Params'\\n\\n    def __repr__(self):\\n        return 'Params'\\n\\n#### Test for Trial ####\\n\\n@pytest.fixture(\\n        params=[\\n            [['hello', 1], [1, '{hello: 1}']],\\n            [['hello', 'str'], ['str', '{hello: str}']],\\n            [['hello', 0.0], [0.0, '{hello: 0.0}']],\\n            [['object', Params(1)], [Params(1), '{object: Params}']],\\n        ]\\n)\\ndef setattr_one_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\n@pytest.fixture(\\n        params=[\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n            [[['int', 1], 1],\\n             [['string', 'str'], 'str'],\\n             [['float', 0.0], 0.0]],\\n        ]\\n)\\ndef setattr_multiple_registered_samples_for_trial(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_trial_instantiation(setattr_one_registered_samples_for_trial):\\n    input = setattr_one_registered_samples_for_trial[0]\\n    expected = setattr_one_registered_samples_for_trial[1]\\n    t = Trial()\\n    setattr(t, *input)\\n    assert getattr(t, input[0]) == expected[0]\\n    assert str(t) == expected[1]\\n\\ndef test_multiple_trial_instantiation(setattr_multiple_registered_samples_for_trial):\\n    t = Trial()\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        setattr(t, *input)\\n    for samples in setattr_multiple_registered_samples_for_trial:\\n        input = samples[0]\\n        expected = samples[1]\\n        assert getattr(t, input[0]) == expected\\n\\n\\n#### Test for TrialGenerator ####\\n\\ndef test_trialgenerator_register():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n\\n    assert len(t) == 1000\\n    assert t[0]._elements == {'ints': 0,\\n                              'strs': 'str0',\\n                              'floats': 0.0}\\n    tt = Trial()\\n    tt.ints = 0\\n    tt.strs = 'str0'\\n    tt.floats = 0.0\\n\\n    assert t[0] == tt\\n\\n    assert t[998]._elements == {'ints': 8,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n    assert t[999]._elements == {'ints': 9,\\n                                'strs': 'str9',\\n                                'floats': 0.9}\\n\\n@pytest.mark.xfail\\ndef test_trialgenerator_register_IndexError():\\n    t = TrialGenerator()\\n    assert len(t) == 0\\n    params = [\\n        ['ints', list(range(10))],\\n        ['strs', [f'str{i}' for i in range(10)]],\\n        ['floats', [0.1*i for i in range(10)]],\\n    ]\\n\\n    for inputs in params:\\n        t.register(inputs[0], inputs[1])\\n    print((t[1000]))\", path='tests/test_trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from pathlib import Path\\n\\nimport numpy as np\\nimport pytest\\n\\nfrom tfdbonas.utils import load_class, is_float\\n\\n\\ndef test_load_class():\\n    path = 'tfdbonas.deep_surrogate_models:SimpleNetwork'\\n    c = load_class(path)\\n    c()\\n\\n@pytest.fixture(\\n    params=[[0.1, True],\\n            [np.float16(0.1), True],\\n            [np.float32(0.1), True],\\n            [np.float64(0.1), True],\\n            [1, False],\\n            [np.int8(1), False],\\n            [np.int16(1), False],\\n            [np.int32(1), False],\\n            [np.int64(1), False],\\n    ]\\n)\\ndef float_types(request):\\n    input = request.param[0]\\n    expected = request.param[1]\\n    return input, expected\\n\\ndef test_is_float(float_types):\\n    input, expected = float_types\\n    assert is_float(input) == expected\", path='tests/test_utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='#!/usr/bin/env python\\nimport warnings\\n\\nimport pytest\\nimport tensorflow as tf\\nimport tensorflow.keras as keras\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nfrom tfdbonas import Searcher, Trial\\n\\n\\ndef create_simple_network_model(hidden_size=128, activation=\\'relu\\'):\\n    model = keras.Sequential([\\n        keras.layers.Flatten(input_shape=(28, 28)),\\n        keras.layers.Dense(hidden_size, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\')\\n    ])\\n    return model\\n\\n\\ndef objectve(trial: Trial):\\n    model = create_simple_network_model(trial.hidden_size)\\n    mnist = tf.keras.datasets.mnist\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n    opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n    model.compile(optimizer=opt,\\n                  loss=\\'sparse_categorical_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n    model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n    out = model.evaluate(x_test,  y_test, verbose=2)\\n    accuracy = out[1]\\n    return accuracy\\n\\n@pytest.mark.heavy\\ndef test_simple_network():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.xfail\\ndef test_simple_network_with_categorical_features():\\n    # (False, reason=\"categorical feature is not supported yet. [idea] I shoud apply embedding layers for categorical inputs.\")\\n    searcher = Searcher()\\n    searcher.register_trial(\\'hidden_size\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n\\n    searcher.register_trial(\\'activation\\', [None, \\'relu\\', \\'tanh\\'])\\n    #\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=3,\\n        n_train_epochs=200,\\n    )\\n    _ = searcher.search(objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                             model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'best_value {}\\'.format(searcher.best_value))\\n\\ndef create_cnn_model(trial):\\n    model = keras.Sequential([\\n        keras.layers.Conv2D(trial.cnn_h1, trial.cnn_k1,\\n                            (trial.cnn_s1, trial.cnn_s1),\\n                            activation=\\'relu\\', input_shape=(28, 28, 1)),\\n        keras.layers.MaxPooling2D(trial.pool_k1),\\n        keras.layers.Conv2D(trial.cnn_h2, trial.cnn_k2,\\n                            (trial.cnn_s2, trial.cnn_s2),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k2),\\n        keras.layers.Conv2D(trial.cnn_h3, trial.cnn_k3,\\n                            (trial.cnn_s3, trial.cnn_s3),\\n                            activation=\\'relu\\'),\\n        keras.layers.MaxPooling2D(trial.pool_k3),\\n        keras.layers.Flatten(),\\n        keras.layers.Dense(trial.fc1, activation=\\'relu\\'),\\n        keras.layers.Dense(10, activation=\\'softmax\\'),\\n    ])\\n    return model\\n\\ndef cnn_objectve(trial: Trial):\\n    try:\\n        model = create_cnn_model(trial)\\n        mnist = tf.keras.datasets.mnist\\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\\n        x_train = x_train.reshape((60000, 28, 28, 1))\\n        x_test = x_test.reshape((10000, 28, 28, 1))\\n        x_train, x_test = x_train / 255.0, x_test / 255.0\\n\\n        opt = tf.keras.optimizers.SGD(learning_rate=trial.lr)\\n        model.compile(optimizer=opt,\\n                      loss=\\'sparse_categorical_crossentropy\\',\\n                      metrics=[\\'accuracy\\'])\\n        mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\\n        model.fit(x_train, y_train, epochs=1, batch_size=trial.batchsize)\\n        out = model.evaluate(x_test,  y_test, verbose=2)\\n        accuracy = out[1]\\n        return accuracy\\n    except Exception:\\n        # Graph construction is failed.\\n        return 0.0\\n\\n@pytest.mark.skipif(True, reason=\\'heavy conputational cost.\\')\\ndef test_cnn_model_with_many_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k2\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [4, 8, 16, 32, 64])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2, 3])\\n    searcher.register_trial(\\'pool_k3\\', [1, 2, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'batchsize\\', [16, 32, 64, 128, 256, 512, 1024])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 516678750\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.skipif(True, reason=\\'memory error (32GB).\\')\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1, 3])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1, 2])\\n    searcher.register_trial(\\'cnn_s2\\', [1, 2])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1, 2])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    # CNN: the number of trials = 73728\\n    # var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n    # MemoryError: Unable to allocate array with shape (73718, 73718) and data type float64\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))\\n\\n@pytest.mark.heavy\\ndef test_cnn_model_with_few_trials():\\n    searcher = Searcher()\\n    searcher.register_trial(\\'cnn_h1\\', [16, 64])\\n    searcher.register_trial(\\'cnn_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s1\\', [1])\\n    searcher.register_trial(\\'pool_k1\\', [1, 3])\\n    searcher.register_trial(\\'cnn_h2\\', [32, 64])\\n    searcher.register_trial(\\'cnn_k2\\', [1])\\n    searcher.register_trial(\\'cnn_s2\\', [1])\\n    searcher.register_trial(\\'pool_k2\\', [2, 3])\\n    searcher.register_trial(\\'cnn_h3\\', [64, 128])\\n    searcher.register_trial(\\'cnn_k3\\', [1, 3])\\n    searcher.register_trial(\\'cnn_s3\\', [1])\\n    searcher.register_trial(\\'pool_k3\\', [1, 3])\\n    searcher.register_trial(\\'fc1\\', [64, 128, 256])\\n    searcher.register_trial(\\'batchsize\\', [32, 64, 128])\\n    searcher.register_trial(\\'lr\\', [0.05, 0.1])\\n    n_trials = 30\\n\\n    model_kwargs = dict(\\n        input_dim=15,\\n        n_train_epochs=200,\\n    )\\n    warnings.warn(\\'CNN: the number of trials = {}\\'.format(len(searcher)))\\n    _ = searcher.search(cnn_objectve,\\n                        n_trials=n_trials,\\n                        deep_surrogate_model=\\'tfdbonas.deep_surrogate_models:SimpleNetwork\\',\\n                        n_random_trials=10,\\n\\n                        model_kwargs=model_kwargs)\\n    assert len(searcher.result) == n_trials\\n    warnings.warn(\\'CNN: results = {}\\'.format(searcher.result))\\n    warnings.warn(\\'CNN: best_trial {}\\'.format(searcher.best_trial))\\n    warnings.warn(\\'CNN: best_value {}\\'.format(searcher.best_value))', path='tests/integrated_test/test_mnist_opt_hyperparams.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.stats\\n\\nfrom .utils import is_float\\n\\ndef _expected_improvement(mean: np.array, sigma: np.array, min_val: np.array):\\n    assert isinstance(mean, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert isinstance(sigma, np.ndarray), f\\'instance type error, {type(mean)}\\'\\n    assert is_float(min_val), f\\'instance type error, {type(mean)}\\'\\n    assert len(mean.shape) == 1, f\\'Invalid shape error, {mean.shape}\\'\\n    assert len(sigma.shape) == 1, f\\'Invalid shape error, {sigma.shape}\\'\\n    assert mean.size == sigma.size, f\\'Invalid shape error, {sigma.size} != {sigma.size}\\'\\n    assert min_val.size == 1, f\\'Invalid shape error, {min_val.size}\\'\\n\\n    dist = scipy.stats.norm(loc=0.0, scale=1.0)\\n    gamma = (min_val - mean) / sigma\\n    pdf = dist.pdf(x=gamma)\\n    cdf = scipy.stats.norm.cdf(x=gamma, loc=0., scale=1.)\\n    ei = (min_val - mean) * cdf + (sigma * pdf)\\n    return ei\\n\\n\\nclass AcquisitonFunctionType(Flag):\\n    EI = auto()\\n\\n\\nclass AcquisitonFunction:\\n    def __init__(self, aftype: AcquisitonFunctionType = AcquisitonFunctionType.EI):\\n        if AcquisitonFunctionType.EI == aftype:\\n            self.af_func = _expected_improvement\\n        else:\\n            raise NotImplementedError(\"EI is only supported\")\\n\\n    def __call__(self, *args, **kwargs):\\n        return self.af_func(*args, **kwargs)', path='tfdbonas/acquistion_functions.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"import uuid\\nimport typing\\n\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.layers as L\\n\\nfrom .trial import Trial\\nfrom .layers import GraphConvolution, GraphGather\\n\\n\\nclass BaseSurrogateModel:\\n    def __init__(self,\\n                 n_train_epochs: int = 100,\\n                 save_path: str = 'network-{uuid.uuid1()}.ckpt'):\\n        self.n_train_epochs = n_train_epochs\\n        self.save_path = save_path\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        raise NotImplementedError\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        raise NotImplementedError\\n\\n\\nclass GCNSurrogateModel(BaseSurrogateModel):\\n    def __init__(self,\\n                 num_nodes: int = 32,\\n                 input_channels: int = 3,\\n                 output_channels: int = 1,\\n                 hidden_channels: int = 64,\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/gcnnet-{uuid.uuid1()}.ckpt'):\\n        super(GCNSurrogateModel, self).__init__(n_train_epochs, save_path)\\n        self.gcn1 = GraphConvolution(16, activation='tanh')\\n        self.gcn2 = GraphConvolution(32, activation='tanh')\\n        self.gcn3 = GraphConvolution(64, activation='tanh')\\n        self.gather = GraphGather()\\n        self.l1 = L.Dense(hidden_channels)\\n        self.l2 = L.Dense(output_channels)\\n        self.bases = None\\n        self.tf_config = tf.compat.v1.ConfigProto(log_device_placement=False,\\n                                                  gpu_options=tf.compat.v1.GPUOptions(\\n                                                      allow_growth=True,\\n                                                  ))\\n        self._build_graph(input_channels, num_nodes, output_channels)\\n        self.vars_to_train = tf.compat.v1.trainable_variables()\\n        self.saver = tf.compat.v1.train.Saver(self.vars_to_train)\\n\\n    def last_layer(self, inputs):\\n        features, adj = inputs\\n        x = self.gcn1([features, adj])\\n        x = self.gcn2([x, adj])\\n        x = self.gcn3([x, adj])\\n        x = self.gather(x)\\n        x = self.l1(x)\\n        self.bases = x\\n        return x\\n\\n    def __call__(self, inputs, adj):\\n        x = self.last_layer([inputs, adj])\\n        x = self.l2(x)\\n        return x\\n\\n    def _build_graph(self, input_channels, num_nodes, output_channles):\\n        self.graph = tf.compat.v1.get_default_graph()\\n        with self.graph.as_default():\\n            self.y_plh = tf.compat.v1.placeholder(tf.float32,\\n                                              shape=[None, output_channles],\\n                                              name='ytrain')\\n            self.x_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, input_channels], name='x')\\n            self.x_adj_plh = tf.compat.v1.placeholder(tf.float32, shape=[1, num_nodes, num_nodes], name='x_adj')\\n            out = self(self.x_plh, self.x_adj_plh)\\n            self.mse_loss = tf.reduce_mean(tf.square(self.y_plh - out))\\n            self.train_loss = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(self.mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        # currently only support batch_size == 1\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                sess.run(tf.compat.v1.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for trial, y in zip(xtrain, ytrain):\\n                        _, (adj, features) = trial.graph\\n                        features = np.expand_dims(features, axis=0) # add batch dimmension\\n                        adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y})\\n\\n\\n                for trial, y in zip(xtrain, ytrain):\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with self.graph.as_default():\\n            with tf.compat.v1.Session(config=self.tf_config) as sess:\\n                self.saver.restore(sess, self.save_path)\\n                for trial in xeval:\\n                    _, (adj, features) = trial.graph\\n                    features = np.expand_dims(features, axis=0) # add batch dimmension\\n                    adj = np.expand_dims(adj, axis=0) # add batch dimmension\\n                    y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh: features, self.x_adj_plh: adj, self.y_plh: y}))\\n                bases = np.concatenate(bases)\\n        return bases\\n\\n\\nclass SimpleNetwork(BaseSurrogateModel):\\n    def __init__(self,\\n                 input_dim: int = 4,\\n                 output_dim: int = 1,\\n                 hidden_dim: int = 64,\\n                 activation='tanh',\\n                 n_train_epochs: int = 100,\\n                 save_path=f'/tmp/simplenetwork-{uuid.uuid1()}.ckpt'):\\n        super(SimpleNetwork, self).__init__(n_train_epochs, save_path)\\n        self.first_layer = tf.keras.models.Sequential([\\n            L.Dense(16, activation),\\n            L.Dense(32, activation),\\n            L.Dense(hidden_dim, activation)])\\n        self.last_layer = L.Dense(output_dim)\\n        self.tf_config = tf.ConfigProto(log_device_placement=False,\\n                                        gpu_options=tf.GPUOptions(\\n                                            allow_growth=True,\\n                                        ))\\n        self.input_dim = input_dim\\n        self.output_dim = output_dim\\n        self.n_train_epochs = n_train_epochs\\n        self._build_graph(input_dim, output_dim)\\n        self.saver = tf.compat.v1.train.Saver()\\n\\n    def __call__(self, x):\\n        self.bases = self.first_layer(x)\\n        return self.last_layer(self.bases)\\n\\n    def _build_graph(self, xdim: int, ydim: int):\\n        if True:\\n            self.y_plh_train = tf.placeholder(tf.float32, shape=[None, ydim], name='ytrain')\\n            self.x_plh_train = tf.placeholder(tf.float32, shape=[None, xdim], name='xtrain')\\n            out = self(self.x_plh_train)\\n            mse_loss = tf.reduce_mean(tf.square(self.y_plh_train - out))\\n            self.train_loss = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mse_loss)\\n\\n    def train(self, xtrain=typing.List[Trial], ytrain=typing.List[float], n_epochs: int=None):\\n        if not n_epochs is None:\\n            n_epochs = self.n_train_epochs\\n        if True:\\n            bases = []\\n            with tf.Session(config=self.tf_config) as sess:\\n                sess.run(tf.global_variables_initializer())\\n                for _ in range(n_epochs):\\n                    for x, y in zip(xtrain, ytrain):\\n                        x = x.to_numpy().reshape(1, self.input_dim)\\n                        y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                        sess.run(self.train_loss, feed_dict={self.x_plh_train: x, self.y_plh_train: y})\\n                for x, y in zip(xtrain, ytrain):\\n                    x = x.to_numpy().reshape(1, self.input_dim)\\n                    y = np.array(y, dtype=np.float32).reshape(1, 1)\\n                    bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n                bases = np.concatenate(bases)\\n                self.saver.save(sess, self.save_path)\\n        return bases\\n\\n    def predict(self, xeval=typing.List[Trial]):\\n        bases = []\\n        with tf.Session(config=self.tf_config) as sess:\\n            self.saver.restore(sess, self.save_path)\\n            for x in xeval:\\n                x = x.to_numpy().reshape(1, self.input_dim)\\n                y = np.ones((1, 1), dtype=np.float32) # dummy input\\n                bases.append(sess.run(self.bases, feed_dict={self.x_plh_train: x, self.y_plh_train: y}))\\n            bases = np.concatenate(bases)\\n        return bases\", path='tfdbonas/deep_surrogate_models.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='# -*- coding: utf-8 -*-\\n#\\n# Copyright 2018-2020 Data61, CSIRO\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras import backend as K\\nfrom tensorflow.keras import activations, initializers, constraints, regularizers\\nfrom tensorflow.keras.layers import Input, Layer, Lambda, Dropout, Reshape\\n\\n\\nclass GraphConvolution(Layer):\\n\\n    \"\"\"\\n    Graph Convolution (GCN) Keras layer.\\n    The implementation is based on the keras-gcn github repo https://github.com/tkipf/keras-gcn.\\n\\n    Original paper: Semi-Supervised Classification with Graph Convolutional Networks. Thomas N. Kipf, Max Welling,\\n    International Conference on Learning Representations (ICLR), 2017 https://github.com/tkipf/gcn\\n\\n    Notes:\\n      - The inputs are tensors with a batch dimension of 1:\\n        Keras requires this batch dimension, and for full-batch methods\\n        we only have a single \"batch\".\\n\\n      - There are three inputs required, the node features, the output\\n        indices (the nodes that are to be selected in the final layer)\\n        and the normalized graph Laplacian matrix\\n\\n      - This class assumes that the normalized Laplacian matrix is passed as\\n        input to the Keras methods.\\n\\n      - The output indices are used when ``final_layer=True`` and the returned outputs\\n        are the final-layer features for the nodes indexed by output indices.\\n\\n      - If ``final_layer=False`` all the node features are output in the same ordering as\\n        given by the adjacency matrix.\\n\\n    Args:\\n        units (int): dimensionality of output feature vectors\\n        activation (str or func): nonlinear activation applied to layer\\'s output to obtain output features\\n        use_bias (bool): toggles an optional bias\\n        final_layer (bool): If False the layer returns output for all nodes,\\n                            if True it returns the subset specified by the indices passed to it.\\n        kernel_initializer (str or func, optional): The initialiser to use for the weights.\\n        kernel_regularizer (str or func, optional): The regulariser to use for the weights.\\n        kernel_constraint (str or func, optional): The constraint to use for the weights.\\n        bias_initializer (str or func, optional): The initialiser to use for the bias.\\n        bias_regularizer (str or func, optional): The regulariser to use for the bias.\\n        bias_constraint (str or func, optional): The constraint to use for the bias.\\n    \"\"\"\\n\\n    def __init__(\\n            self,\\n            units,\\n            activation=None,\\n            use_bias=True,\\n            final_layer=False,\\n            input_dim=None,\\n            kernel_initializer=\"glorot_uniform\",\\n            kernel_regularizer=None,\\n            kernel_constraint=None,\\n            bias_initializer=\"zeros\",\\n            bias_regularizer=None,\\n            bias_constraint=None,\\n            **kwargs\\n    ):\\n        if \"input_shape\" not in kwargs and input_dim is not None:\\n            kwargs[\"input_shape\"] = (input_dim,)\\n\\n        self.units = units\\n        self.activation = activations.get(activation)\\n        self.use_bias = use_bias\\n        self.final_layer = final_layer\\n\\n        self.kernel_initializer = initializers.get(kernel_initializer)\\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\\n        self.kernel_constraint = constraints.get(kernel_constraint)\\n        self.bias_initializer = initializers.get(bias_initializer)\\n        self.bias_regularizer = regularizers.get(bias_regularizer)\\n        self.bias_constraint = constraints.get(bias_constraint)\\n\\n        super().__init__(**kwargs)\\n\\n    def get_config(self):\\n        \"\"\"\\n        Gets class configuration for Keras serialization.\\n        Used by keras model serialization.\\n\\n        Returns:\\n            A dictionary that contains the config of the layer\\n        \"\"\"\\n\\n        config = {\\n            \"units\": self.units,\\n            \"use_bias\": self.use_bias,\\n            \"final_layer\": self.final_layer,\\n            \"activation\": activations.serialize(self.activation),\\n            \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\\n            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\\n            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\\n            \"bias_initializer\": initializers.serialize(self.bias_initializer),\\n            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\\n            \"bias_constraint\": constraints.serialize(self.bias_constraint),\\n        }\\n\\n        base_config = super().get_config()\\n        return {**base_config, **config}\\n\\n    def compute_output_shape(self, input_shapes):\\n        \"\"\"\\n        Computes the output shape of the layer.\\n        Assumes the following inputs:\\n\\n        Args:\\n            input_shapes (tuple of ints)\\n                Shape tuples can include None for free dimensions, instead of an integer.\\n\\n        Returns:\\n            An input shape tuple.\\n        \"\"\"\\n        feature_shape, out_shape, *As_shapes = input_shapes\\n\\n        batch_dim = feature_shape[0]\\n        if self.final_layer:\\n            out_dim = out_shape[1]\\n        else:\\n            out_dim = feature_shape[1]\\n\\n        return batch_dim, out_dim, self.units\\n\\n    def build(self, input_shapes):\\n        \"\"\"\\n        Builds the layer\\n\\n        Args:\\n            input_shapes (list of int): shapes of the layer\\'s inputs (node features and adjacency matrix)\\n\\n        \"\"\"\\n        feat_shape = input_shapes[0]\\n        input_dim = int(feat_shape[-1])\\n\\n        self.kernel = self.add_weight(\\n            shape=(input_dim, self.units),\\n            initializer=self.kernel_initializer,\\n            name=\"kernel\",\\n            regularizer=self.kernel_regularizer,\\n            constraint=self.kernel_constraint,\\n        )\\n\\n        if self.use_bias:\\n            self.bias = self.add_weight(\\n                shape=(self.units,),\\n                initializer=self.bias_initializer,\\n                name=\"bias\",\\n                regularizer=self.bias_regularizer,\\n                constraint=self.bias_constraint,\\n            )\\n        else:\\n            self.bias = None\\n        self.built = True\\n\\n\\n    def call(self, inputs):\\n        \"\"\"\\n        Applies the layer.\\n\\n        Args:\\n            inputs (list): a list of 3 input tensors that includes\\n                node features (size 1 x N x F),\\n                output indices (size 1 x M)\\n                graph adjacency matrix (size N x N),\\n                where N is the number of nodes in the graph, and\\n                F is the dimensionality of node features.\\n\\n        Returns:\\n            Keras Tensor that represents the output of the layer.\\n        \"\"\"\\n        features, *As = inputs\\n        batch_dim, n_nodes, _ = K.int_shape(features)\\n        if batch_dim != 1:\\n            raise ValueError(\\n                \"Currently full-batch methods only support a batch dimension of one\"\\n            )\\n\\n        # Remove singleton batch dimension\\n        features = K.squeeze(features, 0)\\n\\n        # Calculate the layer operation of GCN\\n        A = As[0]\\n        h_graph = K.dot(A, features)\\n        output = K.dot(h_graph, self.kernel)\\n\\n        # Add optional bias & apply activation\\n        if self.bias is not None:\\n            output += self.bias\\n        output = self.activation(output)\\n\\n        return output\\n\\n\\nclass GraphGather(Layer):\\n    # copy from https://github.com/clinfo/kGCN/blob/master/kgcn/layers.py\\n    def __init__(self, **kwargs):\\n        super(GraphGather, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):  # input: batch_size x node_num x #inputs\\n        super(GraphGather, self).build(input_shape)\\n\\n    def call(self, inputs, **kwargs):\\n        return tf.reduce_sum(inputs, axis=1)\\n\\n    def compute_output_shape(self, input_shape):\\n        return input_shape[0], input_shape[2]', path='tfdbonas/layers.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import math\\nimport typing\\nimport random\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\nimport scipy.optimize\\nimport scipy.stats\\n\\nfrom .trial import Trial, TrialGenerator\\nfrom .utils import State, load_class\\nfrom .acquistion_functions import AcquisitonFunction, AcquisitonFunctionType\\n\\n\\nclass OptimizerType(Flag):\\n    DNGO = auto()\\n\\n\\nclass DNGO:\\n    def __init__(self, trial_generator, acq_func_type=AcquisitonFunctionType.EI):\\n        self._trials_indices = list(range(len(trial_generator)))\\n        self.trial_generator = trial_generator\\n        self._state = State.NotInitialized\\n        self._searched_trial_indices: typing.List[int] = []\\n        self.results: typing.Dict[int, float] = {}\\n        self._deep_surrogate_model_restore_path = \\'/tmp/model.ckpt\\'\\n        self.acq_func = AcquisitonFunction(acq_func_type)\\n\\n    def run(self, objective: typing.Callable[[Trial], float],\\n            n_trials: int, **kwargs):\\n        n_random_trials = kwargs[\\'n_random_trials\\']\\n        deep_surrogate_model = kwargs[\\'deep_surrogate_model\\']\\n        model_kwargs = kwargs[\\'model_kwargs\\']\\n        _ = self._random_search(objective, n_random_trials)\\n        results = self._bayes_search(objective,\\n                                     n_trials - n_random_trials,\\n                                     deep_surrogate_model,\\n                                     model_kwargs)\\n        return results\\n\\n    def _random_search(self,\\n                       objective: typing.Callable[[Trial], float],\\n                       n_trials: int) -> typing.List[int]:\\n        assert len(self._trials_indices) >= n_trials, (f\\'len(self._trials_indices) >= n_trials:\\'\\n                                                       f\\' {len(self._trials_indices)} >= {n_trials}\\')\\n        trial_indices = random.sample(self._trials_indices, n_trials)\\n        for i in trial_indices:\\n            self.results[i] = objective(self.trial_generator[i])\\n            self._trials_indices.remove(i)\\n        self._searched_trial_indices += trial_indices\\n        self._state = State.Initialized\\n        return self._trials_indices # return remained trials\\n\\n    def _bayes_search(self,\\n                      objective: typing.Callable[[Trial], float],\\n                      n_trials: int,\\n                      deep_surrogate_model_path: str,\\n                      model_kwargs: typing.Dict) -> typing.List[int]:\\n        deep_surrogate_model_class = load_class(deep_surrogate_model_path)\\n        deep_surrogate_model = deep_surrogate_model_class(**model_kwargs)\\n        assert self._state == State.Initialized, (\\'not initialied: please call \\'\\n                                                  \\'self.random_search() before calling bayes_search.\\')\\n        assert len(self._searched_trial_indices) != 0, \\'Before searching, you have to run random search.\\'\\n        for _ in range(n_trials):\\n            trained_bases = self._train_deep_surrogate_model(\\n                self._searched_trial_indices,\\n                self.results,\\n                deep_surrogate_model)\\n            n_samples = len(self._searched_trial_indices)\\n            n_features = self.trial_generator.n_features\\n            params = self._update_mll_params(trained_bases,\\n                                             self._searched_trial_indices,\\n                                             self.results,\\n                                             n_samples,\\n                                             n_features)\\n            mean, var = self._predict(params, self._trials_indices, deep_surrogate_model)\\n            acq_values = self._calc_acq_value(mean, var, self.results)\\n            next_sample_index = self._trials_indices[np.argmax(acq_values)]\\n            self._searched_trial_indices.append(next_sample_index)\\n            self._trials_indices.remove(next_sample_index)\\n            self.results[next_sample_index] = objective(self.trial_generator[next_sample_index])\\n        return self.results\\n\\n    def _train_deep_surrogate_model(self,\\n                                    searched_trial_indices: typing.List[int],\\n                                    results: typing.Dict[int, float],\\n                                    deep_surrogate_model,\\n                                    n_training_epochs: int = 100):\\n        assert isinstance(n_training_epochs, int), f\\'invalid input type: type(n_training_epochs) {type(n_training_epochs)}\\'\\n        assert len(searched_trial_indices) == len(results), (\\'invalid inputs, searched_trial_indices[{searched_trial_indices}] \\'\\n                                                             \\'and results[{results}] must be the same length.\\')\\n        searched_trials = [self.trial_generator[i] for i in searched_trial_indices]\\n        trained_bases = deep_surrogate_model.train(searched_trials, results, n_training_epochs)\\n        return trained_bases\\n\\n    def _predict_deep_surrogate_model(self,\\n                                      non_searched_trial_indices: typing.List[int],\\n                                      deep_surrogate_model):\\n        non_searched_trials = [self.trial_generator[i] for i in non_searched_trial_indices]\\n        predicted_bases = deep_surrogate_model.predict(non_searched_trials)\\n        return predicted_bases\\n\\n    def _predict(self, params, remained_trial_indicees, deep_surrogate_model):\\n        _, beta = np.exp(params)\\n        predicted_bases = self._predict_deep_surrogate_model(remained_trial_indicees,\\n                                                             deep_surrogate_model)\\n        mean = np.matmul(predicted_bases, self.mat)\\n        var = np.diag(np.matmul(np.matmul(predicted_bases, self.k_inv), predicted_bases.transpose()) + 1 / beta)\\n        return mean, var\\n\\n    def _calc_acq_value(self, mean, var, results):\\n        # TODO: current version is just for EI.\\n        min_val = np.float32(np.min(list(results.values())))\\n        return self.acq_func(mean, var, min_val)\\n\\n    def _update_mll_params(self, bases, searched_trial_indices,\\n                           results, n_samples, n_features):\\n\\n        y_values = np.array([results[i] for i in searched_trial_indices])\\n        params = scipy.optimize.fmin(self._calc_marginal_log_likelihood,\\n                                     np.random.rand(2),\\n                                     args=(bases, y_values, n_samples, n_features))\\n        return params\\n\\n    def _calc_marginal_log_likelihood(self,\\n                                      theta,\\n                                      phi,\\n                                      y_values,\\n                                      n_samples,\\n                                      n_features):\\n        # TODO: input type check\\n        assert theta.size == 2, f\"invalid input: theta => {theta}\"\\n        assert len(theta.shape) == 1, f\"invalid input: theta => {theta}\"\\n        assert y_values.size == n_samples, f\"invalid input: y_values.size => {y_values.size}\"\\n        alpha, beta = np.exp(theta)\\n\\n        # calculate K matrix\\n        identity = np.eye(phi.shape[1])\\n        phi_t = phi.transpose(1, 0)\\n        k_mat = beta * np.matmul(phi_t, phi) + alpha * identity\\n\\n        # calculate mat\\n        k_inv = np.linalg.inv(k_mat)\\n        mat = beta * np.matmul(k_inv, phi_t)\\n        mat = np.matmul(mat, y_values)\\n\\n        self.mat = np.float32(mat)\\n        self.k_inv = np.float32(k_inv)\\n        mll = n_features / 2. * np.log(alpha)\\n        mll += n_samples / 2. * np.log(beta)\\n        mll -= n_samples / 2. * np.log(2 * math.pi)\\n        mll -= beta / 2. * np.linalg.norm(y_values - np.matmul(phi, mat))\\n        mll -= alpha / 2. * mat.dot(mat)\\n        mll -= 0.5 * np.log(np.linalg.det(k_mat))\\n        return -mll', path='tfdbonas/optimizer.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import pathlib\\nimport typing\\n\\nfrom .trial import (Trial,\\n                    TrialGenerator)\\nfrom .optimizer import (OptimizerType,\\n                        DNGO)\\nfrom .utils import State\\n\\n\\n\\nclass Searcher:\\n    def __init__(self, search_algorithm=OptimizerType.DNGO):\\n        self.trial_generator = TrialGenerator()\\n        self.search_algorithm = search_algorithm\\n        self._state = State.NotInitialized\\n\\n    def register_trial(self, name: str, trial: list):\\n        self.trial_generator.register(name, trial)\\n\\n    def search(self,\\n               objective: typing.Callable[[Trial], float],\\n               n_trials: int, **kwargs):\\n\\n        if OptimizerType.DNGO == self.search_algorithm:\\n            Optimizer = DNGO\\n            if not \\'deep_surrogate_model\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'deep_surrogate_model(str)\\' in \\'kwargs\\' as search options\")\\n            if not \\'n_random_trials\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n            if not \\'model_kwargs\\' in list(kwargs.keys()):\\n                raise ValueError(\"set \\'n_random_trials\\' in input \\'kwargs\\' as search options\")\\n\\n        else:\\n            raise NotImplementedError(\"supported optimizer: DNGO\")\\n        optimizer = Optimizer(self.trial_generator)\\n        self.result = optimizer.run(objective, n_trials, **kwargs)\\n        max_value_idx = max(self.result, key=lambda k: self.result[k])\\n        self.best_trial = self.trial_generator[max_value_idx]\\n        self.best_value = self.result[max_value_idx]\\n        return self\\n\\n    def __len__(self):\\n        return len(self.trial_generator)', path='tfdbonas/searcher.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='import typing\\nfrom enum import Flag, auto\\n\\nimport numpy as np\\n\\n\\nclass Params(Flag):\\n    NN = auto()\\n\\n\\nclass Trial:\\n    \\'\\'\\' this class is only accessed by TrialGenerator.\\n    \\'\\'\\'\\n    def __init__(self):\\n        self._elements = {}\\n        del self._elements[\\'_elements\\'] # remove self setattr\\n\\n    def __setattr__(self, name: str, value):\\n        super.__setattr__(self, name, value)\\n        self._elements[name] = value\\n\\n    def __eq__(self, other: dict or \\'Trial\\'):\\n        if isinstance(other, Trial):\\n            return self._elements == other._elements\\n        elif isinstance(other, dict):\\n            return self._elements == other\\n        else:\\n            return NotImplemented\\n    def __len__(self):\\n        \\'\\'\\' return number of elements\\n        \\'\\'\\'\\n        return len(self._elements)\\n\\n    def __str__(self):\\n        o = \"{\"\\n        for k, v in list(self._elements.items()):\\n            if k == \\'_elements\\':\\n                continue\\n            o += f\"{k}: {v}, \"\\n        o = o[:-2] # remove the last comma.\\n        o += \"}\"\\n        return o\\n\\n    def to_dict(self):\\n        return self._elements\\n\\n    def to_numpy(self):\\n        \\'\\'\\'if elements are one values, return\\n        \\'\\'\\'\\n        return np.array([v for k, v in list(self._elements.items())])\\n\\n\\nclass TrialGenerator:\\n    def __init__(self):\\n        self._registered: typing.Dict[str, list] = {}\\n        self._registered_length: typing.Dict[str, int] = {}\\n        self.trial = Trial()\\n        self._len = 1\\n        self._n_features = 0\\n\\n    def register(self, name: str, trials: typing.List) -> None:\\n        assert len(trials) != 0, \"can\\'t accept empty trials.\"\\n        if name in list(self._registered.keys()):\\n            if self._registered[name] == trials:\\n                return\\n            self._len //= len(self._registered[name])\\n            # trials are updated\\n        self._registered[name] = trials\\n        self._registered_length[name] = len(trials)\\n        setattr(self.trial, name, None)\\n        self._len *= len(trials)\\n        self._n_features += 1\\n\\n    @property\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.getter\\n    def n_features(self):\\n        return self._n_features\\n\\n    @n_features.setter\\n    def n_features(self, value):\\n        raise NotImplementedError\\n\\n    def __getitem__(self, index: int) -> Trial:\\n        if index >= self._len:\\n            raise IndexError(f\"len(self) => {self._len}, your index is invalid[{index}].\")\\n        indices: typing.Dict[str, int] = {}\\n        trial = Trial()\\n        for idx, (k, n) in enumerate(self._registered_length.items()):\\n            if (idx + 1) == len(self._registered):\\n                # final key\\n                indices[k] = index % n\\n            else:\\n                remain = index % n\\n                indices[k] = remain\\n                index //= n\\n        for k, n in list(indices.items()):\\n            setattr(trial, k, self._registered[k][n])\\n        return trial\\n\\n    def __len__(self):\\n        if len(self._registered) == 0:\\n            return 0\\n        return self._len\\n\\n    def __str__(self):\\n        o = \"\"\\n        for k, i in list(self._registered.items()):\\n            o += f\"{k} : {i}\\\\n\"\\n        return o', path='tfdbonas/trial.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content=\"from enum import Flag, auto\\nimport importlib\\n\\nfrom .trial import Trial\\n\\nimport numpy as np\\n\\n\\nclass Result:\\n    def __init__(self, n_remenbers: int = 10):\\n        self.trials = []\\n        self.score = []\\n        self.max_score: float = 0.0\\n\\n    def push(self, score: float, trial: Trial):\\n        pass\\n\\n    def top(self) -> float:\\n        return 0\\n\\n\\nclass State(Flag):\\n    Initialized = auto()\\n    NotInitialized = auto()\\n\\ndef is_float(x) -> bool:\\n    if isinstance(x, float):\\n        return True\\n    if isinstance(x, np.float16):\\n        return True\\n    if isinstance(x, np.float32):\\n        return True\\n    if isinstance(x, np.float64):\\n        return True\\n    return False\\n\\ndef load_class(path: str):\\n    ''' load a class from path(: str).\\n    ```\\n    path = 'hoge.hoge.hoge:HogeClass'\\n    HogeClass = load_class(path)\\n    c = HogeClass()\\n    ```\\n    '''\\n    splited_path = path.split(':')\\n    assert len(splited_path) == 2, f'invalid input {splited_path}'\\n    module, class_name = splited_path\\n    module = importlib.import_module(module)\\n    return getattr(module, class_name)\", path='tfdbonas/utils.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from .searcher import Searcher\\nfrom .trial import Trial', path='tfdbonas/__init__.py', repo_name='0h-n0/tfdbonas'),\n",
       " Pandas(content='from pathlib import Path\\n\\n# Flags\\nINP = \"-i\"  # Input path\\nOUP = \"-o\"  # Output path\\nWEI = \"-w\"  # Weights path\\nCNT = \"-c\"  # Count of augmented images to be gen\\nEPO = \"-e\"  # Epochs\\nRET = \"-r\"  # Path to weights that have to be retuned\\nSCA = \"-s\"  # Scale value\\nTHR = \"-t\"  # Distance threshold\\nPRN = \"-p\"  # Print distances\\nMTH = \"-m\"  # Method of detection\\nLND = \"-l\"  # Landmarks to show\\nEMB = \"-e\"  # Embeddings path\\nISC = \"-s\"  # Ignore scanned\\nKNN = \"-k\"  # k value for kNN\\nDIL = \"-d\"  # Use distance loss\\n\\n# Folder names and paths\\nBLW = \"box_line_width\"\\nTLW = \"text_line_width\"\\nBC = \"box_color\"\\nTC = \"text_color\"\\n\\nBOX_PARAMS = {\\n    BLW: 2,\\n    TLW: 1,\\n    BC: (255, 105, 180),\\n    TC: (255, 155, 230)\\n}\\n\\nDATA = Path(\"data\").absolute()\\nT_EXT = \".pt\"\\n\\n# Path to embeddings\\nEMBEDS = DATA/\"embeds\"/(\"embeds\"+T_EXT)\\n\\n# Path to crops\\nCROPS = DATA/\"crops\"\\nCROPS_TRAIN = CROPS/\"train\"\\nCROPS_TEST = CROPS/\"test\"\\nCROPS_AUG_TRAIN = DATA/\"crops_aug\"/\"train\"\\n\\n# Path to images with faces\\nFACE_IMAGES_PATH = DATA/\"faces\"/\"train\"\\n\\n# Default names\\nNAME = \"model\"\\nWEIGHTS = DATA/\"weights\"/(NAME+T_EXT)\\n\\n# Commands\\nAUG = \"augment\"\\nTUN = \"tune\"\\nEMD = \"embed\"\\n# detect is to be combined with image or cam\\nGET = \"detect\"\\nIMG = \"image\"\\nCAM = \"cam\"\\n\\n# test is to be combined with id or detect or acc\\nTST = \"test\"\\nIDN = \"id\"\\nDET = \"detect\"\\nACC = \"acc\"\\n\\next_comm = [GET, TST]\\nget_comm = [IMG, CAM]\\ntst_comm = [IDN, DET, ACC]\\n\\n\\nflag_dict = {\\n    AUG: {\\n        INP: Path,\\n        OUP: Path,\\n        CNT: int\\n    },\\n    EMD: {\\n        INP: Path,\\n        WEI: Path,\\n        OUP: Path\\n    },\\n    TUN: {\\n        INP: Path,\\n        OUP: Path,\\n        RET: Path,\\n        EPO: int,\\n        DIL: None,\\n    },\\n    GET: {\\n        IMG: {\\n            INP: Path,\\n            OUP: Path,\\n            ISC: None\\n        },\\n        CAM: {\\n            OUP: Path\\n        }\\n    },\\n    TST: {\\n        IDN: {\\n            INP: Path,\\n            WEI: Path,\\n            PRN: None,\\n            SCA: float,\\n            THR: float\\n        },\\n        DET: {\\n            MTH: str,\\n            LND: None,\\n            SCA: float\\n        },\\n        ACC: {\\n            INP: Path,\\n            EMB: Path,\\n            WEI: Path,\\n            KNN: int,\\n            THR: float,\\n            PRN: None\\n        }\\n    }\\n}\\n\\ndefault_dict = {\\n    AUG: {\\n        INP: CROPS_TRAIN,\\n        OUP: CROPS_AUG_TRAIN,\\n        CNT: 1000\\n    },\\n    EMD: {\\n        INP: CROPS_TRAIN,\\n        WEI: WEIGHTS,\\n        OUP: EMBEDS\\n    },\\n    TUN: {\\n        INP: CROPS,\\n        OUP: WEIGHTS,\\n        RET: None,\\n        EPO: 20,\\n        DIL: False\\n    },\\n    GET: {\\n        IMG: {\\n            INP: FACE_IMAGES_PATH,\\n            OUP: CROPS_TRAIN,\\n            ISC: False\\n        },\\n        CAM: {\\n            OUP: CROPS_TRAIN\\n        }\\n    },\\n    TST: {\\n        IDN: {\\n            INP: EMBEDS,\\n            WEI: WEIGHTS,\\n            PRN: False,\\n            SCA: 1,\\n            THR: None\\n        },\\n        DET: {\\n            MTH: \"cnn\",\\n            LND: True,\\n            SCA: 1\\n        },\\n        ACC: {\\n            INP: CROPS_TEST,\\n            EMB: EMBEDS,\\n            WEI: WEIGHTS,\\n            KNN: 7,\\n            THR: 0.7,\\n            PRN: False\\n        }\\n    }\\n\\n}', path='constants.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from constants import *\\n\\n\\ndef error(msg):\\n    raise Exception(msg)\\n\\n\\ndef get_sub_dict(arg):\\n    if len(arg) < 2:\\n        error(\"specify command\")\\n    command = arg[1]\\n\\n    if command not in list(flag_dict.keys()):\\n        error(f\"invalid command {command}\")\\n\\n    if command in ext_comm:\\n        sub_comm = arg[2]\\n        if sub_comm not in list(flag_dict[command].keys()):\\n            error(f\"incomplete command {command}\")\\n        else:\\n            return command, sub_comm, flag_dict[command][sub_comm]\\n\\n    else:\\n        return command, None, flag_dict[command]\\n\\n\\ndef get_arg_dict(arg):\\n    command, sub_command, sub_dict = get_sub_dict(arg)\\n    arg_dict = {}\\n\\n    for flag_name in list(sub_dict.keys()):\\n        typ = sub_dict[flag_name]\\n\\n        try:\\n            flag_idx = arg.index(flag_name)\\n        except ValueError:\\n            if sub_command is None:\\n                arg_dict[flag_name] = default_dict[command][flag_name]\\n            else:\\n                arg_dict[flag_name] = default_dict[command][sub_command][flag_name]\\n            continue\\n\\n        if typ is None:\\n            if sub_command is None:\\n                arg_dict[flag_name] = not default_dict[command][flag_name]\\n            else:\\n                arg_dict[flag_name] = not default_dict[command][sub_command][flag_name]\\n        else:\\n            try:\\n                value = typ(arg[flag_idx + 1])\\n                if isinstance(value, Path):\\n                    value = value.absolute()\\n                arg_dict[flag_name] = value\\n            except ValueError or IndexError:\\n                error(f\"invalid value for {arg[flag_idx]}\")\\n                return\\n    return command, sub_command, arg_dict', path='get_flags.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import sys\\nimport torch\\nfrom get_flags import get_arg_dict\\nfrom constants import BOX_PARAMS, AUG, TUN, EMD, GET, IMG, CAM, TST, IDN, DET, ACC, INP, OUP, WEI, CNT, EPO, RET, SCA, THR, PRN, MTH, LND, EMB, ISC, KNN, DIL\\n\\nfrom identify import identification, detection, from_cam, from_images, tune_network, augment, save_embeddings, accuracy\\n\\nfunc_dict = {\\n    AUG: augment,\\n    TUN: tune_network,\\n    GET: {\\n        IMG: from_images,\\n        CAM: from_cam\\n    },\\n    TST: {\\n        IDN: identification,\\n        DET: detection\\n    }\\n}\\n\\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n\\n\\ndef run():\\n    command, sub_command, a = get_arg_dict(sys.argv)\\n\\n    if command == AUG:\\n        augment(a[INP], a[OUP], a[CNT])\\n    elif command == TUN:\\n        tune_network(a[INP], a[OUP], DEVICE, a[EPO], a[RET], a[DIL])\\n    elif command == GET:\\n        if sub_command == IMG:\\n            from_images(a[INP], a[OUP], a[ISC], DEVICE)\\n        elif sub_command == CAM:\\n            from_cam(a[OUP], DEVICE)\\n    elif command == TST:\\n        if sub_command == IDN:\\n            identification(a[INP], a[WEI], DEVICE, None, None,\\n                           a[THR], BOX_PARAMS, a[SCA], a[PRN])\\n        elif sub_command == DET:\\n            detection(BOX_PARAMS, DEVICE, a[SCA], a[MTH], a[LND])\\n        elif sub_command == ACC:\\n            accuracy(a[INP], a[EMB], a[WEI], a[KNN], a[THR], a[PRN], DEVICE)\\n    elif command == EMD:\\n        save_embeddings(a[INP], a[OUP], a[WEI], DEVICE)\\n\\n\\ntry:\\n    run()\\nexcept KeyboardInterrupt:\\n    print(\"exiting\")\\n    exit(0)', path='run.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .cam_test import identification, detection, accuracy\\nfrom .get_faces import from_cam, from_images\\nfrom .models import tune_network\\nfrom .helpers import augment, save_embeddings', path='identify/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\nfrom identify.models.metrics import check_accuracy\\nfrom identify.models.utils.tuner_helpers import get_dataloader\\nfrom identify.helpers import get_model, get_embeddings\\n\\n\\ndef accuracy(input_folder, embed_folder, weights_path, k, thresh, print_dist, device):\\n    model, _ = get_model(weights_path, device)\\n    dataloader = get_dataloader(\\n        input_folder, use_transforms=False, get_split=False, drop_last=False, batch_size=16, shuffle=True)\\n    embed_dict = torch.load(embed_folder)\\n    embeds = embed_dict[\\'embeds\\']\\n    labels = embed_dict[\\'labels\\']\\n    print(f\"checking accuracy, k: {k}, threshold: {thresh}\")\\n    accuracy = check_accuracy(\\n        dataloader, embeds, labels, model, k, thresh, print_dist)\\n    print(f\"accuracy: {accuracy*100:0.3f} %\")', path='identify/cam_test/accuracy.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nGet some stats such as detection fps for detection running on a\\ngiven video stream which may be running using CNN or HOG or may\\nbe bypassed.\\n\"\"\"\\n\\nimport sys\\nimport dlib\\nimport cv2\\nimport time\\nimport numpy as np\\nimport torch\\n\\nfrom identify.models import MTCNN\\nfrom identify.helpers.constants import BLW, TLW, BC, TC\\n\\n\\ndef hog_detector(rgb_image, box_params):\\n    detector = dlib.get_frontal_face_detector()\\n    dets = detector(rgb_image)\\n\\n    for det in dets:\\n        cv2.rectangle(rgb_image, (det.left(), det.top()),\\n                      (det.right(), det.bottom()), box_params[BC], box_params[BLW])\\n\\n    return rgb_image\\n\\n\\ndef cnn_detector(rgb_img, model, landmarks, box_params):\\n    boxes, probs, points = model.detect(rgb_img, landmarks=landmarks)\\n\\n    if probs[0] == None:\\n        return rgb_img\\n\\n    img_boxed = rgb_img.copy()\\n\\n    # Boxing the face\\n    for p, box in zip(probs, boxes):\\n\\n        p1 = (box[0], box[1])\\n        p2 = (box[2], box[3])\\n\\n        cv2.putText(img_boxed, str(p), (p1[0], int(p1[1]+20)), cv2.FONT_HERSHEY_SIMPLEX,\\n                    0.6, box_params[TC], box_params[TLW])\\n        cv2.rectangle(\\n            img_boxed, p1, p2, color=box_params[BC], thickness=box_params[BLW])\\n\\n    # Marking the fiducial points\\n    if landmarks:\\n        for p, point in zip(probs, points):\\n            for cord in point:\\n                cv2.circle(\\n                    img_boxed, (cord[0], cord[1]), 4, (50, 250, 68), -1)\\n\\n    return img_boxed\\n\\n\\ndef detection(box_params, device, scale=1, method=\"cnn\", landmarks=True):\\n    if method == \\'cnn\\':\\n        thresholds = [0.8, 0.9, 0.9]\\n        model = MTCNN(thresholds=thresholds, keep_all=True)\\n\\n    cam = cv2.VideoCapture(0)\\n    times = []\\n\\n    y1 = time.time()\\n    while True:\\n        ret_val, img = cam.read()\\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n        img = cv2.resize(img, (0, 0), fx=scale, fy=scale)\\n\\n        t1 = time.time()\\n        if method == \\'hog\\':\\n            img = hog_detector(img, box_params)\\n        elif method == \\'cnn\\':\\n            img = cnn_detector(img, model, landmarks, box_params)\\n        else:\\n            pass\\n        t2 = time.time()\\n\\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n        times.append(t2 - t1)\\n\\n        cv2.imshow(\\'cam\\', img)\\n        if cv2.waitKey(1) == 27:\\n            break  # esc to quit\\n    y2 = time.time()\\n\\n    cv2.destroyAllWindows()\\n    fps = len(times)/(y2 - y1)\\n\\n    if not method:\\n        method = \\'bypass\\'\\n    print(f\"Using {method}\")\\n    print(f\"avg detection time: {np.array(times).mean()*1000:0.2f} ms\")\\n    print(f\"frames detected: {len(times)}\")\\n    print(f\"cam on for: {y2 - y1:0.2f}\")\\n    print(f\"avg fps: {fps: 0.2f}\")', path='identify/cam_test/detection.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nFlags:\\n    -t: sets the threshold.\\n    -s: sets the input video stream scale (should be less than one).\\n    -w: input path of the trained model weights.\\n    -p: prints the distances.\\n\"\"\"\\n\\nimport os\\nimport cv2\\n\\nimport torch\\nimport numpy as np\\n\\nfrom time import time\\nfrom scipy import stats\\n\\nfrom identify.models import MTCNN\\nfrom identify.helpers import get_model\\nfrom identify.helpers.constants import TC, BC, TLW, BLW\\n\\n\\ndef get_embeddings(embeddings_path):\\n    if (embeddings_path).exists():\\n        return torch.load(embeddings_path)\\n    return None\\n\\n\\ndef predict(embeds, saved_embeds, saved_classes, saved_labels, k, threshold, print_dist):\\n    classes = []\\n    for embed in embeds:\\n        dists = torch.norm(saved_embeds - embed, dim=1)\\n        if print_dist:\\n            print((dists.max(), dists.min()))\\n        knn = torch.topk(dists, k, largest=False)\\n        mask = knn.values < threshold\\n        min_dist = min(knn.values).item()\\n        indices = knn.indices[mask]\\n        try:\\n            mode = stats.mode(saved_labels[indices]).mode[0]\\n            cls = saved_classes[mode]\\n        except IndexError:\\n            cls = \\'unidentified\\'\\n        classes.append((cls, min_dist))\\n    return classes\\n\\n\\ndef detect_identify(model_identifier, model_detector, mean, std, box_params, scale, saved_embeds, saved_labels, saved_classes, k, id_threshold, print_dist):\\n\\n    times = {\\n        \"complete\": [],\\n        \"mtcnn\": [],\\n        \"mtcnn_detect\": [],\\n        \"inception_resnet\": [],\\n        \"prediction\": [],\\n        \"display\": [],\\n        \"total\": []\\n    }\\n\\n    frames_shown = 0\\n    t_x = time()\\n    # Transform after reading frame (scale and to RGB)\\n    def tr_1(i): return cv2.cvtColor(cv2.resize(\\n        i, (0, 0), fx=scale, fy=scale), cv2.COLOR_BGR2RGB)\\n\\n    # Transform before displaying image (to BGR)\\n    def tr_2(i): return cv2.cvtColor(i, cv2.COLOR_RGB2BGR)\\n\\n    # Transform to set tensor from 0 to 1\\n    def tr_3(t): return ((t + 1)*128 - 0.5)/255\\n\\n    # Transform to normalize the tensor using given mean and std.\\n    def tr_4(t): return t\\n    if mean is not None and std is not None:\\n        mean = mean.reshape(1, 3, 1, 1)\\n        std = std.reshape(1, 3, 1, 1)\\n        def tr_4(t): return (t - mean)/std\\n\\n    model_identifier.eval()\\n    model_detector.eval()\\n\\n    vc = cv2.VideoCapture(0)\\n\\n    while True:\\n        frames_shown += 1\\n        t1 = time()\\n\\n        is_read, img = vc.read()\\n        img = tr_1(img)\\n\\n        with torch.no_grad():\\n            t2 = time()\\n            crop_tensors = model_detector(img)\\n\\n            t3 = time()\\n            boxes, probs, _ = model_detector.detect(img)\\n\\n            t4 = time()\\n            if crop_tensors is not None:\\n                crop_tensors = tr_4(tr_3(crop_tensors))\\n                embeds = model_identifier(crop_tensors)\\n\\n            t5 = time()\\n            if crop_tensors is not None:\\n                classes = predict(embeds, saved_embeds,\\n                                  saved_classes, saved_labels, k, id_threshold, print_dist)\\n            t6 = time()\\n\\n        img_boxed = img.copy()\\n        if boxes is not None:\\n            for i, box in enumerate(boxes):\\n                p1 = (box[0], box[1])\\n                p2 = (box[2], box[3])\\n\\n                cls, dist = classes[i]\\n                st = f\"{cls} {dist:0.3f}\"\\n\\n                cv2.putText(img_boxed, st, (int(p1[0]+10), int(p1[1]+20)), cv2.FONT_HERSHEY_COMPLEX,\\n                            box_params[TLW], box_params[TC], 2)\\n                cv2.rectangle(img_boxed, p1, p2, color=box_params[BC],\\n                              thickness=box_params[BLW])\\n\\n        img_boxed = tr_2(img_boxed)\\n\\n        cv2.imshow(\\'cam\\', img_boxed)\\n        t7 = time()\\n\\n        times[\\'complete\\'].append(t7 - t1)\\n        times[\\'mtcnn\\'].append(t3 - t2)\\n        times[\\'mtcnn_detect\\'].append(t4 - t3)\\n        times[\\'inception_resnet\\'].append(t5 - t4)\\n        times[\\'prediction\\'].append(t6 - t5)\\n        times[\\'display\\'].append(t7 - t6)\\n\\n        if cv2.waitKey(1) == 27:\\n            break\\n\\n    vc.release()\\n    cv2.destroyAllWindows()\\n    t_y = time()\\n    times[\\'total\\'].append(t_y - t_x)\\n    return times, frames_shown\\n\\n\\ndef print_stats(times, frames_shown):\\n    lj = 20\\n    for key in list(times.keys()):\\n        if key != \"total\":\\n            avg = np.array(times[key]).mean()*1000\\n            print(f\"{key.ljust(20)} {avg:0.3f} ms\")\\n\\n        else:\\n            tot = times[key][0]\\n            print(f\"{\\'total time\\'.ljust(20)} {tot} s\")\\n            print(f\"{\\'frames:\\'.ljust(20)} {frames_shown}\")\\n            print(f\"{\\'fps\\'.ljust(20)} {frames_shown/tot}\")\\n\\n\\ndef identification(embeddings_path, weights_path, device, mean, std, id_threshold, box_params, scale=1, print_dist=False):\\n    thresholds = [0.8, 0.9, 0.9]\\n    k = 7\\n\\n    em = get_embeddings(embeddings_path)\\n\\n    saved_embeds = em[\"embeds\"]\\n    saved_labels = em[\"labels\"]\\n    saved_classes = em[\"classes\"]\\n\\n    model_identifier = None\\n    if weights_path is not None:\\n        model_identifier, thr = get_model(weights_path, device)\\n    else:\\n        model_identifier, thr = get_model(None, device)\\n    model_detector = MTCNN(thresholds=thresholds,\\n                           device=device, keep_all=True)\\n\\n    if id_threshold is None:\\n        id_threshold = thr\\n\\n    print(f\"distance threshold set at: {id_threshold}\")\\n    times, frames_shown = detect_identify(model_identifier, model_detector, mean, std, box_params, scale, saved_embeds,\\n                                          saved_labels, saved_classes, k, id_threshold, print_dist)\\n\\n    print_stats(times, frames_shown)', path='identify/cam_test/identification.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .detection import detection\\nfrom .identification import identification\\nfrom .accuracy import accuracy', path='identify/cam_test/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nDetect and Store Faces.\\n\\nFlags:\\n    -o: output folder for crops, defaults to preset value if invalid or not present\\n\\nExample:\\n    python get_faces_from_video.py -o crops\\n\\nRead images (using PIL) from the `DATA_PATH` folder,\\ndetect faces in them using MTCNN, crop and transform these faces.\\n\\nGet names for these crops from the CLI.\\nThe names entered in the CLI should be the entire name of the person.\\n\\nStore in folder structure:\\n    DATA \\n      └── CLASSIFIED_CROPS\\n        \\xa0\\xa0 ├── PERSON_ONE\\n           │      └── 000.jpg\\n        \\xa0\\xa0 └── PERSON_TWO\\n                  └── 000.jpg\\n\\n// TODO: Make a GUI interface for this.\\n\"\"\"\\n\\nimport os\\nimport sys\\nimport cv2\\nimport torch\\nimport numpy as np\\n\\nfrom time import time\\nfrom PIL import Image\\nfrom identify.models import MTCNN\\nfrom identify.helpers import get_face_crops, save_face_crops\\n\\n\\ndef get_image_buffer(vc):\\n    \"\"\"\\n    Press c to capture a frame into the buffer.\\n    Press x to stop capture and return the buffer.\\n    \"\"\"\\n    buffer = []\\n    i = 0\\n    while True:\\n        img = vc.read()[1]\\n        cv2.imshow(\\'cam\\', img)\\n        k = cv2.waitKey(1)\\n        if k == ord(\\'c\\'):\\n            i += 1\\n            print(i)\\n            img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\\n            buffer.append(img)\\n        elif k == ord(\\'x\\'):\\n            break\\n    return buffer\\n\\n\\ndef start_capture():\\n    \"\"\"\\n    Captures cam screens into buffer, labels them and returns them.\\n    \"\"\"\\n    vc = cv2.VideoCapture(0)\\n    captures = {}\\n    while True:\\n        print(\"Press c to capture individual.\")\\n        print(\"Press x to stop capture of individual.\")\\n        buffer = get_image_buffer(vc)\\n\\n        if len(buffer) > 0:\\n            name = input(\"Name of individual: \").replace(\\' \\', \\'_\\').lower()\\n            captures[name] = buffer\\n\\n        should_capture = eval(input(\"capture another identity (y/[n]): \"))\\n        if should_capture != \\'y\\':\\n            break\\n    vc.release()\\n    cv2.destroyAllWindows()\\n\\n    return captures\\n\\n\\ndef captures_to_crops(model, captures):\\n    \"\"\"\\n    Converts the cv2 captured faces into jpg crops of \\n    the faces after passing it through the model.\\n    \"\"\"\\n    names = []\\n    crops = []\\n\\n    for k in list(captures.keys()):\\n        crop = get_face_crops(model, captures[k], True)\\n\\n        for c in crop:\\n            names.append(k)\\n            crops.append(c)\\n    return names, torch.stack(crops)\\n\\n\\ndef from_cam(output_folder, device):\\n    \"\"\"\\n    ignore_scanned: \\n        will ignore previously scanned images, stored as a numpy list.\\n        -s flag when calling script will set ignore scanned to True\\n    \"\"\"\\n    model = MTCNN(device=device)\\n\\n    captures = start_capture()\\n    names, crops = captures_to_crops(model, captures)\\n    print(names)\\n    print((crops.shape))\\n\\n    save_face_crops(crops, names, output_folder)\\n    print(\"face crops saved\")', path='identify/get_faces/from_cam.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nDetect and Store Faces.\\n\\nFlags:\\n    -i: input folder of images, defaults to preset value if invalid or not present\\n    -o: output folder for crops, defaults to preset value if invalid or not present\\n    -s: ignore scanned images, defaults to False if not present\\n\\nExample:\\n    python get_faces_from_images.py -s -i data/images -o crops\\n\\nOn entering crop identity name:\\n    - Focus should be on the crop showing window.\\n    - \\'esc\\' to discard a crop.\\n    - Type the name and press \\'return\\' to accept a crop.\\n\\nRead images (using PIL) from the `DATA_PATH` folder,\\ndetect faces in them using MTCNN, crop and transform these faces.\\n\\nGet names for these crops from the CLI.\\nThe names entered in the CLI should be the entire name of the person.\\n\\nStore in folder structure:\\n    DATA \\n      └── CLASSIFIED_CROPS\\n        \\xa0\\xa0 ├── PERSON_ONE\\n           │      └── 000.jpg\\n        \\xa0\\xa0 └── PERSON_TWO\\n                  └── 000.jpg\\n\\n// TODO: Make a GUI interface for this.\\n\"\"\"\\n\\nimport os\\nimport sys\\nimport cv2\\nimport torch\\nimport numpy as np\\n\\nfrom time import time\\nfrom PIL import Image\\nfrom identify.models import MTCNN\\nfrom identify.helpers.constants import SCANNED_IMAGE_LIST, IMG_EXTENSION\\nfrom identify.helpers import get_face_crops, save_face_crops, tensor_to_8b_array\\n\\n\\ndef load_scanned_image_list(input_folder):\\n    # Get a list of scanned images so no rescan.\\n    try:\\n        return np.load(os.path.join(input_folder, SCANNED_IMAGE_LIST)).tolist()\\n    except FileNotFoundError:\\n        return []\\n\\n\\ndef get_image(input_folder, ignore_scanned):\\n    # Get a list of Image.Image(s) from FACE_IMAGES_PATH\\n    sc_list = load_scanned_image_list(input_folder)\\n\\n    if not os.path.isdir(input_folder):\\n        return None\\n\\n    to_filter = []\\n    to_filter = os.listdir(input_folder)\\n\\n    if ignore_scanned:\\n        to_filter = [c for c in to_filter if c not in sc_list]\\n\\n    # For some reason the code doesn\\'t work without conversion to list\\n    # Confound me!\\n    image_names = list([n for n in to_filter if os.path.splitext(\\n        n)[-1].lower() == IMG_EXTENSION])\\n\\n    save_scanned_image_list(sc_list, image_names, input_folder)\\n\\n    images = [Image.open(\\n        os.path.join(input_folder, n)) for n in image_names]\\n\\n    return images\\n\\n\\ndef save_scanned_image_list(sc_list, image_names, input_folder):\\n    # Save list of scanned images so no rescan.\\n    for i in image_names:\\n        if i not in sc_list:\\n            sc_list.append(i)\\n    np.save(os.path.join(input_folder, SCANNED_IMAGE_LIST), sc_list)\\n\\n\\ndef classify_face_crops(crops):\\n    # Get the name for each crop from the CLI\\n    names = []\\n    crops_accepted = []\\n    print(\"Enter full name of the displayed crop.\\\\nPress:\\\\\\n        \\\\n\\\\t1. \\'return\\': accept crop after entering name\\\\\\n        \\\\n\\\\t2. \\'esc\\': to discard invalid crop\")\\n    for i, crop in enumerate(crops):\\n        cv2.imshow(\"face\", tensor_to_8b_array(crop))\\n\\n        name = []\\n        while True:\\n            # Get input\\n            key = cv2.waitKey(1)\\n\\n            if key > 0:\\n                # Show key pressed\\n                print((chr(key)))\\n\\n            if (key >= 65 and key <= 90) or (key >= 97 and key <= 122)\\\\\\n                    or key in [8, 32, 127]:\\n\\n                if key != 8 and key != 127:\\n                    name.append(chr(key))\\n                else:\\n                    # 8: BACKSPACE\\n                    # 127: DELETE\\n                    try:\\n                        name.pop()\\n                    except IndexError:\\n                        print(\"enter name\")\\n            # Accept crop\\n            elif key == 13:\\n                # 13: RETURN\\n                if len(list([i for i in name if i != \\'\\' and i != \\' \\'])) < 1:\\n                    # No empty names.\\n                    print(\"enter name\")\\n                    continue\\n                else:\\n                    names.append(\\'\\'.join(\\n                        name).lower().replace(\\' \\', \\'_\\'))\\n                    crops_accepted.append(crop.clone())\\n                    break\\n            # Discard crop\\n            elif key == 27:\\n                # 27: ESC\\n                break\\n\\n    del crops\\n    return names, torch.stack(crops_accepted)\\n\\n\\ndef from_images(input_folder, output_folder, ignore_scanned, device):\\n    \"\"\"\\n    ignore_scanned: \\n        will ignore previously scanned images, stored as a numpy list.\\n        -s flag when calling script will set ignore scanned to True\\n    \"\"\"\\n    # If detection is incorrect maybe change the MTCNN threshold.\\n    model = MTCNN(keep_all=True, device=device)\\n\\n    # map object of pillow images\\n    images = get_image(input_folder, ignore_scanned)\\n    if images is None:\\n        print(\"no data found\")\\n        return\\n\\n    # pytorch.Tensor\\n    crops = get_face_crops(model, images)\\n    if len(crops) < 1:\\n        print(\"no face crops\")\\n        return\\n\\n    # list of names and accepted crops\\n    names, crops = classify_face_crops(crops)\\n    save_face_crops(crops, names, output_folder)\\n    print(\"face crops saved\")\\n\\n\\n# run_detection()', path='identify/get_faces/from_images.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .from_cam import from_cam\\nfrom .from_images import from_images', path='identify/get_faces/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\ninput_folder:    root of image folder\\noutput_folder:   output folder name\\ncount:           number of samples\\n\"\"\"\\nimport Augmentor\\n\\n\\ndef augment(input_folder, output_folder, count):\\n    p = Augmentor.Pipeline(\\n        input_folder, output_directory=output_folder)\\n    p.rotate(probability=0.5, max_left_rotation=10, max_right_rotation=10)\\n    p.zoom(probability=0.3, min_factor=1.0, max_factor=1.2)\\n    p.histogram_equalisation(0.5)\\n    p.random_brightness(0.4, 0.4, 2)\\n    p.random_contrast(0.5, 0.4, 1)\\n    p.random_color(0.5, 0.2, 1)\\n    p.sample(count)', path='identify/helpers/augment.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='# File ext and formats\\nTEN_FORMAT = \".pt\"\\nIMG_EXTENSION = \".jpg\"\\nIMG_FORMAT = \"jpeg\"\\n\\n# File names\\nSCANNED_IMAGE_LIST = \"scanned.npy\"\\n\\n# SETS\\nSETS = [\\'train\\', \\'valid\\']\\n\\n# Box Params\\nBLW = \"box_line_width\"\\nTLW = \"text_line_width\"\\nBC = \"box_color\"\\nTC = \"text_color\"', path='identify/helpers/constants.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import os\\nimport cv2\\nimport torch\\nimport numpy as np\\n\\nfrom time import time\\nfrom PIL import Image\\nfrom scipy import stats\\n\\n\\nfrom torchvision import datasets\\nfrom identify.models.inception_resnet_v1 import InceptionResnetV1\\nfrom .constants import SETS, IMG_EXTENSION, IMG_FORMAT\\n\\n\\ndef tensor_to_8b_array(ten):\\n    # Crop tensor to cv2 showable numpy array.\\n    np_img = np.uint8((ten.numpy() + 1) * 128 - 0.5).T\\n    HORIZONTAL_FLIP = 1\\n    return cv2.cvtColor(cv2.flip(cv2.rotate\\n                                 (np_img, cv2.ROTATE_90_CLOCKWISE),\\n                                 HORIZONTAL_FLIP), cv2.COLOR_RGB2BGR)\\n\\n\\ndef tensor_to_PIL_img(ten):\\n    # Convert to PIL img for saving, tensors take a lot of space.\\n    # temp = np.uint8((ten + 1)*128 - 0.5).T\\n    # return Image.fromarray(temp).rotate(rot)\\n    return Image.fromarray(cv2.cvtColor(tensor_to_8b_array(ten), cv2.COLOR_BGR2RGB))\\n\\n\\ndef get_face_crops(model, images, is_single=False):\\n    # Crop the faces in the images and return a Tensor of crops.\\n    # Also print mean detection time.\\n    # Shape: (crop_count, 3, 160, 160)\\n\\n    times = []\\n    crops = []\\n\\n    print(\"detecting faces in images, may take a while...\")\\n\\n    for i, image in enumerate(images):\\n        t1 = time()\\n        crop = model(image)\\n        t2 = time()\\n        times.append(t2 - t1)\\n\\n        if crop is not None:\\n            crops.append(crop)\\n\\n    if len(crops) > 0:\\n        if is_single:\\n            crops = torch.stack(crops)\\n        else:\\n            crops = torch.cat(crops)\\n\\n    print()\\n    print(f\"images: {len(times)}, crops: {len(crops)}\")\\n    print(f\"time: {torch.tensor(times).mean() * 1000:0.2f} ms mean per image.\")\\n\\n    return crops\\n\\n\\ndef get_model(weights_path, device):\\n    model = InceptionResnetV1(device=device)\\n    try:\\n        data = torch.load(weights_path)\\n        model.load_state_dict(data[\\'state_dict\\'])\\n        print(\"model weights loaded\")\\n    except:\\n        print(\"using stock weights\")\\n        return InceptionResnetV1(device=device), 2\\n    return model, data[\\'threshold\\']\\n\\n\\ndef create_class_folder(name, output_folder):\\n    # Create folder for a class.\\n    path = os.path.join(output_folder, name)\\n    try:\\n        os.mkdir(path)\\n    except FileExistsError:\\n        return\\n\\n\\ndef get_index_dict(names, output_folder):\\n    # Returns the index of the crop to be saved in a class folder.\\n    # Also creates class folder if it isn\\'t present.\\n    index_dict = {}\\n    create_these = []\\n\\n    # Init index_dict\\n    for name in names:\\n        index_dict[name] = 0\\n\\n    # Create classified crops folder and class folders if not present.\\n    if not os.path.isdir(output_folder):\\n        os.mkdir(output_folder)\\n        create_these = names\\n    # Get first index of new image to be saved.\\n    else:\\n        classes = os.listdir(output_folder)\\n        for name in names:\\n            if not name in classes:\\n                create_these.append(name)\\n                continue\\n            else:\\n                # Get count of .jpg files in a class folder\\n                crop_count = len(list([f for f in os.listdir(os.path.join(output_folder, name)) if os.path.splitext(\\n                    f)[-1] == IMG_EXTENSION]))\\n                index_dict[name] = crop_count\\n\\n    # Create folders for classes if not present\\n    for name in create_these:\\n        index_dict[name] = 0\\n        create_class_folder(name, output_folder)\\n\\n    return index_dict\\n\\n\\ndef save_face_crops(crops, names, output_folder):\\n    # Will save the pytorch tensors as PIL Images (.JPEG)\\n    # Using torch.save takes up a lot of space ~27MB per crop\\n    index_dict = get_index_dict(names, output_folder)\\n\\n    for i, folder_name in enumerate(names):\\n        index = index_dict[folder_name]\\n        index_dict[folder_name] += 1\\n\\n        file_name = f\"{index:03}{IMG_EXTENSION}\"\\n\\n        path = os.path.join(\\n            output_folder, folder_name, file_name)\\n        tensor_to_PIL_img(crops[i]).save(path, IMG_FORMAT)\\n\\n\\ndef get_embeddings(dataloader, model):\\n    \"\"\"\\n    dataloader: pytorch DataLoader of n samples\\n    model: pytorch models used to generate embedding of shape (1,512).\\n\\n    return: \\n        embeds: tensor, shape (n,512)\\n        labels: int tensor, shape (n)\\n    \"\"\"\\n    embeds = []\\n    labels = []\\n    model.eval()\\n    for d in dataloader:\\n        labels.append(d[1])\\n        with torch.no_grad():\\n            embeds.append(model(d[0]))\\n    return torch.cat(embeds), torch.cat(labels)', path='identify/helpers/helpers.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nFlags:\\n  -w: Weights Path   (folder where the weights of the model are saved)\\n  -i: Input Folder   (folder where the crops are saved)\\n  -o: Output Folder  (folder where embeds have to be stored)\\n\"\"\"\\nimport torch\\nfrom pathlib import Path\\nfrom torchvision import datasets, transforms\\nfrom collections import OrderedDict\\n\\nfrom .helpers import get_embeddings, get_model\\nfrom torch.utils.data import DataLoader\\n\\n\\ndef get_dataloader(input_path):\\n    dataset = datasets.ImageFolder(input_path, transform=transforms.ToTensor())\\n    dataloader = DataLoader(dataset, batch_size=16, drop_last=False)\\n    return dataloader\\n\\n\\ndef save_embeddings(input_path, output_path, weights_path, device):\\n    model, _ = get_model(weights_path, device)\\n    dataloader = get_dataloader(input_path)\\n    embeds, labels = get_embeddings(dataloader, model)\\n\\n    path = Path(\\'/\\'.join(output_path.parts[:-1]))\\n    if not path.exists():\\n        path.mkdir(parents=True)\\n\\n    save_this = OrderedDict(\\n        {\"embeds\": embeds, \"labels\": labels, \"classes\": dataloader.dataset.classes})\\n    torch.save(save_this, output_path)\\n    print(f\"embeddings of {len(dataloader.dataset)} crops saved\")', path='identify/helpers/save_embeddings.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .augment import augment\\nfrom .save_embeddings import save_embeddings\\nfrom .helpers import tensor_to_8b_array, tensor_to_PIL_img, \\\\\\n    get_face_crops, get_model, create_class_folder, \\\\\\n    get_index_dict, save_face_crops, get_embeddings', path='identify/helpers/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import os\\nimport torch\\nfrom torch import nn\\nfrom torch.nn import functional as F\\n\\n# Model Definition Code \\n# ______________________________________________________________________________\\nclass BasicConv2d(nn.Module):\\n\\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\\n        super().__init__()\\n        self.conv = nn.Conv2d(\\n            in_planes, out_planes,\\n            kernel_size=kernel_size, stride=stride,\\n            padding=padding, bias=False\\n        ) # verify bias false\\n        self.bn = nn.BatchNorm2d(\\n            out_planes,\\n            eps=0.001, # value found in tensorflow\\n            momentum=0.1, # default pytorch value\\n            affine=True\\n        )\\n        self.relu = nn.ReLU(inplace=False)\\n\\n    def forward(self, x):\\n        x = self.conv(x)\\n        x = self.bn(x)\\n        x = self.relu(x)\\n        return x\\n# ______________________________________________________________________________\\n\\nclass Block35(nn.Module):\\n\\n    def __init__(self, scale=1.0):\\n        super().__init__()\\n\\n        self.scale = scale\\n\\n        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\\n\\n        self.branch1 = nn.Sequential(\\n            BasicConv2d(256, 32, kernel_size=1, stride=1),\\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\\n        )\\n\\n        self.branch2 = nn.Sequential(\\n            BasicConv2d(256, 32, kernel_size=1, stride=1),\\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\\n        )\\n\\n        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\\n        self.relu = nn.ReLU(inplace=False)\\n\\n    def forward(self, x):\\n        x0 = self.branch0(x)\\n        x1 = self.branch1(x)\\n        x2 = self.branch2(x)\\n        out = torch.cat((x0, x1, x2), 1)\\n        out = self.conv2d(out)\\n        out = out * self.scale + x\\n        out = self.relu(out)\\n        return out\\n# ______________________________________________________________________________\\n\\nclass Block17(nn.Module):\\n\\n    def __init__(self, scale=1.0):\\n        super().__init__()\\n\\n        self.scale = scale\\n\\n        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\\n\\n        self.branch1 = nn.Sequential(\\n            BasicConv2d(896, 128, kernel_size=1, stride=1),\\n            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\\n            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\\n        )\\n\\n        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\\n        self.relu = nn.ReLU(inplace=False)\\n\\n    def forward(self, x):\\n        x0 = self.branch0(x)\\n        x1 = self.branch1(x)\\n        out = torch.cat((x0, x1), 1)\\n        out = self.conv2d(out)\\n        out = out * self.scale + x\\n        out = self.relu(out)\\n        return out\\n# ______________________________________________________________________________\\n\\nclass Block8(nn.Module):\\n\\n    def __init__(self, scale=1.0, noReLU=False):\\n        super().__init__()\\n\\n        self.scale = scale\\n        self.noReLU = noReLU\\n\\n        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\\n\\n        self.branch1 = nn.Sequential(\\n            BasicConv2d(1792, 192, kernel_size=1, stride=1),\\n            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\\n            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\\n        )\\n\\n        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\\n        if not self.noReLU:\\n            self.relu = nn.ReLU(inplace=False)\\n\\n    def forward(self, x):\\n        x0 = self.branch0(x)\\n        x1 = self.branch1(x)\\n        out = torch.cat((x0, x1), 1)\\n        out = self.conv2d(out)\\n        out = out * self.scale + x\\n        if not self.noReLU:\\n            out = self.relu(out)\\n        return out\\n# ______________________________________________________________________________\\n\\nclass Mixed_6a(nn.Module):\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\\n\\n        self.branch1 = nn.Sequential(\\n            BasicConv2d(256, 192, kernel_size=1, stride=1),\\n            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\\n            BasicConv2d(192, 256, kernel_size=3, stride=2)\\n        )\\n\\n        self.branch2 = nn.MaxPool2d(3, stride=2)\\n\\n    def forward(self, x):\\n        x0 = self.branch0(x)\\n        x1 = self.branch1(x)\\n        x2 = self.branch2(x)\\n        out = torch.cat((x0, x1, x2), 1)\\n        return out\\n# ______________________________________________________________________________\\n\\nclass Mixed_7a(nn.Module):\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n        self.branch0 = nn.Sequential(\\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\\n        )\\n\\n        self.branch1 = nn.Sequential(\\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\\n            BasicConv2d(256, 256, kernel_size=3, stride=2)\\n        )\\n\\n        self.branch2 = nn.Sequential(\\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\\n            BasicConv2d(256, 256, kernel_size=3, stride=2)\\n        )\\n\\n        self.branch3 = nn.MaxPool2d(3, stride=2)\\n\\n    def forward(self, x):\\n        x0 = self.branch0(x)\\n        x1 = self.branch1(x)\\n        x2 = self.branch2(x)\\n        x3 = self.branch3(x)\\n        out = torch.cat((x0, x1, x2, x3), 1)\\n        return out\\n# ______________________________________________________________________________\\n\\n\"\"\"\\nInception Resnet V1 model definition\\n\"\"\"\\n\\nclass InceptionResnetV1(nn.Module):\\n    def __init__(self, dropout_prob=0.6,device=None):\\n        super(InceptionResnetV1,self).__init__()\\n        \\n        # Define layers\\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\\n        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\\n        self.repeat_1 = nn.Sequential(\\n            Block35(scale=0.17),\\n            Block35(scale=0.17),\\n            Block35(scale=0.17),\\n            Block35(scale=0.17),\\n            Block35(scale=0.17),\\n        )\\n        self.mixed_6a = Mixed_6a()\\n        self.repeat_2 = nn.Sequential(\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n            Block17(scale=0.10),\\n        )\\n        self.mixed_7a = Mixed_7a()\\n        self.repeat_3 = nn.Sequential(\\n            Block8(scale=0.20),\\n            Block8(scale=0.20),\\n            Block8(scale=0.20),\\n            Block8(scale=0.20),\\n            Block8(scale=0.20),\\n        )\\n        self.block8 = Block8(noReLU=True)\\n        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\\n        self.dropout = nn.Dropout(dropout_prob)\\n        self.last_linear = nn.Linear(1792, 512, bias=False)\\n        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\\n\\n        self.device = torch.device(\\'cpu\\')\\n        if device is not None:\\n            self.device = device\\n            self.to(device)\\n\\n        # Load pre trained weights on model init\\n        state_dict_path = os.path.join(os.path.dirname(__file__),\\'./pre_trained_weights/inception_resnet_v1_vggface2.pt\\')\\n        if os.path.isfile(state_dict_path):\\n            state_dict = torch.load(state_dict_path)\\n            self.load_state_dict(state_dict)\\n\\n\\n    def forward(self, x):\\n        \"\"\"Calculate embeddings given a batch of input image tensors.\\n        Arguments:\\n            x {torch.tensor} -- Batch of image tensors representing faces.\\n        Returns:\\n            torch.tensor -- Batch of embeddings.\\n        \"\"\"\\n        x = x.float() # ∵ _thnn_conv2d_forward not supported on CPUType for Byte\\n        x = self.conv2d_1a(x)\\n        x = self.conv2d_2a(x)\\n        x = self.conv2d_2b(x)\\n        x = self.maxpool_3a(x)\\n        x = self.conv2d_3b(x)\\n        x = self.conv2d_4a(x)\\n        x = self.conv2d_4b(x)\\n        x = self.repeat_1(x)\\n        x = self.mixed_6a(x)\\n        x = self.repeat_2(x)\\n        x = self.mixed_7a(x)\\n        x = self.repeat_3(x)\\n        x = self.block8(x)\\n        x = self.avgpool_1a(x)\\n        x = self.dropout(x)\\n        x = self.last_linear(x.view(x.shape[0], -1))\\n        x = self.last_bn(x)\\n        x = F.normalize(x, p=2, dim=1)\\n        return x', path='identify/models/inception_resnet_v1.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\n\\nfrom time import time\\nfrom copy import deepcopy\\n\\n\\nclass DistLoss:\\n    def __init__(self, sim_slink=4.3, dis_thresh=1.9, eps=0.8,\\n                 dist_func=torch.nn.PairwiseDistance(p=2, eps=1e-8)):\\n        \"\"\"\\n        sim_slink: how exp the sim_loss grows after 1\\n        dis_thresh: after this dis_thresh, dis_loss < 0\\n        eps: factor to raise the dissimilar term by.\\n        dis_func: function to calculate distance between two vectors.\\n        \"\"\"\\n        self.sim_slink = sim_slink\\n        self.dis_thresh = dis_thresh\\n        self.eps = eps\\n        self.dist_func = dist_func\\n\\n    def dist_calculator(self, x1, x2):\\n        \"\"\"\\n        Function that calculates the distances between\\n        all the vectors in x1 and x2 which are of the\\n        shape (m1,n) and (m2,n).\\n\\n        n: dimensions in vector\\n        m1, m2: co-ordinates in x1 and x2 \\n\\n        returns: (m2,m1) shape distance matrix\\n        \"\"\"\\n        dists = []\\n        for vec in x1:\\n            dists.append(self.dist_func(x2, vec))\\n        return torch.stack(dists)\\n\\n    def __call__(self, y_vectors: \"vectors from the model\", y: \"labels\"):\\n        \"\"\"\\n        Loss function to be used if the output of the model is \\n        an image embedding and the class of the image is known.\\n\\n        Calculates the distance between all the y_vectors.\\n        If the classes match the distance > sim_thresh increases the loss \\n        If the classes don\\'t match the distance > 1 + dis_offset decreases loss\\n        \"\"\"\\n        \"\"\"\\n        When the dis_thresh is crossed by the dissimilar mean the loss contributed by \\n        that term will be < 0\\n        \"\"\"\\n        def t(n): return torch.tensor(n, requires_grad=True)\\n        rsm = 1e-8\\n\\n        dist_calculator = self.dist_calculator\\n        sim_slink = self.sim_slink\\n        dis_thresh = self.dis_thresh\\n        eps = self.eps\\n\\n        l = y_vectors.size(0)\\n\\n        # Distance between all the y_vectors shape:(l,l)\\n        dist_matrix = dist_calculator(y_vectors, y_vectors)\\n\\n        # Creating a shape:(l,l) mask using labels\\n        y_cross = y.repeat(l).reshape(l, l)\\n        y_mask = torch.eq(y_cross, y_cross.T)\\n\\n        # mean tensor of dissimilar classes distances\\n        dis_mean = dist_matrix[~y_mask].mean()\\n\\n        # Negating the diagonal cause that dist will be (almost) 0\\n        temp_tensor = torch.arange(l)\\n        y_mask[temp_tensor, temp_tensor] = False\\n\\n        # mean tensor of similar classes distances\\n        sim_mean = dist_matrix[y_mask].mean()\\n\\n        # possible nan if batch has all similar or dissimilar classes\\n        if torch.isnan(sim_mean):\\n            sim_mean = t(rsm)\\n        if torch.isnan(dis_mean):\\n            dis_mean = t(rsm)\\n\\n        # Loss contributed by similar and dissimilar classes\\n        sim_loss = sim_mean ** t(sim_slink)\\n        dis_loss = torch.max(t(dis_thresh)/dis_mean - t(1.), t(rsm)) ** t(eps)\\n\\n        loss = torch.log(sim_loss + dis_loss + t(1.))\\n        return loss\\n\\n\\ndef dist_fit(model, optim, train_dl, valid_dl, device, data_count, loss_func=DistLoss(), epochs=25):\\n    start = time()\\n    def time_st(x): return f\"{x//60:0.0f} m {x%60:0.3f} s\"\\n    model = model.to(device)\\n    losses_tr = []\\n    losses_va = []\\n\\n    # Define datasets dict\\n    tr = \\'train\\'\\n    va = \\'valid\\'\\n\\n    sets = [tr, va]\\n    data = {tr: train_dl, va: valid_dl}\\n\\n    least_loss = torch.tensor(float(\\'inf\\'))\\n    best_model_state_dict = deepcopy(model.state_dict())\\n\\n    print(f\"train samples: {data_count[tr]}, valid samples: {data_count[va]}\")\\n    # Add timer\\n    for epoch in range(epochs):\\n        e_start = time()\\n\\n        print((\\n            f\"\\\\nEPOCH: ({epoch + 1}/{epochs})\\\\t{e_start - start:0.3f} s\\\\n\", \"-\"*20))\\n        for phase in sets:\\n            p_start = time()\\n\\n            is_tr = phase == tr\\n            if is_tr:\\n                model.train()\\n            else:\\n                model.eval()\\n\\n            running_loss = 0.\\n\\n            for batch in data[phase]:\\n                X, y = batch\\n                X = X.to(device)\\n                y = y.to(device)\\n\\n                optim.zero_grad()\\n\\n                with torch.set_grad_enabled(is_tr):\\n                    y_vectors = model(X)\\n\\n                    loss = loss_func(y_vectors, y)\\n\\n                    if is_tr:\\n                        loss.backward()\\n                        optim.step()\\n\\n                samp_loss = loss * len(y)\\n                if is_tr:\\n                    losses_tr.append(samp_loss)\\n                else:\\n                    losses_va.append(samp_loss)\\n                running_loss += samp_loss\\n\\n            p_time = time() - p_start\\n            epoch_loss = running_loss / data_count[phase]\\n            print(f\"{phase}: loss {epoch_loss:0.3f}, time {time_st(p_time)}\")\\n\\n            if (not is_tr) and (least_loss > epoch_loss):\\n                least_loss = epoch_loss\\n                best_model_state_dict = deepcopy(model.state_dict())\\n\\n    tot_time = time() - start\\n    print(f\"\\\\nTime taken: {time_st(tot_time)}\")\\n    return model.load_state_dict(best_model_state_dict), losses_tr, losses_va\\n\\n\\ndef std_fit(model, optim, train_dl, valid_dl, device, data_count, loss_func, epochs=25):\\n    start = time()\\n    def time_st(x): return f\"{x//60:0.0f} m {x%60:0.3f} s\"\\n    model = model.to(device)\\n    losses_tr = []\\n    losses_va = []\\n\\n    tr = \\'train\\'\\n    va = \\'valid\\'\\n    data = {tr: train_dl, va: valid_dl}\\n\\n    print(f\"train samples: {data_count[tr]}, valid samples: {data_count[va]}\")\\n    best_accu = 0.0\\n    least_loss = 20\\n    best_model_state_dict = model.state_dict()\\n\\n    # Add timer\\n    for epoch in range(epochs):\\n        e_start = time()\\n\\n        print((\\n            f\"\\\\nEPOCH: ({epoch + 1}/{epochs})\\\\t{e_start - start:0.3f} s\\\\n\", \"-\"*20))\\n        for phase in [tr, va]:\\n            p_start = time()\\n\\n            is_tr = phase == tr\\n            is_va = phase == va\\n\\n            if is_tr:\\n                model.train()\\n            else:\\n                model.eval()\\n\\n            \"\"\"\\n            Loss and accuracy calculated \\n            during a single epoch.\\n            \"\"\"\\n            running_loss = 0.\\n            running_accu = 0\\n\\n            for batch in data[phase]:\\n                X, y = batch\\n                X = X.to(device)\\n                y = y.to(device)\\n\\n                optim.zero_grad()\\n\\n                with torch.set_grad_enabled(is_tr):\\n                    y_val = model(X)\\n                    y_cls = torch.argmax(y_val, dim=1)\\n\\n                    loss = loss_func(y_val, y)\\n\\n                    if is_tr:\\n                        loss.backward()\\n                        optim.step()\\n\\n                \"\"\"\\n                Running Loss:\\n                    Loss calculated over the entire dataset,\\n                    for one epoch. Loss for an epoch will be \\n                    running loss divided by the total number\\n                    of samples (not batches).\\n                Running Accuracy:\\n                    Number of samples the model got right.\\n                    \\n                \"\"\"\\n                samp_loss = loss.item() * len(y)\\n                if is_tr:\\n                    losses_tr.append(samp_loss)\\n                else:\\n                    losses_va.append(samp_loss)\\n\\n                running_loss += samp_loss\\n                running_accu += torch.sum(y_cls == y).item()\\n\\n            p_time = time() - p_start\\n            epoch_loss = running_loss / data_count[phase]\\n            epoch_accu = running_accu / data_count[phase]\\n            print(\\n                f\"{phase}: loss {epoch_loss:0.3f}, accu {epoch_accu:0.3f}, time {time_st(p_time)}\")\\n\\n            if is_va and (epoch_accu > best_accu) or (epoch_accu == best_accu and least_loss > epoch_loss):\\n                best_accu = epoch_accu\\n                least_loss = epoch_loss\\n                best_model_state_dict = deepcopy(model.state_dict())\\n            elif is_va and least_loss > epoch_loss:\\n                least_loss = epoch_loss\\n\\n    tot_time = time() - start\\n    print(\\n        f\"\\\\nTime taken: {time_st(tot_time)}, Best accuracy: {best_accu:0.3f}\")\\n    return model.load_state_dict(best_model_state_dict), losses_tr, losses_va', path='identify/models/model_trainers.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\nimport numpy as np\\nimport os\\nfrom torch import nn\\nfrom .utils.detect_face import detect_face, extract_face\\n\\n\\nclass PNet(nn.Module):\\n    \"\"\"MTCNN PNet.\\n\\n    Keyword Arguments:\\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\\n    \"\"\"\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\\n        self.prelu1 = nn.PReLU(10)\\n        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\\n        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)\\n        self.prelu2 = nn.PReLU(16)\\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\\n        self.prelu3 = nn.PReLU(32)\\n        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)\\n        self.softmax4_1 = nn.Softmax(dim=1)\\n        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)\\n\\n        self.training = False\\n\\n        # Load pre trained weights on model init\\n        state_dict_path = os.path.join(os.path.dirname(\\n            __file__), \\'./pre_trained_weights/pnet.pt\\')\\n        if os.path.isfile(state_dict_path):\\n            state_dict = torch.load(state_dict_path)\\n            self.load_state_dict(state_dict)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.prelu1(x)\\n        x = self.pool1(x)\\n        x = self.conv2(x)\\n        x = self.prelu2(x)\\n        x = self.conv3(x)\\n        x = self.prelu3(x)\\n        a = self.conv4_1(x)\\n        a = self.softmax4_1(a)\\n        b = self.conv4_2(x)\\n        return b, a\\n\\n\\nclass RNet(nn.Module):\\n    \"\"\"MTCNN RNet.\\n\\n    Keyword Arguments:\\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\\n    \"\"\"\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n        self.conv1 = nn.Conv2d(3, 28, kernel_size=3)\\n        self.prelu1 = nn.PReLU(28)\\n        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\\n        self.conv2 = nn.Conv2d(28, 48, kernel_size=3)\\n        self.prelu2 = nn.PReLU(48)\\n        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\\n        self.conv3 = nn.Conv2d(48, 64, kernel_size=2)\\n        self.prelu3 = nn.PReLU(64)\\n        self.dense4 = nn.Linear(576, 128)\\n        self.prelu4 = nn.PReLU(128)\\n        self.dense5_1 = nn.Linear(128, 2)\\n        self.softmax5_1 = nn.Softmax(dim=1)\\n        self.dense5_2 = nn.Linear(128, 4)\\n\\n        self.training = False\\n\\n        # Load pre trained weights on model init\\n        state_dict_path = os.path.join(os.path.dirname(\\n            __file__), \\'./pre_trained_weights/rnet.pt\\')\\n        if os.path.isfile(state_dict_path):\\n            state_dict = torch.load(state_dict_path)\\n            self.load_state_dict(state_dict)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.prelu1(x)\\n        x = self.pool1(x)\\n        x = self.conv2(x)\\n        x = self.prelu2(x)\\n        x = self.pool2(x)\\n        x = self.conv3(x)\\n        x = self.prelu3(x)\\n        x = x.permute(0, 3, 2, 1).contiguous()\\n        x = self.dense4(x.view(x.shape[0], -1))\\n        x = self.prelu4(x)\\n        a = self.dense5_1(x)\\n        a = self.softmax5_1(a)\\n        b = self.dense5_2(x)\\n        return b, a\\n\\n\\nclass ONet(nn.Module):\\n    \"\"\"MTCNN ONet.\\n\\n    Keyword Arguments:\\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\\n    \"\"\"\\n\\n    def __init__(self):\\n        super().__init__()\\n\\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\\n        self.prelu1 = nn.PReLU(32)\\n        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\\n        self.prelu2 = nn.PReLU(64)\\n        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\\n        self.prelu3 = nn.PReLU(64)\\n        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\\n        self.prelu4 = nn.PReLU(128)\\n        self.dense5 = nn.Linear(1152, 256)\\n        self.prelu5 = nn.PReLU(256)\\n        self.dense6_1 = nn.Linear(256, 2)\\n        self.softmax6_1 = nn.Softmax(dim=1)\\n        self.dense6_2 = nn.Linear(256, 4)\\n        self.dense6_3 = nn.Linear(256, 10)\\n\\n        self.training = False\\n\\n        # Load pre trained weights on model init\\n        state_dict_path = os.path.join(os.path.dirname(\\n            __file__), \\'./pre_trained_weights/onet.pt\\')\\n        if os.path.isfile(state_dict_path):\\n            state_dict = torch.load(state_dict_path)\\n            self.load_state_dict(state_dict)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.prelu1(x)\\n        x = self.pool1(x)\\n        x = self.conv2(x)\\n        x = self.prelu2(x)\\n        x = self.pool2(x)\\n        x = self.conv3(x)\\n        x = self.prelu3(x)\\n        x = self.pool3(x)\\n        x = self.conv4(x)\\n        x = self.prelu4(x)\\n        x = x.permute(0, 3, 2, 1).contiguous()\\n        x = self.dense5(x.view(x.shape[0], -1))\\n        x = self.prelu5(x)\\n        a = self.dense6_1(x)\\n        a = self.softmax6_1(a)\\n        b = self.dense6_2(x)\\n        c = self.dense6_3(x)\\n        return b, c, a\\n\\n\\nclass MTCNN(nn.Module):\\n    \"\"\"MTCNN face detection module.\\n    This class loads pretrained P-, R-, and O-nets and returns images cropped to include the face\\n    only, given raw input images of one of the following types:\\n        - PIL image or list of PIL images\\n        - numpy.ndarray (uint8) representing either a single image (3D) or a batch of images (4D).\\n    Cropped faces can optionally be saved to file\\n    also.\\n\\n    Keyword Arguments:\\n        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\\n        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \\n            Note that the application of the margin differs slightly from the davidsandberg/facenet\\n            repo, which applies the margin to the original image before resizing, making the margin\\n            dependent on the original image size (this is a bug in davidsandberg/facenet).\\n            (default: {0})\\n        min_face_size {int} -- Minimum face size to search for. (default: {20})\\n        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\\n        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\\n        post_process {bool} -- Whether or not to post process images tensors before returning.\\n            (default: {True})\\n        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\\n            If False, the face with the highest detection probability is returned.\\n            (default: {True})\\n        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\\n            select_largest parameter. If a save_path is specified, the first face is saved to that\\n            path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\\n        device {torch.device} -- The device on which to run neural net passes. Image tensors and\\n            models are copied to this device before running forward passes. (default: {None})\\n    \"\"\"\\n\\n    def __init__(\\n        self, image_size=160, margin=0, min_face_size=20,\\n        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\\n        select_largest=True, keep_all=False, device=None\\n    ):\\n        super().__init__()\\n\\n        self.image_size = image_size\\n        self.margin = margin\\n        self.min_face_size = min_face_size\\n        self.thresholds = thresholds\\n        self.factor = factor\\n        self.post_process = post_process\\n        self.select_largest = select_largest\\n        self.keep_all = keep_all\\n\\n        self.pnet = PNet()\\n        self.rnet = RNet()\\n        self.onet = ONet()\\n\\n        self.device = torch.device(\\'cpu\\')\\n        if device is not None:\\n            self.device = device\\n            self.to(device)\\n\\n    def forward(self, img, save_path=None, return_prob=False):\\n        \"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\\n        detection and extraction of faces, returning tensors representing detected faces rather\\n        than the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\\n\\n        Arguments:\\n            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, or list.\\n\\n        Keyword Arguments:\\n            save_path {str} -- An optional save path for the cropped image. Note that when\\n                self.post_process=True, although the returned tensor is post processed, the saved\\n                face image is not, so it is a true representation of the face in the input image.\\n                If `img` is a list of images, `save_path` should be a list of equal length.\\n                (default: {None})\\n            return_prob {bool} -- Whether or not to return the detection probability.\\n                (default: {False})\\n\\n        Returns:\\n            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\\n                with dimensions 3 x image_size x image_size. Optionally, the probability that a\\n                face was detected. If self.keep_all is True, n detected faces are returned in an\\n                n x 3 x image_size x image_size tensor with an optional list of detection\\n                probabilities. If `img` is a list of images, the item(s) returned have an extra \\n                dimension (batch) as the first dimension.\\n        Example:\\n        >>> from facenet_pytorch import MTCNN\\n        >>> mtcnn = MTCNN()\\n        >>> face_tensor, prob = mtcnn(img, save_path=\\'face.png\\', return_prob=True)\\n        \"\"\"\\n\\n        # Detect faces\\n        with torch.no_grad():\\n            batch_boxes, batch_probs, _ = self.detect(img)\\n\\n        # Determine if a batch or single image was passed\\n        batch_mode = True\\n        if not isinstance(img, (list, tuple)):\\n            img = [img]\\n            batch_boxes = [batch_boxes]\\n            batch_probs = [batch_probs]\\n            batch_mode = False\\n\\n        # Parse save path(s)\\n        if save_path is not None:\\n            if isinstance(save_path, str):\\n                save_path = [save_path]\\n        else:\\n            save_path = [None for _ in range(len(img))]\\n\\n        # Process all bounding boxes and probabilities\\n        faces, probs = [], []\\n        for im, box_im, prob_im, path_im in zip(img, batch_boxes, batch_probs, save_path):\\n            if box_im is None:\\n                faces.append(None)\\n                probs.append([None] if self.keep_all else None)\\n                continue\\n\\n            if not self.keep_all:\\n                box_im = box_im[[0]]\\n\\n            faces_im = []\\n            for i, box in enumerate(box_im):\\n                face_path = path_im\\n                if path_im is not None and i > 0:\\n                    save_name, ext = os.path.splitext(path_im)\\n                    face_path = save_name + \\'_\\' + str(i + 1) + ext\\n\\n                face = extract_face(im, box, self.image_size,\\n                                    self.margin, face_path)\\n                if self.post_process:\\n                    face = fixed_image_standardization(face)\\n                faces_im.append(face)\\n\\n            if self.keep_all:\\n                faces_im = torch.stack(faces_im)\\n            else:\\n                faces_im = faces_im[0]\\n                prob_im = prob_im[0]\\n\\n            faces.append(faces_im)\\n            probs.append(prob_im)\\n\\n        if not batch_mode:\\n            faces = faces[0]\\n            probs = probs[0]\\n\\n        if return_prob:\\n            return faces, probs\\n        else:\\n            return faces\\n\\n    def detect(self, img, landmarks=False):\\n        \"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\\n        This method is used by the forward method and is also useful for face detection tasks\\n        that require lower-level handling of bounding boxes and facial landmarks (e.g., face\\n        tracking). The functionality of the forward function can be emulated by using this method\\n        followed by the extract_face() function.\\n\\n        Arguments:\\n            img {PIL.Image, np.ndarray, or list} -- A PIL image or a list of PIL images.\\n        Keyword Arguments:\\n            landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.\\n                (default: {False})\\n\\n        Returns:\\n            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\\n                Nx4 array of bounding boxes and a length N list of detection probabilities.\\n                Returned boxes will be sorted in descending order by detection probability if\\n                self.select_largest=False, otherwise the largest face will be returned first.\\n                If `img` is a list of images, the items returned have an extra dimension\\n                (batch) as the first dimension. Optionally, a third item, the facial landmarks,\\n                are returned if `landmarks=True`.\\n        Example:\\n        >>> from PIL import Image, ImageDraw\\n        >>> from facenet_pytorch import MTCNN, extract_face\\n        >>> mtcnn = MTCNN(keep_all=True)\\n        >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)\\n        >>> # Draw boxes and save faces\\n        >>> img_draw = img.copy()\\n        >>> draw = ImageDraw.Draw(img_draw)\\n        >>> for i, (box, point) in enumerate(zip(boxes, points)):\\n        ...     draw.rectangle(box.tolist(), width=5)\\n        ...     for p in point:\\n        ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\\n        ...     extract_face(img, box, save_path=\\'detected_face_{}.png\\'.format(i))\\n        >>> img_draw.save(\\'annotated_faces.png\\')\\n        \"\"\"\\n\\n        with torch.no_grad():\\n            batch_boxes, batch_points = detect_face(\\n                img, self.min_face_size,\\n                self.pnet, self.rnet, self.onet,\\n                self.thresholds, self.factor,\\n                self.device\\n            )\\n\\n        boxes, probs, points = [], [], []\\n        for box, point in zip(batch_boxes, batch_points):\\n            box = np.array(box)\\n            point = np.array(point)\\n            if len(box) == 0:\\n                boxes.append(None)\\n                probs.append([None])\\n                points.append(None)\\n            elif self.select_largest:\\n                box_order = np.argsort(\\n                    (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1]))[::-1]\\n                box = box[box_order]\\n                point = point[box_order]\\n                boxes.append(box[:, :4])\\n                probs.append(box[:, 4])\\n                points.append(point)\\n            else:\\n                boxes.append(box[:, :4])\\n                probs.append(box[:, 4])\\n                points.append(point)\\n        boxes = np.array(boxes)\\n        probs = np.array(probs)\\n        points = np.array(points)\\n\\n        if not isinstance(img, (list, tuple)):\\n            boxes = boxes[0]\\n            probs = probs[0]\\n            points = points[0]\\n\\n        if landmarks:\\n            return boxes, probs, points\\n\\n        return boxes, probs, None\\n\\n\\ndef fixed_image_standardization(image_tensor):\\n    processed_tensor = (image_tensor - 127.5) / 128.0\\n    return processed_tensor\\n\\n\\ndef prewhiten(x):\\n    mean = x.mean()\\n    std = x.std()\\n    std_adj = std.clamp(min=1.0/(float(x.numel())**0.5))\\n    y = (x - mean) / std_adj\\n    return y', path='identify/models/mtcnn.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='\"\"\"\\nFlags:\\n  -i: Input Folder   (folder where the training and test data are saved in subfolders \\'train\\', \\'test\\')\\n  -o: Output Folder  (folder where the model state_dict is to be stored along with the threshold.)\\n  -n: Name           (name of the weights file)\\n  -e: Epochs         (number of epochs to train the model for.) \\n  -r: Retune         (if the model was previously tuned, tune it more else will tune from scratch.)\\n  -d: Use Dist Loss  (Uses DistLoss to train the model.)\\n\"\"\"\\n\\nimport os\\nimport torch\\nimport numpy as np\\n\\nfrom pathlib import Path\\nfrom copy import deepcopy\\nfrom scipy import stats\\n\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\\nfrom torchvision import models, datasets, transforms\\n\\nfrom .model_trainers import dist_fit, std_fit\\nfrom .inception_resnet_v1 import InceptionResnetV1\\nfrom .metrics import check_accuracy, show_embed_metrics\\nfrom .utils.tuner_helpers import get_mean_std, get_dataloader\\nfrom identify.helpers import get_embeddings\\nfrom identify.helpers.constants import SETS, TEN_FORMAT\\n\\n\\ndef get_weight(dataset):\\n    # Calculating the weight (due to unbalanced dataset)\\n    weight = []\\n    l = len(dataset)\\n    for i, _ in enumerate(dataset.classes):\\n        weight.append(1/np.count_nonzero(np.array(dataset.targets) == i))\\n    return torch.tensor(weight)\\n\\n\\ndef run_tune(model, train_dl, valid_dl, test_dl, data_count, device, epochs, dist_loss):\\n    # Freeze all layers\\n    for param in model.parameters():\\n        param.requires_grad = False\\n    # Thaw last layer\\n    for param in model.last_linear.parameters():\\n        param.requires_grad = True\\n\\n    if not dist_loss:\\n        in_features = model.last_linear.out_features\\n        out_features = len(train_dl.dataset.classes)\\n\\n        model_ex = nn.Sequential(\\n            model,\\n            nn.Linear(in_features, out_features),\\n            nn.LogSoftmax(dim=1)\\n        )\\n\\n        params = list(model_ex[0].last_linear.parameters()) + \\\\\\n            list(model_ex[1].parameters())\\n        optim = torch.optim.Adam(params, lr=0.0005)\\n        loss_func = nn.CrossEntropyLoss(get_weight(train_dl.dataset))\\n        _ = std_fit(model_ex, optim, train_dl, valid_dl,\\n                    device, data_count, loss_func=loss_func, epochs=epochs)\\n\\n    else:\\n        optim = torch.optim.Adam(\\n            params=model.last_linear.parameters(), lr=0.0007)\\n        _ = dist_fit(model, optim, train_dl, valid_dl,\\n                     device, data_count, epochs=epochs)\\n\\n\\ndef test_model(model, test_dl, embeds, labels,  k, thresh):\\n    accuracy = check_accuracy(test_dl, embeds, labels, model, k, thresh)\\n    print(\\n        f\"\\\\nAccuracy at k={k}, threshold={thresh:0.2f}: {accuracy*100:0.3f} %\")\\n\\n\\ndef get_threshold(model, embeds, labels):\\n    clearance = 0.1\\n    thresh = show_embed_metrics(embeds, labels)\\n    thresh = np.round(thresh.item()+clearance, 3)\\n\\n    print(f\"threshold: {thresh:0.4f}\")\\n    return torch.tensor(thresh)\\n\\n\\ndef save_values(model, thresh, output_folder):\\n\\n    path = Path(\\'/\\'.join(output_folder.parts[:-1]))\\n    if not path.exists():\\n        path.mkdir(parents=True)\\n\\n    data = {\"state_dict\": model.state_dict(), \"threshold\": thresh}\\n    torch.save(data, output_folder)\\n    print(\"model and threshold saved\")\\n\\n\\ndef tune_network(input_folder, output_folder, device, epochs=25, retune=None, dist_loss=False):\\n    TR, VA = SETS\\n    # Set k for accuracy testing.\\n    k = 7\\n\\n    # # Set data input paths.\\n    train_path = input_folder/\"train\"\\n    test_path = input_folder/\"test\"\\n\\n    model = InceptionResnetV1(device=device)\\n    if retune is not None:\\n        try:\\n            state_dict = torch.load(retune)[\\'state_dict\\']\\n            print(\"model weights loaded\")\\n            model.load_state_dict(state_dict)\\n        except FileNotFoundError:\\n            pass\\n\\n    # Load all the dataloaders\\n    dloaders, data_count = get_dataloader(train_path)\\n    train_dl = dloaders[TR]\\n    valid_dl = dloaders[VA]\\n    test_dl = get_dataloader(test_path, use_transforms=False,\\n                             get_split=False, drop_last=False, batch_size=16)\\n    embed_dl = get_dataloader(train_path, use_transforms=False,\\n                              get_split=False, drop_last=False, batch_size=16)\\n\\n    if epochs > 0:\\n        # Function that calls fit using Adam optimiser and the passed parameters.\\n        run_tune(model, train_dl, valid_dl, test_dl,\\n                 data_count, device, epochs, dist_loss)\\n\\n    # Embeddings used to calculate threshold and check accuracy.\\n    embeds, labels = get_embeddings(embed_dl, model)\\n\\n    # Calculate threshold.\\n    thresh = get_threshold(model, embeds, labels)\\n\\n    # Get accuracy of the model (kNN using train data as neighbours).\\n    test_model(model, test_dl, embeds, labels, k, thresh)\\n\\n    # Save the state_dict and threshold as .pt files.\\n    save_values(model, thresh, output_folder)', path='identify/models/tune_network.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .mtcnn import MTCNN\\nfrom .inception_resnet_v1 import InceptionResnetV1\\nfrom .tune_network import tune_network\\nfrom .metrics import check_accuracy', path='identify/models/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\nfrom math import ceil\\nfrom scipy.stats import mode\\n\\n\\ndef predict(crops, embeds, labels, model, k=7, threshold=0.7, print_dist=False):\\n    \"\"\"\\n    crops: tensors, shape (m, 3, 160, 160).\\n    embeds: tensors, shape (n, 512).\\n    labels: int tensors, shape (n).\\n    model: pytorch models used to generate embedding of shape (1,512).\\n    k: neighbour classes to check.\\n    threshold: distance more than this is invalid\\n\\n    return: int tensor, shape (m)\\n    \"\"\"\\n    assert crops.shape[1] == 3, \"invalid input shape\"\\n\\n    inf = torch.tensor(float(\\'inf\\'))\\n    classes = []\\n    model.eval()\\n    with torch.no_grad():\\n        new_embeds = model(crops)\\n        for new_embed in new_embeds:\\n            dists = torch.norm(\\n                embeds - new_embed.reshape(1, *new_embed.shape), dim=1)\\n            knn = torch.topk(dists, k, largest=False)\\n            mask = dists[knn.indices] <= threshold\\n            # Indices of distances below threshold\\n            indices = knn.indices[mask]\\n            k_classes = labels[indices]\\n            try:\\n                classes.append(mode(k_classes).mode[0])\\n            except IndexError:\\n                classes.append(-1)\\n            if print_dist:\\n                print(\\n                    f\"max: {dists.max().item():0.4f}, min: {dists.min().item():0.4f}\")\\n    return torch.tensor(classes)\\n\\n\\ndef check_accuracy(dataloader, embeds, labels, model, k=7, thresh=0.7, print_dist=False):\\n    \"\"\"\\n    dataloader: pytorch DataLoader (test dataloader)\\n    embeds: tensors, shape (n, 512).\\n    labels: int tensors, shape (n).\\n    model: pytorch models used to generate embedding of shape (1,512).\\n    k: neighbour classes to check.\\n    threshold: distance more than this is invalid\\n\\n    return: float\\n    \"\"\"\\n    batch_count = ceil(len(dataloader.dataset)/dataloader.batch_size)\\n    accuracy = 0\\n\\n    for batch in dataloader:\\n        crops_t, labels_t = batch\\n        bs = torch.tensor(len(labels_t)).float()\\n        classes = predict(crops_t, embeds, labels,\\n                          model, k, thresh, print_dist)\\n        batch_accuracy = (classes == labels_t).sum().float()/bs\\n        accuracy += batch_accuracy\\n    accuracy /= batch_count\\n    return accuracy.item()', path='identify/models/metrics/check_accuracy.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\n\\n\\ndef get_cross_dist(embeds_1, embeds_2):\\n    \"\"\"\\n    Calculates the (L2) distance between all the embeddings in all the embeds.\\n    embed_1 shape (m, v)\\n    embed_2 shape (n, v)\\n\\n    return shape(m,n)\\n    \"\"\"\\n    embeds_cross_dist = []\\n    for embed in embeds_1:\\n        dists = torch.norm(embeds_2 - embed, dim=1)\\n        embeds_cross_dist.append(dists)\\n    return torch.stack(embeds_cross_dist)\\n\\n\\ndef show_min_max(embeds_cross_dist, labels_1, labels_2, show_sim):\\n    # I am aware that there maybe better non for loopy way of doing this.\\n    if show_sim:\\n        print(\"SHOWING SIMILAR\")\\n    else:\\n        print(\"SHOWING DISSIMILAR\")\\n    max_dist = []\\n    min_dist = []\\n    log_dist = []\\n    for i, vec in enumerate(embeds_cross_dist):\\n        dists = []\\n        for j, dist in enumerate(vec):\\n            if labels_1[i] == labels_2[j] and i != j and show_sim:\\n                log_dist.append(dist)\\n                dists.append(dist)\\n            elif not show_sim and labels_1[i] != labels_2[j]:\\n                log_dist.append(dist)\\n                dists.append(dist)\\n\\n        if len(dists) > 0:\\n            mx = max(dists)\\n            mn = min(dists)\\n            max_dist.append(mx)\\n            min_dist.append(mn)\\n\\n    overall = torch.tensor(log_dist).mean()\\n    mean_min = torch.tensor(min_dist).mean()\\n    print(\\'---\\')\\n    print(f\"alltime max(max) = {max(max_dist)}\")\\n    print(f\"alltime min(max) = {min(max_dist)}\")\\n    print(f\"mean of max      = {torch.tensor(max_dist).mean()}\")\\n    print()\\n    print(f\"alltime min(min) = {min(min_dist)}\")\\n    print(f\"alltime max(min) = {max(min_dist)}\")\\n    print(f\"mean of min      = {mean_min}\")\\n    print()\\n    print(f\"overall mean     = {overall}\")\\n    print()\\n\\n    if show_sim:\\n        return torch.tensor(max_dist).mean(), overall, mean_min\\n    else:\\n        return torch.tensor(min_dist).mean(), overall, None\\n\\n\\ndef show_embed_metrics(embeds, labels):\\n    \"\"\"\\n    Calls the min max function, shows stats on embeddings generated from a model \\n    Scores returned can be used to calculate threshold\\n    \"\"\"\\n    print(f\\'Showing embedding distance metrics, {len(labels)} embeds: \\')\\n    dist_matr = get_cross_dist(embeds, embeds)\\n    _, overall, mean_min = show_min_max(\\n        dist_matr, labels, labels, show_sim=True)\\n    print(\\'---\\')\\n    _, _, _ = show_min_max(dist_matr, labels, labels, show_sim=False)\\n    print(\\'---\\')\\n    return overall', path='identify/models/metrics/embed_metrics.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='from .check_accuracy import check_accuracy\\nfrom .embed_metrics import show_embed_metrics', path='identify/models/metrics/__init__.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\nfrom torch.nn.functional import interpolate\\nfrom torchvision.transforms import functional as F\\nfrom torchvision.ops.boxes import batched_nms\\nimport cv2\\nfrom PIL import Image\\nimport numpy as np\\nimport os\\n\\n\\ndef detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):\\n    if isinstance(imgs, (np.ndarray, torch.Tensor)):\\n        imgs = torch.as_tensor(imgs, device=device)\\n        if len(imgs.shape) == 3:\\n            imgs = imgs.unsqueeze(0)\\n    else:\\n        if not isinstance(imgs, (list, tuple)):\\n            imgs = [imgs]\\n        if any(img.size != imgs[0].size for img in imgs):\\n            raise Exception(\"MTCNN batch processing only compatible with equal-dimension images.\")\\n        imgs = np.stack([np.uint8(img) for img in imgs])\\n\\n    imgs = torch.as_tensor(imgs, device=device)\\n\\n    imgs = imgs.permute(0, 3, 1, 2).float()\\n\\n    batch_size = len(imgs)\\n    h, w = imgs.shape[2:4]\\n    m = 12.0 / minsize\\n    minl = min(h, w)\\n    minl = minl * m\\n\\n    # Create scale pyramid\\n    scale_i = m\\n    scales = []\\n    while minl >= 12:\\n        scales.append(scale_i)\\n        scale_i = scale_i * factor\\n        minl = minl * factor\\n\\n    # First stage\\n    boxes = []\\n    image_inds = []\\n    all_inds = []\\n    all_i = 0\\n    for scale in scales:\\n        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))\\n        im_data = (im_data - 127.5) * 0.0078125\\n        reg, probs = pnet(im_data)\\n    \\n        boxes_scale, image_inds_scale = generateBoundingBox(reg, probs[:, 1], scale, threshold[0])\\n        boxes.append(boxes_scale)\\n        image_inds.append(image_inds_scale)\\n        all_inds.append(all_i + image_inds_scale)\\n        all_i += batch_size\\n\\n    boxes = torch.cat(boxes, dim=0)\\n    image_inds = torch.cat(image_inds, dim=0).cpu()\\n    all_inds = torch.cat(all_inds, dim=0)\\n\\n    # NMS within each scale + image\\n    pick = batched_nms(boxes[:, :4], boxes[:, 4], all_inds, 0.5)\\n    boxes, image_inds = boxes[pick], image_inds[pick]\\n    \\n    # NMS within each image\\n    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\\n    boxes, image_inds = boxes[pick], image_inds[pick]\\n\\n    regw = boxes[:, 2] - boxes[:, 0]\\n    regh = boxes[:, 3] - boxes[:, 1]\\n    qq1 = boxes[:, 0] + boxes[:, 5] * regw\\n    qq2 = boxes[:, 1] + boxes[:, 6] * regh\\n    qq3 = boxes[:, 2] + boxes[:, 7] * regw\\n    qq4 = boxes[:, 3] + boxes[:, 8] * regh\\n    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)\\n    boxes = rerec(boxes)\\n    y, ey, x, ex = pad(boxes, w, h)\\n    \\n    # Second stage\\n    if len(boxes) > 0:\\n        im_data = []\\n        for k in range(len(y)):\\n            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\\n                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\\n                im_data.append(imresample(img_k, (24, 24)))\\n        im_data = torch.cat(im_data, dim=0)\\n        im_data = (im_data - 127.5) * 0.0078125\\n        out = rnet(im_data)\\n\\n        out0 = out[0].permute(1, 0)\\n        out1 = out[1].permute(1, 0)\\n        score = out1[1, :]\\n        ipass = score > threshold[1]\\n        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\\n        image_inds = image_inds[ipass]\\n        mv = out0[:, ipass].permute(1, 0)\\n\\n        # NMS within each image\\n        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\\n        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]\\n        boxes = bbreg(boxes, mv)\\n        boxes = rerec(boxes)\\n\\n    # Third stage\\n    points = torch.zeros(0, 5, 2, device=device)\\n    if len(boxes) > 0:\\n        y, ey, x, ex = pad(boxes, w, h)\\n        im_data = []\\n        for k in range(len(y)):\\n            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\\n                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\\n                im_data.append(imresample(img_k, (48, 48)))\\n        im_data = torch.cat(im_data, dim=0)\\n        im_data = (im_data - 127.5) * 0.0078125\\n        out = onet(im_data)\\n\\n        out0 = out[0].permute(1, 0)\\n        out1 = out[1].permute(1, 0)\\n        out2 = out[2].permute(1, 0)\\n        score = out2[1, :]\\n        points = out1\\n        ipass = score > threshold[2]\\n        points = points[:, ipass]\\n        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\\n        image_inds = image_inds[ipass]\\n        mv = out0[:, ipass].permute(1, 0)\\n\\n        w_i = boxes[:, 2] - boxes[:, 0] + 1\\n        h_i = boxes[:, 3] - boxes[:, 1] + 1\\n        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1\\n        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1\\n        points = torch.stack((points_x, points_y)).permute(2, 1, 0)\\n        boxes = bbreg(boxes, mv)\\n\\n        # NMS within each image using \"Min\" strategy\\n        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\\n        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, \\'Min\\')\\n        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]\\n\\n    boxes = boxes.cpu().numpy()\\n    points = points.cpu().numpy()\\n\\n    batch_boxes = []\\n    batch_points = []\\n    for b_i in range(batch_size):\\n        b_i_inds = np.where(image_inds == b_i)\\n        batch_boxes.append(boxes[b_i_inds].copy())\\n        batch_points.append(points[b_i_inds].copy())\\n\\n    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\\n\\n    return batch_boxes, batch_points\\n\\n\\ndef bbreg(boundingbox, reg):\\n    if reg.shape[1] == 1:\\n        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))\\n\\n    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\\n    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\\n    b1 = boundingbox[:, 0] + reg[:, 0] * w\\n    b2 = boundingbox[:, 1] + reg[:, 1] * h\\n    b3 = boundingbox[:, 2] + reg[:, 2] * w\\n    b4 = boundingbox[:, 3] + reg[:, 3] * h\\n    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)\\n\\n    return boundingbox\\n\\n\\ndef generateBoundingBox(reg, probs, scale, thresh):\\n    stride = 2\\n    cellsize = 12\\n\\n    reg = reg.permute(1, 0, 2, 3)\\n\\n    mask = probs >= thresh\\n    mask_inds = mask.nonzero()\\n    image_inds = mask_inds[:, 0]\\n    score = probs[mask]\\n    reg = reg[:, mask].permute(1, 0)\\n    bb = mask_inds[:, 1:].float().flip(1)\\n    q1 = ((stride * bb + 1) / scale).floor()\\n    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()\\n    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)\\n    return boundingbox, image_inds\\n\\n\\ndef nms_numpy(boxes, scores, threshold, method):\\n    if boxes.size == 0:\\n        return np.empty((0, 3))\\n\\n    x1 = boxes[:, 0].copy()\\n    y1 = boxes[:, 1].copy()\\n    x2 = boxes[:, 2].copy()\\n    y2 = boxes[:, 3].copy()\\n    s = scores\\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\\n\\n    I = np.argsort(s)\\n    pick = np.zeros_like(s, dtype=np.int16)\\n    counter = 0\\n    while I.size > 0:\\n        i = I[-1]\\n        pick[counter] = i\\n        counter += 1\\n        idx = I[0:-1]\\n\\n        xx1 = np.maximum(x1[i], x1[idx]).copy()\\n        yy1 = np.maximum(y1[i], y1[idx]).copy()\\n        xx2 = np.minimum(x2[i], x2[idx]).copy()\\n        yy2 = np.minimum(y2[i], y2[idx]).copy()\\n\\n        w = np.maximum(0.0, xx2 - xx1 + 1).copy()\\n        h = np.maximum(0.0, yy2 - yy1 + 1).copy()\\n\\n        inter = w * h\\n        if method is \"Min\":\\n            o = inter / np.minimum(area[i], area[idx])\\n        else:\\n            o = inter / (area[i] + area[idx] - inter)\\n        I = I[np.where(o <= threshold)]\\n\\n    pick = pick[:counter].copy()\\n    return pick\\n\\n\\ndef batched_nms_numpy(boxes, scores, idxs, threshold, method):\\n    device = boxes.device\\n    if boxes.numel() == 0:\\n        return torch.empty((0,), dtype=torch.int64, device=device)\\n    # strategy: in order to perform NMS independently per class.\\n    # we add an offset to all the boxes. The offset is dependent\\n    # only on the class idx, and is large enough so that boxes\\n    # from different classes do not overlap\\n    max_coordinate = boxes.max()\\n    offsets = idxs.to(boxes) * (max_coordinate + 1)\\n    boxes_for_nms = boxes + offsets[:, None]\\n    boxes_for_nms = boxes_for_nms.cpu().numpy()\\n    scores = scores.cpu().numpy()\\n    keep = nms_numpy(boxes_for_nms, scores, threshold, method)\\n    return torch.as_tensor(keep, dtype=torch.long, device=device)\\n\\n\\ndef pad(boxes, w, h):\\n    boxes = boxes.trunc().int().cpu().numpy()\\n    x = boxes[:, 0]\\n    y = boxes[:, 1]\\n    ex = boxes[:, 2]\\n    ey = boxes[:, 3]\\n\\n    x[x < 1] = 1\\n    y[y < 1] = 1\\n    ex[ex > w] = w\\n    ey[ey > h] = h\\n\\n    return y, ey, x, ex\\n\\n\\ndef rerec(bboxA):\\n    h = bboxA[:, 3] - bboxA[:, 1]\\n    w = bboxA[:, 2] - bboxA[:, 0]\\n    \\n    l = torch.max(w, h)\\n    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\\n    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\\n    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)\\n\\n    return bboxA\\n\\n\\ndef imresample(img, sz):\\n    im_data = interpolate(img, size=sz, mode=\"area\")\\n    return im_data\\n\\n\\ndef crop_resize(img, box, image_size):\\n    if isinstance(img, np.ndarray):\\n        out = cv2.resize(\\n            img[box[1]:box[3], box[0]:box[2]],\\n            (image_size, image_size),\\n            interpolation=cv2.INTER_AREA\\n        ).copy()\\n    else:\\n        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\\n    return out\\n\\n\\ndef save_img(img, path):\\n    if isinstance(img, np.ndarray):\\n        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\\n    else:\\n        img.save(path)\\n\\n\\ndef get_size(img):\\n    if isinstance(img, np.ndarray):\\n        return img.shape[1::-1]\\n    else:\\n        return img.size\\n\\n\\ndef extract_face(img, box, image_size=160, margin=0, save_path=None):\\n    \"\"\"Extract face + margin from PIL Image given bounding box.\\n    \\n    Arguments:\\n        img {PIL.Image} -- A PIL Image.\\n        box {numpy.ndarray} -- Four-element bounding box.\\n        image_size {int} -- Output image size in pixels. The image will be square.\\n        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \\n            Note that the application of the margin differs slightly from the davidsandberg/facenet\\n            repo, which applies the margin to the original image before resizing, making the margin\\n            dependent on the original image size.\\n        save_path {str} -- Save path for extracted face image. (default: {None})\\n    \\n    Returns:\\n        torch.tensor -- tensor representing the extracted face.\\n    \"\"\"\\n    margin = [\\n        margin * (box[2] - box[0]) / (image_size - margin),\\n        margin * (box[3] - box[1]) / (image_size - margin),\\n    ]\\n    raw_image_size = get_size(img)\\n    box = [\\n        int(max(box[0] - margin[0] / 2, 0)),\\n        int(max(box[1] - margin[1] / 2, 0)),\\n        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\\n        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\\n    ]\\n\\n    face = crop_resize(img, box, image_size)\\n\\n    if save_path is not None:\\n        os.makedirs(os.path.dirname(save_path) + \"/\", exist_ok=True)\\n        save_img(face, save_path)\\n\\n    face = F.to_tensor(np.float32(face))\\n\\n    return face', path='identify/models/utils/detect_face.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='import torch\\nimport numpy as np\\nfrom torch.utils.data import DataLoader\\nfrom torch.utils.data.sampler import SubsetRandomSampler\\nfrom torchvision.datasets import ImageFolder\\nfrom torchvision.transforms import Compose, ColorJitter, ToTensor, Normalize\\nfrom sklearn.model_selection import train_test_split\\nfrom identify.helpers.constants import SETS\\n\\n\\ndef get_mean_std(path):\\n    \"\"\"\\n    returns mean and std of image\\n    data located at the given path.\\n    \"\"\"\\n    dataset = ImageFolder(path)\\n    tr = ToTensor()\\n    imgs = []\\n    for i in dataset:\\n        imgs.append(tr(i[0]))\\n    # dataset\\n    imgs = torch.stack(imgs)\\n    mean = imgs.mean(axis=(0, 2, 3))\\n    std = imgs.std(axis=(0, 2, 3))\\n    return mean, std\\n\\n\\ndef get_dataloader(data_path, use_transforms=True, get_split=True, drop_last=True, batch_size=5, shuffle=False):\\n    # Returns DataLoader and datacount if using sampler (for split).\\n    mean, std = get_mean_std(data_path)\\n    data_trans = None\\n    if use_transforms:\\n        data_trans = Compose([\\n            ColorJitter(0.3, 0.3, 0.3),\\n            ToTensor(),\\n            Normalize(mean, std)\\n        ])\\n    else:\\n        data_trans = Compose([\\n            ToTensor(),\\n            Normalize(mean, std)\\n        ])\\n\\n    dataset = ImageFolder(\\n        data_path, transform=data_trans)\\n\\n    if not get_split:\\n\\n        return DataLoader(dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\\n    else:\\n        targets = np.array(dataset.targets)\\n        idx = train_test_split(torch.arange(\\n            len(targets)), shuffle=True, stratify=targets, random_state=34, test_size=0.2)\\n\\n        g = np.gcd(len(idx[0]), len(idx[1]))\\n        batch_size = g if g > 4 and g < 32 else 10\\n        if len(idx[0]) < 128:\\n            batch_size = 5\\n\\n        indices = {x: idx[i] for i, x in enumerate(SETS)}\\n        samplers = {x: SubsetRandomSampler(indices[x]) for x in SETS}\\n        dataloaders = {x: DataLoader(dataset, int(\\n            batch_size), sampler=samplers[x], drop_last=True) for x in SETS}\\n        datacount = {x: len(indices[x]) for x in SETS}\\n        return dataloaders, datacount', path='identify/models/utils/tuner_helpers.py', repo_name='18alantom/identify'),\n",
       " Pandas(content='# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n# ==============================================================================\\nr\"\"\"Downloads and converts a particular dataset.\\n\\nUsage:\\n```shell\\n\\n$ python download_and_convert_data.py \\\\\\n    --dataset_name=mnist \\\\\\n    --dataset_dir=/tmp/mnist\\n\\n$ python download_and_convert_data.py \\\\\\n    --dataset_name=cifar10 \\\\\\n    --dataset_dir=/tmp/cifar10\\n\\n$ python download_and_convert_data.py \\\\\\n    --dataset_name=flowers \\\\\\n    --dataset_dir=/tmp/flowers\\n```\\n\"\"\"\\n\\n\\n\\n\\nimport tensorflow as tf\\n\\nfrom .datasets import download_and_convert_cifar10\\nfrom .datasets import download_and_convert_flowers\\nfrom .datasets import download_and_convert_mnist\\nfrom .datasets import convert_quiz\\n\\nFLAGS = tf.app.flags.FLAGS\\n\\ntf.app.flags.DEFINE_string(\\n    \\'dataset_name\\',\\n    None,\\n    \\'The name of the dataset to convert, one of \"cifar10\", \"flowers\", \"mnist\".\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'dataset_dir\\',\\n    None,\\n    \\'The directory where the output TFRecords and temporary files are saved.\\')\\n\\n\\ndef main(_):\\n    if not FLAGS.dataset_name:\\n        raise ValueError(\\'You must supply the dataset name with --dataset_name\\')\\n    if not FLAGS.dataset_dir:\\n        raise ValueError(\\'You must supply the dataset directory with --dataset_dir\\')\\n\\n    if FLAGS.dataset_name == \\'cifar10\\':\\n        download_and_convert_cifar10.run(FLAGS.dataset_dir)\\n    elif FLAGS.dataset_name == \\'flowers\\':\\n        download_and_convert_flowers.run(FLAGS.dataset_dir)\\n    elif FLAGS.dataset_name == \\'mnist\\':\\n        download_and_convert_mnist.run(FLAGS.dataset_dir)\\n    elif FLAGS.dataset_name == \\'quiz\\':\\n        convert_quiz.run(FLAGS.dataset_dir)\\n    else:\\n        raise ValueError(\\n            \\'dataset_name [%s] was not recognized.\\' % FLAGS.dataset_name)\\n\\n\\nif __name__ == \\'__main__\\':\\n    tf.app.run()', path='download_and_convert_data.py', repo_name='0492wzl/tensorflow_slim_densenet'),\n",
       " Pandas(content='# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n# ==============================================================================\\n\"\"\"Generic evaluation script that evaluates a model using a given dataset.\"\"\"\\n\\n\\n\\n\\n\\nimport math\\nimport tensorflow as tf\\n\\nfrom .datasets import dataset_factory\\nfrom .nets import nets_factory\\nfrom .preprocessing import preprocessing_factory\\n\\nslim = tf.contrib.slim\\n\\ntf.app.flags.DEFINE_integer(\\n    \\'batch_size\\', 50, \\'The number of samples in each batch.\\')\\n\\ntf.app.flags.DEFINE_integer(\\n    \\'max_num_batches\\', None,\\n    \\'Max number of batches to evaluate by default use all.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'master\\', \\'\\', \\'The address of the TensorFlow master to use.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'checkpoint_path\\', \\'/tmp/tfmodel/\\',\\n    \\'The directory where the model was written to or an absolute path to a \\'\\n    \\'checkpoint file.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'eval_dir\\', \\'/tmp/tfmodel/\\', \\'Directory where the results are saved to.\\')\\n\\ntf.app.flags.DEFINE_integer(\\n    \\'num_preprocessing_threads\\', 4,\\n    \\'The number of threads used to create the batches.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'dataset_name\\', \\'imagenet\\', \\'The name of the dataset to load.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'dataset_split_name\\', \\'test\\', \\'The name of the train/test split.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'dataset_dir\\', None, \\'The directory where the dataset files are stored.\\')\\n\\ntf.app.flags.DEFINE_integer(\\n    \\'labels_offset\\', 0,\\n    \\'An offset for the labels in the dataset. This flag is primarily used to \\'\\n    \\'evaluate the VGG and ResNet architectures which do not use a background \\'\\n    \\'class for the ImageNet dataset.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'model_name\\', \\'inception_v3\\', \\'The name of the architecture to evaluate.\\')\\n\\ntf.app.flags.DEFINE_string(\\n    \\'preprocessing_name\\', None, \\'The name of the preprocessing to use. If left \\'\\n    \\'as `None`, then the model_name flag is used.\\')\\n\\ntf.app.flags.DEFINE_float(\\n    \\'moving_average_decay\\', None,\\n    \\'The decay to use for the moving average.\\'\\n    \\'If left as None, then moving averages are not used.\\')\\n\\ntf.app.flags.DEFINE_integer(\\n    \\'eval_image_size\\', None, \\'Eval image size\\')\\n\\nFLAGS = tf.app.flags.FLAGS\\n\\n\\ndef main(_):\\n  if not FLAGS.dataset_dir:\\n    raise ValueError(\\'You must supply the dataset directory with --dataset_dir\\')\\n\\n  tf.logging.set_verbosity(tf.logging.INFO)\\n  with tf.Graph().as_default():\\n    tf_global_step = slim.get_or_create_global_step()\\n\\n    ######################\\n    # Select the dataset #\\n    ######################\\n    dataset = dataset_factory.get_dataset(\\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\\n\\n    ####################\\n    # Select the model #\\n    ####################\\n    network_fn = nets_factory.get_network_fn(\\n        FLAGS.model_name,\\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\\n        is_training=False)\\n\\n    ##############################################################\\n    # Create a dataset provider that loads data from the dataset #\\n    ##############################################################\\n    provider = slim.dataset_data_provider.DatasetDataProvider(\\n        dataset,\\n        shuffle=False,\\n        common_queue_capacity=2 * FLAGS.batch_size,\\n        common_queue_min=FLAGS.batch_size)\\n    [image, label] = provider.get([\\'image\\', \\'label\\'])\\n    label -= FLAGS.labels_offset\\n\\n    #####################################\\n    # Select the preprocessing function #\\n    #####################################\\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\\n        preprocessing_name,\\n        is_training=False)\\n\\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\\n\\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\\n\\n    images, labels = tf.train.batch(\\n        [image, label],\\n        batch_size=FLAGS.batch_size,\\n        num_threads=FLAGS.num_preprocessing_threads,\\n        capacity=5 * FLAGS.batch_size)\\n\\n    ####################\\n    # Define the model #\\n    ####################\\n    logits, _ = network_fn(images)\\n\\n    if FLAGS.moving_average_decay:\\n      variable_averages = tf.train.ExponentialMovingAverage(\\n          FLAGS.moving_average_decay, tf_global_step)\\n      variables_to_restore = variable_averages.variables_to_restore(\\n          slim.get_model_variables())\\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\\n    else:\\n      variables_to_restore = slim.get_variables_to_restore()\\n\\n    predictions = tf.argmax(logits, 1)\\n    labels = tf.squeeze(labels)\\n\\n    # Define the metrics:\\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\\n        \\'Accuracy\\': slim.metrics.streaming_accuracy(predictions, labels),\\n        \\'Recall_5\\': slim.metrics.streaming_recall_at_k(\\n            logits, labels, 5),\\n    })\\n\\n    # Print the summaries to screen.\\n    for name, value in list(names_to_values.items()):\\n      summary_name = \\'eval/%s\\' % name\\n      op = tf.summary.scalar(summary_name, value, collections=[])\\n      op = tf.Print(op, [value], summary_name)\\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\\n\\n    # TODO(sguada) use num_epochs=1\\n    if FLAGS.max_num_batches:\\n      num_batches = FLAGS.max_num_batches\\n    else:\\n      # This ensures that we make a single pass over all of the data.\\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\\n\\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\\n    else:\\n      checkpoint_path = FLAGS.checkpoint_path\\n\\n    tf.logging.info(\\'Evaluating %s\\' % checkpoint_path)\\n\\n    slim.evaluation.evaluate_once(\\n        master=FLAGS.master,\\n        checkpoint_path=checkpoint_path,\\n        logdir=FLAGS.eval_dir,\\n        num_evals=num_batches,\\n        eval_op=list(names_to_updates.values()),\\n        variables_to_restore=variables_to_restore)\\n\\n\\nif __name__ == \\'__main__\\':\\n  tf.app.run()', path='eval_image_classifier.py', repo_name='0492wzl/tensorflow_slim_densenet')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(selected_repos_python_files_df.iloc[:100].itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18aa8eb27d146fa8f191548dd403f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "making python functions df:   0%|          | 0/3169680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 4min 58s, sys: 42.4 s, total: 2h 5min 41s\n",
      "Wall time: 2h 5min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>function_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>failure</td>\n",
       "      <td>def failure(message):\\n    print('{}failure: {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>success</td>\n",
       "      <td>def success(message):\\n    print('{}success: {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>get_best_score</td>\n",
       "      <td>def get_best_score(log_file):\\n    scores = []...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>run</td>\n",
       "      <td>def run(dir_, score=None):\\n    config_file = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>BPE</td>\n",
       "      <td>class BPE(object):\\n\\n    def encode(self, ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537187</th>\n",
       "      <td>blessengeorge/compare_gan</td>\n",
       "      <td>compare_gan/src/multi_gan/visualize_gan.py</td>\n",
       "      <td>GetMultiGANGeneratorsOp</td>\n",
       "      <td>def GetMultiGANGeneratorsOp(graph, gan_type, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537188</th>\n",
       "      <td>blessengeorge/compare_gan</td>\n",
       "      <td>compare_gan/src/multi_gan/visualize_gan.py</td>\n",
       "      <td>EvalCheckpoint</td>\n",
       "      <td>def EvalCheckpoint(checkpoint_path, task_workd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537189</th>\n",
       "      <td>blessengeorge/compare_gan</td>\n",
       "      <td>compare_gan/src/multi_gan/visualize_gan.py</td>\n",
       "      <td>GetModelDir</td>\n",
       "      <td>def GetModelDir(options):\\n    'Returns the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537190</th>\n",
       "      <td>blessengeorge/compare_gan</td>\n",
       "      <td>compare_gan/src/multi_gan/visualize_gan.py</td>\n",
       "      <td>EvalTask</td>\n",
       "      <td>def EvalTask(options, task_workdir, out_dir):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537191</th>\n",
       "      <td>blessengeorge/compare_gan</td>\n",
       "      <td>compare_gan/src/multi_gan/visualize_gan.py</td>\n",
       "      <td>main</td>\n",
       "      <td>def main(unused_argv):\\n    gan_lib.MODELS.upd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12537192 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          repo_name  \\\n",
       "0                   trangvu/ape-npi   \n",
       "1                   trangvu/ape-npi   \n",
       "2                   trangvu/ape-npi   \n",
       "3                   trangvu/ape-npi   \n",
       "4                   trangvu/ape-npi   \n",
       "...                             ...   \n",
       "12537187  blessengeorge/compare_gan   \n",
       "12537188  blessengeorge/compare_gan   \n",
       "12537189  blessengeorge/compare_gan   \n",
       "12537190  blessengeorge/compare_gan   \n",
       "12537191  blessengeorge/compare_gan   \n",
       "\n",
       "                                                path            function_name  \\\n",
       "0                                       run-tests.py                  failure   \n",
       "1                                       run-tests.py                  success   \n",
       "2                                       run-tests.py           get_best_score   \n",
       "3                                       run-tests.py                      run   \n",
       "4                               scripts/apply_bpe.py                      BPE   \n",
       "...                                              ...                      ...   \n",
       "12537187  compare_gan/src/multi_gan/visualize_gan.py  GetMultiGANGeneratorsOp   \n",
       "12537188  compare_gan/src/multi_gan/visualize_gan.py           EvalCheckpoint   \n",
       "12537189  compare_gan/src/multi_gan/visualize_gan.py              GetModelDir   \n",
       "12537190  compare_gan/src/multi_gan/visualize_gan.py                 EvalTask   \n",
       "12537191  compare_gan/src/multi_gan/visualize_gan.py                     main   \n",
       "\n",
       "                                              function_code  \n",
       "0         def failure(message):\\n    print('{}failure: {...  \n",
       "1         def success(message):\\n    print('{}success: {...  \n",
       "2         def get_best_score(log_file):\\n    scores = []...  \n",
       "3         def run(dir_, score=None):\\n    config_file = ...  \n",
       "4         class BPE(object):\\n\\n    def encode(self, ori...  \n",
       "...                                                     ...  \n",
       "12537187  def GetMultiGANGeneratorsOp(graph, gan_type, a...  \n",
       "12537188  def EvalCheckpoint(checkpoint_path, task_workd...  \n",
       "12537189  def GetModelDir(options):\\n    'Returns the mo...  \n",
       "12537190  def EvalTask(options, task_workdir, out_dir):\\...  \n",
       "12537191  def main(unused_argv):\\n    gan_lib.MODELS.upd...  \n",
       "\n",
       "[12537192 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_function_data_df(python_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m python_functions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/python_functions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1250\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1248\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1250\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "python_functions_df = pd.read_csv(\"output/python_functions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python_functions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpython_functions_df\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_functions_df' is not defined"
     ]
    }
   ],
   "source": [
    "python_functions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python_functions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpython_functions_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_functions_df' is not defined"
     ]
    }
   ],
   "source": [
    "python_functions_df[\"repo_name\"].value_counts()  # .plot.hist()\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python_functions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m repo_function_counts \u001b[38;5;241m=\u001b[39m \u001b[43mpython_functions_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()  \u001b[38;5;66;03m# .plot.hist()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m small_repos \u001b[38;5;241m=\u001b[39m repo_function_counts[repo_function_counts \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_functions_df' is not defined"
     ]
    }
   ],
   "source": [
    "repo_function_counts = python_functions_df[\"repo_name\"].value_counts()  # .plot.hist()\n",
    "small_repos = repo_function_counts[repo_function_counts < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python_functions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m repo_sample \u001b[38;5;241m=\u001b[39m \u001b[43mpython_functions_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_functions_df' is not defined"
     ]
    }
   ],
   "source": [
    "repo_sample = python_functions_df[\"repo_name\"].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'repo_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m python_files_df[python_files_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mrepo_sample\u001b[49m)]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repo_sample' is not defined"
     ]
    }
   ],
   "source": [
    "python_files_df[python_files_df[\"repo_name\"].isin(repo_sample)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python_functions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m small_repo_functions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpython_functions_df\u001b[49m[\n\u001b[1;32m      2\u001b[0m     python_functions_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(small_repos\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m      3\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_functions_df' is not defined"
     ]
    }
   ],
   "source": [
    "small_repo_functions_df = python_functions_df[\n",
    "    python_functions_df[\"repo_name\"].isin(small_repos.index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-dbd5717aacfb>:1: DtypeWarning: Columns (2,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")\n"
     ]
    }
   ],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  ['Dictionary Learning']\n",
       "1                                    ['Contour Detection']\n",
       "2        ['Named Entity Recognition', 'Sentiment Analys...\n",
       "3        ['Named Entity Recognition', 'Sentiment Analys...\n",
       "4        ['Heart Rate Variability', 'Image-to-Image Tra...\n",
       "                               ...                        \n",
       "36204                                   ['Style Transfer']\n",
       "36205                                   ['Style Transfer']\n",
       "36206                                   ['Style Transfer']\n",
       "36207                                   ['Style Transfer']\n",
       "36208    ['Denoising', 'Domain Adaptation', 'Representa...\n",
       "Name: tasks, Length: 36209, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m functions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/python_functions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1250\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1248\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1250\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "functions_df = pd.read_csv(\"output/python_functions.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_functions_df = functions_df[functions_df[\"repo_name\"] == \"trangvu/ape-npi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutil.feature_extraction import embeddings_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/kuba/.local/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK2at6Tensor6deviceEv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-309f8a7e2e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"microsoft/codebert-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/mlutil/mlutil/feature_extraction/embeddings_torch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, model, tokenizer, aggregation, device, batch_size, max_length, truncation)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2939\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__version__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2941\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1888\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1887\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    263\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m from ..tapas.modeling_tapas import (\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mTapasForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mTapasForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# soft dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_scatter_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_scatter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BUILD_DOCS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/kuba/.local/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK2at6Tensor6deviceEv"
     ]
    }
   ],
   "source": [
    "embedding_model = embeddings_torch.TransformerVectorizer(\n",
    "    model_type=\"microsoft/codebert-base\", batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embeddings = embedding_model.transform(list(example_functions_df[\"function_code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentRemover:\n",
    "    def __init__(self, pca):\n",
    "        self.pca = pca\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.pca.fit(X)\n",
    "        transformed_embeddings = self.pca.transform(X)\n",
    "        transformed_embeddings[:, 0] = 0\n",
    "        return self.pca.inverse_transform(transformed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentroidSummarizer:\n",
    "    def __init__(self, embedder, summary_size=10, pc_remover=None):\n",
    "        self.embedder = embedder\n",
    "        self.clusterer = cluster.KMeans(summary_size, n_jobs=-1)\n",
    "        self.pc_remover = pc_remover\n",
    "\n",
    "    def fit_pc_remover(self, texts):\n",
    "        text_embeddings = self.embedder.transform(texts)\n",
    "        pca = decomposition.PCA()\n",
    "        pca.fit(text_embeddings)\n",
    "        self.pc_remover = PrincipalComponentRemover(pca)\n",
    "\n",
    "    def summarize(self, texts, **kwargs):\n",
    "        text_embeddings = self.embedder.transform(texts, **kwargs)\n",
    "        if self.pc_remover:\n",
    "            text_embeddings = self.pc_remover.transform(text_embeddings)\n",
    "        self.clusterer.fit(text_embeddings)\n",
    "        centroids = self.clusterer.cluster_centers_\n",
    "        centroid_distances = metrics.pairwise.euclidean_distances(\n",
    "            centroids, text_embeddings\n",
    "        )\n",
    "        closest_idxs = centroid_distances.argsort(axis=1)\n",
    "        return np.array(texts)[closest_idxs[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_function_counts = functions_df[\"repo_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_function_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_repos = functions_df[\"repo_name\"].unique()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_repos = [repo for repo in selected_repos if repo_function_counts[repo] < 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutil import summarization\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_sample = functions_df.sample(50000)\n",
    "function_sample_embeddings = embedding_model.transform(\n",
    "    list(function_sample[\"function_code\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(function_sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_remover = PrincipalComponentRemover(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics, decomposition, mixture\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PrincipalComponentRemover:\n",
    "    def __init__(self, pca, zeroed_out_pcs=1):\n",
    "        self.pca = pca\n",
    "        self.zeroed_out_pcs = zeroed_out_pcs\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.pca.fit(X)\n",
    "        transformed_embeddings = self.pca.transform(X)\n",
    "        transformed_embeddings[:, : self.zeroed_out_pcs] = 0\n",
    "        return self.pca.inverse_transform(transformed_embeddings)\n",
    "\n",
    "\n",
    "class CentroidSummarizer:\n",
    "    def __init__(\n",
    "        self, embedder, n_clusters=10, pc_remover=None, clusterer_cls=cluster.KMeans\n",
    "    ):\n",
    "        self.embedder = embedder\n",
    "        self.clusterer = clusterer_cls(n_clusters)\n",
    "        self.pc_remover = pc_remover\n",
    "\n",
    "    def fit_clusterer(self, texts=None, text_embeddings=None, **kwargs):\n",
    "        assert not texts is None or not text_embeddings is None\n",
    "        if text_embeddings is None:\n",
    "            text_embeddings = self.embedder.transform(texts)\n",
    "        self.clusterer.fit(text_embeddings)\n",
    "\n",
    "    def fit_pc_remover(self, texts=None, text_embeddings=None, zeroed_out_pcs=2):\n",
    "        assert not texts is None or not text_embeddings is None\n",
    "        if text_embeddings is None:\n",
    "            text_embeddings = self.embedder.transform(texts)\n",
    "        pca = decomposition.PCA()\n",
    "        pca.fit(text_embeddings)\n",
    "        self.pc_remover = PrincipalComponentRemover(pca, zeroed_out_pcs=zeroed_out_pcs)\n",
    "\n",
    "    def summarize(self, texts, n_summaries=1, fit_clusterer=False, **kwargs):\n",
    "        text_embeddings = self.embedder.transform(texts, **kwargs)\n",
    "        if self.pc_remover:\n",
    "            text_embeddings = self.pc_remover.transform(text_embeddings)\n",
    "        if fit_clusterer:\n",
    "            self.clusterer.fit(text_embeddings)\n",
    "        centroids = self.clusterer.cluster_centers_\n",
    "        centroid_distances = metrics.pairwise.cosine_distances(\n",
    "            centroids, text_embeddings\n",
    "        )\n",
    "        closest_idxs_x, closest_idxs_y = np.unravel_index(\n",
    "            centroid_distances.argsort(axis=None), centroid_distances.shape\n",
    "        )\n",
    "        return np.array(texts)[closest_idxs_y[:n_summaries]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = mlutil.summarization.CentroidSummarizer(\n",
    "    embedding_model, n_clusters=500\n",
    ")  # pca.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer.fit_pc_remover(text_embeddings=function_sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "summarizer.fit_clusterer(text_embeddings=function_sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutup\n",
    "\n",
    "shutup.please()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:17<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "summaries = [\n",
    "    summarizer.summarize(\n",
    "        list(functions_df[\"function_code\"][functions_df[\"repo_name\"] == repo]),\n",
    "        verbose=False,\n",
    "    )\n",
    "    for repo in tqdm.tqdm(selected_repos)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_typical_functions_df = pd.DataFrame(\n",
    "    {\"repo\": selected_repos, \"summary\": summaries}\n",
    ").explode(\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repo_typical_functions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_typical_functions_df[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer.clusterer.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_df[\"tasks\"] = functions_with_tasks_df[\"tasks\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_functions_with_tasks_df = functions_with_tasks_df.explode(\n",
    "    column=\"tasks\"\n",
    ")  # .apply(ast.literal_eval).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_functions_with_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_embedder = SentenceTransformer(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedder = sentence_embedder.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(exploded_functions_with_tasks_df[\"function_code\"]).str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embeddings = sentence_embedder.encode(\n",
    "    list(exploded_functions_with_tasks_df[\"function_code\"].iloc[:n_samples])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = sentence_embedder.encode(\n",
    "    exploded_functions_with_tasks_df[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util as sentence_transformer_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = set(exploded_functions_with_tasks_df[\"tasks\"].iloc[:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = sentence_embedder.encode(\n",
    "    exploded_functions_with_tasks_df[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores = sentence_transformer_util.pytorch_cos_sim(\n",
    "    query_embeddings, code_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_scores = torch.diag(cos_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between task and code embeddings for all tasks and matching tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(cos_scores[:n_samples].reshape(-1), label=\"all\", hist_kws=dict(alpha=0.5))\n",
    "sns.distplot(diag_scores, label=\"matching\", hist_kws=dict(alpha=0.5))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_similarities_added_df = exploded_functions_with_tasks_df.iloc[\n",
    "    :n_samples\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_similarities_added_df[\"similarity\"] = torch.diag(cos_scores).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    function_similarities_added_df.groupby(\"function_name\").agg(\"max\")[\"similarity\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_functions_with_tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster functions by tasks, path, function name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "from sklearn import metrics, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fasttext_model = fasttext.load_model(\"output/python_files_fasttext_dim200.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlutil.feature_extraction import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedder = embeddings.FastTextVectorizer(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_with_path = (\n",
    "    functions_with_tasks_df[\"path\"].str.replace(\"/\", \" \")\n",
    "    + \" \"\n",
    "    + functions_with_tasks_df[\"function_name\"]\n",
    "    + \" : \"\n",
    "    + functions_with_tasks_df[\"tasks\"].apply(\" \".join)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_path_embeddings = fasttext_embedder.transform(fname_with_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_path_cos_sim = metrics.pairwise.cosine_similarity(\n",
    "    fname_path_embeddings, fname_path_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_with_path.iloc[(-fname_path_cos_sim[100]).argsort()[:20]].str.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans.fit_predict(fname_path_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    print(fname_with_path[cluster_labels == i][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "from sklearn import model_selection\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import tqdm\n",
    "from transformers import RobertaForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/codebert-base-mlm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name,truncation = True, padding=True, max_length=100)\n",
    "model = RobertaForMaskedLM.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Tell pytorch to run this model on the GPU.\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens([\"<tasks>\", \"<end_tasks>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,  # I don't know if this is the only way to set up mask for mlm task....\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_about = (\n",
    "    \"<tasks> \"\n",
    "    + functions_with_tasks_df[\"tasks\"].apply(\" \".join).str.lower()\n",
    "    + \" <end_tasks>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks = function_about + \" \" + functions_with_tasks_df[\"function_code\"]\n",
    "functions_with_tasks = (\n",
    "    functions_with_tasks.str.replace(\"\\n\", \"<n>\")\n",
    "    .str.split()\n",
    "    .apply(lambda words: \" \".join(words[:200]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_train, functions_with_tasks_val = model_selection.train_test_split(\n",
    "    functions_with_tasks, test_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_train.to_csv(\n",
    "    \"tmp/functions_about_train.txt\", index=False, header=False\n",
    ")\n",
    "functions_with_tasks_val.to_csv(\n",
    "    \"tmp/functions_about_val.txt\", index=False, header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 tmp/functions_about.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"tmp/functions_about_train.txt\",  # I reorgnasized data to line by line form as the tutorial, which is stupid. but  I also tried TensorDataset, it got errors\n",
    "    block_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"tmp/functions_about_val.txt\",\n",
    "    block_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"tmp/codebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=cpu_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = functions_with_tasks_train.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = example.replace(\"anomaly detection\", \"<mask> <mask>\")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity_score(scoring_model, tokenizer, example, maxlen=500):\n",
    "    inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(scoring_model.device)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"][:, :maxlen]\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, :maxlen]\n",
    "    scoring_model_out = scoring_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return scoring_model_out[0].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_model = RobertaForCausalLM.from_pretrained(\n",
    "    \"tmp/codebert\",\n",
    "    is_decoder=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptn_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "gptn_scoring_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptn_scoring_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating whether model confuses tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_permuted_train = functions_with_tasks_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_permuted_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_tasks_train = functions_with_tasks_permuted_train.str.findall(\n",
    "    r\"(<tasks>.*<end_tasks>)\"\n",
    ").apply(lambda l: l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_tasks_train = permuted_tasks_train.sample(\n",
    "    len(permuted_tasks_train)\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_tasks_train.index = functions_with_tasks_permuted_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_permuted_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_permuted_train = (\n",
    "    permuted_tasks_train\n",
    "    + \" \"\n",
    "    + functions_with_tasks_permuted_train.str.replace(\n",
    "        r\"(FUNCTION_IS_ABOUT.*END_FUNCTION_IS_ABOUT)\", \"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            \"original\": get_perplexity_score(\n",
    "                gptn_scoring_model, gptn_tokenizer, original\n",
    "            ),\n",
    "            \"permuted\": get_perplexity_score(\n",
    "                gptn_scoring_model, gptn_tokenizer, permuted\n",
    "            ),\n",
    "        }\n",
    "        for (original, permuted) in tqdm.tqdm(\n",
    "            zip(\n",
    "                functions_with_tasks_train.sample(1000).values,\n",
    "                functions_with_tasks_permuted_train.values,\n",
    "            ),\n",
    "            total=1000,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scores_df[\"original\"] < scores_df[\"permuted\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity_score(\n",
    "    scoring_model, tokenizer, functions_with_tasks_permuted_train.iloc[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity_score(scoring_model, tokenizer, functions_with_tasks_train.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_tasks_permuted_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity_score(scoring_model, tokenizer, functions_with_tasks_train.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = example\n",
    "inputs = tokenizer(input_txt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sent, maxlen=512):\n",
    "\n",
    "    token_ids = tokenizer.encode(sent, return_tensors=\"pt\")[:, :maxlen]\n",
    "    masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "    masked_pos = [mask.item() for mask in masked_position]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(token_ids)\n",
    "\n",
    "    last_hidden_state = output[0].squeeze()\n",
    "\n",
    "    list_of_list = []\n",
    "    for index, mask_index in enumerate(masked_pos):\n",
    "        mask_hidden_state = last_hidden_state[mask_index]\n",
    "        idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "        list_of_list.append(words)\n",
    "        print(\"Mask \", index + 1, \"Guesses : \", words)\n",
    "\n",
    "    best_guess = \"\"\n",
    "    for j in list_of_list:\n",
    "        best_guess = best_guess + \" \" + j[0]\n",
    "\n",
    "    return best_guess.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(example.replace(\"clustering\", \"<mask>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-2.7B\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "C:  def f1(x, y): return x + y\n",
    "Q: what does this function do\n",
    "A: addition\n",
    "###\n",
    "C:  def f2(x, y): return x * y\n",
    "Q: what does this function do\n",
    "A: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator(t, do_sample=True, min_length=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_codes = (\n",
    "    functions_with_tasks_df[\"function_code\"]\n",
    "    .str.split()\n",
    "    .apply(lambda words: \" \".join(words[:100]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_tasks = functions_with_tasks_df[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_appended_tasks = (\n",
    "    function_codes + \"\\n tasks: \" + function_tasks.apply(\", \".join) + \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_with_appended_tasks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_example = \" \".join(functions_with_appended_tasks.sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(whole_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = whole_example.split(\"tasks\")[-1][1:-1]\n",
    "cut_length = len(whole_example.split(\"tasks\")[-1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_example = whole_example[:-cut_length].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generator(whole_example, do_sample=True, min_length=3, max_length=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = functions_with_tasks_train.iloc[0]\n",
    "example = example.replace(\"atari games\", \"reinforcement learning\")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity_score(scoring_model, tokenizer, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.save_pretrained(\"tmp/codebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "for k in range(2):\n",
    "    predicted_index = [sorted_idx[i, k].item() for i in range(0, 24)]\n",
    "    predicted_token = [\n",
    "        tokenizer.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1, 24)\n",
    "    ]\n",
    "    print(predicted_token)\n",
    "    words = [tokenizer.decode(t) for t in predicted_token]\n",
    "    print(\"\".join(predicted_token).replace(\"Ġ\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities = pd.DataFrame(\n",
    "    data=None,\n",
    "    index=minhash_amenable_fnames_with_path.values,\n",
    "    columns=minhash_amenable_fnames_with_path.values,\n",
    ")\n",
    "for (s1, s2) in tqdm.tqdm(\n",
    "    itertools.product(\n",
    "        minhash_amenable_fnames_with_path.values,\n",
    "        minhash_amenable_fnames_with_path.values,\n",
    "    ),\n",
    "    total=n ** 2,\n",
    "):\n",
    "    pairwise_similarities.loc[s1, s2] = shingles.text_similarity(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitext mining approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

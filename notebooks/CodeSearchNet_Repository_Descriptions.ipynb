{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "WARNING:root:tensorflow or tensorflow-hub not found, loading tfhub models won't work\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import mlutil.parallel\n",
    "from io import StringIO\n",
    "import sys\n",
    "import pypi_cli\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251820 ../data/python/train.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../data/python/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"repo\": \"smdabdoub/phylotoast\", \"path\": \"phylotoast/util.py\", \"func_name\": \"split_phylogeny\", \"original_string\": \"def split_phylogeny(p, level=\\\"s\\\"):\\n    \\\"\\\"\\\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \\\"\\\"\\\"\\n    level = level+\\\"__\\\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\\\";\\\")[0]\", \"language\": \"python\", \"code\": \"def split_phylogeny(p, level=\\\"s\\\"):\\n    \\\"\\\"\\\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \\\"\\\"\\\"\\n    level = level+\\\"__\\\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\\\";\\\")[0]\", \"code_tokens\": [\"def\", \"split_phylogeny\", \"(\", \"p\", \",\", \"level\", \"=\", \"\\\"s\\\"\", \")\", \":\", \"level\", \"=\", \"level\", \"+\", \"\\\"__\\\"\", \"result\", \"=\", \"p\", \".\", \"split\", \"(\", \"level\", \")\", \"return\", \"result\", \"[\", \"0\", \"]\", \"+\", \"level\", \"+\", \"result\", \"[\", \"1\", \"]\", \".\", \"split\", \"(\", \"\\\";\\\"\", \")\", \"[\", \"0\", \"]\"], \"docstring\": \"Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\", \"docstring_tokens\": [\"Return\", \"either\", \"the\", \"full\", \"or\", \"truncated\", \"version\", \"of\", \"a\", \"QIIME\", \"-\", \"formatted\", \"taxonomy\", \"string\", \".\"], \"sha\": \"0b74ef171e6a84761710548501dfac71285a58a3\", \"url\": \"https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L159-L177\", \"partition\": \"train\"}\r\n",
      "{\"repo\": \"smdabdoub/phylotoast\", \"path\": \"phylotoast/util.py\", \"func_name\": \"ensure_dir\", \"original_string\": \"def ensure_dir(d):\\n    \\\"\\\"\\\"\\n    Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn't exist\\n             already.\\n    \\\"\\\"\\\"\\n    if not os.path.exists(d):\\n        try:\\n            os.makedirs(d)\\n        except OSError as oe:\\n            # should not happen with os.makedirs\\n            # ENOENT: No such file or directory\\n            if os.errno == errno.ENOENT:\\n                msg = twdd(\\\"\\\"\\\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\\\"\\\"\\\")\\n                return msg.format(d)\\n            else:\\n                msg = twdd(\\\"\\\"\\\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\\\"\\\"\\\")\\n                return msg.format(d, oe.strerror)\", \"language\": \"python\", \"code\": \"def ensure_dir(d):\\n    \\\"\\\"\\\"\\n    Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn't exist\\n             already.\\n    \\\"\\\"\\\"\\n    if not os.path.exists(d):\\n        try:\\n            os.makedirs(d)\\n        except OSError as oe:\\n            # should not happen with os.makedirs\\n            # ENOENT: No such file or directory\\n            if os.errno == errno.ENOENT:\\n                msg = twdd(\\\"\\\"\\\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\\\"\\\"\\\")\\n                return msg.format(d)\\n            else:\\n                msg = twdd(\\\"\\\"\\\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\\\"\\\"\\\")\\n                return msg.format(d, oe.strerror)\", \"code_tokens\": [\"def\", \"ensure_dir\", \"(\", \"d\", \")\", \":\", \"if\", \"not\", \"os\", \".\", \"path\", \".\", \"exists\", \"(\", \"d\", \")\", \":\", \"try\", \":\", \"os\", \".\", \"makedirs\", \"(\", \"d\", \")\", \"except\", \"OSError\", \"as\", \"oe\", \":\", \"# should not happen with os.makedirs\", \"# ENOENT: No such file or directory\", \"if\", \"os\", \".\", \"errno\", \"==\", \"errno\", \".\", \"ENOENT\", \":\", \"msg\", \"=\", \"twdd\", \"(\", \"\\\"\\\"\\\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\\\"\\\"\\\"\", \")\", \"return\", \"msg\", \".\", \"format\", \"(\", \"d\", \")\", \"else\", \":\", \"msg\", \"=\", \"twdd\", \"(\", \"\\\"\\\"\\\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\\\"\\\"\\\"\", \")\", \"return\", \"msg\", \".\", \"format\", \"(\", \"d\", \",\", \"oe\", \".\", \"strerror\", \")\"], \"docstring\": \"Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn't exist\\n             already.\", \"docstring_tokens\": [\"Check\", \"to\", \"make\", \"sure\", \"the\", \"supplied\", \"directory\", \"path\", \"does\", \"not\", \"exist\", \"if\", \"so\", \"create\", \"it\", \".\", \"The\", \"method\", \"catches\", \"OSError\", \"exceptions\", \"and\", \"returns\", \"a\", \"descriptive\", \"message\", \"instead\", \"of\", \"re\", \"-\", \"raising\", \"the\", \"error\", \".\"], \"sha\": \"0b74ef171e6a84761710548501dfac71285a58a3\", \"url\": \"https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L180-L206\", \"partition\": \"train\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head  -2 ../data/python/train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codesearch_df = pd.concat([\n",
    "    pd.read_json('../data/python/train.jsonl', lines=True),\n",
    "    pd.read_json('../data/python/valid.jsonl', lines=True),\n",
    "    pd.read_json('../data/python/test.jsonl', lines=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['saltstack/salt', 'mitsei/dlkit', 'google/grr', 'bcbio/bcbio-nextgen',\n",
       "       'materialsproject/pymatgen', 'tensorflow/tensor2tensor',\n",
       "       'iotile/coretools', 'pandas-dev/pandas', 'cloud9ers/gurumate',\n",
       "       'spyder-ide/spyder', 'pypa/pipenv', 'apple/turicreate', 'gem/oq-engine',\n",
       "       'pantsbuild/pants', 'log2timeline/plaso',\n",
       "       'googleapis/google-cloud-python', 'inasafe/inasafe', 'gwastro/pycbc',\n",
       "       'apache/incubator-mxnet', 'senaite/senaite.core'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codesearch_df['repo'].value_counts().index[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github page project descriptions\n",
    "\n",
    "Most repositories have easily accesible descriptions on github.\n",
    "\n",
    "Github page HTML has description in 'title' tag.\n",
    "\n",
    "The problem with this approach is github's rate limit (we're not using API for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    return requests.get(url).text\n",
    "\n",
    "\n",
    "def get_short_description(repo):\n",
    "    url = 'http://www.github.com/{}'.format(repo)\n",
    "    html = get_html(url)\n",
    "    parsed_html = bs4.BeautifulSoup(html)\n",
    "    return parsed_html.find('title').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'allenai/allennlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.github.com/{}?'.format(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_short_description('allenai/allennlp') == 'GitHub - allenai/allennlp: An open-source NLP research library, built on PyTorch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.Series(all_codesearch_df['repo'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12361,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:50<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "descriptions = []\n",
    "for repo in tqdm.tqdm(repos[:50]):\n",
    "    descriptions.append(get_short_description(repo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mlutil/parallel.py\u001b[0m in \u001b[0;36mmapp\u001b[0;34m(f, iterable, executor_cls, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mexecutor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread_wakeup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "descriptions_p = list(mlutil.parallel.mapp(get_short_description, repos[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'descriptions_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6bec6df168ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescriptions_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rate limit · GitHub'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'descriptions_p' is not defined"
     ]
    }
   ],
   "source": [
    "descriptions_p.index('Rate limit · GitHub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPI project descriptions with pypi_cli\n",
    "\n",
    "Most of dataset repositories are registered in PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pypi_package_description(package_name, part=2):\n",
    "    temp_out = StringIO()\n",
    "    sys.stdout = temp_out\n",
    "    try:\n",
    "        pypi_cli.info([package_name])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    stdout = sys.stdout.getvalue().split('\\n')\n",
    "    if len(stdout) > part:\n",
    "        description = stdout[part]\n",
    "    else:\n",
    "        description = None\n",
    "    sys.stdout = sys.__stdout__\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tensors and Dynamic neural networks in Python with strong GPU acceleration'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pypi_package_description('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pypi_repo_description(repo):\n",
    "    print(repo.split('/'))\n",
    "    return get_pypi_package_description(repo.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An open-source NLP research library, built on PyTorch.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pypi_repo_description('allenai/allennlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pypi_descriptions = []\n",
    "for repo in tqdm.tqdm(repos[:100]):\n",
    "    pypi_descriptions.append(get_pypi_package_description(repo.split('/')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlutil.parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "t_start = time.time()\n",
    "pypi_descriptions_p = list(mlutil.parallel.mapp(get_pypi_repo_description, repos))\n",
    "t_end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long did it take to retrieve PyPI descriptions (minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.13"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((t_end - t_start) / 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_with_descriptions = [repo for (repo, n) in zip(repos, pypi_descriptions_p) if not n is None]\n",
    "descriptions = [desc for (repo, desc) in zip(repos, pypi_descriptions_p) if not (desc is None or desc == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repos_with_no_pypi_description = len(repos) - len(repos_with_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repositories without pypi description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14.04%'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(round(100 * n_repos_with_no_pypi_description / len(pypi_descriptions_p), 2)) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tools for phylogenetic data analysis including visualization and cluster-computing support.',\n",
       " None,\n",
       " 'Open-source algorithms for data-driven building analysis and control',\n",
       " 'Bootstrap Python package',\n",
       " 'Extremely fast and easy feature based HTML generator.',\n",
       " None,\n",
       " 'Connection utilities',\n",
       " 'Python library to work with Steam',\n",
       " 'Distributed Network Packet Analysis Pipeline for Layer 2, 3 and 4 Frames',\n",
       " 'Django Simple Multilingual Support for Models.']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pypi_descriptions_p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smdabdoub/phylotoast</td>\n",
       "      <td>phylotoast/util.py</td>\n",
       "      <td>split_phylogeny</td>\n",
       "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
       "      <td>[def, split_phylogeny, (, p, ,, level, =, \"s\",...</td>\n",
       "      <td>Return either the full or truncated version of...</td>\n",
       "      <td>[Return, either, the, full, or, truncated, ver...</td>\n",
       "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
       "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smdabdoub/phylotoast</td>\n",
       "      <td>phylotoast/util.py</td>\n",
       "      <td>ensure_dir</td>\n",
       "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
       "      <td>python</td>\n",
       "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
       "      <td>[def, ensure_dir, (, d, ), :, if, not, os, ., ...</td>\n",
       "      <td>Check to make sure the supplied directory path...</td>\n",
       "      <td>[Check, to, make, sure, the, supplied, directo...</td>\n",
       "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
       "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smdabdoub/phylotoast</td>\n",
       "      <td>phylotoast/util.py</td>\n",
       "      <td>file_handle</td>\n",
       "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
       "      <td>[def, file_handle, (, fnh, ,, mode, =, \"rU\", )...</td>\n",
       "      <td>Takes either a file path or an open file handl...</td>\n",
       "      <td>[Takes, either, a, file, path, or, an, open, f...</td>\n",
       "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
       "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smdabdoub/phylotoast</td>\n",
       "      <td>phylotoast/util.py</td>\n",
       "      <td>gather_categories</td>\n",
       "      <td>def gather_categories(imap, header, categories...</td>\n",
       "      <td>python</td>\n",
       "      <td>def gather_categories(imap, header, categories...</td>\n",
       "      <td>[def, gather_categories, (, imap, ,, header, ,...</td>\n",
       "      <td>Find the user specified categories in the map ...</td>\n",
       "      <td>[Find, the, user, specified, categories, in, t...</td>\n",
       "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
       "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smdabdoub/phylotoast</td>\n",
       "      <td>phylotoast/util.py</td>\n",
       "      <td>parse_unifrac</td>\n",
       "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
       "      <td>python</td>\n",
       "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
       "      <td>[def, parse_unifrac, (, unifracFN, ), :, with,...</td>\n",
       "      <td>Parses the unifrac results file into a diction...</td>\n",
       "      <td>[Parses, the, unifrac, results, file, into, a,...</td>\n",
       "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
       "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   repo                path          func_name  \\\n",
       "0  smdabdoub/phylotoast  phylotoast/util.py    split_phylogeny   \n",
       "1  smdabdoub/phylotoast  phylotoast/util.py         ensure_dir   \n",
       "2  smdabdoub/phylotoast  phylotoast/util.py        file_handle   \n",
       "3  smdabdoub/phylotoast  phylotoast/util.py  gather_categories   \n",
       "4  smdabdoub/phylotoast  phylotoast/util.py      parse_unifrac   \n",
       "\n",
       "                                     original_string language  \\\n",
       "0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...   python   \n",
       "1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...   python   \n",
       "2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...   python   \n",
       "3  def gather_categories(imap, header, categories...   python   \n",
       "4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...   python   \n",
       "\n",
       "                                                code  \\\n",
       "0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...   \n",
       "1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...   \n",
       "2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...   \n",
       "3  def gather_categories(imap, header, categories...   \n",
       "4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  [def, split_phylogeny, (, p, ,, level, =, \"s\",...   \n",
       "1  [def, ensure_dir, (, d, ), :, if, not, os, ., ...   \n",
       "2  [def, file_handle, (, fnh, ,, mode, =, \"rU\", )...   \n",
       "3  [def, gather_categories, (, imap, ,, header, ,...   \n",
       "4  [def, parse_unifrac, (, unifracFN, ), :, with,...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0  Return either the full or truncated version of...   \n",
       "1  Check to make sure the supplied directory path...   \n",
       "2  Takes either a file path or an open file handl...   \n",
       "3  Find the user specified categories in the map ...   \n",
       "4  Parses the unifrac results file into a diction...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0  [Return, either, the, full, or, truncated, ver...   \n",
       "1  [Check, to, make, sure, the, supplied, directo...   \n",
       "2  [Takes, either, a, file, path, or, an, open, f...   \n",
       "3  [Find, the, user, specified, categories, in, t...   \n",
       "4  [Parses, the, unifrac, results, file, into, a,...   \n",
       "\n",
       "                                        sha  \\\n",
       "0  0b74ef171e6a84761710548501dfac71285a58a3   \n",
       "1  0b74ef171e6a84761710548501dfac71285a58a3   \n",
       "2  0b74ef171e6a84761710548501dfac71285a58a3   \n",
       "3  0b74ef171e6a84761710548501dfac71285a58a3   \n",
       "4  0b74ef171e6a84761710548501dfac71285a58a3   \n",
       "\n",
       "                                                 url partition  \n",
       "0  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
       "1  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
       "2  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
       "3  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
       "4  https://github.com/smdabdoub/phylotoast/blob/0...     train  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_codesearch_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - retrieval by bag of words from descriptions\n",
    "\n",
    "Fit bag of words model to descriptions, then match 'descriptions' obtained from concatenating comments from all functions from repository\n",
    "\n",
    "The assumptions here are very optimistic, since we're only matching features from known repositories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10625"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repos_with_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "codesearch_df = all_codesearch_df[all_codesearch_df['repo'].isin(repos_with_descriptions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_codesearch_df = codesearch_df[codesearch_df['partition'] == 'train']\n",
    "val_codesearch_df = codesearch_df[codesearch_df['partition'] == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test', 'train', 'valid'}"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_codesearch_df['partition'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['repo', 'path', 'func_name', 'original_string', 'language', 'code',\n",
       "       'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url',\n",
       "       'partition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_codesearch_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_descriptions = codesearch_df.groupby('repo')['docstring'].agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_bow_vectors = tfidf_vectorizer.fit_transform(descriptions).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectors = tfidf_vectorizer.transform(repo_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_docstring_distances = metrics.pairwise.cosine_distances(description_bow_vectors, bow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "description_closest_docstring_indices = pd.DataFrame(np.argsort(description_docstring_distances, axis=1)[:,:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>path</th>\n",
       "      <th>func_name</th>\n",
       "      <th>original_string</th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>sha</th>\n",
       "      <th>url</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo, path, func_name, original_string, language, code, code_tokens, docstring, docstring_tokens, sha, url, partition]\n",
       "Index: []"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_codesearch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_closest_docstrings = description_closest_docstring_indices.apply(lambda s: codesearch_df['repo'].iloc[s].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12315,  5357,  5496,  4823, 11440,  8370, 13226,  6336, 12316,\n",
       "       13100,  5350,  4103,    34, 12463,  4410, 12314, 13227,  2647,\n",
       "        2646,  4959,  5358, 12997,  9230,  1915, 11506,  4264,  2969,\n",
       "        1121,  2011,  7498,  9228,  2661,  4822,  9519,  4429, 11504,\n",
       "        4746,  1764, 11505,  1126,  1125,  8736, 10521,  9999,  9229,\n",
       "         862,  2854,  1765, 11446,  3451])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_description_closest_docstring_indices.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10625, 11178)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall at 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at100= [\n",
    "    repo in potential_repos\n",
    "    for (repo, potential_repos) in zip(repos, description_closest_docstrings.values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006795566701723162"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_at50) / len(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pomysły\n",
    "\n",
    "- jako baseline wyszukiwanie zanurzeń komentarzy\n",
    "- wyszukiwanie po cechach korzystających z CodeBERTA\n",
    "- wytrenowanie modelu do rankingu (?) \n",
    "\n",
    "### Ogólne pomysły na podejścia\n",
    "\n",
    "Problem wydaje się dobrze opisywać podejście 'pairwise': mamy cechy jednego typu i drugiego typu, uczymy się dopasowywać jedne do drugich\n",
    "\n",
    "- agregowanie cech kodu z repozytoriów\n",
    "- agregowanie cech pochodzących od importów (Import2Vec) - **pokaż mi co importujesz, a powiem na jaki temat jest repo**\n",
    "- ogólniej cechy dla specjalnych tokenów: nazwy funkcji, nazwy argumentów\n",
    "- sprytniejsza agregacja - np hierarchiczna (np zrobić hiperboliczną wersję zanurzeń)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

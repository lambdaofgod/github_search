{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b176eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator \n",
    "\n",
    "import sentence_transformers\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260a2fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab34cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09c849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b99f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_t5_path = \"output/doc_id_generation_model/best_checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f016967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base-multi-sum\")#(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29db5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666c68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1412d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb4b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_path_summary(texts, n_beams=16, max_length=256, max_label_length=64, min_length=32):\n",
    "    inputs = tokenizer(texts, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "    summaries = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "                     attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "                     length_penalty=0.8, num_beams=n_beams, max_length=max_label_length, min_length=min_length)\n",
    "    return tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba8aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "import ast\n",
    "from github_search import seq2seq_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "960cf8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df = pd.read_csv(\"output/papers_with_readmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b316840",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d33c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tasks = paperswithcode_df['tasks'].apply(lambda ts: \", \".join(ast.literal_eval(ts)))\n",
    "repo_tasks = pd.DataFrame({\"repo\": paperswithcode_df['repo'], \"tasks\": repo_tasks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d6c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df = repo_tasks.merge(files_df, on='repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682b6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_dataset = datasets.load_from_disk(\"output/seq2seq_hf_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f56dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_area_ds= datasets.Dataset.from_pandas(pd.read_csv(\"data/paperswithcode_tasks.csv\").dropna()[['area', 'task_description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aafb7f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>tasks</th>\n",
       "      <th>similarity</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>torchdiffeq/_impl/odeint.py</td>\n",
       "      <td>from .tsit5 import Tsit5Solver\\nfrom .dopri5 import Dopri5Solver\\nfrom .fixed_grid import Euler, Midpoint, RK4\\nfrom .fixed_adams import AdamsBash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.402928</td>\n",
       "      <td>torchdiffeq/_impl/rk_common.py</td>\n",
       "      <td># Based on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/integrate\\nimport collections\\nfrom .misc import _scaled_dot_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.400493</td>\n",
       "      <td>torchdiffeq/_impl/interp.py</td>\n",
       "      <td>import torch\\nfrom .misc import _convert_to_tensor, _dot_product\\n\\n\\ndef _interp_fit(y0, y1, y_mid, f0, f1, dt):\\n    \"\"\"Fit coefficients for 4th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.343509</td>\n",
       "      <td>torchdiffeq/_impl/adjoint.py</td>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nfrom . import odeint\\nfrom .misc import _flatten, _flatten_convert_none_to_zeros\\n\\n\\nclass OdeintAdjointMeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.298963</td>\n",
       "      <td>torchdiffeq/_impl/misc.py</td>\n",
       "      <td>import warnings\\nimport torch\\n\\n\\ndef _flatten(sequence):\\n    flat = [p.contiguous().view(-1) for p in sequence]\\n    return torch.cat(flat) if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       repo  \\\n",
       "0  000Justin000/torchdiffeq   \n",
       "1  000Justin000/torchdiffeq   \n",
       "2  000Justin000/torchdiffeq   \n",
       "3  000Justin000/torchdiffeq   \n",
       "4  000Justin000/torchdiffeq   \n",
       "\n",
       "                                                                                               tasks  \\\n",
       "0  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "1  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "2  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "3  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "4  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "\n",
       "   similarity                            path  \\\n",
       "0    0.536278     torchdiffeq/_impl/odeint.py   \n",
       "1    0.402928  torchdiffeq/_impl/rk_common.py   \n",
       "2    0.400493     torchdiffeq/_impl/interp.py   \n",
       "3    0.343509    torchdiffeq/_impl/adjoint.py   \n",
       "4    0.298963       torchdiffeq/_impl/misc.py   \n",
       "\n",
       "                                                                                                                                                 content  \n",
       "0  from .tsit5 import Tsit5Solver\\nfrom .dopri5 import Dopri5Solver\\nfrom .fixed_grid import Euler, Midpoint, RK4\\nfrom .fixed_adams import AdamsBash...  \n",
       "1  # Based on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/integrate\\nimport collections\\nfrom .misc import _scaled_dot_pr...  \n",
       "2  import torch\\nfrom .misc import _convert_to_tensor, _dot_product\\n\\n\\ndef _interp_fit(y0, y1, y_mid, f0, f1, dt):\\n    \"\"\"Fit coefficients for 4th...  \n",
       "3  import torch\\nimport torch.nn as nn\\nfrom . import odeint\\nfrom .misc import _flatten, _flatten_convert_none_to_zeros\\n\\n\\nclass OdeintAdjointMeth...  \n",
       "4  import warnings\\nimport torch\\n\\n\\ndef _flatten(sequence):\\n    flat = [p.contiguous().view(-1) for p in sequence]\\n    return torch.cat(flat) if ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb34f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plbart_str = \"uclanlp/plbart-single_task-en_python\"#uclanlp/plbart-python-en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d42a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea3e5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = sentence_transformers.cross_encoder.CrossEncoder(\"microsoft/codebert-base\", max_length=512, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2cdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df = files_with_tasks_df.iloc[::25].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f125944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>repo</th>\n",
       "      <th>tasks</th>\n",
       "      <th>similarity</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000Justin000/torchdiffeq</td>\n",
       "      <td>latent variable models, multivariate time series forecasting, multivariate time series imputation</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>torchdiffeq/_impl/odeint.py</td>\n",
       "      <td>from .tsit5 import Tsit5Solver\\nfrom .dopri5 import Dopri5Solver\\nfrom .fixed_grid import Euler, Midpoint, RK4\\nfrom .fixed_adams import AdamsBash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>011235813/SEPT</td>\n",
       "      <td>variational inference</td>\n",
       "      <td>0.291831</td>\n",
       "      <td>alg/train_baseline.py</td>\n",
       "      <td>\"\"\"\\nBaseline 1: \\nTraining: train a single policy on all training instances, for many episodes per instance.\\nTest: execute the trained policy on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>011235813/hierarchical-marl</td>\n",
       "      <td>multi agent reinforcement learning, q learning</td>\n",
       "      <td>0.160598</td>\n",
       "      <td>test/test_env_selfplay.py</td>\n",
       "      <td>import sys\\nimport random\\nimport json\\n\\nsys.path.append('../env/')\\n\\nimport numpy as np\\nimport tensorflow as tf\\n\\nimport env_wrapper\\n\\nwith ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>0492wzl/tensorflow_slim_densenet</td>\n",
       "      <td>crowd counting, image classification, object recognition, person re identification</td>\n",
       "      <td>0.181351</td>\n",
       "      <td>datasets/download_and_convert_cifar10.py</td>\n",
       "      <td># Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>08173021/YOLOv4</td>\n",
       "      <td>data augmentation, object detection</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>demo_onnx.py</td>\n",
       "      <td>import sys\\nimport onnx\\nimport os\\nimport argparse\\nimport numpy as np\\nimport cv2\\nimport onnxruntime\\nfrom tool.utils import *\\n\\n\\ndef main(on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                              repo  \\\n",
       "0      0          000Justin000/torchdiffeq   \n",
       "1     25                    011235813/SEPT   \n",
       "2     50       011235813/hierarchical-marl   \n",
       "3     75  0492wzl/tensorflow_slim_densenet   \n",
       "4    100                   08173021/YOLOv4   \n",
       "\n",
       "                                                                                               tasks  \\\n",
       "0  latent variable models, multivariate time series forecasting, multivariate time series imputation   \n",
       "1                                                                              variational inference   \n",
       "2                                                     multi agent reinforcement learning, q learning   \n",
       "3                 crowd counting, image classification, object recognition, person re identification   \n",
       "4                                                                data augmentation, object detection   \n",
       "\n",
       "   similarity                                      path  \\\n",
       "0    0.536278               torchdiffeq/_impl/odeint.py   \n",
       "1    0.291831                     alg/train_baseline.py   \n",
       "2    0.160598                 test/test_env_selfplay.py   \n",
       "3    0.181351  datasets/download_and_convert_cifar10.py   \n",
       "4    0.100642                              demo_onnx.py   \n",
       "\n",
       "                                                                                                                                                 content  \n",
       "0  from .tsit5 import Tsit5Solver\\nfrom .dopri5 import Dopri5Solver\\nfrom .fixed_grid import Euler, Midpoint, RK4\\nfrom .fixed_adams import AdamsBash...  \n",
       "1  \"\"\"\\nBaseline 1: \\nTraining: train a single policy on all training instances, for many episodes per instance.\\nTest: execute the trained policy on...  \n",
       "2  import sys\\nimport random\\nimport json\\n\\nsys.path.append('../env/')\\n\\nimport numpy as np\\nimport tensorflow as tf\\n\\nimport env_wrapper\\n\\nwith ...  \n",
       "3  # Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not...  \n",
       "4  import sys\\nimport onnx\\nimport os\\nimport argparse\\nimport numpy as np\\nimport cv2\\nimport onnxruntime\\nfrom tool.utils import *\\n\\n\\ndef main(on...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2857304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object detection            800\n",
       "semantic segmentation       761\n",
       "image classification        612\n",
       "question answering          357\n",
       "language modelling          351\n",
       "                           ... \n",
       "person search                 1\n",
       "mathematical proofs           1\n",
       "brain image segmentation      1\n",
       "genre classification          1\n",
       "physical simulations          1\n",
       "Name: tasks, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_files_with_tasks_df['tasks'].str.split(\", \").explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae5bdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000Justin000/torchdiffeq                              1\n",
       "maxdel/rmt                                            1\n",
       "mboudiaf/RePRI-for-Few-Shot-Segmentation              1\n",
       "mbsariyildiz/gmn-zsl                                  1\n",
       "mcahny/Deep-Video-Inpainting                          1\n",
       "                                                     ..\n",
       "aubreychen9012/cAAE                                   1\n",
       "audiofhrozen/motion_dance                             1\n",
       "augu0093/Voice-Conversion-Project                     1\n",
       "aurelien-peden/Deep-Learning-paper-implementations    1\n",
       "zzzace2000/mimic-preprocess                           1\n",
       "Name: repo, Length: 9308, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_files_with_tasks_df['repo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a5cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPairDataPreprocessor:\n",
    "    \n",
    "    def __init__(self, first_text_columns, second_text_columns, add_first_text_columns_prompts=True, add_second_text_columns_prompts=False):\n",
    "        self.first_text_columns = first_text_columns\n",
    "        self.second_text_columns = second_text_columns \n",
    "        self.first_interpolation_str = self.get_interpolation_str(first_text_columns, add_first_text_columns_prompts)\n",
    "        self.second_interpolation_str = self.get_interpolation_str(second_text_columns, add_second_text_columns_prompts)\n",
    "    \n",
    "    def get_interpolation_str(self, columns, add_prompt):\n",
    "        if add_prompt:\n",
    "            return \"\\n\".join(f\"# {col}: \" + \"{}\" for col in columns)\n",
    "        else:\n",
    "            return \"\\n\".join([\"{}\" for __ in range(len(columns))])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        first_pretty_interpolation_str = \"\\t\" + self.first_interpolation_str.replace(\"\\n\", \"\\n\\t\")\n",
    "        second_pretty_interpolation_str = \"\\t\" + self.second_interpolation_str.replace(\"\\n\", \"\\n\\t\")\n",
    "        return (f\"{self.__class__.__name__}\\n\" +\n",
    "            f\"first text columns: {self.first_text_columns}\\n\" +  \n",
    "            f\"second text columns: {self.second_text_columns}\\n\" +  \n",
    "            f\"pattern:\\n {first_pretty_interpolation_str}\\n{second_pretty_interpolation_str}\"\n",
    "        )\n",
    "       \n",
    "    def prepare_input_examples(self, df, label):\n",
    "        return [\n",
    "            sentence_transformers.InputExample(\n",
    "                texts=[self.first_interpolation_str.format(*values),  self.second_interpolation_str.format(*second_values)],\n",
    "                label=label\n",
    "            )\n",
    "            for values, second_values in zip(zip(*[df[col] for col in self.first_text_columns]), zip(* [df[col] for col in self.second_text_columns]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d947d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_content_preprocessor = TextPairDataPreprocessor(first_text_columns=[\"repo\", \"path\", \"tasks\"], second_text_columns=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "003c6270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextPairDataPreprocessor\n",
       "first text columns: ['repo', 'path', 'tasks']\n",
       "second text columns: ['content']\n",
       "pattern:\n",
       " \t# repo: {}\n",
       "\t# path: {}\n",
       "\t# tasks: {}\n",
       "\t{}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path_task_content_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a337f378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# repo: 000Justin000/torchdiffeq\\n# path: torchdiffeq/_impl/odeint.py\\n# tasks: latent variable models, multivariate time series forecasting, multivariate time series imputation',\n",
       " 'from .tsit5 import Tsit5Solver\\nfrom .dopri5 import Dopri5Solver\\nfrom .fixed_grid import Euler, Midpoint, RK4\\nfrom .fixed_adams import AdamsBashforth, AdamsBashforthMoulton\\nfrom .adams import VariableCoefficientAdamsBashforth\\nfrom .misc import _check_inputs\\n\\nSOLVERS = {\\n    \\'explicit_adams\\': AdamsBashforth,\\n    \\'fixed_adams\\': AdamsBashforthMoulton,\\n    \\'adams\\': VariableCoefficientAdamsBashforth,\\n    \\'tsit5\\': Tsit5Solver,\\n    \\'dopri5\\': Dopri5Solver,\\n    \\'euler\\': Euler,\\n    \\'midpoint\\': Midpoint,\\n    \\'rk4\\': RK4,\\n}\\n\\n\\ndef odeint(func, y0, t, rtol=1e-7, atol=1e-9, method=None, options=None):\\n    \"\"\"Integrate a system of ordinary differential equations.\\n\\n    Solves the initial value problem for a non-stiff system of first order ODEs:\\n        ```\\n        dy/dt = func(t, y), y(t[0]) = y0\\n        ```\\n    where y is a Tensor of any shape.\\n\\n    Output dtypes and numerical precision are based on the dtypes of the inputs `y0`.\\n\\n    Args:\\n        func: Function that maps a Tensor holding the state `y` and a scalar Tensor\\n            `t` into a Tensor of state derivatives with respect to time.\\n        y0: N-D Tensor giving starting value of `y` at time point `t[0]`. May\\n            have any floating point or complex dtype.\\n        t: 1-D Tensor holding a sequence of time points for which to solve for\\n            `y`. The initial time point should be the first element of this sequence,\\n            and each time must be larger than the previous time. May have any floating\\n            point dtype. Converted to a Tensor with float64 dtype.\\n        rtol: optional float64 Tensor specifying an upper bound on relative error,\\n            per element of `y`.\\n        atol: optional float64 Tensor specifying an upper bound on absolute error,\\n            per element of `y`.\\n        method: optional string indicating the integration method to use.\\n        options: optional dict of configuring options for the indicated integration\\n            method. Can only be provided if a `method` is explicitly set.\\n        name: Optional name for this operation.\\n\\n    Returns:\\n        y: Tensor, where the first dimension corresponds to different\\n            time points. Contains the solved value of y for each desired time point in\\n            `t`, with the initial value `y0` being the first element along the first\\n            dimension.\\n\\n    Raises:\\n        ValueError: if an invalid `method` is provided.\\n        TypeError: if `options` is supplied without `method`, or if `t` or `y0` has\\n            an invalid dtype.\\n    \"\"\"\\n\\n    tensor_input, func, y0, t = _check_inputs(func, y0, t)\\n\\n    if options is None:\\n        options = {}\\n    elif method is None:\\n        raise ValueError(\\'cannot supply `options` without specifying `method`\\')\\n\\n    if method is None:\\n        method = \\'dopri5\\'\\n\\n    solver = SOLVERS[method](func, y0, rtol=rtol, atol=atol, **options)\\n    solution = solver.integrate(t)\\n\\n    if tensor_input:\\n        solution = solution[0]\\n    return solution']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_tasks_df, 1)[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "635fa76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_input_examples = repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_tasks_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb72f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_permuted_tasks_df = sample_files_with_tasks_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b93f90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_permuted_tasks_df['tasks'] = sample_files_with_tasks_df['tasks'].sample(len(sample_files_with_tasks_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679d0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_input_examples = repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_permuted_tasks_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc8443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_examples = positive_input_examples + negative_input_examples\n",
    "input_example_labels = np.ones(len(input_examples))\n",
    "input_example_labels[len(positive_input_examples):] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd88d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_examples, test_input_examples = model_selection.train_test_split(input_examples, stratify=input_example_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4d23728",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95b82c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = pd.Series([\" \".join(ie.texts) for ie in input_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc008bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = files_with_tasks_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9e75c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_input_examples, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "548b15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "177c3a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5177917120859395"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83222ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42a8a551074cd9b3b525a7d9a516dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a137fd69024219a43a239d14ce4d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 score: 0.487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96864411424c491c92f5f7a0a80a98b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 score: 0.764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0313b07d484a8b902bb46c2feb6f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 score: 0.821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8113563a9c0a45839396bfa73d05c03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 score: 0.829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ef3b1ce42b4f58b9e19564fda540a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 score: 0.82\n"
     ]
    }
   ],
   "source": [
    "cross_encoder.fit(train_dataloader,\n",
    "  epochs=5,\n",
    "  evaluator=evaluator,\n",
    "  use_amp=True,\n",
    "  callback=lambda score, epoch, steps: print(\"epoch {} score: {}\".format(epoch, round(score, 3)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be921ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.save(\"output/sbert/cross_encoder_repo_path_task_10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "010d3da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder at 0x7fc86d5d6c10>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05761104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9308"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1a9c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_predicted_scores = cross_encoder.predict([ex.texts for ex in positive_input_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d02f3024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9308.000000\n",
       "mean        0.765778\n",
       "std         0.271908\n",
       "min         0.056812\n",
       "25%         0.558808\n",
       "50%         0.923891\n",
       "75%         0.973130\n",
       "max         0.981595\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(positive_predicted_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a73486a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(positive_predicted_scores).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de6e5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9308.000000\n",
       "mean        0.236007\n",
       "std         0.202585\n",
       "min         0.050964\n",
       "25%         0.090538\n",
       "50%         0.168475\n",
       "75%         0.304945\n",
       "max         0.981317\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_predicted_scores = cross_encoder.predict([ex.texts for ex in negative_input_examples])\n",
    "pd.Series(negative_predicted_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5682890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(negative_predicted_scores).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.predict([\n",
    "    [\"# tasks: neural networks \\n\", positive_input_pairs[0][1]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_without_content =[[f\"#repo: {repo}\\n #path: {path}\", f\"# tasks: {tasks}\"]\n",
    "    for (repo, tasks, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['tasks'],\n",
    "        sample_files_with_tasks_df['path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d91dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_without_content[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_scores = cross_encoder.predict([\n",
    "    [f\"#repo: {repo}\\n #path: {path}\", f\"#tasks: {tasks}\"]\n",
    "    for (repo, tasks, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['tasks'],\n",
    "        sample_files_with_tasks_df['path'])]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_content_scores = cross_encoder.predict([\n",
    "    [f\"#repo: {repo}\\n #path: {path}\", f\"#content: {content}\"]\n",
    "    for (repo, content, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['content'],\n",
    "        sample_files_with_tasks_df['path'])]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "1f49bfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4654.000000\n",
       "mean        0.537175\n",
       "std         0.137356\n",
       "min         0.259086\n",
       "25%         0.426667\n",
       "50%         0.517186\n",
       "75%         0.635878\n",
       "max         0.931464\n",
       "dtype: float64"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(repo_path_content_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3251c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4654.000000\n",
       "mean        0.473706\n",
       "std         0.309223\n",
       "min         0.147257\n",
       "25%         0.222930\n",
       "50%         0.270712\n",
       "75%         0.900784\n",
       "max         0.937792\n",
       "dtype: float64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(repo_path_task_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b24572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92510b46",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_files_with_tasks_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:6307\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[1;32m   6304\u001b[0m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[1;32m   6306\u001b[0m     by \u001b[38;5;241m=\u001b[39m by[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 6307\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6309\u001b[0m     \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[1;32m   6310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6311\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   6312\u001b[0m         \u001b[38;5;66;03m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:1848\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "sample_files_with_tasks_df.sort_values(\"score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e92e322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4654.000000\n",
       "mean        0.070864\n",
       "std         0.185280\n",
       "min         0.011906\n",
       "25%         0.018969\n",
       "50%         0.022650\n",
       "75%         0.030266\n",
       "max         0.997173\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8929556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__, tokenizer = seq2seq_utils.get_seq2seq_model_with_tokenizer(\"Salesforce/codet5-base-multi-sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52417cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.RobertaTokenizerFast.from_pretrained(\"Salesforce/codet5-base-multi-sum\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = tokenizer(\"<PATH_TASK_SEP>\", add_special_tokens=False)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "inputs = tokenizer(example_contents, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "             attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "             length_penalty=0.8, num_beams=10, max_length=64, min_length=8)#, decoder_start_token_id=decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ae3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[7].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68052fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_paths = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in output_paths], columns=[\"path\", \"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in example_doc_idxs], columns=[\"path\", \"tasks\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = [content + \" predict docID: \" for (content, doc_id) in zip(example_contents, example_doc_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09881498",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "sru_contents = '''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "import sru\n",
    "\n",
    "\n",
    "rnn_class_type_mapping = {\"lstm\": nn.LSTM, \"sru\": sru.SRU}\n",
    "\n",
    "\n",
    "class SentenceRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    sentence_transformers RNN wrapper\n",
    "    \"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction_contents = '''\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "class IncrementalHyperbolicMDS:\n",
    "    def __init__(self, n_components, dtype=\"float16\"):\n",
    "        self.ipca = decomposition.IncrementalPCA(n_components=n_components)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def partial_fit(self, D):\n",
    "        Y = -np.cosh(D)\n",
    "        self.ipca.partial_fit(Y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5624232",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_contents = '''\n",
    "#export \n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import compose, feature_extraction, metrics\n",
    "from functools import reduce, partial\n",
    "import attr\n",
    "from typing import Union\n",
    "import umap\n",
    "import altair\n",
    "\n",
    "\n",
    "from game_recommender import steam_data\n",
    "'''# + \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a31db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mining_content = '''\n",
    "def get_wordnet_similarity(\n",
    "    word, another_word, similarity_method=\"resnik\", pos=None, ic=None\n",
    "):\n",
    "    if ic is None:\n",
    "        ic = wordnet_ic.ic(\"ic-semcor.dat\")\n",
    "    assert similarity_method in [\n",
    "        \"lin\",\n",
    "        \"jcn\",\n",
    "        \"resnik\",\n",
    "    ], \"Unsupported similarity method: \" + str(similarity_method)\n",
    "    word_synset = wn.synsets(word, pos)[0]\n",
    "    another_word_synset = wn.synsets(another_word, pos)[0]\n",
    "    if similarity_method == \"lin\":\n",
    "        return word_synset.lin_similarity(another_word_synset, ic)\n",
    "    elif similarity_method == \"jcn\":\n",
    "        return word_synset.jcn_similarity(another_word_synset, ic)\n",
    "    else:\n",
    "        return word_synset.res_similarity(another_word_synset, ic)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_content = '''\n",
    "import numpy as np\n",
    "import attr\n",
    "from toolz import partial\n",
    "from scarce_learn.zero_shot import zsl_base\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from scarce_learn.zero_shot import torch_util\n",
    "\n",
    "\n",
    "class DEVISELayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_class_features, margin, init_weights_std=0.1):\n",
    "        super(DEVISELayer, self).__init__()\n",
    "        init_weights = init_weights_std * torch.randn(n_features, n_class_features) \n",
    "        self.weights = nn.Parameter(data=init_weights.cuda())\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, X, y, label_embeddings):\n",
    "        loss = torch.Tensor([0]).cuda()\n",
    "        for i in range(X.shape[0]):\n",
    "            loss += self._devise_loss(X[i], y[i], label_embeddings)\n",
    "        return loss\n",
    "\n",
    "    def _devise_loss(self, embedding, label, label_embeddings):\n",
    "        indicator = torch.ones(label_embeddings.shape[0], dtype=bool)\n",
    "        indicator[label] = 0\n",
    "        per_class_loss = torch_util.similarity_based_hinge_loss(self.weights, embedding, label, label_embeddings)\n",
    "        return nn.ReLU()(self.margin + per_class_loss).sum()\n",
    "\n",
    "    def predict(self, X, label_embeddings):\n",
    "        class_similarities = torch_util.bilinear_feature_similarity(self.weights, X, label_embeddings)\n",
    "        return torch.argmax(class_similarities, axis=1)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf856b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_content = '''\n",
    "import attr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "try:\n",
    "    import numba\n",
    "except ImportError as e:\n",
    "    logging.warning(\n",
    "        \"numba not found, you'll not be able to use mlutil.evolutionary_algorithms.multiobjective\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bounded_gaussian_noise_mutation(x, n_mutants, lo=0, hi=1, sigma=1e-2):\n",
    "    noise = sigma * np.random.randn(n_mutants, x.shape[-1])\n",
    "    return np.clip(x + noise, lo, hi)\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class NSGAII:\n",
    "\n",
    "    optimized_function = attr.ib()\n",
    "    chromosome_size: int = attr.ib()\n",
    "    mutation_function = attr.ib(default=bounded_gaussian_noise_mutation)\n",
    "    random_initializer = attr.ib(default=np.random.rand)\n",
    "    population_bounds = attr.ib(default=(0, 1))\n",
    "    objective_names = attr.ib(default=(\"1st objective\", \"2nd objective\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content = '''\n",
    "import numpy as np\n",
    "import ot\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def get_stem_vectors(filtered_stems, keyed_vectors):\n",
    "    return np.vstack(\n",
    "        [\n",
    "            np.mean([keyed_vectors[w] for w in stem_list], axis=0)\n",
    "            for stem_list in filtered_stems\n",
    "            if len(stem_list) > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_word_vector_optimal_transport(\n",
    "    word_vectors1, word_vectors2, ot_method=ot.sinkhorn, reg=0.01, normalize_dists=True\n",
    "):\n",
    "    cost = cosine_distances(word_vectors1, word_vectors2)\n",
    "    height, width = cost.shape\n",
    "    a = np.ones(height)\n",
    "    b = np.ones(width)\n",
    "    if normalize_dists:\n",
    "        a = a / a.sum()\n",
    "        b = b / b.sum()\n",
    "    ot_matrix = ot_method(a, b, cost, reg=reg)\n",
    "    return ot_matrix, (ot_matrix * cost).sum()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0054f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_content = \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bba08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(path_content, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=5, max_length=128, min_length=32)[0].tolist(), top_k=0, top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"_nbdev.py\", \"haystack_search.py\", \"rss_feeds.py\", \"zero_shot_learning.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3443af",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_series = \"lambdaofgod <REPO_NAME_SEP> pytorch_hackathon <REPO_PATH_SEP> pytorch_hackathon <PATH_TASK_SEP> \" + pd.Series(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(paths_series[3], return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=10, max_length=128, min_length=16)[0].tolist(), #, top_p=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adffae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = seq2seq_dataset[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30600be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89464bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(examples, tokenize=False):\n",
    "    if tokenize:\n",
    "        inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n",
    "                       max_length=128, return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = {\"input_ids\": torch.tensor(examples[\"input_ids\"]).to(model.device), \"attention_mask\": torch.tensor(examples[\"attention_mask\"]).to(model.device)}\n",
    "    with torch.no_grad():\n",
    "        model_output = model.encoder(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e383480",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset = seq2seq_dataset.train_test_split(test_size=10000)['test'].map(embed_text,\n",
    "    batched=True,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset[0].keys()#['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d020ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"metric learning\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64132b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"implements zero-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encoder(**tokenizer(query_text, return_tensors=\"pt\").to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = query_emb.last_hidden_state.mean(axis=1)[0].to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de69b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_emb = np.array(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484a003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.iloc[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample = files_with_tasks_df.iloc[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79993171",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe968361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "[embs_dataset[\"contents\"][i] for i in list(embs_dataset.search(\"embedding\", query=query_emb.astype(\"float32\")).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552207c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(**(tokenizer(example_imports, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=20, max_length=128, min_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00747d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = model.encoder(**inputs.to(model.device)) #decoder_input_ids=torch.tensor([[decoder_start_token_id]]).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = files_with_tasks_df['tasks'].str.split(',').explode().str.strip().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8322a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bdf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode((-lm_output.logits).argsort()[:,:,:5][0,0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc0eb8",
   "metadata": {},
   "source": [
    "# TODO wyawianie istotnych informacji z konfigw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_doc_ids = get_predicted_path_summary(example_contents, max_length=64, min_length=16, max_label_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tasks = [d for d in generated_doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeda099",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tasks = [d for d in example_doc_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in zip(predicted_tasks, true_tasks, example_contents):\n",
    "    print(\"#\")\n",
    "    print(p[2])\n",
    "    print(p[0])\n",
    "    print(p[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5a9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp python_call_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa24ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import csrgraph as cg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from github_search import python_tokens, paperswithcode_tasks\n",
    "from mlutil.feature_extraction import embeddings\n",
    "from mlutil import prototype_selection\n",
    "import mlutil\n",
    "from mlutil.feature_extraction import embeddings \n",
    "\n",
    "import ast\n",
    "import astunparse\n",
    "import csrgraph\n",
    "import nodevectors\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import igraph\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "\n",
    "from operator import itemgetter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f5db22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3842518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df = pd.read_csv('data/python_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e21480a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "      <th>sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport subprocess\\nimp...</td>\n",
       "      <td>5364c4a113de40cb17e40b052fa012ad7daba2bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n# Author: Rico Sennric...</td>\n",
       "      <td>261dd0ffc2e10ee32ea99ca80c99cde50a6e7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/concat-bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\npar...</td>\n",
       "      <td>e6bc04ce4b16c0b91d00fc576ed5d261c711760f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/copy-model.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport argparse\\nimpor...</td>\n",
       "      <td>56014b1516fd6df606d500b5efc190efbe8d5203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/coverage.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nfro...</td>\n",
       "      <td>f008bc3a194503c2e0c68ea2d5389db81905f6bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560178</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/convert_from_depre.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>4ed7125b8568a0f9c60a3bf4670747ec4c497942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560179</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/demo.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# -------------------...</td>\n",
       "      <td>2bd89335df588e010bbb22370274dcbd04bcb407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560180</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/reval.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# -------------------...</td>\n",
       "      <td>612ae317fa5362b2b03b667cc2a8e17323d1a16f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560181</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/test_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>c9cfc53f46f7f9d6c0885ffcad8727f86716ce22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560182</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560183 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                owner repo_name                    file_path  \\\n",
       "0             trangvu   ape-npi                 run-tests.py   \n",
       "1             trangvu   ape-npi         scripts/apply_bpe.py   \n",
       "2             trangvu   ape-npi        scripts/concat-bpe.py   \n",
       "3             trangvu   ape-npi        scripts/copy-model.py   \n",
       "4             trangvu   ape-npi          scripts/coverage.py   \n",
       "...               ...       ...                          ...   \n",
       "560178  KyeongmoonKim        sb  tools/convert_from_depre.py   \n",
       "560179  KyeongmoonKim        sb                tools/demo.py   \n",
       "560180  KyeongmoonKim        sb               tools/reval.py   \n",
       "560181  KyeongmoonKim        sb            tools/test_net.py   \n",
       "560182  KyeongmoonKim        sb        tools/trainval_net.py   \n",
       "\n",
       "                                                  content  \\\n",
       "0       #!/usr/bin/env python3\\nimport subprocess\\nimp...   \n",
       "1       #!/usr/bin/env python3\\n# Author: Rico Sennric...   \n",
       "2       #!/usr/bin/env python3\\n\\nimport argparse\\npar...   \n",
       "3       #!/usr/bin/env python3\\nimport argparse\\nimpor...   \n",
       "4       #!/usr/bin/env python3\\n\\nimport argparse\\nfro...   \n",
       "...                                                   ...   \n",
       "560178  # --------------------------------------------...   \n",
       "560179  #!/usr/bin/env python\\n\\n# -------------------...   \n",
       "560180  #!/usr/bin/env python\\n\\n# -------------------...   \n",
       "560181  # --------------------------------------------...   \n",
       "560182  # --------------------------------------------...   \n",
       "\n",
       "                                             sha  \n",
       "0       5364c4a113de40cb17e40b052fa012ad7daba2bd  \n",
       "1       261dd0ffc2e10ee32ea99ca80c99cde50a6e7636  \n",
       "2       e6bc04ce4b16c0b91d00fc576ed5d261c711760f  \n",
       "3       56014b1516fd6df606d500b5efc190efbe8d5203  \n",
       "4       f008bc3a194503c2e0c68ea2d5389db81905f6bd  \n",
       "...                                          ...  \n",
       "560178  4ed7125b8568a0f9c60a3bf4670747ec4c497942  \n",
       "560179  2bd89335df588e010bbb22370274dcbd04bcb407  \n",
       "560180  612ae317fa5362b2b03b667cc2a8e17323d1a16f  \n",
       "560181  c9cfc53f46f7f9d6c0885ffcad8727f86716ce22  \n",
       "560182  e7b4dab93b6e543402f25f53696e918f3e3d0b5a  \n",
       "\n",
       "[560183 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb990fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df, all_papers_df = paperswithcode_tasks.get_paperswithcode_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e621479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search/github_search/paperswithcode_tasks.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_with_repo_with_biggest_tasks_df['most_common_task'] = papers_with_repo_with_biggest_tasks_df['tasks'].apply(\n"
     ]
    }
   ],
   "source": [
    "papers_with_repo_df = paperswithcode_tasks.get_papers_with_biggest_tasks_df(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4324ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_with_repo_df['repo_name'] = papers_with_repo_df['repo'].str.split(\"/\").apply(lambda ps: ps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87729e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_url</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url_abs</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>mentioned_in_github</th>\n",
       "      <th>mentioned_in_paper</th>\n",
       "      <th>paper_arxiv_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url_abs</th>\n",
       "      <th>paper_url_pdf</th>\n",
       "      <th>repo</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>most_common_task</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://paperswithcode.com/paper/procedural-3d...</td>\n",
       "      <td>2010.06411</td>\n",
       "      <td>Procedural 3D Terrain Generation using Generat...</td>\n",
       "      <td>Procedural 3D Terrain generation has become a ...</td>\n",
       "      <td>https://arxiv.org/abs/2010.06411v1</td>\n",
       "      <td>https://arxiv.org/pdf/2010.06411v1.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Emmanouil Panagiotou, Eleni Charou]</td>\n",
       "      <td>[Colorization, Image-to-Image Translation]</td>\n",
       "      <td>2020-10-13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2010.06411</td>\n",
       "      <td>Procedural 3D Terrain Generation using Generat...</td>\n",
       "      <td>https://arxiv.org/abs/2010.06411v1</td>\n",
       "      <td>https://arxiv.org/pdf/2010.06411v1.pdf</td>\n",
       "      <td>Panagiotou/Procedural3DTerrain</td>\n",
       "      <td>https://github.com/Panagiotou/Procedural3DTerrain</td>\n",
       "      <td>Image-to-Image Translation</td>\n",
       "      <td>Procedural3DTerrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://paperswithcode.com/paper/multiscale-sp...</td>\n",
       "      <td>2003.11265</td>\n",
       "      <td>Multiscale Sparsifying Transform Learning for ...</td>\n",
       "      <td>The data-driven sparse methods such as synthes...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11265v4</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11265v4.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ashkan Abbasi, Amirhassan Monadjemi, Leyuan F...</td>\n",
       "      <td>[Denoising, Dictionary Learning, Image Denoising]</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2003.11265</td>\n",
       "      <td>Multiscale Sparsifying Transform Learning for ...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11265v4</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11265v4.pdf</td>\n",
       "      <td>ashkan-abbasi66/MTLD</td>\n",
       "      <td>https://github.com/ashkan-abbasi66/MTLD</td>\n",
       "      <td>Denoising</td>\n",
       "      <td>MTLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://paperswithcode.com/paper/word2vec-opti...</td>\n",
       "      <td>2003.11645</td>\n",
       "      <td>Word2Vec: Optimal Hyper-Parameters and Their I...</td>\n",
       "      <td>Word2Vec is a prominent model for natural lang...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11645v2</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11645v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Tosin P. Adewumi, Foteini Liwicki, Marcus Liw...</td>\n",
       "      <td>[Named Entity Recognition, Sentiment Analysis]</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2003.11645</td>\n",
       "      <td>Word2Vec: Optimal Hyper-Parameters and Their I...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11645v2</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11645v2.pdf</td>\n",
       "      <td>phnk/D7047E</td>\n",
       "      <td>https://github.com/phnk/D7047E</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>D7047E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://paperswithcode.com/paper/word2vec-opti...</td>\n",
       "      <td>2003.11645</td>\n",
       "      <td>Word2Vec: Optimal Hyper-Parameters and Their I...</td>\n",
       "      <td>Word2Vec is a prominent model for natural lang...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11645v2</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11645v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Tosin P. Adewumi, Foteini Liwicki, Marcus Liw...</td>\n",
       "      <td>[Named Entity Recognition, Sentiment Analysis]</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2003.11645</td>\n",
       "      <td>Word2Vec: Optimal Hyper-Parameters and Their I...</td>\n",
       "      <td>https://arxiv.org/abs/2003.11645v2</td>\n",
       "      <td>https://arxiv.org/pdf/2003.11645v2.pdf</td>\n",
       "      <td>tosingithub/sdesk</td>\n",
       "      <td>https://github.com/tosingithub/sdesk</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>sdesk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://paperswithcode.com/paper/evaluating-se...</td>\n",
       "      <td>1903.07377</td>\n",
       "      <td>Evaluating Sequence-to-Sequence Models for Han...</td>\n",
       "      <td>Encoder-decoder models have become an effectiv...</td>\n",
       "      <td>https://arxiv.org/abs/1903.07377v2</td>\n",
       "      <td>https://arxiv.org/pdf/1903.07377v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Johannes Michael, Roger Labahn, Tobias Grünin...</td>\n",
       "      <td>[Image Captioning, Keyword Spotting, Language ...</td>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.07377</td>\n",
       "      <td>Evaluating Sequence-to-Sequence Models for Han...</td>\n",
       "      <td>https://arxiv.org/abs/1903.07377v2</td>\n",
       "      <td>https://arxiv.org/pdf/1903.07377v2.pdf</td>\n",
       "      <td>jamestjw/OCR</td>\n",
       "      <td>https://github.com/jamestjw/OCR</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>OCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74224</th>\n",
       "      <td>https://paperswithcode.com/paper/arbitrary-sty...</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>Gatys et al. recently introduced a neural algo...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Xun Huang, Serge Belongie]</td>\n",
       "      <td>[Style Transfer]</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>bethgelab/stylize-datasets</td>\n",
       "      <td>https://github.com/bethgelab/stylize-datasets</td>\n",
       "      <td>Style Transfer</td>\n",
       "      <td>stylize-datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74225</th>\n",
       "      <td>https://paperswithcode.com/paper/arbitrary-sty...</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>Gatys et al. recently introduced a neural algo...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Xun Huang, Serge Belongie]</td>\n",
       "      <td>[Style Transfer]</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>cryu854/ArbitraryStyle-tfjs</td>\n",
       "      <td>https://github.com/cryu854/ArbitraryStyle-tfjs</td>\n",
       "      <td>Style Transfer</td>\n",
       "      <td>ArbitraryStyle-tfjs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74226</th>\n",
       "      <td>https://paperswithcode.com/paper/arbitrary-sty...</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>Gatys et al. recently introduced a neural algo...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>ICCV 2017 10</td>\n",
       "      <td>[Xun Huang, Serge Belongie]</td>\n",
       "      <td>[Style Transfer]</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1703.06868</td>\n",
       "      <td>Arbitrary Style Transfer in Real-time with Ada...</td>\n",
       "      <td>http://arxiv.org/abs/1703.06868v2</td>\n",
       "      <td>http://arxiv.org/pdf/1703.06868v2.pdf</td>\n",
       "      <td>srihari-humbarwadi/adain-tensorflow2.x</td>\n",
       "      <td>https://github.com/srihari-humbarwadi/adain-te...</td>\n",
       "      <td>Style Transfer</td>\n",
       "      <td>adain-tensorflow2.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74227</th>\n",
       "      <td>https://paperswithcode.com/paper/variational-q...</td>\n",
       "      <td>2006.02336</td>\n",
       "      <td>Variational Quantum Singular Value Decomposition</td>\n",
       "      <td>Singular value decomposition is central to man...</td>\n",
       "      <td>https://arxiv.org/abs/2006.02336v2</td>\n",
       "      <td>https://arxiv.org/pdf/2006.02336v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Xin Wang, Zhixin Song, Youle Wang]</td>\n",
       "      <td>[Image Compression, Recommendation Systems]</td>\n",
       "      <td>2020-06-03</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2006.02336</td>\n",
       "      <td>Variational Quantum Singular Value Decomposition</td>\n",
       "      <td>https://arxiv.org/abs/2006.02336v2</td>\n",
       "      <td>https://arxiv.org/pdf/2006.02336v2.pdf</td>\n",
       "      <td>PaddlePaddle/Quantum</td>\n",
       "      <td>https://github.com/PaddlePaddle/Quantum</td>\n",
       "      <td>Recommendation Systems</td>\n",
       "      <td>Quantum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74228</th>\n",
       "      <td>https://paperswithcode.com/paper/domain-advers...</td>\n",
       "      <td>1412.4446</td>\n",
       "      <td>Domain-Adversarial Neural Networks</td>\n",
       "      <td>We introduce a new representation learning alg...</td>\n",
       "      <td>http://arxiv.org/abs/1412.4446v2</td>\n",
       "      <td>http://arxiv.org/pdf/1412.4446v2.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Hana Ajakan, Pascal Germain, Hugo Larochelle,...</td>\n",
       "      <td>[Denoising, Domain Adaptation, Representation ...</td>\n",
       "      <td>2014-12-15</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1412.4446</td>\n",
       "      <td>Domain-Adversarial Neural Networks</td>\n",
       "      <td>http://arxiv.org/abs/1412.4446v2</td>\n",
       "      <td>http://arxiv.org/pdf/1412.4446v2.pdf</td>\n",
       "      <td>Kano-Wu/Domain-Adversarial-Neural-Networks</td>\n",
       "      <td>https://github.com/Kano-Wu/Domain-Adversarial-...</td>\n",
       "      <td>Representation Learning</td>\n",
       "      <td>Domain-Adversarial-Neural-Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49503 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper_url    arxiv_id  \\\n",
       "2      https://paperswithcode.com/paper/procedural-3d...  2010.06411   \n",
       "6      https://paperswithcode.com/paper/multiscale-sp...  2003.11265   \n",
       "7      https://paperswithcode.com/paper/word2vec-opti...  2003.11645   \n",
       "8      https://paperswithcode.com/paper/word2vec-opti...  2003.11645   \n",
       "9      https://paperswithcode.com/paper/evaluating-se...  1903.07377   \n",
       "...                                                  ...         ...   \n",
       "74224  https://paperswithcode.com/paper/arbitrary-sty...  1703.06868   \n",
       "74225  https://paperswithcode.com/paper/arbitrary-sty...  1703.06868   \n",
       "74226  https://paperswithcode.com/paper/arbitrary-sty...  1703.06868   \n",
       "74227  https://paperswithcode.com/paper/variational-q...  2006.02336   \n",
       "74228  https://paperswithcode.com/paper/domain-advers...   1412.4446   \n",
       "\n",
       "                                                   title  \\\n",
       "2      Procedural 3D Terrain Generation using Generat...   \n",
       "6      Multiscale Sparsifying Transform Learning for ...   \n",
       "7      Word2Vec: Optimal Hyper-Parameters and Their I...   \n",
       "8      Word2Vec: Optimal Hyper-Parameters and Their I...   \n",
       "9      Evaluating Sequence-to-Sequence Models for Han...   \n",
       "...                                                  ...   \n",
       "74224  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74225  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74226  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74227   Variational Quantum Singular Value Decomposition   \n",
       "74228                 Domain-Adversarial Neural Networks   \n",
       "\n",
       "                                                abstract  \\\n",
       "2      Procedural 3D Terrain generation has become a ...   \n",
       "6      The data-driven sparse methods such as synthes...   \n",
       "7      Word2Vec is a prominent model for natural lang...   \n",
       "8      Word2Vec is a prominent model for natural lang...   \n",
       "9      Encoder-decoder models have become an effectiv...   \n",
       "...                                                  ...   \n",
       "74224  Gatys et al. recently introduced a neural algo...   \n",
       "74225  Gatys et al. recently introduced a neural algo...   \n",
       "74226  Gatys et al. recently introduced a neural algo...   \n",
       "74227  Singular value decomposition is central to man...   \n",
       "74228  We introduce a new representation learning alg...   \n",
       "\n",
       "                                  url_abs  \\\n",
       "2      https://arxiv.org/abs/2010.06411v1   \n",
       "6      https://arxiv.org/abs/2003.11265v4   \n",
       "7      https://arxiv.org/abs/2003.11645v2   \n",
       "8      https://arxiv.org/abs/2003.11645v2   \n",
       "9      https://arxiv.org/abs/1903.07377v2   \n",
       "...                                   ...   \n",
       "74224   http://arxiv.org/abs/1703.06868v2   \n",
       "74225   http://arxiv.org/abs/1703.06868v2   \n",
       "74226   http://arxiv.org/abs/1703.06868v2   \n",
       "74227  https://arxiv.org/abs/2006.02336v2   \n",
       "74228    http://arxiv.org/abs/1412.4446v2   \n",
       "\n",
       "                                      url_pdf    proceeding  \\\n",
       "2      https://arxiv.org/pdf/2010.06411v1.pdf          None   \n",
       "6      https://arxiv.org/pdf/2003.11265v4.pdf          None   \n",
       "7      https://arxiv.org/pdf/2003.11645v2.pdf          None   \n",
       "8      https://arxiv.org/pdf/2003.11645v2.pdf          None   \n",
       "9      https://arxiv.org/pdf/1903.07377v2.pdf          None   \n",
       "...                                       ...           ...   \n",
       "74224   http://arxiv.org/pdf/1703.06868v2.pdf  ICCV 2017 10   \n",
       "74225   http://arxiv.org/pdf/1703.06868v2.pdf  ICCV 2017 10   \n",
       "74226   http://arxiv.org/pdf/1703.06868v2.pdf  ICCV 2017 10   \n",
       "74227  https://arxiv.org/pdf/2006.02336v2.pdf          None   \n",
       "74228    http://arxiv.org/pdf/1412.4446v2.pdf          None   \n",
       "\n",
       "                                                 authors  \\\n",
       "2                   [Emmanouil Panagiotou, Eleni Charou]   \n",
       "6      [Ashkan Abbasi, Amirhassan Monadjemi, Leyuan F...   \n",
       "7      [Tosin P. Adewumi, Foteini Liwicki, Marcus Liw...   \n",
       "8      [Tosin P. Adewumi, Foteini Liwicki, Marcus Liw...   \n",
       "9      [Johannes Michael, Roger Labahn, Tobias Grünin...   \n",
       "...                                                  ...   \n",
       "74224                        [Xun Huang, Serge Belongie]   \n",
       "74225                        [Xun Huang, Serge Belongie]   \n",
       "74226                        [Xun Huang, Serge Belongie]   \n",
       "74227                [Xin Wang, Zhixin Song, Youle Wang]   \n",
       "74228  [Hana Ajakan, Pascal Germain, Hugo Larochelle,...   \n",
       "\n",
       "                                                   tasks       date  ...  \\\n",
       "2             [Colorization, Image-to-Image Translation] 2020-10-13  ...   \n",
       "6      [Denoising, Dictionary Learning, Image Denoising] 2020-03-25  ...   \n",
       "7         [Named Entity Recognition, Sentiment Analysis] 2020-03-23  ...   \n",
       "8         [Named Entity Recognition, Sentiment Analysis] 2020-03-23  ...   \n",
       "9      [Image Captioning, Keyword Spotting, Language ... 2019-03-18  ...   \n",
       "...                                                  ...        ...  ...   \n",
       "74224                                   [Style Transfer] 2017-03-20  ...   \n",
       "74225                                   [Style Transfer] 2017-03-20  ...   \n",
       "74226                                   [Style Transfer] 2017-03-20  ...   \n",
       "74227        [Image Compression, Recommendation Systems] 2020-06-03  ...   \n",
       "74228  [Denoising, Domain Adaptation, Representation ... 2014-12-15  ...   \n",
       "\n",
       "      mentioned_in_github mentioned_in_paper  paper_arxiv_id  \\\n",
       "2                   False               True      2010.06411   \n",
       "6                    True              False      2003.11265   \n",
       "7                    True              False      2003.11645   \n",
       "8                   False               True      2003.11645   \n",
       "9                    True              False      1903.07377   \n",
       "...                   ...                ...             ...   \n",
       "74224                True              False      1703.06868   \n",
       "74225               False              False      1703.06868   \n",
       "74226                True              False      1703.06868   \n",
       "74227                True               True      2006.02336   \n",
       "74228                True              False       1412.4446   \n",
       "\n",
       "                                             paper_title  \\\n",
       "2      Procedural 3D Terrain Generation using Generat...   \n",
       "6      Multiscale Sparsifying Transform Learning for ...   \n",
       "7      Word2Vec: Optimal Hyper-Parameters and Their I...   \n",
       "8      Word2Vec: Optimal Hyper-Parameters and Their I...   \n",
       "9      Evaluating Sequence-to-Sequence Models for Han...   \n",
       "...                                                  ...   \n",
       "74224  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74225  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74226  Arbitrary Style Transfer in Real-time with Ada...   \n",
       "74227   Variational Quantum Singular Value Decomposition   \n",
       "74228                 Domain-Adversarial Neural Networks   \n",
       "\n",
       "                            paper_url_abs  \\\n",
       "2      https://arxiv.org/abs/2010.06411v1   \n",
       "6      https://arxiv.org/abs/2003.11265v4   \n",
       "7      https://arxiv.org/abs/2003.11645v2   \n",
       "8      https://arxiv.org/abs/2003.11645v2   \n",
       "9      https://arxiv.org/abs/1903.07377v2   \n",
       "...                                   ...   \n",
       "74224   http://arxiv.org/abs/1703.06868v2   \n",
       "74225   http://arxiv.org/abs/1703.06868v2   \n",
       "74226   http://arxiv.org/abs/1703.06868v2   \n",
       "74227  https://arxiv.org/abs/2006.02336v2   \n",
       "74228    http://arxiv.org/abs/1412.4446v2   \n",
       "\n",
       "                                paper_url_pdf  \\\n",
       "2      https://arxiv.org/pdf/2010.06411v1.pdf   \n",
       "6      https://arxiv.org/pdf/2003.11265v4.pdf   \n",
       "7      https://arxiv.org/pdf/2003.11645v2.pdf   \n",
       "8      https://arxiv.org/pdf/2003.11645v2.pdf   \n",
       "9      https://arxiv.org/pdf/1903.07377v2.pdf   \n",
       "...                                       ...   \n",
       "74224   http://arxiv.org/pdf/1703.06868v2.pdf   \n",
       "74225   http://arxiv.org/pdf/1703.06868v2.pdf   \n",
       "74226   http://arxiv.org/pdf/1703.06868v2.pdf   \n",
       "74227  https://arxiv.org/pdf/2006.02336v2.pdf   \n",
       "74228    http://arxiv.org/pdf/1412.4446v2.pdf   \n",
       "\n",
       "                                             repo  \\\n",
       "2                  Panagiotou/Procedural3DTerrain   \n",
       "6                            ashkan-abbasi66/MTLD   \n",
       "7                                     phnk/D7047E   \n",
       "8                               tosingithub/sdesk   \n",
       "9                                    jamestjw/OCR   \n",
       "...                                           ...   \n",
       "74224                  bethgelab/stylize-datasets   \n",
       "74225                 cryu854/ArbitraryStyle-tfjs   \n",
       "74226      srihari-humbarwadi/adain-tensorflow2.x   \n",
       "74227                        PaddlePaddle/Quantum   \n",
       "74228  Kano-Wu/Domain-Adversarial-Neural-Networks   \n",
       "\n",
       "                                                repo_url  \\\n",
       "2      https://github.com/Panagiotou/Procedural3DTerrain   \n",
       "6                https://github.com/ashkan-abbasi66/MTLD   \n",
       "7                         https://github.com/phnk/D7047E   \n",
       "8                   https://github.com/tosingithub/sdesk   \n",
       "9                        https://github.com/jamestjw/OCR   \n",
       "...                                                  ...   \n",
       "74224      https://github.com/bethgelab/stylize-datasets   \n",
       "74225     https://github.com/cryu854/ArbitraryStyle-tfjs   \n",
       "74226  https://github.com/srihari-humbarwadi/adain-te...   \n",
       "74227            https://github.com/PaddlePaddle/Quantum   \n",
       "74228  https://github.com/Kano-Wu/Domain-Adversarial-...   \n",
       "\n",
       "                 most_common_task                           repo_name  \n",
       "2      Image-to-Image Translation                 Procedural3DTerrain  \n",
       "6                       Denoising                                MTLD  \n",
       "7              Sentiment Analysis                              D7047E  \n",
       "8              Sentiment Analysis                               sdesk  \n",
       "9              Language Modelling                                 OCR  \n",
       "...                           ...                                 ...  \n",
       "74224              Style Transfer                    stylize-datasets  \n",
       "74225              Style Transfer                 ArbitraryStyle-tfjs  \n",
       "74226              Style Transfer                 adain-tensorflow2.x  \n",
       "74227      Recommendation Systems                             Quantum  \n",
       "74228     Representation Learning  Domain-Adversarial-Neural-Networks  \n",
       "\n",
       "[49503 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_repo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cede59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "      <th>sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport subprocess\\nimp...</td>\n",
       "      <td>5364c4a113de40cb17e40b052fa012ad7daba2bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n# Author: Rico Sennric...</td>\n",
       "      <td>261dd0ffc2e10ee32ea99ca80c99cde50a6e7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/concat-bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\npar...</td>\n",
       "      <td>e6bc04ce4b16c0b91d00fc576ed5d261c711760f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/copy-model.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport argparse\\nimpor...</td>\n",
       "      <td>56014b1516fd6df606d500b5efc190efbe8d5203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/coverage.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nfro...</td>\n",
       "      <td>f008bc3a194503c2e0c68ea2d5389db81905f6bd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560178</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/convert_from_depre.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>4ed7125b8568a0f9c60a3bf4670747ec4c497942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560179</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/demo.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# -------------------...</td>\n",
       "      <td>2bd89335df588e010bbb22370274dcbd04bcb407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560180</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/reval.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# -------------------...</td>\n",
       "      <td>612ae317fa5362b2b03b667cc2a8e17323d1a16f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560181</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/test_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>c9cfc53f46f7f9d6c0885ffcad8727f86716ce22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560182</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560183 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                owner repo_name                    file_path  \\\n",
       "0             trangvu   ape-npi                 run-tests.py   \n",
       "1             trangvu   ape-npi         scripts/apply_bpe.py   \n",
       "2             trangvu   ape-npi        scripts/concat-bpe.py   \n",
       "3             trangvu   ape-npi        scripts/copy-model.py   \n",
       "4             trangvu   ape-npi          scripts/coverage.py   \n",
       "...               ...       ...                          ...   \n",
       "560178  KyeongmoonKim        sb  tools/convert_from_depre.py   \n",
       "560179  KyeongmoonKim        sb                tools/demo.py   \n",
       "560180  KyeongmoonKim        sb               tools/reval.py   \n",
       "560181  KyeongmoonKim        sb            tools/test_net.py   \n",
       "560182  KyeongmoonKim        sb        tools/trainval_net.py   \n",
       "\n",
       "                                                  content  \\\n",
       "0       #!/usr/bin/env python3\\nimport subprocess\\nimp...   \n",
       "1       #!/usr/bin/env python3\\n# Author: Rico Sennric...   \n",
       "2       #!/usr/bin/env python3\\n\\nimport argparse\\npar...   \n",
       "3       #!/usr/bin/env python3\\nimport argparse\\nimpor...   \n",
       "4       #!/usr/bin/env python3\\n\\nimport argparse\\nfro...   \n",
       "...                                                   ...   \n",
       "560178  # --------------------------------------------...   \n",
       "560179  #!/usr/bin/env python\\n\\n# -------------------...   \n",
       "560180  #!/usr/bin/env python\\n\\n# -------------------...   \n",
       "560181  # --------------------------------------------...   \n",
       "560182  # --------------------------------------------...   \n",
       "\n",
       "                                             sha  \n",
       "0       5364c4a113de40cb17e40b052fa012ad7daba2bd  \n",
       "1       261dd0ffc2e10ee32ea99ca80c99cde50a6e7636  \n",
       "2       e6bc04ce4b16c0b91d00fc576ed5d261c711760f  \n",
       "3       56014b1516fd6df606d500b5efc190efbe8d5203  \n",
       "4       f008bc3a194503c2e0c68ea2d5389db81905f6bd  \n",
       "...                                          ...  \n",
       "560178  4ed7125b8568a0f9c60a3bf4670747ec4c497942  \n",
       "560179  2bd89335df588e010bbb22370274dcbd04bcb407  \n",
       "560180  612ae317fa5362b2b03b667cc2a8e17323d1a16f  \n",
       "560181  c9cfc53f46f7f9d6c0885ffcad8727f86716ce22  \n",
       "560182  e7b4dab93b6e543402f25f53696e918f3e3d0b5a  \n",
       "\n",
       "[560183 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab8c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df = python_files_df.merge(papers_with_repo_df[['repo_name', 'tasks']], on=['repo_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b838642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "      <th>sha</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>run-tests.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport subprocess\\nimp...</td>\n",
       "      <td>5364c4a113de40cb17e40b052fa012ad7daba2bd</td>\n",
       "      <td>[Automatic Post-Editing, Machine Translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/apply_bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n# Author: Rico Sennric...</td>\n",
       "      <td>261dd0ffc2e10ee32ea99ca80c99cde50a6e7636</td>\n",
       "      <td>[Automatic Post-Editing, Machine Translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/concat-bpe.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\npar...</td>\n",
       "      <td>e6bc04ce4b16c0b91d00fc576ed5d261c711760f</td>\n",
       "      <td>[Automatic Post-Editing, Machine Translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/copy-model.py</td>\n",
       "      <td>#!/usr/bin/env python3\\nimport argparse\\nimpor...</td>\n",
       "      <td>56014b1516fd6df606d500b5efc190efbe8d5203</td>\n",
       "      <td>[Automatic Post-Editing, Machine Translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu</td>\n",
       "      <td>ape-npi</td>\n",
       "      <td>scripts/coverage.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\nimport argparse\\nfro...</td>\n",
       "      <td>f008bc3a194503c2e0c68ea2d5389db81905f6bd</td>\n",
       "      <td>[Automatic Post-Editing, Machine Translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39906459</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/test_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>c9cfc53f46f7f9d6c0885ffcad8727f86716ce22</td>\n",
       "      <td>[Object Detection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39906460</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "      <td>[Object Detection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39906461</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "      <td>[Object Detection]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39906462</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "      <td>[Object Detection, Real-Time Object Detection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39906463</th>\n",
       "      <td>KyeongmoonKim</td>\n",
       "      <td>sb</td>\n",
       "      <td>tools/trainval_net.py</td>\n",
       "      <td># --------------------------------------------...</td>\n",
       "      <td>e7b4dab93b6e543402f25f53696e918f3e3d0b5a</td>\n",
       "      <td>[Object Detection]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39906464 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  owner repo_name              file_path  \\\n",
       "0               trangvu   ape-npi           run-tests.py   \n",
       "1               trangvu   ape-npi   scripts/apply_bpe.py   \n",
       "2               trangvu   ape-npi  scripts/concat-bpe.py   \n",
       "3               trangvu   ape-npi  scripts/copy-model.py   \n",
       "4               trangvu   ape-npi    scripts/coverage.py   \n",
       "...                 ...       ...                    ...   \n",
       "39906459  KyeongmoonKim        sb      tools/test_net.py   \n",
       "39906460  KyeongmoonKim        sb  tools/trainval_net.py   \n",
       "39906461  KyeongmoonKim        sb  tools/trainval_net.py   \n",
       "39906462  KyeongmoonKim        sb  tools/trainval_net.py   \n",
       "39906463  KyeongmoonKim        sb  tools/trainval_net.py   \n",
       "\n",
       "                                                    content  \\\n",
       "0         #!/usr/bin/env python3\\nimport subprocess\\nimp...   \n",
       "1         #!/usr/bin/env python3\\n# Author: Rico Sennric...   \n",
       "2         #!/usr/bin/env python3\\n\\nimport argparse\\npar...   \n",
       "3         #!/usr/bin/env python3\\nimport argparse\\nimpor...   \n",
       "4         #!/usr/bin/env python3\\n\\nimport argparse\\nfro...   \n",
       "...                                                     ...   \n",
       "39906459  # --------------------------------------------...   \n",
       "39906460  # --------------------------------------------...   \n",
       "39906461  # --------------------------------------------...   \n",
       "39906462  # --------------------------------------------...   \n",
       "39906463  # --------------------------------------------...   \n",
       "\n",
       "                                               sha  \\\n",
       "0         5364c4a113de40cb17e40b052fa012ad7daba2bd   \n",
       "1         261dd0ffc2e10ee32ea99ca80c99cde50a6e7636   \n",
       "2         e6bc04ce4b16c0b91d00fc576ed5d261c711760f   \n",
       "3         56014b1516fd6df606d500b5efc190efbe8d5203   \n",
       "4         f008bc3a194503c2e0c68ea2d5389db81905f6bd   \n",
       "...                                            ...   \n",
       "39906459  c9cfc53f46f7f9d6c0885ffcad8727f86716ce22   \n",
       "39906460  e7b4dab93b6e543402f25f53696e918f3e3d0b5a   \n",
       "39906461  e7b4dab93b6e543402f25f53696e918f3e3d0b5a   \n",
       "39906462  e7b4dab93b6e543402f25f53696e918f3e3d0b5a   \n",
       "39906463  e7b4dab93b6e543402f25f53696e918f3e3d0b5a   \n",
       "\n",
       "                                                      tasks  \n",
       "0             [Automatic Post-Editing, Machine Translation]  \n",
       "1             [Automatic Post-Editing, Machine Translation]  \n",
       "2             [Automatic Post-Editing, Machine Translation]  \n",
       "3             [Automatic Post-Editing, Machine Translation]  \n",
       "4             [Automatic Post-Editing, Machine Translation]  \n",
       "...                                                     ...  \n",
       "39906459                                 [Object Detection]  \n",
       "39906460                                 [Object Detection]  \n",
       "39906461                                 [Object Detection]  \n",
       "39906462  [Object Detection, Real-Time Object Detection,...  \n",
       "39906463                                 [Object Detection]  \n",
       "\n",
       "[39906464 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1638278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe7211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def foo():\n",
    "    bar()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3073e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_ast = (ast.parse(code).body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f54428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foo'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.parse(code).body[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b2334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = ast.parse(code).body[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6792a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = exprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d4855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_ast = ast.parse(\"word = tuple(orig) + ''\").body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2669f1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.BinOp at 0x7fa10b42dd90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr_ast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "829099fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_calls_from_expr_or_assign(expr):\n",
    "    try:\n",
    "        if type(expr) is ast.Name:\n",
    "            return [expr.id]\n",
    "        if type(expr) is ast.Call:\n",
    "            return get_calls_from_expr_or_assign(expr.func) + get_calls_from_expr_or_assign(expr.args)\n",
    "        elif type(expr) is ast.Attribute:\n",
    "            return [expr.attr]\n",
    "        elif type(expr) is ast.BinOp:\n",
    "            return get_calls_from_expr_or_assign(expr.left) + get_calls_from_expr_or_assign(expr.right)\n",
    "        elif type(expr) is ast.Expr:\n",
    "            return get_calls_from_expr_or_assign(expr.value)\n",
    "        elif type(expr) is ast.Assign:\n",
    "            return get_calls_from_expr_or_assign(expr.value)\n",
    "        elif type(expr) is ast.While or type(expr) is ast.If:\n",
    "            return (\n",
    "                get_calls_from_expr_or_assign(expr.test) +\n",
    "                get_calls_from_expr_or_assign(expr.orelse) +\n",
    "                get_calls_from_expr_or_assign(expr.body)\n",
    "            )\n",
    "        elif type(expr) is ast.For:\n",
    "            return (\n",
    "                get_calls_from_expr_or_assign(expr.target) +\n",
    "                get_calls_from_expr_or_assign(expr.body) +\n",
    "                get_calls_from_expr_or_assign(expr.iter)\n",
    "            )\n",
    "        elif type(expr) is ast.Try:\n",
    "            return get_calls_from_expr_or_assign(expr.body)\n",
    "        elif type(expr) is list:\n",
    "            return [res for subexpr in expr for res in get_calls_from_expr_or_assign(subexpr)]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(str(astunparse.unparse(expr)), ' ', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07514328",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_code = \"\"\"def encode(self, orig):\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    while True:\n",
    "        bigram = min(pairs, key=lambda pair: self.bpe_codes.get(pair, float('inf')))\n",
    "        if bigram not in self.bpe_codes:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
    "\n",
    "    return word\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eb4c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ast = ast.parse(encode_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66009bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_body = encode_ast.body[0].body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2899f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_def = ast.parse(code).body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3128527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_ast.Expr at 0x7fa10eca8580>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_def.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1798f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def zip_dicts(dicts):\n",
    "    res = {}\n",
    "    for d in dicts:\n",
    "        res = {**res, **d}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_ast_function_calls(code_ast, calls={}):\n",
    "    if type(code_ast) is ast.FunctionDef:\n",
    "        return {code_ast.name: frozenset(\n",
    "            sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])\n",
    "        )}\n",
    "    elif type(code_ast) is ast.ClassDef:\n",
    "        return {code_ast.name: frozenset(\n",
    "            sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])\n",
    "        )}\n",
    "    elif hasattr(code_ast, 'body'):\n",
    "        return zip_dicts([get_ast_function_calls(item) for item in code_ast.body])\n",
    "    else: return {}\n",
    "    \n",
    "    \n",
    "def get_function_calls(code):\n",
    "    code_ast = ast.parse(code)\n",
    "    return get_ast_function_calls(code_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f6d3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ast.parse(\n",
    "\"new_word.extend(word[i:j])\"\n",
    ").body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84a81235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encode': frozenset({'append',\n",
       "            'bigram',\n",
       "            'endswith',\n",
       "            'extend',\n",
       "            'first',\n",
       "            'get_pairs',\n",
       "            'i',\n",
       "            'index',\n",
       "            'j',\n",
       "            'min',\n",
       "            'new_word',\n",
       "            'orig',\n",
       "            'pairs',\n",
       "            'second',\n",
       "            'tuple',\n",
       "            'word'})}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ast_function_calls(encode_ast.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95a5b4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encode': frozenset({'append',\n",
       "            'bigram',\n",
       "            'endswith',\n",
       "            'extend',\n",
       "            'first',\n",
       "            'get_pairs',\n",
       "            'i',\n",
       "            'index',\n",
       "            'j',\n",
       "            'min',\n",
       "            'new_word',\n",
       "            'orig',\n",
       "            'pairs',\n",
       "            'second',\n",
       "            'tuple',\n",
       "            'word'})}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ast_function_calls(ast.parse(encode_code).body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6449cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': frozenset({'bar'})}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function_calls(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "722e0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_function_calls(code) == {\"foo\": frozenset([\"bar\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b9d2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ast.parse(code).body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b02102e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "# Author: Rico Sennrich\n",
      "\n",
      "\"\"\"Use operations learned with learn_bpe.py to encode a new text.\n",
      "The text will not be smaller, but use only a fixed vocabulary, with rare words\n",
      "encoded as variable-length sequences of subword units.\n",
      "\n",
      "Reference:\n",
      "Rico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\n",
      "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import codecs\n",
      "import argparse\n",
      "import functools\n",
      "from collections import defaultdict\n",
      "\n",
      "class BPE(object):\n",
      "    def encode(self, orig):\n",
      "        \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n",
      "        \"\"\"\n",
      "        word = tuple(orig) + ('</w>',)\n",
      "        pairs = get_pairs(word)\n",
      "\n",
      "        while True:\n",
      "            bigram = min(pairs, key=lambda pair: self.bpe_codes.get(pair, float('inf')))\n",
      "            if bigram not in self.bpe_codes:\n",
      "                break\n",
      "            first, second = bigram\n",
      "            new_word = []\n",
      "            i = 0\n",
      "            while i < len(word):\n",
      "                try:\n",
      "                    j = word.index(first, i)\n",
      "                    new_word.extend(word[i:j])\n",
      "                    i = j\n",
      "                except:\n",
      "                    new_word.extend(word[i:])\n",
      "                    break\n",
      "\n",
      "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
      "                    new_word.append(first + second)\n",
      "                    i += 2\n",
      "                else:\n",
      "                    new_word.append(word[i])\n",
      "                    i += 1\n",
      "            new_word = tuple(new_word)\n",
      "            word = new_word\n",
      "            if len(word) == 1:\n",
      "                break\n",
      "            else:\n",
      "                pairs = get_pairs(word)\n",
      "\n",
      "        # don't print end-of-word symbols\n",
      "        if word[-1] == '</w>':\n",
      "            word = word[:-1]\n",
      "        elif word[-1].endswith('</w>'):\n",
      "            word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
      "\n",
      "        return word\n",
      "\n",
      "    def __init__(self, codes, separator='@@'):\n",
      "        self.encode = functools.lru_cache(maxsize=65536)(self.encode)\n",
      "        self.bpe_codes = [tuple(item.split()) for item in codes]\n",
      "        # some hacking to deal with duplicates (only consider first instance)\n",
      "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
      "\n",
      "        self.separator = separator\n",
      "\n",
      "    def segment(self, sentence):\n",
      "        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n",
      "\n",
      "        output = []\n",
      "        for word in sentence.split():\n",
      "            new_word = self.encode(word)\n",
      "\n",
      "            for item in new_word[:-1]:\n",
      "                output.append(item + self.separator)\n",
      "            output.append(new_word[-1])\n",
      "\n",
      "        return ' '.join(output)\n",
      "\n",
      "\n",
      "def create_parser():\n",
      "    parser = argparse.ArgumentParser(\n",
      "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
      "        description=\"learn BPE-based word segmentation\")\n",
      "\n",
      "    parser.add_argument(\n",
      "        '--input', '-i', type=argparse.FileType('r'), default=sys.stdin,\n",
      "        metavar='PATH',\n",
      "        help=\"Input file (default: standard input).\")\n",
      "    parser.add_argument(\n",
      "        '--codes', '-c', type=argparse.FileType('r'), metavar='PATH',\n",
      "        required=True,\n",
      "        help=\"File with BPE codes (created by learn_bpe.py).\")\n",
      "    parser.add_argument(\n",
      "        '--output', '-o', type=argparse.FileType('w'), default=sys.stdout,\n",
      "        metavar='PATH',\n",
      "        help=\"Output file (default: standard output)\")\n",
      "    parser.add_argument(\n",
      "        '--separator', '-s', type=str, default='@@', metavar='STR',\n",
      "        help=\"Separator between non-final subword units (default: '%(default)s'))\")\n",
      "\n",
      "    return parser\n",
      "\n",
      "def get_pairs(word):\n",
      "    \"\"\"Return set of symbol pairs in a word.\n",
      "\n",
      "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
      "    \"\"\"\n",
      "    pairs = set()\n",
      "    prev_char = word[0]\n",
      "    for char in word[1:]:\n",
      "        pairs.add((prev_char, char))\n",
      "        prev_char = char\n",
      "    return pairs\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = create_parser()\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    bpe = BPE(args.codes, args.separator)\n",
      "\n",
      "    for line in args.input:\n",
      "        args.output.write(bpe.segment(line).strip())\n",
      "        args.output.write('\\n')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_file = python_files_df['content'].iloc[1]\n",
    "print(example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "165e3c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def get_pairs(word):\n",
      "    'Return set of symbol pairs in a word.\\n\\n    word is represented as tuple of symbols (symbols being variable-length strings)\\n    '\n",
      "    pairs = set()\n",
      "    prev_char = word[0]\n",
      "    for char in word[1:]:\n",
      "        pairs.add((prev_char, char))\n",
      "        prev_char = char\n",
      "    return pairs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_code = ast.parse(example_file).body[-2]\n",
    "print(astunparse.unparse(fn_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a441bfd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BPE': frozenset(),\n",
       " 'create_parser': frozenset({'ArgumentParser', 'add_argument'}),\n",
       " 'get_pairs': frozenset({'add', 'char', 'set'})}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function_calls(example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef7c2d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items = ast.parse(\"a.append(1)\").body\n",
    "expr = items[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dce14c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 178 ms, total: 49.8 s\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "python_files_df['clean_content'] = python_files_df['content'].str.replace(\"429: Too Many Requests\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a7d020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df['clean_content'] = python_files_df['clean_content'].apply(lambda s: np.nan if s == \"\" else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1997fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_sample_files_df(python_files_df, n_files=100):\n",
    "    \"sample n_files from each repo\"\n",
    "    return python_files_df.dropna().groupby('repo_name').apply(lambda df: df.iloc[:min(df.shape[0], 100)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34065632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = python_files_df.dropna().groupby('repo_name').apply(lambda df: df.iloc[:min(df.shape[0], 100)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a01fc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_files = sample_df['clean_content']\n",
    "example_filenames = sample_df['file_path'].apply(os.path.basename).str.replace('.py', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec02834c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['owner', 'repo_name', 'file_path', 'content', 'sha', 'tasks',\n",
       "       'clean_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b7933c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_repo_task_edges(python_files_df):\n",
    "    task_exploded_df = python_files_df[['repo_name', 'tasks']].explode('tasks')\n",
    "    task_exploded_df['tasks'] = task_exploded_df['tasks'].str.replace(\"2D \", \"\").str.replace(\"3D \", \"\").str.replace(\"4D \", \"\").str.replace(\"6D \", \"\")\n",
    "    return task_exploded_df.groupby(\"tasks\").apply(lambda df: {df['tasks'].iloc[0]: frozenset(df['repo_name'].values)}).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef59f31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['owner', 'repo_name', 'file_path', 'content', 'sha', 'tasks',\n",
       "       'clean_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1953d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_task_edges = get_repo_task_edges(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "700b84a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'': frozenset({'deepmind-research', 'pytorch-image-models'})},\n",
       " {'AMR Parsing': frozenset({'stog'})},\n",
       " {'AMR-to-Text Generation': frozenset({'OpenNMT-py'})},\n",
       " {'Abstractive Text Summarization': frozenset({'BERT_text_summarisation',\n",
       "             'Chatbot',\n",
       "             'CopyCat-abstractive-opinion-summarizer',\n",
       "             'Pointer_Transformer_Generator',\n",
       "             'bottom-up-summary',\n",
       "             'fairseq',\n",
       "             'google-research',\n",
       "             'onnxt5',\n",
       "             'politicalsynthesis',\n",
       "             'qags',\n",
       "             'shmoop-corpus',\n",
       "             'swap-net',\n",
       "             'teaser_generate',\n",
       "             'tensor2tensor',\n",
       "             'transformers'})},\n",
       " {'Abuse Detection': frozenset({'multi-task-offensive-language-detection'})},\n",
       " {'Abusive Language': frozenset({'emo2vec_wassa_paper',\n",
       "             'hate-speech-detection'})},\n",
       " {'Accented Speech Recognition': frozenset({'DeepSpeech',\n",
       "             'STT',\n",
       "             'audio2score',\n",
       "             'models'})},\n",
       " {'Acoustic Scene Classification': frozenset({'audio_environment_description',\n",
       "             'icassp19'})},\n",
       " {'Action Classification': frozenset({'Activity-Recognition-with-CNN-and-RNN',\n",
       "             'FTGAN',\n",
       "             'GWSDR',\n",
       "             'MFF-pytorch',\n",
       "             'Octconv-ResNet-tensorflow',\n",
       "             'SDN',\n",
       "             'SoccerNet-code',\n",
       "             'ashus_projects',\n",
       "             'caffe',\n",
       "             'charades',\n",
       "             'charades-algorithms',\n",
       "             'detectron',\n",
       "             'google-research',\n",
       "             'keras-kinetics-i3d',\n",
       "             'octconv_resnet',\n",
       "             'pytorch-coviar',\n",
       "             'representation-flow-cvpr19'})},\n",
       " {'Action Classification ': frozenset({'FTGAN',\n",
       "             'GWSDR',\n",
       "             'MFF-pytorch',\n",
       "             'SDN',\n",
       "             'ashus_projects',\n",
       "             'keras-kinetics-i3d'})}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_task_edges[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fef6e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def try_run(f): \n",
    "    def _maybe_failed_f(args):\n",
    "        try:\n",
    "            return f(args)\n",
    "        except:\n",
    "            return None\n",
    "    return _maybe_failed_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "551d9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_upper_level_edges(upper_level_names, lower_level_edges):\n",
    "    return [\n",
    "        {\n",
    "            file_name: \n",
    "            frozenset(\n",
    "                [\n",
    "                    function \n",
    "                    for function in lower_level_edges.keys()\n",
    "                ])\n",
    "        }\n",
    "        for (file_name, lower_level_edges) in zip(upper_level_names, lower_level_edges)\n",
    "        if type(lower_level_edges) is dict\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_records(function_edges):\n",
    "    return [{\n",
    "        'calling_function': calling_fn, \n",
    "        'called_function': called_fn\n",
    "    }\n",
    "        for calling_fn in function_edges.keys()\n",
    "        for called_fn in function_edges[calling_fn]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26010ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "example_node_names = [\n",
    "    'train', 'fit', 'test', '__init__', 'DistilBertForMaskedLM',\n",
    "    'tensorflow', 'transformers', 'mmdetection', 'Recommenders-movielens',\n",
    "    'hate-speech-detection', 'variational-dropout-sparsifies-dnn']\n",
    "\n",
    "\n",
    "def get_nodes(call_graph, node_names):\n",
    "    selected_nodes = call_graph.names[call_graph.names.isin(node_names)]\n",
    "    return selected_nodes\n",
    "\n",
    "\n",
    "def get_node_embedding_model_and_results(node_embeddings, call_graph, example_nodes, selected_names=None, topk=10):\n",
    "    if selected_names is None:\n",
    "        selected_nodes = call_graph.names\n",
    "    else:\n",
    "        selected_nodes = get_node_indices(call_graph, selected_names)\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    for (i, node_name) in enumerate(example_nodes.values):\n",
    "        distances = metrics.euclidean_distances(node_embeddings[example_nodes.index[[i]]], node_embeddings) \n",
    "        results.append((call_graph.names[np.argsort(distances)[0,:topk]].values))\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.index =  example_nodes.values\n",
    "    return results_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "074f3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_dependency_edges(python_files_df, content_column='clean_content'):\n",
    "    sample_files_df = python_files_df.dropna().groupby('repo_name').apply(lambda df: df.iloc[:min(df.shape[0], 100)]).reset_index(drop=True)\n",
    "    filenames = sample_files_df['file_path'].apply(os.path.basename).str.replace('.py', '')\n",
    "    files = sample_files_df[content_column]\n",
    "    function_edges = files.apply(try_run(get_function_calls)).dropna()\n",
    "    function_edges = function_edges.to_list()\n",
    "    filename_edges = get_upper_level_edges(filenames, function_edges)\n",
    "    repo_edges_dict = filenames.groupby(sample_files_df['repo_name']).agg(lambda args: frozenset(args)).to_dict()\n",
    "    raw_repo_edges = [{k: repo_edges_dict[k]} for k in repo_edges_dict]\n",
    "    root_edges = [{\n",
    "        '<ROOT>': [repo for edges in raw_repo_edges for repo in repo_edges_dict.keys()]\n",
    "        }\n",
    "    ]\n",
    "    repo_task_edges = get_repo_task_edges(sample_files_df)\n",
    "    all_edges = root_edges + repo_task_edges + raw_repo_edges +  filename_edges + function_edges\n",
    "    return all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "615812be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = get_dependency_edges(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d88e6d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 284 ms, total: 1.35 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "records  = [ record\n",
    "    for edges in all_edges \n",
    "    for record in make_records(edges)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bce5e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66812"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(map(itemgetter('calling_function'), records)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7b82935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def make_call_df(records):\n",
    "    call_records_df = pd.DataFrame.from_records(records)\n",
    "    call_records_df.columns = ['source', 'destination'] \n",
    "    call_records_df = call_records_df[\n",
    "        (call_records_df['source'].apply(len) > 0) &\n",
    "        (call_records_df['source'] != 'null') &\n",
    "        (call_records_df['destination'].apply(len) > 0) &\n",
    "        (call_records_df['source'] != call_records_df['destination'])\n",
    "    ]\n",
    "    return call_records_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3803c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df = make_call_df(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f524ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 844725 entries, 0 to 10035947\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   source       844725 non-null  object\n",
      " 1   destination  844725 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 19.3+ MB\n"
     ]
    }
   ],
   "source": [
    "call_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f126127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df.to_csv('data/call_records.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3951e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(call_records_df.iloc[:,[1,0]].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7c3e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def make_igraph(call_records_df):\n",
    "    vertices = list(set(call_records_df['source'].unique()).union(call_records_df['destination'].unique()))\n",
    "    edges = [\n",
    "        (source, destination)\n",
    "        for (source, destination) in zip(\n",
    "            call_records_df['source'].values,\n",
    "            call_records_df['destination'].values\n",
    "        )\n",
    "    ]\n",
    "    graph = igraph.Graph()\n",
    "    graph.add_vertices(vertices)\n",
    "    graph.add_edges(edges)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06b5eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = make_igraph(call_records_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f066e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(graph, open(\"data/call_igraph.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e57536d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198517"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1e3e8",
   "metadata": {},
   "source": [
    "## nodevectors & csrgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c61e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_graph = cg.read_edgelist('data/function_calls.csv', sep=',')\n",
    "example_nodes = get_nodes(call_graph, example_node_names)\n",
    "pickle.dump(call_graph, open('call_csrgraph.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_repo_node_names = [\n",
    "    'fairseq', 'mmdetection', 'Recommenders-movielens',\n",
    "]\n",
    "example_node_names = [\n",
    "    'train', 'fit', 'test', '__init__', 'DistilBertForMaskedLM',\n",
    "    'tensorflow', \n",
    "    'hate-speech-detection', 'variational-dropout-sparsifies-dnn']\n",
    "\n",
    "\n",
    "\n",
    "def get_nodes(call_graph, node_names):\n",
    "    selected_nodes = call_graph.names[call_graph.names.isin(node_names)]\n",
    "    return selected_nodes\n",
    "\n",
    "\n",
    "example_nodes = get_nodes(call_graph, example_node_names)\n",
    "example_repo_nodes = get_nodes(call_graph, example_repo_node_names)\n",
    "\n",
    "\n",
    "def get_node_embedding_model_and_results(node_embeddings, call_graph, example_nodes, selected_names=None, topk=10):\n",
    "    if selected_names is None:\n",
    "        selected_nodes = call_graph.names\n",
    "    else:\n",
    "        selected_nodes = get_node_indices(call_graph, selected_names)\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    for (i, node_name) in enumerate(example_nodes.values):\n",
    "        distances = metrics.euclidean_distances(node_embeddings[example_nodes.index[[i]]], node_embeddings) \n",
    "        results.append((call_graph.names[np.argsort(distances)[0,:topk]].values))\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.index =  example_nodes.values\n",
    "    return results_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df[call_records_df['source'] == 'fairseq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af944cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df[call_records_df['destination'] == 'fairseq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df[call_records_df['source'] == '<ROOT>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5265466",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_graph.mat.diagonal().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e319ba2",
   "metadata": {},
   "source": [
    "%%time\n",
    "ggvec_model = nodevectors.GGVec(n_components=100, max_epoch=1000)\n",
    "ggvec_embeddings = ggvec_model.fit_transform(call_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prone_model = nodevectors.ProNE(n_components=100, step=100)\n",
    "prone_embeddings = prone_model.fit_transform(call_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91129a9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "get_node_embedding_model_and_results(ggvec_embeddings, call_graph, example_repo_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_embedding_model_and_results(prone_embeddings, call_graph, example_repo_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c06899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "prone_kv = KeyedVectors(prone_embeddings.shape[1])\n",
    "prone_kv.add(call_graph.names, prone_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prone_kv.save(\"data/prone_embeddings.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "prone_kv.most_similar(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89819fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in prone_kv.vocab.keys() if 'allenn' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(repo_edges[0].values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(repo_edges[0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_graph.names[call_graph.names.str.contains('allennlp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for item in repo_edges for (k, v) in item.items() if 'basic_allennlp' in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "prone_kv.most_similar(\"num_block_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aeae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prone_closest_repos.md', 'w') as f:\n",
    "    f.write(get_node_embedding_model_and_results(prone_embeddings, call_graph, example_repo_nodes).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2dc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prone_closest_nodes.md', 'w') as f:\n",
    "    f.write(get_node_embedding_model_and_results(prone_embeddings, call_graph, example_nodes).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prone_closest_nodes.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_embedding_model_and_results(prone_embeddings, call_graph, example_nodes).to_csv(open('prone_closest_nodes.md', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_embedding_model_and_results(ggvec_embeddings, call_graph, example_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_embedding_model_and_results(prone_embeddings, call_graph, example_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import poincare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_edges = list(call_records_df.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_records_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd933278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = poincare.PoincareModel(example_edges, negative=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6020ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.kv.descendants(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f75c65",
   "metadata": {},
   "source": [
    "# Hyperbolic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fa366",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c03a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0af37",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- expore call graph\n",
    "- better call graph\n",
    "- Node2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb9d69",
   "metadata": {},
   "source": [
    "Exploring call graph\n",
    "\n",
    "what is the problem?\n",
    "\n",
    "Graph is not even connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ec31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb20744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88394a10-b5ca-43ad-accc-5ab18dc62170",
   "metadata": {},
   "source": [
    "# default_exp python_call_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a09f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import csrgraph as cg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from github_search import python_tokens, paperswithcode_tasks\n",
    "from mlutil.feature_extraction import embeddings\n",
    "from mlutil import prototype_selection\n",
    "import mlutil\n",
    "from mlutil.feature_extraction import embeddings\n",
    "\n",
    "import ast\n",
    "import astunparse\n",
    "import csrgraph\n",
    "import nodevectors\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import igraph\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "\n",
    "from operator import itemgetter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6610ed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a25443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 2.96 s, total: 49.2 s\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "python_files_df = pd.read_csv(\"data/crawled_python_files.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3102709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402272, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afea21bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import pickle\\n\\nimport tensorflow as tf\\nimpo...</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import tensorflow as tf\\nimport math\\nfrom ten...</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import tensorflow as tf\\nfrom tensorflow.contr...</td>\n",
       "      <td>translate/conv_lstm.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import tensorflow as tf\\nfrom translate import...</td>\n",
       "      <td>translate/beam_search.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from tensorflow.python.ops import init_ops\\nfr...</td>\n",
       "      <td>translate/rnn.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  import pickle\\n\\nimport tensorflow as tf\\nimpo...   \n",
       "1  import tensorflow as tf\\nimport math\\nfrom ten...   \n",
       "2  import tensorflow as tf\\nfrom tensorflow.contr...   \n",
       "3  import tensorflow as tf\\nfrom translate import...   \n",
       "4  from tensorflow.python.ops import init_ops\\nfr...   \n",
       "\n",
       "                        path        repo_name  \n",
       "0  translate/import_graph.py  trangvu/ape-npi  \n",
       "1        translate/models.py  trangvu/ape-npi  \n",
       "2     translate/conv_lstm.py  trangvu/ape-npi  \n",
       "3   translate/beam_search.py  trangvu/ape-npi  \n",
       "4           translate/rnn.py  trangvu/ape-npi  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410ec59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78df2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "def foo():\n",
    "    bar()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bb83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_ast = ast.parse(code).body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34abf5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.parse(code).body[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfb6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = ast.parse(code).body[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcd7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = exprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4279559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_ast = ast.parse(\"word = tuple(orig) + ''\").body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1831c3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.BinOp at 0x7fbec904aa00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr_ast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "018ca67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_calls_from_expr_or_assign(expr):\n",
    "    try:\n",
    "        if type(expr) is ast.Name:\n",
    "            return []\n",
    "        if type(expr) is ast.Call:\n",
    "            # function call\n",
    "            if type(expr.func) is ast.Name:\n",
    "                name = [expr.func.id]\n",
    "            # method\n",
    "            elif type(expr.func) is ast.Attribute:\n",
    "                name = [expr.func.attr]\n",
    "            else:\n",
    "                name = []\n",
    "            return name + get_calls_from_expr_or_assign(expr.args)\n",
    "        elif type(expr) is ast.Attribute:\n",
    "            return [expr.attr]\n",
    "        elif type(expr) is ast.BinOp:\n",
    "            return get_calls_from_expr_or_assign(\n",
    "                expr.left\n",
    "            ) + get_calls_from_expr_or_assign(expr.right)\n",
    "        elif type(expr) is ast.Expr:\n",
    "            return get_calls_from_expr_or_assign(expr.value)\n",
    "        elif type(expr) is ast.Assign:\n",
    "            return get_calls_from_expr_or_assign(expr.value)\n",
    "        elif type(expr) is ast.While or type(expr) is ast.If:\n",
    "            return (\n",
    "                get_calls_from_expr_or_assign(expr.test)\n",
    "                + get_calls_from_expr_or_assign(expr.orelse)\n",
    "                + get_calls_from_expr_or_assign(expr.body)\n",
    "            )\n",
    "        elif type(expr) is ast.For:\n",
    "            return (\n",
    "                get_calls_from_expr_or_assign(expr.target)\n",
    "                + get_calls_from_expr_or_assign(expr.body)\n",
    "                + get_calls_from_expr_or_assign(expr.iter)\n",
    "            )\n",
    "        elif type(expr) is ast.Try:\n",
    "            return get_calls_from_expr_or_assign(expr.body)\n",
    "        elif type(expr) is ast.If:\n",
    "            return get_calls_from_expr_or_assign(\n",
    "                expr.body\n",
    "            ) + get_calls_from_expr_or_assign(expr.orelse)\n",
    "        elif type(expr) is list:\n",
    "            return [\n",
    "                res\n",
    "                for subexpr in expr\n",
    "                for res in get_calls_from_expr_or_assign(subexpr)\n",
    "            ]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logging.info(str(astunparse.unparse(expr)), \" \", str(e))\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "703a15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_code = \"\"\"def encode(self, orig):\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    while True:\n",
    "        bigram = min(pairs, key=lambda pair: self.bpe_codes.get(pair, float('inf')))\n",
    "        if bigram not in self.bpe_codes:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
    "\n",
    "    return word\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "72bacc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ast = ast.parse(encode_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a152945",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_body = encode_ast.body[0].body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "367adcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_def = ast.parse(code).body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eafb2bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.Module at 0x7fbec74f7670>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "389a9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def zip_dicts(dicts):\n",
    "    res = {}\n",
    "    for d in dicts:\n",
    "        res = {**res, **d}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_ast_function_calls(code_ast, calls={}):\n",
    "    if type(code_ast) is ast.FunctionDef:\n",
    "        return {\n",
    "            code_ast.name: frozenset(\n",
    "                sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])\n",
    "            )\n",
    "        }\n",
    "    elif type(code_ast) is ast.ClassDef:\n",
    "        return {\n",
    "            code_ast.name: frozenset(\n",
    "                sum([get_calls_from_expr_or_assign(expr) for expr in code_ast.body], [])\n",
    "            )\n",
    "        }\n",
    "    elif hasattr(code_ast, \"body\"):\n",
    "        return zip_dicts([get_ast_function_calls(item) for item in code_ast.body])\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_function_calls(code):\n",
    "    code_ast = ast.parse(code)\n",
    "    return get_ast_function_calls(code_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d920a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ast.parse(\"new_word.extend(word[i:j])\").body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b5549f01-54b3-4279-97b7-ca7f1505c13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_calls_from_expr_or_assign(ast.parse(\"word[i:j]\").body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ec8e107-c864-43a1-af54-cbf3f4bec118",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exp_text = \"def foo(x): f(g(x))\"\n",
    "raw_exp = ast.parse(\"def foo(x): f(g(x))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "79dc52e9-5d3b-42a2-bf51-f8fd02d99553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encode': frozenset({'append',\n",
       "            'endswith',\n",
       "            'extend',\n",
       "            'get_pairs',\n",
       "            'index',\n",
       "            'min',\n",
       "            'tuple'})}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ast_function_calls(encode_ast.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "161fe4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encode': frozenset({'append',\n",
       "            'bigram',\n",
       "            'endswith',\n",
       "            'extend',\n",
       "            'first',\n",
       "            'get_pairs',\n",
       "            'i',\n",
       "            'index',\n",
       "            'j',\n",
       "            'min',\n",
       "            'new_word',\n",
       "            'orig',\n",
       "            'pairs',\n",
       "            'second',\n",
       "            'tuple',\n",
       "            'word'})}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ast_function_calls(encode_ast.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "39c646be-bb0a-4cce-bcb5-1e5b14e0a4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': frozenset({'f', 'g', 'x'})}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function_calls(raw_exp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ddbc49b4-ad9c-43b7-afe3-9f0a55a3ef79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-99fdba0c8577>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ast_function_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdeps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[deps] = get_ast_function_calls(raw_exp).values()\n",
    "assert deps == frozenset([\"f\", \"g\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf6e0b83-036d-4137-a9e5-a913b8560557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'f', 'g', 'x'})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2448247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def foo(x):\n",
      "    f(g(x))\n",
      " <class '_ast.FunctionDef'>\n"
     ]
    }
   ],
   "source": [
    "for item in raw_exp.body:\n",
    "    print(astunparse.unparse(item), type(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c84a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': frozenset({'bar'})}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function_calls(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ad714ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_function_calls(code) == {\"foo\": frozenset([\"bar\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b45be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ast.parse(code).body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "349449b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\n",
      "import math\n",
      "from tensorflow.contrib.rnn import BasicLSTMCell, RNNCell, DropoutWrapper, MultiRNNCell\n",
      "from translate.rnn import stack_bidirectional_dynamic_rnn, CellInitializer, GRUCell, DropoutGRUCell, PLSTM\n",
      "from translate.rnn import get_state_size\n",
      "from translate.beam_search import get_weights\n",
      "from translate import utils, beam_search\n",
      "from translate.conv_lstm import BasicConvLSTMCell\n",
      "\n",
      "\n",
      "def auto_reuse(fun):\n",
      "    \"\"\"\n",
      "    Wrapper that automatically handles the `reuse' parameter.\n",
      "    This is rather risky, as it can lead to reusing variables\n",
      "    by mistake.\n",
      "    \"\"\"\n",
      "\n",
      "    def fun_(*args, **kwargs):\n",
      "        try:\n",
      "            return fun(*args, **kwargs)\n",
      "        except ValueError as e:\n",
      "            if 'reuse' in str(e):\n",
      "                with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
      "                    return fun(*args, **kwargs)\n",
      "            else:\n",
      "                raise e\n",
      "\n",
      "    return fun_\n",
      "\n",
      "\n",
      "get_variable = auto_reuse(tf.get_variable)\n",
      "dense = auto_reuse(tf.layers.dense)\n",
      "\n",
      "\n",
      "class CellWrapper(RNNCell):\n",
      "    \"\"\"\n",
      "    Wrapper around LayerNormBasicLSTMCell, BasicLSTMCell and MultiRNNCell, to keep\n",
      "    the state_is_tuple=False behavior (soon to be deprecated).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, cell):\n",
      "        super(CellWrapper, self).__init__()\n",
      "        self.cell = cell\n",
      "        self.num_splits = len(cell.state_size) if isinstance(cell.state_size, tuple) else 1\n",
      "\n",
      "    @property\n",
      "    def state_size(self):\n",
      "        return sum(self.cell.state_size)\n",
      "\n",
      "    @property\n",
      "    def output_size(self):\n",
      "        return self.cell.output_size\n",
      "\n",
      "    def __call__(self, inputs, state, scope=None):\n",
      "        state = tf.split(value=state, num_or_size_splits=self.num_splits, axis=1)\n",
      "        new_h, new_state = self.cell(inputs, state, scope=scope)\n",
      "        return new_h, tf.concat(new_state, 1)\n",
      "\n",
      "\n",
      "def multi_encoder(encoder_inputs, encoders, encoder_input_length, other_inputs=None, training=True, **kwargs):\n",
      "    \"\"\"\n",
      "    Build multiple encoders according to the configuration in `encoders`, reading from `encoder_inputs`.\n",
      "    The result is a list of the outputs produced by those encoders (for each time-step), and their final state.\n",
      "\n",
      "    :param encoder_inputs: list of tensors of shape (batch_size, input_length), one tensor for each encoder.\n",
      "    :param encoders: list of encoder configurations\n",
      "    :param encoder_input_length: list of tensors of shape (batch_size,) (one tensor for each encoder)\n",
      "    :return:\n",
      "      encoder outputs: a list of tensors of shape (batch_size, input_length, encoder_cell_size), hidden states of the\n",
      "        encoders.\n",
      "      encoder state: concatenation of the final states of all encoders, tensor of shape (batch_size, sum_of_state_sizes)\n",
      "      new_encoder_input_length: list of tensors of shape (batch_size,) with the true length of the encoder outputs.\n",
      "        May be different than `encoder_input_length` because of maxout strides, and time pooling.\n",
      "    \"\"\"\n",
      "    encoder_states = []\n",
      "    encoder_outputs = []\n",
      "    new_encoder_input_length = []\n",
      "\n",
      "    for i, encoder in enumerate(encoders):\n",
      "\n",
      "        # create embeddings in the global scope (allows sharing between encoder and decoder)\n",
      "        weight_scale = encoder.embedding_weight_scale or encoder.weight_scale\n",
      "        if weight_scale is None:\n",
      "            initializer = None  # FIXME\n",
      "        elif encoder.embedding_initializer == 'uniform' or (encoder.embedding_initializer is None\n",
      "                                                            and encoder.initializer == 'uniform'):\n",
      "            initializer = tf.random_uniform_initializer(minval=-weight_scale, maxval=weight_scale)\n",
      "        else:\n",
      "            initializer = tf.random_normal_initializer(stddev=weight_scale)\n",
      "\n",
      "        with tf.device('/cpu:0'):  # embeddings can take a very large amount of memory, so\n",
      "            # storing them in GPU memory can be impractical\n",
      "            if encoder.binary:\n",
      "                embeddings = None  # inputs are token ids, which need to be mapped to vectors (embeddings)\n",
      "            else:\n",
      "                embedding_shape = [encoder.vocab_size, encoder.embedding_size]\n",
      "                embeddings = get_variable('embedding_{}'.format(encoder.name), shape=embedding_shape,\n",
      "                                          initializer=initializer)\n",
      "            if encoder.pos_embedding_size:\n",
      "                pos_embedding_shape = [encoder.max_len + 1, encoder.pos_embedding_size]\n",
      "                pos_embeddings = get_variable('pos_embedding_{}'.format(encoder.name), shape=pos_embedding_shape,\n",
      "                                              initializer=initializer)\n",
      "            else:\n",
      "                pos_embeddings = None\n",
      "\n",
      "        if encoder.use_lstm is False:\n",
      "            encoder.cell_type = 'GRU'\n",
      "\n",
      "        cell_output_size, cell_state_size = get_state_size(encoder.cell_type, encoder.cell_size,\n",
      "                                                           encoder.lstm_proj_size)\n",
      "\n",
      "        with tf.variable_scope('encoder_{}'.format(encoder.name)):\n",
      "            encoder_inputs_ = encoder_inputs[i]\n",
      "            initial_inputs = encoder_inputs_\n",
      "            encoder_input_length_ = encoder_input_length[i]\n",
      "\n",
      "            def get_cell(input_size=None, reuse=False):\n",
      "                if encoder.cell_type.lower() == 'lstm':\n",
      "                    cell = CellWrapper(BasicLSTMCell(encoder.cell_size, reuse=reuse))\n",
      "                elif encoder.cell_type.lower() == 'plstm':\n",
      "                    cell = PLSTM(encoder.cell_size, reuse=reuse, fact_size=encoder.lstm_fact_size,\n",
      "                                 proj_size=encoder.lstm_proj_size)\n",
      "                elif encoder.cell_type.lower() == 'dropoutgru':\n",
      "                    cell = DropoutGRUCell(encoder.cell_size, reuse=reuse, layer_norm=encoder.layer_norm,\n",
      "                                          input_size=input_size, input_keep_prob=encoder.rnn_input_keep_prob,\n",
      "                                          state_keep_prob=encoder.rnn_state_keep_prob)\n",
      "                else:\n",
      "                    cell = GRUCell(encoder.cell_size, reuse=reuse, layer_norm=encoder.layer_norm)\n",
      "\n",
      "                if encoder.use_dropout and encoder.cell_type.lower() != 'dropoutgru':\n",
      "                    cell = DropoutWrapper(cell, input_keep_prob=encoder.rnn_input_keep_prob,\n",
      "                                          output_keep_prob=encoder.rnn_output_keep_prob,\n",
      "                                          state_keep_prob=encoder.rnn_state_keep_prob,\n",
      "                                          variational_recurrent=encoder.pervasive_dropout,\n",
      "                                          dtype=tf.float32, input_size=input_size)\n",
      "                return cell\n",
      "\n",
      "            batch_size = tf.shape(encoder_inputs_)[0]\n",
      "            time_steps = tf.shape(encoder_inputs_)[1]\n",
      "\n",
      "            if embeddings is not None:\n",
      "                flat_inputs = tf.reshape(encoder_inputs_, [tf.multiply(batch_size, time_steps)])\n",
      "                flat_inputs = tf.nn.embedding_lookup(embeddings, flat_inputs)\n",
      "                encoder_inputs_ = tf.reshape(flat_inputs,\n",
      "                                             tf.stack([batch_size, time_steps, flat_inputs.get_shape()[1].value]))\n",
      "            if pos_embeddings is not None:\n",
      "                pos_inputs_ = tf.range(time_steps, dtype=tf.int32)\n",
      "                pos_inputs_ = tf.nn.embedding_lookup(pos_embeddings, pos_inputs_)\n",
      "                pos_inputs_ = tf.tile(tf.expand_dims(pos_inputs_, axis=0), [batch_size, 1, 1])\n",
      "                encoder_inputs_ = tf.concat([encoder_inputs_, pos_inputs_], axis=2)\n",
      "\n",
      "            if other_inputs is not None:\n",
      "                encoder_inputs_ = tf.concat([encoder_inputs_, other_inputs], axis=2)\n",
      "\n",
      "            if encoder.use_dropout:\n",
      "                noise_shape = [1, time_steps, 1] if encoder.pervasive_dropout else [batch_size, time_steps, 1]\n",
      "                encoder_inputs_ = tf.nn.dropout(encoder_inputs_, keep_prob=encoder.word_keep_prob,\n",
      "                                                noise_shape=noise_shape)\n",
      "\n",
      "                size = tf.shape(encoder_inputs_)[2]\n",
      "                noise_shape = [1, 1, size] if encoder.pervasive_dropout else [batch_size, time_steps, size]\n",
      "                encoder_inputs_ = tf.nn.dropout(encoder_inputs_, keep_prob=encoder.embedding_keep_prob,\n",
      "                                                noise_shape=noise_shape)\n",
      "\n",
      "            if encoder.input_layers:\n",
      "                for j, layer_size in enumerate(encoder.input_layers):\n",
      "                    if encoder.input_layer_activation is not None and encoder.input_layer_activation.lower() == 'relu':\n",
      "                        activation = tf.nn.relu\n",
      "                    else:\n",
      "                        activation = tf.tanh\n",
      "\n",
      "                    if encoder.batch_norm:\n",
      "                        encoder_inputs_ = tf.layers.batch_normalization(encoder_inputs_, training=training,\n",
      "                                                                        name='input_batch_norm_{}'.format(j + 1))\n",
      "\n",
      "                    encoder_inputs_ = dense(encoder_inputs_, layer_size, activation=activation, use_bias=True,\n",
      "                                            name='layer_{}'.format(j))\n",
      "                    if encoder.use_dropout:\n",
      "                        encoder_inputs_ = tf.nn.dropout(encoder_inputs_, keep_prob=encoder.input_layer_keep_prob)\n",
      "\n",
      "            if encoder.conv_filters:\n",
      "                encoder_inputs_ = tf.expand_dims(encoder_inputs_, axis=3)\n",
      "\n",
      "                for k, out_channels in enumerate(encoder.conv_filters, 1):\n",
      "                    in_channels = encoder_inputs_.get_shape()[-1].value\n",
      "                    filter_height, filter_width = encoder.conv_size\n",
      "\n",
      "                    strides = encoder.conv_strides or [1, 1]\n",
      "                    strides = [1] + strides + [1]\n",
      "\n",
      "                    filter_ = get_variable('filter_{}'.format(k),\n",
      "                                           [filter_height, filter_width, in_channels, out_channels])\n",
      "                    encoder_inputs_ = tf.nn.conv2d(encoder_inputs_, filter_, strides, padding='SAME')\n",
      "\n",
      "                    if encoder.batch_norm:\n",
      "                        encoder_inputs_ = tf.layers.batch_normalization(encoder_inputs_, training=training,\n",
      "                                                                        name='conv_batch_norm_{}'.format(k))\n",
      "                    if encoder.conv_activation is not None and encoder.conv_activation.lower() == 'relu':\n",
      "                        encoder_inputs_ = tf.nn.relu(encoder_inputs_)\n",
      "\n",
      "                    encoder_input_length_ = tf.to_int32(tf.ceil(encoder_input_length_ / strides[1]))\n",
      "\n",
      "                feature_size = encoder_inputs_.shape[2].value\n",
      "                channels = encoder_inputs_.shape[3].value\n",
      "                time_steps = tf.shape(encoder_inputs_)[1]\n",
      "\n",
      "                encoder_inputs_ = tf.reshape(encoder_inputs_, [batch_size, time_steps, feature_size * channels])\n",
      "                conv_outputs_ = encoder_inputs_\n",
      "\n",
      "                if encoder.conv_lstm_size:\n",
      "                    cell = BasicConvLSTMCell([feature_size, channels], encoder.conv_lstm_size, 1)\n",
      "                    encoder_inputs_, _ = tf.nn.bidirectional_dynamic_rnn(\n",
      "                        cell, cell, encoder_inputs_,\n",
      "                        dtype=tf.float32\n",
      "                    )\n",
      "                    encoder_inputs_ = tf.concat(encoder_inputs_, axis=2)\n",
      "\n",
      "            if encoder.convolutions:\n",
      "                if encoder.binary:\n",
      "                    raise NotImplementedError\n",
      "\n",
      "                pad = tf.nn.embedding_lookup(embeddings, utils.BOS_ID)\n",
      "                pad = tf.expand_dims(tf.expand_dims(pad, axis=0), axis=1)\n",
      "                pad = tf.tile(pad, [batch_size, 1, 1])\n",
      "\n",
      "                # Fully Character-Level NMT without Explicit Segmentation, Lee et al. 2016\n",
      "                inputs = []\n",
      "\n",
      "                for w, filter_size in enumerate(encoder.convolutions, 1):\n",
      "                    filter_ = get_variable('filter_{}'.format(w), [w, encoder.embedding_size, filter_size])\n",
      "\n",
      "                    if w > 1:\n",
      "                        right = (w - 1) // 2\n",
      "                        left = (w - 1) - right\n",
      "                        pad_right = tf.tile(pad, [1, right, 1])\n",
      "                        pad_left = tf.tile(pad, [1, left, 1])\n",
      "                        inputs_ = tf.concat([pad_left, encoder_inputs_, pad_right], axis=1)\n",
      "                    else:\n",
      "                        inputs_ = encoder_inputs_\n",
      "\n",
      "                    inputs_ = tf.nn.convolution(inputs_, filter=filter_, padding='VALID')\n",
      "                    inputs.append(inputs_)\n",
      "\n",
      "                encoder_inputs_ = tf.concat(inputs, axis=2)\n",
      "                # if encoder.convolution_activation.lower() == 'relu':\n",
      "                encoder_inputs_ = tf.nn.relu(encoder_inputs_)\n",
      "\n",
      "            if encoder.maxout_stride:\n",
      "                if encoder.binary:\n",
      "                    raise NotImplementedError\n",
      "\n",
      "                stride = encoder.maxout_stride\n",
      "                k = tf.to_int32(tf.ceil(time_steps / stride) * stride) - time_steps  # TODO: simpler\n",
      "                pad = tf.zeros([batch_size, k, tf.shape(encoder_inputs_)[2]])\n",
      "                encoder_inputs_ = tf.concat([encoder_inputs_, pad], axis=1)\n",
      "                encoder_inputs_ = tf.nn.pool(encoder_inputs_, window_shape=[stride], pooling_type='MAX',\n",
      "                                             padding='VALID', strides=[stride])\n",
      "                encoder_input_length_ = tf.to_int32(tf.ceil(encoder_input_length_ / stride))\n",
      "\n",
      "            if encoder.highway_layers:\n",
      "                x = encoder_inputs_\n",
      "                for j in range(encoder.highway_layers):\n",
      "                    size = x.shape[2].value\n",
      "\n",
      "                    with tf.variable_scope('highway_{}'.format(j + 1)):\n",
      "                        g = tf.layers.dense(x, size, activation=tf.nn.sigmoid, use_bias=True, name='g')\n",
      "                        y = tf.layers.dense(x, size, activation=tf.nn.relu, use_bias=True, name='y')\n",
      "                        x = g * y + (1 - g) * x\n",
      "\n",
      "                encoder_inputs_ = x\n",
      "\n",
      "            # Contrary to Theano's RNN implementation, states after the sequence length are zero\n",
      "            # (while Theano repeats last state)\n",
      "            inter_layer_keep_prob = None if not encoder.use_dropout else encoder.inter_layer_keep_prob\n",
      "\n",
      "            parameters = dict(\n",
      "                inputs=encoder_inputs_, sequence_length=encoder_input_length_,\n",
      "                dtype=tf.float32, parallel_iterations=encoder.parallel_iterations,\n",
      "                inter_layers=encoder.inter_layers, inter_layer_activation=encoder.inter_layer_activation,\n",
      "                batch_norm=encoder.batch_norm, inter_layer_keep_prob=inter_layer_keep_prob,\n",
      "                pervasive_dropout=encoder.pervasive_dropout, training=training\n",
      "            )\n",
      "\n",
      "            input_size = encoder_inputs_.get_shape()[2].value\n",
      "\n",
      "            def get_initial_state(name='initial_state'):\n",
      "                if encoder.train_initial_states:\n",
      "                    initial_state = get_variable(name, initializer=tf.zeros(cell_state_size))\n",
      "                    return tf.tile(tf.expand_dims(initial_state, axis=0), [batch_size, 1])\n",
      "                else:\n",
      "                    return None\n",
      "\n",
      "            if encoder.bidir:\n",
      "                rnn = lambda reuse: stack_bidirectional_dynamic_rnn(\n",
      "                    cells_fw=[get_cell(input_size if j == 0 else 2 * cell_output_size, reuse=reuse)\n",
      "                              for j in range(encoder.layers)],\n",
      "                    cells_bw=[get_cell(input_size if j == 0 else 2 * cell_output_size, reuse=reuse)\n",
      "                              for j in range(encoder.layers)],\n",
      "                    initial_states_fw=[get_initial_state('initial_state_fw')] * encoder.layers,\n",
      "                    initial_states_bw=[get_initial_state('initial_state_bw')] * encoder.layers,\n",
      "                    time_pooling=encoder.time_pooling, pooling_avg=encoder.pooling_avg,\n",
      "                    **parameters)\n",
      "\n",
      "                initializer = CellInitializer(encoder.cell_size) if encoder.orthogonal_init else None\n",
      "                with tf.variable_scope(tf.get_variable_scope(), initializer=initializer):\n",
      "                    try:\n",
      "                        encoder_outputs_, _, encoder_states_ = rnn(reuse=False)\n",
      "                    except ValueError:  # Multi-task scenario where we're reusing the same RNN parameters\n",
      "                        encoder_outputs_, _, encoder_states_ = rnn(reuse=True)\n",
      "            else:\n",
      "                if encoder.time_pooling or encoder.final_state == 'concat_last':\n",
      "                    raise NotImplementedError\n",
      "\n",
      "                if encoder.layers > 1:\n",
      "                    cell = MultiRNNCell([get_cell(input_size if j == 0 else cell_output_size)\n",
      "                                         for j in range(encoder.layers)])\n",
      "                    initial_state = (get_initial_state(),) * encoder.layers\n",
      "                else:\n",
      "                    cell = get_cell(input_size)\n",
      "                    initial_state = get_initial_state()\n",
      "\n",
      "                encoder_outputs_, encoder_states_ = auto_reuse(tf.nn.dynamic_rnn)(cell=cell,\n",
      "                                                                                  initial_state=initial_state,\n",
      "                                                                                  **parameters)\n",
      "\n",
      "            if encoder.time_pooling:\n",
      "                for stride in encoder.time_pooling[:encoder.layers - 1]:\n",
      "                    encoder_input_length_ = (encoder_input_length_ + stride - 1) // stride  # rounding up\n",
      "\n",
      "            last_backward = encoder_outputs_[:, 0, cell_output_size:]\n",
      "            indices = tf.stack([tf.range(batch_size), encoder_input_length_ - 1], axis=1)\n",
      "            last_forward = tf.gather_nd(encoder_outputs_[:, :, :cell_output_size], indices)\n",
      "            last_forward.set_shape([None, cell_output_size])\n",
      "\n",
      "            if encoder.final_state == 'concat_last':  # concats last states of all backward layers (full LSTM states)\n",
      "                encoder_state_ = tf.concat(encoder_states_, axis=1)\n",
      "            elif encoder.final_state == 'average':\n",
      "                mask = tf.sequence_mask(encoder_input_length_, maxlen=tf.shape(encoder_outputs_)[1], dtype=tf.float32)\n",
      "                mask = tf.expand_dims(mask, axis=2)\n",
      "                encoder_state_ = tf.reduce_sum(mask * encoder_outputs_, axis=1) / tf.reduce_sum(mask, axis=1)\n",
      "            elif encoder.final_state == 'average_inputs':\n",
      "                mask = tf.sequence_mask(encoder_input_length_, maxlen=tf.shape(encoder_inputs_)[1], dtype=tf.float32)\n",
      "                mask = tf.expand_dims(mask, axis=2)\n",
      "                encoder_state_ = tf.reduce_sum(mask * encoder_inputs_, axis=1) / tf.reduce_sum(mask, axis=1)\n",
      "            elif encoder.bidir and encoder.final_state == 'last_both':\n",
      "                encoder_state_ = tf.concat([last_forward, last_backward], axis=1)\n",
      "            elif encoder.final_state == 'none':\n",
      "                encoder_state_ = tf.zeros(shape=[batch_size, 0])\n",
      "            elif encoder.bidir and not encoder.final_state == 'last_forward':  # last backward hidden state\n",
      "                encoder_state_ = last_backward\n",
      "            else:  # last forward hidden state\n",
      "                encoder_state_ = last_forward\n",
      "\n",
      "            if encoder.bidir and encoder.bidir_projection:\n",
      "                encoder_outputs_ = dense(encoder_outputs_, cell_output_size, use_bias=False, name='bidir_projection')\n",
      "\n",
      "            if encoder.attend_inputs:\n",
      "                encoder_outputs.append(encoder_inputs_)\n",
      "            elif encoder.attend_both:\n",
      "                encoder_outputs.append(tf.concat([encoder_inputs_, encoder_outputs_], axis=2))\n",
      "            else:\n",
      "                encoder_outputs.append(encoder_outputs_)\n",
      "\n",
      "            encoder_states.append(encoder_state_)\n",
      "            new_encoder_input_length.append(encoder_input_length_)\n",
      "\n",
      "    encoder_state = tf.concat(encoder_states, 1)\n",
      "    return encoder_outputs, encoder_state, new_encoder_input_length\n",
      "\n",
      "\n",
      "def compute_energy(hidden, state, encoder, time=None, input_length=None, prev_weights=None, **kwargs):\n",
      "    batch_size = tf.shape(hidden)[0]\n",
      "    time_steps = tf.shape(hidden)[1]\n",
      "\n",
      "    if encoder.attn_keep_prob is not None:\n",
      "        state_noise_shape = [1, tf.shape(state)[1]] if encoder.pervasive_dropout else None\n",
      "        state = tf.nn.dropout(state, keep_prob=encoder.attn_keep_prob, noise_shape=state_noise_shape)\n",
      "        hidden_noise_shape = [1, 1, tf.shape(hidden)[2]] if encoder.pervasive_dropout else None\n",
      "        hidden = tf.nn.dropout(hidden, keep_prob=encoder.attn_keep_prob, noise_shape=hidden_noise_shape)\n",
      "\n",
      "    if encoder.mult_attn:\n",
      "        state = dense(state, encoder.attn_size, use_bias=False, name='state')\n",
      "        hidden = dense(hidden, encoder.attn_size, use_bias=False, name='hidden')\n",
      "        return tf.einsum('ijk,ik->ij', hidden, state)\n",
      "\n",
      "    y = dense(state, encoder.attn_size, use_bias=not encoder.layer_norm, name='W_a')\n",
      "    y = tf.expand_dims(y, axis=1)\n",
      "\n",
      "    if encoder.layer_norm:\n",
      "        y = tf.contrib.layers.layer_norm(y, scope='layer_norm_state')\n",
      "        hidden = tf.contrib.layers.layer_norm(hidden, center=False, scope='layer_norm_hidden')\n",
      "\n",
      "    y += dense(hidden, encoder.attn_size, use_bias=False, name='U_a')\n",
      "\n",
      "    if encoder.position_bias and input_length is not None and time is not None:\n",
      "        src_pos = tf.tile(tf.expand_dims(tf.range(time_steps), axis=0), [batch_size, 1])\n",
      "        trg_pos = tf.tile(tf.reshape(time, [1, 1]), [batch_size, time_steps])\n",
      "        src_len = tf.tile(tf.expand_dims(input_length, axis=1), [1, time_steps])  # - 1\n",
      "        pos_feats = tf.to_float(tf.stack([src_pos, trg_pos, src_len], axis=2))\n",
      "        pos_feats = tf.log(1 + pos_feats)\n",
      "\n",
      "        y += dense(pos_feats, encoder.attn_size, use_bias=False, name='P_a')\n",
      "\n",
      "    if encoder.attn_filters:\n",
      "        filter_shape = [encoder.attn_filter_length * 2 + 1, 1, 1, encoder.attn_filters]\n",
      "        filter_ = get_variable('filter', filter_shape)\n",
      "        prev_weights = tf.reshape(prev_weights, tf.stack([batch_size, time_steps, 1, 1]))\n",
      "        conv = tf.nn.conv2d(prev_weights, filter_, [1, 1, 1, 1], 'SAME')\n",
      "        conv = tf.squeeze(conv, axis=2)\n",
      "\n",
      "        y += dense(conv, encoder.attn_size, use_bias=False, name='C_a')\n",
      "\n",
      "    v = get_variable('v_a', [encoder.attn_size])\n",
      "    return tf.reduce_sum(v * tf.tanh(y), axis=2)\n",
      "\n",
      "\n",
      "def global_attention(state, hidden_states, encoder, encoder_input_length, scope=None, context=None, **kwargs):\n",
      "    with tf.variable_scope(scope or 'attention_{}'.format(encoder.name)):\n",
      "        if context is not None and encoder.use_context:\n",
      "            state = tf.concat([state, context], axis=1)\n",
      "\n",
      "        e = compute_energy(hidden_states, state, encoder, input_length=encoder_input_length, **kwargs)\n",
      "        mask = tf.sequence_mask(encoder_input_length, maxlen=tf.shape(hidden_states)[1], dtype=tf.float32)\n",
      "        e *= mask\n",
      "\n",
      "        if encoder.attn_norm_fun == 'none':\n",
      "            weights = e\n",
      "        elif encoder.attn_norm_fun == 'sigmoid':\n",
      "            weights = tf.nn.sigmoid(e)\n",
      "        elif encoder.attn_norm_fun == 'max':\n",
      "            weights = tf.one_hot(tf.argmax(e, -1), depth=tf.shape(e)[1])\n",
      "        else:\n",
      "            e -= tf.reduce_max(e, axis=1, keep_dims=True)\n",
      "            T = encoder.attn_temperature or 1.0\n",
      "            exp = tf.exp(e / T) * mask\n",
      "            weights = exp / tf.reduce_sum(exp, axis=-1, keep_dims=True)\n",
      "\n",
      "        weighted_average = tf.reduce_sum(tf.expand_dims(weights, 2) * hidden_states, axis=1)\n",
      "\n",
      "        return weighted_average, weights\n",
      "\n",
      "\n",
      "def no_attention(state, hidden_states, *args, **kwargs):\n",
      "    batch_size = tf.shape(state)[0]\n",
      "    weighted_average = tf.zeros(shape=tf.stack([batch_size, 0]))\n",
      "    weights = tf.zeros(shape=[batch_size, tf.shape(hidden_states)[1]])\n",
      "    return weighted_average, weights\n",
      "\n",
      "\n",
      "def average_attention(hidden_states, encoder_input_length, *args, **kwargs):\n",
      "    # attention with fixed weights (average of all hidden states)\n",
      "    lengths = tf.to_float(tf.expand_dims(encoder_input_length, axis=1))\n",
      "    mask = tf.sequence_mask(encoder_input_length, maxlen=tf.shape(hidden_states)[1])\n",
      "    weights = tf.to_float(mask) / lengths\n",
      "    weighted_average = tf.reduce_sum(hidden_states * tf.expand_dims(weights, axis=2), axis=1)\n",
      "    return weighted_average, weights\n",
      "\n",
      "\n",
      "def last_state_attention(hidden_states, encoder_input_length, *args, **kwargs):\n",
      "    weights = tf.one_hot(encoder_input_length - 1, tf.shape(hidden_states)[1])\n",
      "    weights = tf.to_float(weights)\n",
      "\n",
      "    weighted_average = tf.reduce_sum(hidden_states * tf.expand_dims(weights, axis=2), axis=1)\n",
      "    return weighted_average, weights\n",
      "\n",
      "\n",
      "def local_attention(state, hidden_states, encoder, encoder_input_length, pos=None, scope=None, context=None, **kwargs):\n",
      "    batch_size = tf.shape(state)[0]\n",
      "    attn_length = tf.shape(hidden_states)[1]\n",
      "\n",
      "    if context is not None and encoder.use_context:\n",
      "        state = tf.concat([state, context], axis=1)\n",
      "\n",
      "    state_size = state.get_shape()[1].value\n",
      "\n",
      "    with tf.variable_scope(scope or 'attention_{}'.format(encoder.name)):\n",
      "        encoder_input_length = tf.to_float(tf.expand_dims(encoder_input_length, axis=1))\n",
      "\n",
      "        if pos is not None:\n",
      "            pos = tf.reshape(pos, [-1, 1])\n",
      "            pos = tf.minimum(pos, encoder_input_length - 1)\n",
      "\n",
      "        if pos is not None and encoder.attn_window_size > 0:\n",
      "            # `pred_edits` scenario, where we know the aligned pos\n",
      "            # when the windows size is non-zero, we concatenate consecutive encoder states\n",
      "            # and map it to the right attention vector size.\n",
      "            weights = tf.to_float(tf.one_hot(tf.to_int32(tf.squeeze(pos, axis=1)), depth=attn_length))\n",
      "\n",
      "            weighted_average = []\n",
      "            for offset in range(-encoder.attn_window_size, encoder.attn_window_size + 1):\n",
      "                pos_ = pos + offset\n",
      "                pos_ = tf.minimum(pos_, encoder_input_length - 1)\n",
      "                pos_ = tf.maximum(pos_, 0)  # TODO: when pos is < 0, use <S> or </S>\n",
      "                weights_ = tf.to_float(tf.one_hot(tf.to_int32(tf.squeeze(pos_, axis=1)), depth=attn_length))\n",
      "                weighted_average_ = tf.reduce_sum(tf.expand_dims(weights_, axis=2) * hidden_states, axis=1)\n",
      "                weighted_average.append(weighted_average_)\n",
      "\n",
      "            weighted_average = tf.concat(weighted_average, axis=1)\n",
      "            weighted_average = dense(weighted_average, encoder.attn_size)\n",
      "        elif pos is not None:\n",
      "            weights = tf.to_float(tf.one_hot(tf.to_int32(tf.squeeze(pos, axis=1)), depth=attn_length))\n",
      "            weighted_average = tf.reduce_sum(tf.expand_dims(weights, axis=2) * hidden_states, axis=1)\n",
      "        else:\n",
      "            # Local attention of Luong et al. (http://arxiv.org/abs/1508.04025)\n",
      "            wp = get_variable('Wp', [state_size, state_size])\n",
      "            vp = get_variable('vp', [state_size, 1])\n",
      "\n",
      "            pos = tf.nn.sigmoid(tf.matmul(tf.nn.tanh(tf.matmul(state, wp)), vp))\n",
      "            pos = tf.floor(encoder_input_length * pos)\n",
      "            pos = tf.reshape(pos, [-1, 1])\n",
      "            pos = tf.minimum(pos, encoder_input_length - 1)\n",
      "\n",
      "            idx = tf.tile(tf.to_float(tf.range(attn_length)), tf.stack([batch_size]))\n",
      "            idx = tf.reshape(idx, [-1, attn_length])\n",
      "\n",
      "            low = pos - encoder.attn_window_size\n",
      "            high = pos + encoder.attn_window_size\n",
      "\n",
      "            mlow = tf.to_float(idx < low)\n",
      "            mhigh = tf.to_float(idx > high)\n",
      "            m = mlow + mhigh\n",
      "            m += tf.to_float(idx >= encoder_input_length)\n",
      "\n",
      "            mask = tf.to_float(tf.equal(m, 0.0))\n",
      "\n",
      "            e = compute_energy(hidden_states, state, encoder, input_length=encoder_input_length, **kwargs)\n",
      "            weights = softmax(e, mask=mask)\n",
      "\n",
      "            if encoder.attn_window_size > 0:\n",
      "                sigma = encoder.attn_window_size / 2\n",
      "                numerator = -tf.pow((idx - pos), tf.convert_to_tensor(2, dtype=tf.float32))\n",
      "                div = tf.truediv(numerator, 2 * sigma ** 2)\n",
      "\n",
      "                weights *= tf.exp(div)  # result of the truncated normal distribution\n",
      "                # normalize to keep a probability distribution\n",
      "                # weights /= (tf.reduce_sum(weights, axis=1, keep_dims=True) + 10e-12)\n",
      "\n",
      "            weighted_average = tf.reduce_sum(tf.expand_dims(weights, axis=2) * hidden_states, axis=1)\n",
      "\n",
      "        return weighted_average, weights\n",
      "\n",
      "\n",
      "def attention(encoder, scope=None, **kwargs):\n",
      "    attention_functions = {\n",
      "        'global': global_attention,\n",
      "        'local': local_attention,\n",
      "        'none': no_attention,\n",
      "        'average': average_attention,\n",
      "        'last_state': last_state_attention\n",
      "    }\n",
      "    attention_function = attention_functions.get(encoder.attention_type, global_attention)\n",
      "\n",
      "    context_vectors = []\n",
      "    weights = []\n",
      "\n",
      "    attn_heads = encoder.attn_heads or 1\n",
      "    scope = scope or 'attention_{}'.format(encoder.name)\n",
      "    for i in range(attn_heads):\n",
      "        scope_ = scope if i == 0 else scope + '_{}'.format(i + 1)\n",
      "\n",
      "        context_vector, weights_ = attention_function(encoder=encoder, scope=scope_, **kwargs)\n",
      "        context_vectors.append(context_vector)\n",
      "        weights.append(weights_)\n",
      "\n",
      "    context_vector = tf.concat(context_vectors, axis=-1)\n",
      "    weights = sum(weights) / len(weights)\n",
      "\n",
      "    if encoder.attn_mapping:\n",
      "        with tf.variable_scope(scope):\n",
      "            context_vector = dense(context_vector, encoder.attn_mapping, use_bias=False, name='output')\n",
      "\n",
      "    return context_vector, weights\n",
      "\n",
      "\n",
      "def multi_attention(state, hidden_states, encoders, encoder_input_length, pos=None, aggregation_method='sum',\n",
      "                    prev_weights=None, **kwargs):\n",
      "    attns = []\n",
      "    weights = []\n",
      "\n",
      "    context_vector = None\n",
      "    for i, (hidden, encoder, input_length) in enumerate(zip(hidden_states, encoders, encoder_input_length)):\n",
      "        pos_ = pos[i] if pos is not None else None\n",
      "        prev_weights_ = prev_weights[i] if prev_weights is not None else None\n",
      "\n",
      "        hidden = beam_search.resize_like(hidden, state)\n",
      "        input_length = beam_search.resize_like(input_length, state)\n",
      "\n",
      "        context_vector, weights_ = attention(state=state, hidden_states=hidden, encoder=encoder,\n",
      "                                             encoder_input_length=input_length, pos=pos_, context=context_vector,\n",
      "                                             prev_weights=prev_weights_, **kwargs)\n",
      "        attns.append(context_vector)\n",
      "        weights.append(weights_)\n",
      "\n",
      "    if aggregation_method == 'sum':\n",
      "        context_vector = tf.reduce_sum(tf.stack(attns, axis=2), axis=2)\n",
      "    else:\n",
      "        context_vector = tf.concat(attns, axis=1)\n",
      "\n",
      "    return context_vector, weights\n",
      "\n",
      "\n",
      "def attention_decoder(decoder_inputs, initial_state, attention_states, encoders, decoder, encoder_input_length,\n",
      "                      feed_previous=0.0, align_encoder_id=0, feed_argmax=True, training=True, **kwargs):\n",
      "    \"\"\"\n",
      "    :param decoder_inputs: int32 tensor of shape (batch_size, output_length)\n",
      "    :param initial_state: initial state of the decoder (usually the final state of the encoder),\n",
      "      as a float32 tensor of shape (batch_size, initial_state_size). This state is mapped to the\n",
      "      correct state size for the decoder.\n",
      "    :param attention_states: list of tensors of shape (batch_size, input_length, encoder_cell_size),\n",
      "      the hidden states of the encoder(s) (one tensor for each encoder).\n",
      "    :param encoders: configuration of the encoders\n",
      "    :param decoder: configuration of the decoder\n",
      "    :param encoder_input_length: list of int32 tensors of shape (batch_size,), tells for each encoder,\n",
      "     the true length of each sequence in the batch (sequences in the same batch are padded to all have the same\n",
      "     length).\n",
      "    :param feed_previous: scalar tensor corresponding to the probability to use previous decoder output\n",
      "      instead of the ground truth as input for the decoder (1 when decoding, between 0 and 1 when training)\n",
      "    :param feed_argmax: boolean tensor, when True the greedy decoder outputs the word with the highest\n",
      "    probability (argmax). When False, it samples a word from the probability distribution (softmax).\n",
      "    :param align_encoder_id: outputs attention weights for this encoder. Also used when predicting edit operations\n",
      "    (pred_edits), to specifify which encoder reads the sequence to post-edit (MT).\n",
      "\n",
      "    :return:\n",
      "      outputs of the decoder as a tensor of shape (batch_size, output_length, decoder_cell_size)\n",
      "      attention weights as a tensor of shape (output_length, encoders, batch_size, input_length)\n",
      "    \"\"\"\n",
      "\n",
      "    cell_output_size, cell_state_size = get_state_size(decoder.cell_type, decoder.cell_size,\n",
      "                                                       decoder.lstm_proj_size, decoder.layers)\n",
      "    utils.log(\"{} {}\".format(cell_output_size, cell_state_size))\n",
      "\n",
      "    assert not decoder.pred_maxout_layer or cell_output_size % 2 == 0, 'cell size must be a multiple of 2'\n",
      "\n",
      "    if decoder.use_lstm is False:\n",
      "        decoder.cell_type = 'GRU'\n",
      "\n",
      "    embedding_shape = [decoder.vocab_size, decoder.embedding_size]\n",
      "    weight_scale = decoder.embedding_weight_scale or decoder.weight_scale\n",
      "    if weight_scale is None:\n",
      "        initializer = None  # FIXME\n",
      "    elif decoder.embedding_initializer == 'uniform' or (decoder.embedding_initializer is None\n",
      "                                                        and decoder.initializer == 'uniform'):\n",
      "        initializer = tf.random_uniform_initializer(minval=-weight_scale, maxval=weight_scale)\n",
      "    else:\n",
      "        initializer = tf.random_normal_initializer(stddev=weight_scale)\n",
      "\n",
      "    with tf.device('/cpu:0'):\n",
      "        if decoder.name == \"edits\":\n",
      "            embedding_name = \"mt\"\n",
      "        else:\n",
      "            embedding_name = decoder.name\n",
      "        embedding = get_variable('embedding_{}'.format(embedding_name), shape=embedding_shape, initializer=initializer)\n",
      "\n",
      "    input_shape = tf.shape(decoder_inputs)\n",
      "    batch_size = input_shape[0]\n",
      "    time_steps = input_shape[1]\n",
      "\n",
      "    scope_name = 'decoder_{}'.format(decoder.name)\n",
      "    scope_name += '/' + '_'.join(encoder.name for encoder in encoders)\n",
      "\n",
      "    def embed(input_):\n",
      "        embedded_input = tf.nn.embedding_lookup(embedding, input_)\n",
      "\n",
      "        if decoder.use_dropout and decoder.word_keep_prob is not None:\n",
      "            noise_shape = [1, 1] if decoder.pervasive_dropout else [tf.shape(input_)[0], 1]\n",
      "            embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder.word_keep_prob, noise_shape=noise_shape)\n",
      "        if decoder.use_dropout and decoder.embedding_keep_prob is not None:\n",
      "            size = tf.shape(embedded_input)[1]\n",
      "            noise_shape = [1, size] if decoder.pervasive_dropout else [tf.shape(input_)[0], size]\n",
      "            embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder.embedding_keep_prob,\n",
      "                                           noise_shape=noise_shape)\n",
      "\n",
      "        return embedded_input\n",
      "\n",
      "    def get_cell(input_size=None, reuse=False):\n",
      "        cells = []\n",
      "\n",
      "        for j in range(decoder.layers):\n",
      "            input_size_ = input_size if j == 0 else cell_output_size\n",
      "\n",
      "            if decoder.cell_type.lower() == 'lstm':\n",
      "                cell = CellWrapper(BasicLSTMCell(decoder.cell_size, reuse=reuse))\n",
      "            elif decoder.cell_type.lower() == 'plstm':\n",
      "                cell = PLSTM(decoder.cell_size, reuse=reuse, fact_size=decoder.lstm_fact_size,\n",
      "                             proj_size=decoder.lstm_proj_size)\n",
      "            elif decoder.cell_type.lower() == 'dropoutgru':\n",
      "                cell = DropoutGRUCell(decoder.cell_size, reuse=reuse, layer_norm=decoder.layer_norm,\n",
      "                                      input_size=input_size_, input_keep_prob=decoder.rnn_input_keep_prob,\n",
      "                                      state_keep_prob=decoder.rnn_state_keep_prob)\n",
      "            else:\n",
      "                cell = GRUCell(decoder.cell_size, reuse=reuse, layer_norm=decoder.layer_norm)\n",
      "\n",
      "            if decoder.use_dropout and decoder.cell_type.lower() != 'dropoutgru':\n",
      "                cell = DropoutWrapper(cell, input_keep_prob=decoder.rnn_input_keep_prob,\n",
      "                                      output_keep_prob=decoder.rnn_output_keep_prob,\n",
      "                                      state_keep_prob=decoder.rnn_state_keep_prob,\n",
      "                                      variational_recurrent=decoder.pervasive_dropout,\n",
      "                                      dtype=tf.float32, input_size=input_size_)\n",
      "            cells.append(cell)\n",
      "\n",
      "        if len(cells) == 1:\n",
      "            return cells[0]\n",
      "        else:\n",
      "            return CellWrapper(MultiRNNCell(cells))\n",
      "\n",
      "    def look(time, state, input_, prev_weights=None, pos=None, context=None):\n",
      "        prev_weights_ = [prev_weights if i == align_encoder_id else None for i in range(len(encoders))]\n",
      "        pos_ = None\n",
      "        if decoder.pred_edits:\n",
      "            pos_ = [pos if i == align_encoder_id else None for i in range(len(encoders))]\n",
      "        if decoder.attn_prev_word:\n",
      "            state = tf.concat([state, input_], axis=1)\n",
      "\n",
      "        if decoder.attn_prev_attn and context is not None:\n",
      "            state = tf.concat([state, context], axis=1)\n",
      "\n",
      "        if decoder.hidden_state_scaling:\n",
      "            attention_states_ = [states * decoder.hidden_state_scaling for states in attention_states]\n",
      "        else:\n",
      "            attention_states_ = attention_states\n",
      "\n",
      "        parameters = dict(hidden_states=attention_states_, encoder_input_length=encoder_input_length,\n",
      "                          encoders=encoders, aggregation_method=decoder.aggregation_method)\n",
      "        context, new_weights = multi_attention(state, time=time, pos=pos_, prev_weights=prev_weights_, **parameters)\n",
      "\n",
      "        if decoder.context_mapping:\n",
      "            with tf.variable_scope(scope_name):\n",
      "                activation = tf.nn.tanh if decoder.context_mapping_activation == 'tanh' else None\n",
      "                use_bias = not decoder.context_mapping_no_bias\n",
      "                context = dense(context, decoder.context_mapping, use_bias=use_bias, activation=activation,\n",
      "                                name='context_mapping')\n",
      "\n",
      "        return context, new_weights[align_encoder_id]\n",
      "\n",
      "    def update(state, input_, context=None, symbol=None):\n",
      "        if context is not None and decoder.rnn_feed_attn:\n",
      "            input_ = tf.concat([input_, context], axis=1)\n",
      "        input_size = input_.get_shape()[1].value\n",
      "\n",
      "        initializer = CellInitializer(decoder.cell_size) if decoder.orthogonal_init else None\n",
      "        with tf.variable_scope(tf.get_variable_scope(), initializer=initializer):\n",
      "            try:\n",
      "                output, new_state = get_cell(input_size)(input_, state)\n",
      "            except ValueError:  # auto_reuse doesn't work with LSTM cells\n",
      "                output, new_state = get_cell(input_size, reuse=True)(input_, state)\n",
      "\n",
      "        if decoder.skip_update and decoder.pred_edits and symbol is not None:\n",
      "            is_del = tf.equal(symbol, utils.DEL_ID)\n",
      "            new_state = tf.where(is_del, state, new_state)\n",
      "\n",
      "        if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "            output = new_state\n",
      "\n",
      "        return output, new_state\n",
      "\n",
      "    def update_pos(pos, symbol, max_pos=None):\n",
      "        if not decoder.pred_edits:\n",
      "            return pos\n",
      "\n",
      "        is_keep = tf.equal(symbol, utils.KEEP_ID)\n",
      "        is_del = tf.equal(symbol, utils.DEL_ID)\n",
      "        is_not_ins = tf.logical_or(is_keep, is_del)\n",
      "\n",
      "        pos = beam_search.resize_like(pos, symbol)\n",
      "        max_pos = beam_search.resize_like(max_pos, symbol)\n",
      "\n",
      "        pos += tf.to_float(is_not_ins)\n",
      "        if max_pos is not None:\n",
      "            pos = tf.minimum(pos, tf.to_float(max_pos))\n",
      "        return pos\n",
      "\n",
      "    def generate(state, input_, context):\n",
      "        if decoder.pred_use_lstm_state is False:  # for back-compatibility\n",
      "            state = state[:, -cell_output_size:]\n",
      "\n",
      "        projection_input = [state, context]\n",
      "        if decoder.use_previous_word:\n",
      "            projection_input.insert(1, input_)  # for back-compatibility\n",
      "\n",
      "        output_ = tf.concat(projection_input, axis=1)\n",
      "\n",
      "        if decoder.pred_deep_layer:\n",
      "            deep_layer_size = decoder.pred_deep_layer_size or decoder.embedding_size\n",
      "            if decoder.layer_norm:\n",
      "                output_ = dense(output_, deep_layer_size, use_bias=False, name='deep_output')\n",
      "                output_ = tf.contrib.layers.layer_norm(output_, activation_fn=tf.nn.tanh, scope='output_layer_norm')\n",
      "            else:\n",
      "                output_ = dense(output_, deep_layer_size, activation=tf.tanh, use_bias=True, name='deep_output')\n",
      "\n",
      "            if decoder.use_dropout:\n",
      "                size = tf.shape(output_)[1]\n",
      "                noise_shape = [1, size] if decoder.pervasive_dropout else None\n",
      "                output_ = tf.nn.dropout(output_, keep_prob=decoder.deep_layer_keep_prob, noise_shape=noise_shape)\n",
      "        else:\n",
      "            if decoder.pred_maxout_layer:\n",
      "                maxout_size = decoder.maxout_size or cell_output_size\n",
      "                output_ = dense(output_, maxout_size, use_bias=True, name='maxout')\n",
      "                if decoder.old_maxout:  # for back-compatibility with old models\n",
      "                    output_ = tf.nn.pool(tf.expand_dims(output_, axis=2), window_shape=[2], pooling_type='MAX',\n",
      "                                         padding='SAME', strides=[2])\n",
      "                    output_ = tf.squeeze(output_, axis=2)\n",
      "                else:\n",
      "                    output_ = tf.maximum(*tf.split(output_, num_or_size_splits=2, axis=1))\n",
      "\n",
      "            if decoder.pred_embed_proj:\n",
      "                # intermediate projection to embedding size (before projecting to vocabulary size)\n",
      "                # this is useful to reduce the number of parameters, and\n",
      "                # to use the output embeddings for output projection (tie_embeddings parameter)\n",
      "                output_ = dense(output_, decoder.embedding_size, use_bias=False, name='softmax0')\n",
      "\n",
      "        if decoder.tie_embeddings and (decoder.pred_embed_proj or decoder.pred_deep_layer):\n",
      "            bias = get_variable('softmax1/bias', shape=[decoder.vocab_size])\n",
      "            output_ = tf.matmul(output_, tf.transpose(embedding)) + bias\n",
      "        else:\n",
      "            output_ = dense(output_, decoder.vocab_size, use_bias=True, name='softmax1')\n",
      "        return output_\n",
      "\n",
      "    if decoder.use_dropout:  # FIXME: why no pervasive dropout here?\n",
      "        initial_state = tf.nn.dropout(initial_state, keep_prob=decoder.initial_state_keep_prob)\n",
      "\n",
      "    with tf.variable_scope(scope_name):\n",
      "        activation_fn = None if decoder.initial_state == 'linear' else tf.nn.tanh\n",
      "        if decoder.initial_state == 'trained':\n",
      "            initial_state = get_variable(shape=[cell_state_size], name='initial_state')\n",
      "            initial_state = tf.tile(tf.expand_dims(initial_state, axis=0), [batch_size, 1])\n",
      "        elif decoder.initial_state == 'zero':\n",
      "            initial_state = tf.zeros(shape=[batch_size, cell_state_size])\n",
      "        elif decoder.layer_norm:\n",
      "            initial_state = dense(initial_state, cell_state_size, use_bias=False, name='initial_state_projection')\n",
      "            initial_state = tf.contrib.layers.layer_norm(initial_state, activation_fn=activation_fn,\n",
      "                                                         scope='initial_state_layer_norm')\n",
      "        else:\n",
      "            initial_state = dense(initial_state, cell_state_size, use_bias=True, name='initial_state_projection',\n",
      "                                  activation=activation_fn)\n",
      "\n",
      "    if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "        initial_output = initial_state\n",
      "    else:\n",
      "        # Last layer's state is the right-most part. Output is the left-most part of an LSTM's state.\n",
      "        initial_output = initial_state[:, -cell_output_size:]\n",
      "\n",
      "    time = tf.constant(0, dtype=tf.int32, name='time')\n",
      "    outputs = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    samples = tf.TensorArray(dtype=tf.int64, size=time_steps)\n",
      "    inputs = tf.TensorArray(dtype=tf.int64, size=time_steps).unstack(tf.to_int64(tf.transpose(decoder_inputs)))\n",
      "\n",
      "    states = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    weights = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    attns = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "\n",
      "    initial_symbol = inputs.read(0)  # first symbol is BOS\n",
      "    initial_input = embed(initial_symbol)\n",
      "    initial_pos = tf.zeros([batch_size], tf.float32)\n",
      "    initial_weights = tf.zeros(tf.shape(attention_states[align_encoder_id])[:2])\n",
      "    zero_context = tf.zeros(shape=tf.shape(attention_states[align_encoder_id][:, 0]))  # FIXME\n",
      "\n",
      "    with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "        initial_context, _ = look(0, initial_output, initial_input, pos=initial_pos, prev_weights=initial_weights,\n",
      "                                  context=zero_context)\n",
      "    initial_data = tf.concat([initial_state, initial_context, tf.expand_dims(initial_pos, axis=1), initial_weights],\n",
      "                             axis=1)\n",
      "    context_size = initial_context.shape[1].value\n",
      "\n",
      "    def get_logits(state, ids, time):  # for beam-search decoding\n",
      "        with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "            state, context, pos, prev_weights = tf.split(state, [cell_state_size, context_size, 1, -1], axis=1)\n",
      "            input_ = embed(ids)\n",
      "\n",
      "            pos = tf.squeeze(pos, axis=1)\n",
      "            pos = tf.cond(tf.equal(time, 0),\n",
      "                          lambda: pos,\n",
      "                          lambda: update_pos(pos, ids, encoder_input_length[align_encoder_id]))\n",
      "\n",
      "            if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "                output = state\n",
      "            else:\n",
      "                # Output is always the right-most part of the state (even with multi-layer RNNs)\n",
      "                # However, this only works at test time, because different dropout operations can be used\n",
      "                # on state and output.\n",
      "                output = state[:, -cell_output_size:]\n",
      "\n",
      "            if decoder.conditional_rnn:\n",
      "                with tf.variable_scope('conditional_1'):\n",
      "                    output, state = update(state, input_)\n",
      "            elif decoder.update_first:\n",
      "                output, state = update(state, input_, None, ids)\n",
      "            elif decoder.generate_first:\n",
      "                output, state = tf.cond(tf.equal(time, 0),\n",
      "                                        lambda: (output, state),\n",
      "                                        lambda: update(state, input_, context, ids))\n",
      "\n",
      "            context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context)\n",
      "\n",
      "            if decoder.conditional_rnn:\n",
      "                with tf.variable_scope('conditional_2'):\n",
      "                    output, state = update(state, context)\n",
      "            elif not decoder.generate_first:\n",
      "                output, state = update(state, input_, context, ids)\n",
      "\n",
      "            logits = generate(output, input_, context)\n",
      "\n",
      "            pos = tf.expand_dims(pos, axis=1)\n",
      "            state = tf.concat([state, context, pos, new_weights], axis=1)\n",
      "            return state, logits\n",
      "\n",
      "    def _time_step(time, input_, input_symbol, pos, state, output, outputs, states, weights, attns, prev_weights,\n",
      "                   samples, context):\n",
      "        if decoder.conditional_rnn:\n",
      "            with tf.variable_scope('conditional_1'):\n",
      "                output, state = update(state, input_)\n",
      "        elif decoder.update_first:\n",
      "            output, state = update(state, input_, None, input_symbol)\n",
      "\n",
      "        context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context)\n",
      "\n",
      "        if decoder.conditional_rnn:\n",
      "            with tf.variable_scope('conditional_2'):\n",
      "                output, state = update(state, context)\n",
      "        elif not decoder.generate_first:\n",
      "            output, state = update(state, input_, context, input_symbol)\n",
      "\n",
      "        output_ = generate(output, input_, context)\n",
      "\n",
      "        argmax = lambda: tf.argmax(output_, 1)\n",
      "        target = lambda: inputs.read(time + 1)\n",
      "        softmax = lambda: tf.squeeze(tf.multinomial(tf.log(tf.nn.softmax(output_)), num_samples=1),\n",
      "                                     axis=1)\n",
      "\n",
      "        use_target = tf.logical_and(time < time_steps - 1, tf.random_uniform([]) >= feed_previous)\n",
      "        predicted_symbol = tf.case([\n",
      "            (use_target, target),\n",
      "            (tf.logical_not(feed_argmax), softmax)],\n",
      "            default=argmax)  # default case is useful for beam-search\n",
      "\n",
      "        predicted_symbol.set_shape([None])\n",
      "        predicted_symbol = tf.stop_gradient(predicted_symbol)\n",
      "\n",
      "        input_ = embed(predicted_symbol)\n",
      "        pos = update_pos(pos, predicted_symbol, encoder_input_length[align_encoder_id])\n",
      "\n",
      "        samples = samples.write(time, predicted_symbol)\n",
      "        attns = attns.write(time, context)\n",
      "        weights = weights.write(time, new_weights)\n",
      "        states = states.write(time, state)\n",
      "        outputs = outputs.write(time, output_)\n",
      "\n",
      "        if not decoder.conditional_rnn and not decoder.update_first and decoder.generate_first:\n",
      "            output, state = update(state, input_, context, predicted_symbol)\n",
      "\n",
      "        return (time + 1, input_, predicted_symbol, pos, state, output, outputs, states, weights, attns, new_weights,\n",
      "                samples, context)\n",
      "\n",
      "    with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "        _, _, _, new_pos, new_state, _, outputs, states, weights, attns, new_weights, samples, _ = tf.while_loop(\n",
      "            cond=lambda time, *_: time < time_steps,\n",
      "            body=_time_step,\n",
      "            loop_vars=(time, initial_input, initial_symbol, initial_pos, initial_state, initial_output, outputs,\n",
      "                       weights, states, attns, initial_weights, samples, initial_context),\n",
      "            parallel_iterations=decoder.parallel_iterations,\n",
      "            swap_memory=decoder.swap_memory)\n",
      "\n",
      "    outputs = outputs.stack()\n",
      "    weights = weights.stack()  # batch_size, encoders, output time, input time\n",
      "    states = states.stack()\n",
      "    attns = attns.stack()\n",
      "    samples = samples.stack()\n",
      "\n",
      "    # put batch_size as first dimension\n",
      "    outputs = tf.transpose(outputs, perm=(1, 0, 2))\n",
      "    weights = tf.transpose(weights, perm=(1, 0, 2))\n",
      "    states = tf.transpose(states, perm=(1, 0, 2))\n",
      "    attns = tf.transpose(attns, perm=(1, 0, 2))\n",
      "    samples = tf.transpose(samples)\n",
      "\n",
      "    return outputs, weights, states, attns, samples, get_logits, initial_data\n",
      "\n",
      "\n",
      "def attention_execution_decoder(decoder_inputs, initial_state, attention_states, encoders, decoder,\n",
      "                                encoder_input_length,\n",
      "                                feed_previous=0.0, align_encoder_id=0, feed_argmax=True, training=True, **kwargs):\n",
      "    \"\"\"\n",
      "    :param decoder_inputs: int32 tensor of shape (batch_size, output_length)\n",
      "    :param initial_state: initial state of the decoder (usually the final state of the encoder),\n",
      "      as a float32 tensor of shape (batch_size, initial_state_size). This state is mapped to the\n",
      "      correct state size for the decoder.\n",
      "    :param attention_states: list of tensors of shape (batch_size, input_length, encoder_cell_size),\n",
      "      the hidden states of the encoder(s) (one tensor for each encoder).\n",
      "    :param encoders: configuration of the encoders\n",
      "    :param decoder: configuration of the decoder\n",
      "    :param encoder_input_length: list of int32 tensors of shape (batch_size,), tells for each encoder,\n",
      "     the true length of each sequence in the batch (sequences in the same batch are padded to all have the same\n",
      "     length).\n",
      "    :param feed_previous: scalar tensor corresponding to the probability to use previous decoder output\n",
      "      instead of the ground truth as input for the decoder (1 when decoding, between 0 and 1 when training)\n",
      "    :param feed_argmax: boolean tensor, when True the greedy decoder outputs the word with the highest\n",
      "    probability (argmax). When False, it samples a word from the probability distribution (softmax).\n",
      "    :param align_encoder_id: outputs attention weights for this encoder. Also used when predicting edit operations\n",
      "    (pred_edits), to specifify which encoder reads the sequence to post-edit (MT).\n",
      "\n",
      "    :return:\n",
      "      outputs of the decoder as a tensor of shape (batch_size, output_length, decoder_cell_size)\n",
      "      attention weights as a tensor of shape (output_length, encoders, batch_size, input_length)\n",
      "    \"\"\"\n",
      "\n",
      "    cell_output_size, cell_state_size = get_state_size(decoder.cell_type, decoder.cell_size,\n",
      "                                                       decoder.lstm_proj_size, decoder.layers)\n",
      "\n",
      "    assert not decoder.pred_maxout_layer or cell_output_size % 2 == 0, 'cell size must be a multiple of 2'\n",
      "\n",
      "    if decoder.use_lstm is False:\n",
      "        decoder.cell_type = 'GRU'\n",
      "\n",
      "    embedding_shape = [decoder.vocab_size, decoder.embedding_size]\n",
      "    weight_scale = decoder.embedding_weight_scale or decoder.weight_scale\n",
      "    if weight_scale is None:\n",
      "        initializer = None  # FIXME\n",
      "    elif decoder.embedding_initializer == 'uniform' or (decoder.embedding_initializer is None\n",
      "                                                        and decoder.initializer == 'uniform'):\n",
      "        initializer = tf.random_uniform_initializer(minval=-weight_scale, maxval=weight_scale)\n",
      "    else:\n",
      "        initializer = tf.random_normal_initializer(stddev=weight_scale)\n",
      "\n",
      "    with tf.device('/cpu:0'):\n",
      "        if decoder.name == \"edits\":\n",
      "            embedding_name = \"mt\"\n",
      "        else:\n",
      "            embedding_name = decoder.name\n",
      "        embedding = get_variable('embedding_{}'.format(embedding_name), shape=embedding_shape, initializer=initializer)\n",
      "\n",
      "    input_shape = tf.shape(decoder_inputs)\n",
      "    batch_size = input_shape[0]\n",
      "    time_steps = input_shape[1]\n",
      "\n",
      "    scope_name = 'decoder_{}'.format(decoder.name)\n",
      "    scope_name += '/' + '_'.join(encoder.name for encoder in encoders)\n",
      "\n",
      "    def embed(input_):\n",
      "        embedded_input = tf.nn.embedding_lookup(embedding, input_)\n",
      "\n",
      "        if decoder.use_dropout and decoder.word_keep_prob is not None:\n",
      "            noise_shape = [1, 1] if decoder.pervasive_dropout else [tf.shape(input_)[0], 1]\n",
      "            embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder.word_keep_prob, noise_shape=noise_shape)\n",
      "        if decoder.use_dropout and decoder.embedding_keep_prob is not None:\n",
      "            size = tf.shape(embedded_input)[1]\n",
      "            noise_shape = [1, size] if decoder.pervasive_dropout else [tf.shape(input_)[0], size]\n",
      "            embedded_input = tf.nn.dropout(embedded_input, keep_prob=decoder.embedding_keep_prob,\n",
      "                                           noise_shape=noise_shape)\n",
      "\n",
      "        return embedded_input\n",
      "\n",
      "    def get_cell(input_size=None, reuse=False):\n",
      "        cells = []\n",
      "\n",
      "        for j in range(decoder.layers):\n",
      "            input_size_ = input_size if j == 0 else cell_output_size\n",
      "\n",
      "            if decoder.cell_type.lower() == 'lstm':\n",
      "                cell = CellWrapper(BasicLSTMCell(decoder.cell_size, reuse=reuse))\n",
      "            elif decoder.cell_type.lower() == 'plstm':\n",
      "                cell = PLSTM(decoder.cell_size, reuse=reuse, fact_size=decoder.lstm_fact_size,\n",
      "                             proj_size=decoder.lstm_proj_size)\n",
      "            elif decoder.cell_type.lower() == 'dropoutgru':\n",
      "                cell = DropoutGRUCell(decoder.cell_size, reuse=reuse, layer_norm=decoder.layer_norm,\n",
      "                                      input_size=input_size_, input_keep_prob=decoder.rnn_input_keep_prob,\n",
      "                                      state_keep_prob=decoder.rnn_state_keep_prob)\n",
      "            else:\n",
      "                cell = GRUCell(decoder.cell_size, reuse=reuse, layer_norm=decoder.layer_norm)\n",
      "\n",
      "            if decoder.use_dropout and decoder.cell_type.lower() != 'dropoutgru':\n",
      "                cell = DropoutWrapper(cell, input_keep_prob=decoder.rnn_input_keep_prob,\n",
      "                                      output_keep_prob=decoder.rnn_output_keep_prob,\n",
      "                                      state_keep_prob=decoder.rnn_state_keep_prob,\n",
      "                                      variational_recurrent=decoder.pervasive_dropout,\n",
      "                                      dtype=tf.float32, input_size=input_size_)\n",
      "            cells.append(cell)\n",
      "\n",
      "        if len(cells) == 1:\n",
      "            return cells[0]\n",
      "        else:\n",
      "            return CellWrapper(MultiRNNCell(cells))\n",
      "\n",
      "    def look(time, state, input_, prev_weights=None, pos=None, context=None):\n",
      "        prev_weights_ = [prev_weights if i == align_encoder_id else None for i in range(len(encoders))]\n",
      "        pos_ = None\n",
      "        if decoder.pred_edits:\n",
      "            pos_ = [pos if i == align_encoder_id else None for i in range(len(encoders))]\n",
      "        if decoder.attn_prev_word:\n",
      "            state = tf.concat([state, input_], axis=1)\n",
      "\n",
      "        if decoder.attn_prev_attn and context is not None:\n",
      "            state = tf.concat([state, context], axis=1)\n",
      "\n",
      "        # attention_states = encoder hidden states\n",
      "        if decoder.hidden_state_scaling:\n",
      "            attention_states_ = [states * decoder.hidden_state_scaling for states in attention_states]\n",
      "        else:\n",
      "            attention_states_ = attention_states\n",
      "\n",
      "        parameters = dict(hidden_states=attention_states_, encoder_input_length=encoder_input_length,\n",
      "                          encoders=encoders, aggregation_method=decoder.aggregation_method)\n",
      "        context, new_weights = multi_attention(state, time=time, pos=pos_, prev_weights=prev_weights_, **parameters)\n",
      "\n",
      "        if decoder.context_mapping:\n",
      "            with tf.variable_scope(scope_name):\n",
      "                activation = tf.nn.tanh if decoder.context_mapping_activation == 'tanh' else None\n",
      "                use_bias = not decoder.context_mapping_no_bias\n",
      "                context = dense(context, decoder.context_mapping, use_bias=use_bias, activation=activation,\n",
      "                                name='context_mapping')\n",
      "\n",
      "        return context, new_weights[align_encoder_id]\n",
      "\n",
      "    def update(state, input_, context=None, symbol=None, lm_state=None):\n",
      "        # TODO: add g_i (last hidden state of RNN LM)\n",
      "        if context is not None and lm_state is not None and decoder.rnn_feed_attn:\n",
      "            input_ = tf.concat([input_, context, lm_state], axis=1)\n",
      "        input_size = input_.get_shape()[1].value\n",
      "\n",
      "        initializer = CellInitializer(decoder.cell_size) if decoder.orthogonal_init else None\n",
      "        with tf.variable_scope(tf.get_variable_scope(), initializer=initializer):\n",
      "            try:\n",
      "                output, new_state = get_cell(input_size)(input_, state)\n",
      "            except ValueError:  # auto_reuse doesn't work with LSTM cells\n",
      "                output, new_state = get_cell(input_size, reuse=True)(input_, state)\n",
      "\n",
      "        if decoder.skip_update and decoder.pred_edits and symbol is not None:\n",
      "            is_del = tf.equal(symbol, utils.DEL_ID)\n",
      "            new_state = tf.where(is_del, state, new_state)\n",
      "\n",
      "        if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "            output = new_state\n",
      "\n",
      "        return output, new_state\n",
      "\n",
      "    def update_pos(pos, symbol, max_pos=None):\n",
      "        if not decoder.pred_edits:\n",
      "            return pos\n",
      "\n",
      "        is_keep = tf.equal(symbol, utils.KEEP_ID)\n",
      "        is_del = tf.equal(symbol, utils.DEL_ID)\n",
      "        is_not_ins = tf.logical_or(is_keep, is_del)\n",
      "\n",
      "        pos = beam_search.resize_like(pos, symbol)\n",
      "        max_pos = beam_search.resize_like(max_pos, symbol)\n",
      "\n",
      "        pos += tf.to_float(is_not_ins)\n",
      "        if max_pos is not None:\n",
      "            pos = tf.minimum(pos, tf.to_float(max_pos))\n",
      "        return pos\n",
      "\n",
      "    def generate(state, input_, context):\n",
      "        if decoder.pred_use_lstm_state is False:  # for back-compatibility\n",
      "            state = state[:, -cell_output_size:]\n",
      "\n",
      "        projection_input = [state, context]\n",
      "        if decoder.use_previous_word:\n",
      "            projection_input.insert(1, input_)  # for back-compatibility\n",
      "\n",
      "        output_ = tf.concat(projection_input, axis=1)\n",
      "\n",
      "        if decoder.pred_deep_layer:\n",
      "            deep_layer_size = decoder.pred_deep_layer_size or decoder.embedding_size\n",
      "            if decoder.layer_norm:\n",
      "                output_ = dense(output_, deep_layer_size, use_bias=False, name='deep_output')\n",
      "                output_ = tf.contrib.layers.layer_norm(output_, activation_fn=tf.nn.tanh, scope='output_layer_norm')\n",
      "            else:\n",
      "                output_ = dense(output_, deep_layer_size, activation=tf.tanh, use_bias=True, name='deep_output')\n",
      "\n",
      "            if decoder.use_dropout:\n",
      "                size = tf.shape(output_)[1]\n",
      "                noise_shape = [1, size] if decoder.pervasive_dropout else None\n",
      "                output_ = tf.nn.dropout(output_, keep_prob=decoder.deep_layer_keep_prob, noise_shape=noise_shape)\n",
      "        else:\n",
      "            if decoder.pred_maxout_layer:\n",
      "                maxout_size = decoder.maxout_size or cell_output_size\n",
      "                output_ = dense(output_, maxout_size, use_bias=True, name='maxout')\n",
      "                if decoder.old_maxout:  # for back-compatibility with old models\n",
      "                    output_ = tf.nn.pool(tf.expand_dims(output_, axis=2), window_shape=[2], pooling_type='MAX',\n",
      "                                         padding='SAME', strides=[2])\n",
      "                    output_ = tf.squeeze(output_, axis=2)\n",
      "                else:\n",
      "                    output_ = tf.maximum(*tf.split(output_, num_or_size_splits=2, axis=1))\n",
      "\n",
      "            if decoder.pred_embed_proj:\n",
      "                # intermediate projection to embedding size (before projecting to vocabulary size)\n",
      "                # this is useful to reduce the number of parameters, and\n",
      "                # to use the output embeddings for output projection (tie_embeddings parameter)\n",
      "                output_ = dense(output_, decoder.embedding_size, use_bias=False, name='softmax0')\n",
      "\n",
      "        if decoder.tie_embeddings and (decoder.pred_embed_proj or decoder.pred_deep_layer):\n",
      "            bias = get_variable('softmax1/bias', shape=[decoder.vocab_size])\n",
      "            output_ = tf.matmul(output_, tf.transpose(embedding)) + bias\n",
      "        else:\n",
      "            output_ = dense(output_, decoder.vocab_size, use_bias=True, name='softmax1')\n",
      "        return output_\n",
      "\n",
      "    def execute(symbol, input, lm_state):\n",
      "        # predicted_symbol = KEEP -> feed input to RNN_LM\n",
      "        # predicted_symbol = DEL -> do nothing, return current lm_state\n",
      "        # predicted_symbol = new word -> feed that word to RNN_LM\n",
      "        is_keep = tf.equal(symbol, utils.KEEP_ID)\n",
      "        is_del = tf.equal(symbol, utils.DEL_ID)\n",
      "        is_not_ins = tf.logical_or(is_keep, is_del)\n",
      "\n",
      "        new_input = tf.where(is_not_ins, embed(input), embed(symbol))\n",
      "        input_size = new_input.get_shape()[1].value\n",
      "        initializer = CellInitializer(decoder.cell_size) if decoder.orthogonal_init else None\n",
      "        with tf.variable_scope(tf.get_variable_scope(), initializer=initializer):\n",
      "            try:\n",
      "                lm_output, new_lm_state = get_cell(input_size)(new_input, lm_state)\n",
      "            except ValueError:  # auto_reuse doesn't work with LSTM cells\n",
      "                lm_output, new_lm_state = get_cell(input_size, reuse=True)(new_input, lm_state)\n",
      "        if decoder.skip_update and decoder.pred_edits and symbol is not None:\n",
      "            new_lm_state = tf.where(is_del, lm_state, new_lm_state)\n",
      "\n",
      "        return lm_output, new_lm_state\n",
      "\n",
      "    if decoder.use_dropout:  # FIXME: why no pervasive dropout here?\n",
      "        initial_state = tf.nn.dropout(initial_state, keep_prob=decoder.initial_state_keep_prob)\n",
      "\n",
      "    with tf.variable_scope(scope_name):\n",
      "        activation_fn = None if decoder.initial_state == 'linear' else tf.nn.tanh\n",
      "        if decoder.initial_state == 'trained':\n",
      "            initial_state = get_variable(shape=[cell_state_size], name='initial_state')\n",
      "            initial_state = tf.tile(tf.expand_dims(initial_state, axis=0), [batch_size, 1])\n",
      "        elif decoder.initial_state == 'zero':\n",
      "            initial_state = tf.zeros(shape=[batch_size, cell_state_size])\n",
      "        elif decoder.layer_norm:\n",
      "            initial_state = dense(initial_state, cell_state_size, use_bias=False, name='initial_state_projection')\n",
      "            initial_state = tf.contrib.layers.layer_norm(initial_state, activation_fn=activation_fn,\n",
      "                                                         scope='initial_state_layer_norm')\n",
      "        else:\n",
      "            initial_state = dense(initial_state, cell_state_size, use_bias=True, name='initial_state_projection',\n",
      "                                  activation=activation_fn)\n",
      "\n",
      "    if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "        initial_output = initial_state\n",
      "    else:\n",
      "        # Last layer's state is the right-most part. Output is the left-most part of an LSTM's state.\n",
      "        initial_output = initial_state[:, -cell_output_size:]\n",
      "\n",
      "    time = tf.constant(0, dtype=tf.int32, name='time')\n",
      "    outputs = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    samples = tf.TensorArray(dtype=tf.int64, size=time_steps)\n",
      "    inputs = tf.TensorArray(dtype=tf.int64, size=time_steps).unstack(tf.to_int64(tf.transpose(decoder_inputs)))\n",
      "\n",
      "    states = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    weights = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "    attns = tf.TensorArray(dtype=tf.float32, size=time_steps)\n",
      "\n",
      "    encoder_inputs = kwargs['encoder_inputs'][0]\n",
      "    mt_inputs = decoder_inputs\n",
      "    max_pos = time_steps\n",
      "    # mt_inputs = tf.transpose(encoder_inputs, [1, 0])  # length x batch size\n",
      "    index_range = tf.range(batch_size)\n",
      "    initial_lm_state = tf.zeros(shape=[batch_size, cell_state_size])\n",
      "\n",
      "    initial_symbol = inputs.read(0)  # first symbol is BOS\n",
      "    initial_input = embed(initial_symbol)\n",
      "    initial_pos = tf.zeros([batch_size], tf.float32)\n",
      "    initial_weights = tf.zeros(tf.shape(attention_states[align_encoder_id])[:2])\n",
      "    zero_context = tf.zeros(shape=tf.shape(attention_states[align_encoder_id][:, 0]))  # FIXME\n",
      "\n",
      "    with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "        initial_context, _ = look(0, initial_output, initial_input, pos=initial_pos, prev_weights=initial_weights,\n",
      "                                  context=zero_context)\n",
      "    # TODO: initial_data [c_i, c'_i, g_i]\n",
      "    initial_data = tf.concat([initial_state, initial_context, initial_lm_state, tf.expand_dims(initial_pos, axis=1), initial_weights],\n",
      "                             axis=1)\n",
      "    context_size = initial_context.shape[1].value\n",
      "    lm_state_size = initial_lm_state.shape[1].value\n",
      "\n",
      "    def get_logits(state, ids, time):  # for beam-search decoding\n",
      "        with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "            state, context, lm_state, pos, prev_weights = tf.split(state, [cell_state_size, context_size, lm_state_size, 1, -1], axis=1)\n",
      "            input_ = embed(ids)\n",
      "\n",
      "            pos = tf.squeeze(pos, axis=1)\n",
      "            pos = tf.cond(tf.equal(time, 0),\n",
      "                          lambda: pos,\n",
      "                          lambda: update_pos(pos, ids, encoder_input_length[align_encoder_id]))\n",
      "\n",
      "            if decoder.cell_type.lower() == 'lstm' and decoder.use_lstm_full_state:\n",
      "                output = state\n",
      "            else:\n",
      "                # Output is always the right-most part of the state (even with multi-layer RNNs)\n",
      "                # However, this only works at test time, because different dropout operations can be used\n",
      "                # on state and output.\n",
      "                output = state[:, -cell_output_size:]\n",
      "\n",
      "            if decoder.conditional_rnn:\n",
      "                with tf.variable_scope('conditional_1'):\n",
      "                    output, state = update(state, input_)\n",
      "            elif decoder.update_first:\n",
      "                output, state = update(state, input_, None, ids, lm_state)\n",
      "            elif decoder.generate_first:\n",
      "                output, state = tf.cond(tf.equal(time, 0),\n",
      "                                        lambda: (output, state),\n",
      "                                        lambda: update(state, input_, context, ids, lm_state))\n",
      "\n",
      "            context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context)\n",
      "\n",
      "            if decoder.conditional_rnn:\n",
      "                with tf.variable_scope('conditional_2'):\n",
      "                    output, state = update(state, context)\n",
      "            elif not decoder.generate_first:\n",
      "                output, state = update(state, input_, context, ids)\n",
      "\n",
      "            logits = generate(output, input_, context)\n",
      "\n",
      "            argmax = lambda: tf.argmax(logits, 1)\n",
      "            target = lambda: inputs.read(time + 1)\n",
      "            softmax = lambda: tf.squeeze(tf.multinomial(tf.log(tf.nn.softmax(logits)), num_samples=1),\n",
      "                                         axis=1)\n",
      "\n",
      "            use_target = tf.logical_and(time < time_steps - 1, tf.random_uniform([]) >= feed_previous)\n",
      "            predicted_symbol = tf.case([\n",
      "                (use_target, target),\n",
      "                (tf.logical_not(feed_argmax), softmax)],\n",
      "                default=argmax)  # default case is useful for beam-search\n",
      "\n",
      "            with tf.variable_scope(\"rnn_lm\"):\n",
      "                lm_output, lm_state = execute(predicted_symbol, ids, lm_state)\n",
      "\n",
      "            pos = tf.expand_dims(pos, axis=1)\n",
      "            state = tf.concat([state, context, lm_state, pos, new_weights], axis=1)\n",
      "            return state, logits\n",
      "\n",
      "    def _time_step(time, input_, input_symbol, pos, state, output, outputs, states, weights, attns, prev_weights,\n",
      "                   samples, context, lm_state):\n",
      "        if decoder.conditional_rnn:\n",
      "            with tf.variable_scope('conditional_1'):\n",
      "                output, state = update(state, input_)\n",
      "        elif decoder.update_first:\n",
      "            with tf.variable_scope('op_decoder_rnn'):\n",
      "                output, state = update(state, input_, None, input_symbol, lm_state=lm_state)\n",
      "\n",
      "        # compute attention and context vector: c_src\n",
      "        context, new_weights = look(time, output, input_, pos=pos, prev_weights=prev_weights, context=context)\n",
      "\n",
      "        # feed to LSTM cell\n",
      "        if decoder.conditional_rnn:\n",
      "            with tf.variable_scope('conditional_2'):\n",
      "                output, state = update(state, context)\n",
      "        elif not decoder.generate_first:\n",
      "            with tf.variable_scope('op_decoder_rnn'):\n",
      "                output, state = update(state, input_, context, input_symbol, lm_state=lm_state)\n",
      "\n",
      "        # generate operation (projection output to vocab)\n",
      "        output_ = generate(output, input_, context)\n",
      "\n",
      "        argmax = lambda: tf.argmax(output_, 1)\n",
      "        target = lambda: inputs.read(time + 1)\n",
      "        softmax = lambda: tf.squeeze(tf.multinomial(tf.log(tf.nn.softmax(output_)), num_samples=1),\n",
      "                                     axis=1)\n",
      "\n",
      "        use_target = tf.logical_and(time < time_steps - 1, tf.random_uniform([]) >= feed_previous)\n",
      "        predicted_symbol = tf.case([\n",
      "            (use_target, target),\n",
      "            (tf.logical_not(feed_argmax), softmax)],\n",
      "            default=argmax)  # default case is useful for beam-search\n",
      "\n",
      "        predicted_symbol.set_shape([None])\n",
      "        predicted_symbol = tf.stop_gradient(predicted_symbol)\n",
      "\n",
      "        # TODO: feed predicted_symbol to RNN LM\n",
      "        index = tf.stack([index_range, tf.cast(pos, tf.int32)], axis=1)\n",
      "        current_mt_symbol = tf.gather_nd(mt_inputs, index)\n",
      "        with tf.variable_scope(\"rnn_lm\"):\n",
      "            lm_output, lm_state = execute(predicted_symbol, current_mt_symbol, lm_state)\n",
      "\n",
      "        input_ = embed(predicted_symbol)\n",
      "        pos = update_pos(pos, predicted_symbol, encoder_input_length[align_encoder_id])\n",
      "\n",
      "        samples = samples.write(time, predicted_symbol)\n",
      "        attns = attns.write(time, context)\n",
      "        weights = weights.write(time, new_weights)\n",
      "        states = states.write(time, state)\n",
      "        outputs = outputs.write(time, output_)\n",
      "\n",
      "        if not decoder.conditional_rnn and not decoder.update_first and decoder.generate_first:\n",
      "            output, state = update(state, input_, context, predicted_symbol, lm_state=lm_state)\n",
      "\n",
      "        return (time + 1, input_, predicted_symbol, pos, state, output, outputs, states, weights, attns, new_weights,\n",
      "                samples, context, lm_state)\n",
      "\n",
      "    with tf.variable_scope('decoder_{}'.format(decoder.name)):\n",
      "        # TODO: check order states and weights\n",
      "        _, _, _, new_pos, new_state, _, outputs, states, weights, attns, new_weights, samples, _, _ = tf.while_loop(\n",
      "            cond=lambda time, *_: time < time_steps,\n",
      "            body=_time_step,\n",
      "            loop_vars=(time, initial_input, initial_symbol, initial_pos, initial_state, initial_output, outputs,\n",
      "                       weights, states, attns, initial_weights, samples, initial_context, initial_lm_state),\n",
      "            parallel_iterations=decoder.parallel_iterations,\n",
      "            swap_memory=decoder.swap_memory)\n",
      "\n",
      "    outputs = outputs.stack()\n",
      "    weights = weights.stack()  # batch_size, encoders, output time, input time\n",
      "    states = states.stack()\n",
      "    attns = attns.stack()\n",
      "    samples = samples.stack()\n",
      "\n",
      "    # put batch_size as first dimension\n",
      "    outputs = tf.transpose(outputs, perm=(1, 0, 2))\n",
      "    weights = tf.transpose(weights, perm=(1, 0, 2))\n",
      "    states = tf.transpose(states, perm=(1, 0, 2))\n",
      "    attns = tf.transpose(attns, perm=(1, 0, 2))\n",
      "    samples = tf.transpose(samples)\n",
      "\n",
      "    return outputs, weights, states, attns, samples, get_logits, initial_data\n",
      "\n",
      "\n",
      "def encoder_decoder(encoders, decoders, encoder_inputs, targets, feed_previous, align_encoder_id=0,\n",
      "                    encoder_input_length=None, feed_argmax=True, rewards=None, use_baseline=True,\n",
      "                    training=True, global_step=None,\n",
      "                    monotonicity_weight=None, monotonicity_dist=None, monotonicity_decay=None, **kwargs):\n",
      "    decoder = decoders[0]\n",
      "    targets = targets[0]  # single decoder\n",
      "\n",
      "    if encoder_input_length is None:\n",
      "        encoder_input_length = []\n",
      "        for encoder_inputs_ in encoder_inputs:\n",
      "            mask = get_weights(encoder_inputs_, utils.EOS_ID, include_first_eos=True)\n",
      "            encoder_input_length.append(tf.to_int32(tf.reduce_sum(mask, axis=1)))\n",
      "\n",
      "    parameters = dict(encoders=encoders, decoder=decoder, encoder_inputs=encoder_inputs,\n",
      "                      feed_argmax=feed_argmax, training=training)\n",
      "\n",
      "    attention_states, encoder_state, encoder_input_length = multi_encoder(\n",
      "        encoder_input_length=encoder_input_length, **parameters)\n",
      "\n",
      "    outputs, attention_weights, _, _, samples, beam_fun, initial_data = attention_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state, feed_previous=feed_previous,\n",
      "        decoder_inputs=targets[:, :-1], align_encoder_id=align_encoder_id, encoder_input_length=encoder_input_length,\n",
      "        **parameters\n",
      "    )\n",
      "\n",
      "    if use_baseline:\n",
      "        baseline_rewards = reinforce_baseline(outputs, rewards)  # FIXME: use logits or decoder outputs?\n",
      "        baseline_weights = get_weights(samples, utils.EOS_ID, include_first_eos=False)\n",
      "        baseline_loss_ = baseline_loss(rewards=baseline_rewards, weights=baseline_weights)\n",
      "    else:\n",
      "        baseline_rewards = rewards\n",
      "        baseline_loss_ = tf.constant(0.0)\n",
      "\n",
      "    reinforce_weights = get_weights(samples, utils.EOS_ID, include_first_eos=True)\n",
      "    reinforce_loss = sequence_loss(logits=outputs, targets=samples, weights=reinforce_weights,\n",
      "                                   rewards=baseline_rewards)\n",
      "\n",
      "    trg_mask = get_weights(targets[:, 1:], utils.EOS_ID, include_first_eos=True)\n",
      "    xent_loss = sequence_loss(logits=outputs, targets=targets[:, 1:], weights=trg_mask)\n",
      "\n",
      "    if monotonicity_weight:\n",
      "        monotonicity_dist = monotonicity_dist or 1.0\n",
      "\n",
      "        batch_size = tf.shape(attention_weights)[0]\n",
      "        src_len = tf.shape(attention_weights)[2]\n",
      "        trg_len = tf.shape(attention_weights)[1]\n",
      "\n",
      "        src_indices = tf.tile(tf.reshape(tf.range(src_len), shape=[1, 1, src_len]), [batch_size, trg_len, 1])\n",
      "        trg_indices = tf.tile(tf.reshape(tf.range(trg_len), shape=[1, trg_len, 1]), [batch_size, 1, src_len])\n",
      "\n",
      "        source_length = encoder_input_length[0]\n",
      "        target_length = tf.to_int32(tf.reduce_sum(trg_mask, axis=1))\n",
      "        true_src_len = tf.reshape(source_length, shape=[batch_size, 1, 1]) - 1\n",
      "        true_trg_len = tf.reshape(target_length, shape=[batch_size, 1, 1]) - 1\n",
      "\n",
      "        src_mask = tf.to_float(tf.sequence_mask(source_length, maxlen=src_len))\n",
      "        mask = tf.matmul(tf.expand_dims(trg_mask, axis=2), tf.expand_dims(src_mask, axis=1))\n",
      "\n",
      "        monotonous = tf.sqrt(((true_trg_len * src_indices - true_src_len * trg_indices) ** 2)\n",
      "                             / (true_trg_len ** 2 + true_src_len ** 2))\n",
      "        monotonous = tf.to_float(monotonous < monotonicity_dist)\n",
      "        non_monotonous = (1 - monotonous) * mask\n",
      "        attn_loss = tf.reduce_sum(attention_weights * tf.stop_gradient(non_monotonous)) / tf.to_float(batch_size)\n",
      "\n",
      "        if monotonicity_decay:\n",
      "            decay = tf.stop_gradient(0.5 ** (tf.to_float(global_step) / monotonicity_decay))\n",
      "        else:\n",
      "            decay = 1.0\n",
      "\n",
      "        xent_loss += monotonicity_weight * decay * attn_loss\n",
      "\n",
      "    losses = [xent_loss, reinforce_loss, baseline_loss_]\n",
      "\n",
      "    return losses, [outputs], encoder_state, attention_states, attention_weights, samples, beam_fun, initial_data\n",
      "\n",
      "\n",
      "def reconstruction_encoder_decoder(encoders, decoders, encoder_inputs, targets, feed_previous,\n",
      "                                   encoder_input_length=None, training=True, reconstruction_weight=1.0,\n",
      "                                   reconstruction_attn_weight=0.05, **kwargs):\n",
      "    encoders = encoders[:1]\n",
      "\n",
      "    if encoder_input_length is None:\n",
      "        weights = get_weights(encoder_inputs[0], utils.EOS_ID, include_first_eos=True)\n",
      "        encoder_input_length = [tf.to_int32(tf.reduce_sum(weights, axis=1))]\n",
      "\n",
      "    attention_states, encoder_state, encoder_input_length = multi_encoder(\n",
      "        encoder_input_length=encoder_input_length, encoders=encoders, encoder_inputs=encoder_inputs,\n",
      "        training=training)\n",
      "\n",
      "    outputs, attention_weights, states, _, samples, beam_fun, initial_data = attention_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state, feed_previous=feed_previous,\n",
      "        decoder_inputs=targets[0][:, :-1], encoder_input_length=encoder_input_length,\n",
      "        decoder=decoders[0], training=training, encoders=encoders\n",
      "    )\n",
      "\n",
      "    target_weights = get_weights(targets[0][:, 1:], utils.EOS_ID, include_first_eos=True)\n",
      "    target_length = [tf.to_int32(tf.reduce_sum(target_weights, axis=1))]\n",
      "\n",
      "    xent_loss = sequence_loss(logits=outputs, targets=targets[0][:, 1:], weights=target_weights)\n",
      "\n",
      "    reconstructed_outputs, reconstructed_weights, _, _, _, _, _ = attention_decoder(\n",
      "        attention_states=[states], initial_state=states[:, -1, :], feed_previous=feed_previous,\n",
      "        decoder_inputs=targets[1][:, :-1], encoder_input_length=target_length,\n",
      "        decoder=decoders[1], training=training, encoders=decoders[:1]\n",
      "    )\n",
      "\n",
      "    target_weights = get_weights(targets[1][:, 1:], utils.EOS_ID, include_first_eos=True)\n",
      "    xent_loss += reconstruction_weight * sequence_loss(logits=reconstructed_outputs, targets=targets[1][:, 1:],\n",
      "                                                       weights=target_weights)\n",
      "\n",
      "    max_src_len = tf.shape(reconstructed_weights)[1]\n",
      "    batch_size = tf.shape(reconstructed_weights)[0]\n",
      "\n",
      "    attn_loss = tf.matmul(reconstructed_weights, attention_weights) - tf.eye(max_src_len)\n",
      "\n",
      "    src_mask = tf.sequence_mask(encoder_input_length[0], maxlen=max_src_len, dtype=tf.float32)\n",
      "    src_mask = tf.einsum('ij,ik->ijk', src_mask, src_mask)\n",
      "    attn_loss *= tf.to_float(src_mask)  # don't take padding words into account\n",
      "\n",
      "    attn_loss = tf.norm(attn_loss) / tf.to_float(batch_size)\n",
      "    xent_loss += reconstruction_attn_weight * attn_loss\n",
      "\n",
      "    attention_weights = [attention_weights, reconstructed_weights]\n",
      "    losses = [xent_loss, None, None]\n",
      "    return losses, [outputs], encoder_state, attention_states, attention_weights, samples, beam_fun, initial_data\n",
      "\n",
      "\n",
      "def chained_encoder_decoder(encoders, decoders, encoder_inputs, targets, feed_previous,\n",
      "                            chaining_strategy=None, align_encoder_id=0, chaining_non_linearity=False,\n",
      "                            chaining_loss_ratio=1.0, chaining_stop_gradient=False, training=True, **kwargs):\n",
      "    decoder = decoders[0]\n",
      "    targets = targets[0]  # single decoder\n",
      "\n",
      "    assert len(encoders) == 2\n",
      "\n",
      "    encoder_input_length = []\n",
      "    input_weights = []\n",
      "    for encoder_inputs_ in encoder_inputs:\n",
      "        weights = get_weights(encoder_inputs_, utils.EOS_ID, include_first_eos=True)\n",
      "        input_weights.append(weights)\n",
      "        encoder_input_length.append(tf.to_int32(tf.reduce_sum(weights, axis=1)))\n",
      "\n",
      "    target_weights = get_weights(targets[:, 1:], utils.EOS_ID, include_first_eos=True)\n",
      "\n",
      "    parameters = dict(encoders=encoders[1:], decoder=encoders[0], training=training)\n",
      "\n",
      "    attention_states, encoder_state, encoder_input_length[1:] = multi_encoder(\n",
      "        encoder_inputs[1:], encoder_input_length=encoder_input_length[1:], **parameters)\n",
      "\n",
      "    decoder_inputs = encoder_inputs[0][:, :-1]\n",
      "    batch_size = tf.shape(decoder_inputs)[0]\n",
      "\n",
      "    pad = tf.ones(shape=tf.stack([batch_size, 1]), dtype=tf.int32) * utils.BOS_ID\n",
      "    decoder_inputs = tf.concat([pad, decoder_inputs], axis=1)\n",
      "\n",
      "    outputs, _, states, attns, _, _, _ = attention_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state, decoder_inputs=decoder_inputs,\n",
      "        encoder_input_length=encoder_input_length[1:], **parameters\n",
      "    )\n",
      "\n",
      "    chaining_loss = sequence_loss(logits=outputs, targets=encoder_inputs[0], weights=input_weights[0])\n",
      "\n",
      "    if 'lstm' in decoder.cell_type.lower():\n",
      "        size = states.get_shape()[2].value\n",
      "        decoder_outputs = states[:, :, size // 2:]\n",
      "    else:\n",
      "        decoder_outputs = states\n",
      "\n",
      "    if chaining_strategy == 'share_states':\n",
      "        other_inputs = states\n",
      "    elif chaining_strategy == 'share_outputs':\n",
      "        other_inputs = decoder_outputs\n",
      "    else:\n",
      "        other_inputs = None\n",
      "\n",
      "    if other_inputs is not None and chaining_stop_gradient:\n",
      "        other_inputs = tf.stop_gradient(other_inputs)\n",
      "\n",
      "    parameters = dict(encoders=encoders[:1], decoder=decoder, encoder_inputs=encoder_inputs[:1],\n",
      "                      other_inputs=other_inputs, training=training)\n",
      "\n",
      "    attention_states, encoder_state, encoder_input_length[:1] = multi_encoder(\n",
      "        encoder_input_length=encoder_input_length[:1], **parameters)\n",
      "\n",
      "    if chaining_stop_gradient:\n",
      "        attns = tf.stop_gradient(attns)\n",
      "        states = tf.stop_gradient(states)\n",
      "        decoder_outputs = tf.stop_gradient(decoder_outputs)\n",
      "\n",
      "    if chaining_strategy == 'concat_attns':\n",
      "        attention_states[0] = tf.concat([attention_states[0], attns], axis=2)\n",
      "    elif chaining_strategy == 'concat_states':\n",
      "        attention_states[0] = tf.concat([attention_states[0], states], axis=2)\n",
      "    elif chaining_strategy == 'sum_attns':\n",
      "        attention_states[0] += attns\n",
      "    elif chaining_strategy in ('map_attns', 'map_states', 'map_outputs'):\n",
      "        if chaining_strategy == 'map_attns':\n",
      "            x = attns\n",
      "        elif chaining_strategy == 'map_outputs':\n",
      "            x = decoder_outputs\n",
      "        else:\n",
      "            x = states\n",
      "\n",
      "        shape = [x.get_shape()[-1], attention_states[0].get_shape()[-1]]\n",
      "\n",
      "        w = tf.get_variable(\"map_attns/matrix\", shape=shape)\n",
      "        b = tf.get_variable(\"map_attns/bias\", shape=shape[-1:])\n",
      "\n",
      "        x = tf.einsum('ijk,kl->ijl', x, w) + b\n",
      "        if chaining_non_linearity:\n",
      "            x = tf.nn.tanh(x)\n",
      "\n",
      "        attention_states[0] += x\n",
      "\n",
      "    outputs, attention_weights, _, _, samples, beam_fun, initial_data = attention_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state,\n",
      "        feed_previous=feed_previous, decoder_inputs=targets[:, :-1],\n",
      "        align_encoder_id=align_encoder_id, encoder_input_length=encoder_input_length[:1],\n",
      "        **parameters\n",
      "    )\n",
      "\n",
      "    xent_loss = sequence_loss(logits=outputs, targets=targets[:, 1:],\n",
      "                              weights=target_weights)\n",
      "\n",
      "    if chaining_loss is not None and chaining_loss_ratio:\n",
      "        xent_loss += chaining_loss_ratio * chaining_loss\n",
      "\n",
      "    losses = [xent_loss, None, None]\n",
      "\n",
      "    return losses, [outputs], encoder_state, attention_states, attention_weights, samples, beam_fun, initial_data\n",
      "\n",
      "\n",
      "def chained_encoder_decoder_execute(encoders, decoders, encoder_inputs, targets, feed_previous,\n",
      "                                    chaining_strategy=None, align_encoder_id=0, chaining_non_linearity=False,\n",
      "                                    chaining_loss_ratio=1.0, chaining_stop_gradient=False, training=True, **kwargs):\n",
      "    decoder = decoders[0]\n",
      "    targets = targets[0]  # single decoder\n",
      "\n",
      "    assert len(encoders) == 2\n",
      "\n",
      "    encoder_input_length = []\n",
      "    input_weights = []\n",
      "    for encoder_inputs_ in encoder_inputs:\n",
      "        weights = get_weights(encoder_inputs_, utils.EOS_ID, include_first_eos=True)\n",
      "        input_weights.append(weights)\n",
      "        encoder_input_length.append(tf.to_int32(tf.reduce_sum(weights, axis=1)))\n",
      "\n",
      "    target_weights = get_weights(targets[:, 1:], utils.EOS_ID, include_first_eos=True)\n",
      "\n",
      "    parameters = dict(encoders=encoders[1:], decoder=encoders[0], training=training)\n",
      "\n",
      "    # src encoder\n",
      "    attention_states, encoder_state, encoder_input_length[1:] = multi_encoder(\n",
      "        encoder_inputs[1:], encoder_input_length=encoder_input_length[1:], **parameters)\n",
      "\n",
      "    decoder_inputs = encoder_inputs[0][:, :-1]\n",
      "    batch_size = tf.shape(decoder_inputs)[0]\n",
      "\n",
      "    pad = tf.ones(shape=tf.stack([batch_size, 1]), dtype=tf.int32) * utils.BOS_ID\n",
      "    decoder_inputs = tf.concat([pad, decoder_inputs], axis=1)\n",
      "\n",
      "    # src -> mt decoder\n",
      "    outputs, _, states, attns, _, _, _ = attention_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state, decoder_inputs=decoder_inputs,\n",
      "        encoder_input_length=encoder_input_length[1:], **parameters\n",
      "    )\n",
      "\n",
      "    chaining_loss = sequence_loss(logits=outputs, targets=encoder_inputs[0], weights=input_weights[0])\n",
      "\n",
      "    if 'lstm' in decoder.cell_type.lower():\n",
      "        size = states.get_shape()[2].value\n",
      "        decoder_outputs = states[:, :, size // 2:]\n",
      "    else:\n",
      "        decoder_outputs = states\n",
      "\n",
      "    if chaining_strategy == 'share_states':\n",
      "        other_inputs = states\n",
      "    elif chaining_strategy == 'share_outputs':\n",
      "        other_inputs = decoder_outputs\n",
      "    else:\n",
      "        other_inputs = None\n",
      "\n",
      "    if other_inputs is not None and chaining_stop_gradient:\n",
      "        other_inputs = tf.stop_gradient(other_inputs)\n",
      "\n",
      "    parameters = dict(encoders=encoders[:1], decoder=decoder, encoder_inputs=encoder_inputs[:1],\n",
      "                      other_inputs=other_inputs, training=training)\n",
      "\n",
      "    # mt encoder\n",
      "    attention_states, encoder_state, encoder_input_length[:1] = multi_encoder(\n",
      "        encoder_input_length=encoder_input_length[:1], **parameters)\n",
      "\n",
      "    if chaining_stop_gradient:\n",
      "        attns = tf.stop_gradient(attns)\n",
      "        states = tf.stop_gradient(states)\n",
      "        decoder_outputs = tf.stop_gradient(decoder_outputs)\n",
      "\n",
      "    if chaining_strategy == 'concat_attns':\n",
      "        attention_states[0] = tf.concat([attention_states[0], attns], axis=2)\n",
      "    elif chaining_strategy == 'concat_states':\n",
      "        attention_states[0] = tf.concat([attention_states[0], states], axis=2)\n",
      "    elif chaining_strategy == 'sum_attns':\n",
      "        attention_states[0] += attns\n",
      "    elif chaining_strategy in ('map_attns', 'map_states', 'map_outputs'):\n",
      "        if chaining_strategy == 'map_attns':\n",
      "            x = attns\n",
      "        elif chaining_strategy == 'map_outputs':\n",
      "            x = decoder_outputs\n",
      "        else:\n",
      "            x = states\n",
      "\n",
      "        shape = [x.get_shape()[-1], attention_states[0].get_shape()[-1]]\n",
      "\n",
      "        w = tf.get_variable(\"map_attns/matrix\", shape=shape)\n",
      "        b = tf.get_variable(\"map_attns/bias\", shape=shape[-1:])\n",
      "\n",
      "        x = tf.einsum('ijk,kl->ijl', x, w) + b\n",
      "        if chaining_non_linearity:\n",
      "            x = tf.nn.tanh(x)\n",
      "\n",
      "        attention_states[0] += x\n",
      "\n",
      "    outputs, attention_weights, _, _, samples, beam_fun, initial_data = attention_execution_decoder(\n",
      "        attention_states=attention_states, initial_state=encoder_state,\n",
      "        feed_previous=feed_previous, decoder_inputs=targets[:, :-1],\n",
      "        align_encoder_id=align_encoder_id, encoder_input_length=encoder_input_length[:1],\n",
      "        **parameters\n",
      "    )\n",
      "\n",
      "    xent_loss = sequence_loss(logits=outputs, targets=targets[:, 1:],\n",
      "                              weights=target_weights)\n",
      "\n",
      "    if chaining_loss is not None and chaining_loss_ratio:\n",
      "        xent_loss += chaining_loss_ratio * chaining_loss\n",
      "\n",
      "    losses = [xent_loss, None, None]\n",
      "\n",
      "    return losses, [outputs], encoder_state, attention_states, attention_weights, samples, beam_fun, initial_data\n",
      "\n",
      "\n",
      "def softmax(logits, dim=-1, mask=None):\n",
      "    e = tf.exp(logits)\n",
      "    if mask is not None:\n",
      "        e *= mask\n",
      "\n",
      "    return e / tf.clip_by_value(tf.reduce_sum(e, axis=dim, keep_dims=True), 10e-37, 10e+37)\n",
      "\n",
      "\n",
      "def sequence_loss(logits, targets, weights, average_across_timesteps=False, average_across_batch=True, rewards=None):\n",
      "    batch_size = tf.shape(targets)[0]\n",
      "    time_steps = tf.shape(targets)[1]\n",
      "\n",
      "    logits_ = tf.reshape(logits, tf.stack([time_steps * batch_size, logits.get_shape()[2].value]))\n",
      "    targets_ = tf.reshape(targets, tf.stack([time_steps * batch_size]))\n",
      "\n",
      "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_, labels=targets_)\n",
      "    crossent = tf.reshape(crossent, tf.stack([batch_size, time_steps]))\n",
      "\n",
      "    if rewards is not None:\n",
      "        crossent *= tf.stop_gradient(rewards)\n",
      "\n",
      "    log_perp = tf.reduce_sum(crossent * weights, axis=1)\n",
      "\n",
      "    if average_across_timesteps:\n",
      "        total_size = tf.reduce_sum(weights, axis=1)\n",
      "        total_size += 1e-12  # just to avoid division by 0 for all-0 weights\n",
      "        log_perp /= total_size\n",
      "\n",
      "    cost = tf.reduce_sum(log_perp)\n",
      "\n",
      "    if average_across_batch:\n",
      "        return cost / tf.to_float(batch_size)\n",
      "    else:\n",
      "        return cost\n",
      "\n",
      "\n",
      "def reinforce_baseline(decoder_states, reward):\n",
      "    \"\"\"\n",
      "    Center the reward by computing a baseline reward over decoder states.\n",
      "\n",
      "    :param decoder_states: internal states of the decoder, tensor of shape (batch_size, time_steps, state_size)\n",
      "    :param reward: reward for each time step, tensor of shape (batch_size, time_steps)\n",
      "    :return: reward - computed baseline, tensor of shape (batch_size, time_steps)\n",
      "    \"\"\"\n",
      "    # batch_size = tf.shape(decoder_states)[0]\n",
      "    # time_steps = tf.shape(decoder_states)[1]\n",
      "    # state_size = decoder_states.get_shape()[2]\n",
      "    # states = tf.reshape(decoder_states, shape=tf.stack([batch_size * time_steps, state_size]))\n",
      "\n",
      "    baseline = dense(tf.stop_gradient(decoder_states), units=1, activation=None, name='reward_baseline',\n",
      "                     kernel_initializer=tf.constant_initializer(0.01))\n",
      "    baseline = tf.squeeze(baseline, axis=2)\n",
      "\n",
      "    # baseline = tf.reshape(baseline, shape=tf.stack([batch_size, time_steps]))\n",
      "    return reward - baseline\n",
      "\n",
      "\n",
      "def baseline_loss(rewards, weights, average_across_timesteps=False, average_across_batch=True):\n",
      "    \"\"\"\n",
      "    :param rewards: tensor of shape (batch_size, time_steps)\n",
      "    :param weights: tensor of shape (batch_size, time_steps)\n",
      "    \"\"\"\n",
      "    batch_size = tf.shape(rewards)[0]\n",
      "\n",
      "    cost = rewards ** 2\n",
      "    cost = tf.reduce_sum(cost * weights, axis=1)\n",
      "\n",
      "    if average_across_timesteps:\n",
      "        total_size = tf.reduce_sum(weights, axis=1)\n",
      "        total_size += 1e-12  # just to avoid division by 0 for all-0 weights\n",
      "        cost /= total_size\n",
      "\n",
      "    cost = tf.reduce_sum(cost)\n",
      "\n",
      "    if average_across_batch:\n",
      "        cost /= tf.to_float(batch_size)\n",
      "\n",
      "    return cost\n"
     ]
    }
   ],
   "source": [
    "example_file = python_files_df[\"content\"].iloc[1]\n",
    "print(example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b7ae8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def reinforce_baseline(decoder_states, reward):\n",
      "    '\\n    Center the reward by computing a baseline reward over decoder states.\\n\\n    :param decoder_states: internal states of the decoder, tensor of shape (batch_size, time_steps, state_size)\\n    :param reward: reward for each time step, tensor of shape (batch_size, time_steps)\\n    :return: reward - computed baseline, tensor of shape (batch_size, time_steps)\\n    '\n",
      "    baseline = dense(tf.stop_gradient(decoder_states), units=1, activation=None, name='reward_baseline', kernel_initializer=tf.constant_initializer(0.01))\n",
      "    baseline = tf.squeeze(baseline, axis=2)\n",
      "    return (reward - baseline)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_code = ast.parse(example_file).body[-2]\n",
    "print(astunparse.unparse(fn_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "855e760b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_reuse': frozenset(),\n",
       " 'CellWrapper': frozenset(),\n",
       " 'multi_encoder': frozenset({'cell_size',\n",
       "            'cell_type',\n",
       "            'concat',\n",
       "            'encoder_states',\n",
       "            'encoders',\n",
       "            'enumerate',\n",
       "            'get_state_size',\n",
       "            'lstm_proj_size',\n",
       "            'random_normal_initializer',\n",
       "            'random_uniform_initializer'}),\n",
       " 'compute_energy': frozenset({'attn_filters',\n",
       "            'attn_size',\n",
       "            'conv',\n",
       "            'conv2d',\n",
       "            'dense',\n",
       "            'dropout',\n",
       "            'expand_dims',\n",
       "            'filter_',\n",
       "            'filter_shape',\n",
       "            'get_variable',\n",
       "            'hidden',\n",
       "            'input_length',\n",
       "            'layer_norm',\n",
       "            'log',\n",
       "            'mult_attn',\n",
       "            'pos_feats',\n",
       "            'prev_weights',\n",
       "            'range',\n",
       "            'reshape',\n",
       "            'squeeze',\n",
       "            'stack',\n",
       "            'state',\n",
       "            'tile',\n",
       "            'time',\n",
       "            'time_steps',\n",
       "            'to_float',\n",
       "            'y'}),\n",
       " 'global_attention': frozenset(),\n",
       " 'no_attention': frozenset({'zeros'}),\n",
       " 'average_attention': frozenset({'encoder_input_length',\n",
       "            'expand_dims',\n",
       "            'hidden_states',\n",
       "            'lengths',\n",
       "            'mask',\n",
       "            'reduce_sum',\n",
       "            'sequence_mask',\n",
       "            'to_float',\n",
       "            'weights'}),\n",
       " 'last_state_attention': frozenset({'encoder_input_length',\n",
       "            'expand_dims',\n",
       "            'hidden_states',\n",
       "            'one_hot',\n",
       "            'reduce_sum',\n",
       "            'to_float',\n",
       "            'weights'}),\n",
       " 'local_attention': frozenset({'concat', 'value'}),\n",
       " 'attention': frozenset({'append',\n",
       "            'attention_function',\n",
       "            'attention_type',\n",
       "            'attn_heads',\n",
       "            'attn_mapping',\n",
       "            'concat',\n",
       "            'context_vector',\n",
       "            'context_vectors',\n",
       "            'get',\n",
       "            'global_attention',\n",
       "            'i',\n",
       "            'len',\n",
       "            'range',\n",
       "            'sum',\n",
       "            'weights',\n",
       "            'weights_'}),\n",
       " 'multi_attention': frozenset({'append',\n",
       "            'attention',\n",
       "            'attns',\n",
       "            'concat',\n",
       "            'context_vector',\n",
       "            'encoder_input_length',\n",
       "            'encoders',\n",
       "            'enumerate',\n",
       "            'hidden',\n",
       "            'hidden_states',\n",
       "            'input_length',\n",
       "            'reduce_sum',\n",
       "            'resize_like',\n",
       "            'stack',\n",
       "            'state',\n",
       "            'weights_',\n",
       "            'zip'}),\n",
       " 'attention_decoder': frozenset({'TensorArray',\n",
       "            'attns',\n",
       "            'cell_output_size',\n",
       "            'cell_size',\n",
       "            'cell_state_size',\n",
       "            'cell_type',\n",
       "            'concat',\n",
       "            'constant',\n",
       "            'decoder_inputs',\n",
       "            'dropout',\n",
       "            'embed',\n",
       "            'float32',\n",
       "            'format',\n",
       "            'get_state_size',\n",
       "            'initial_state',\n",
       "            'initial_symbol',\n",
       "            'layers',\n",
       "            'log',\n",
       "            'lstm_proj_size',\n",
       "            'name',\n",
       "            'outputs',\n",
       "            'random_normal_initializer',\n",
       "            'random_uniform_initializer',\n",
       "            'read',\n",
       "            'samples',\n",
       "            'shape',\n",
       "            'stack',\n",
       "            'states',\n",
       "            'to_int64',\n",
       "            'transpose',\n",
       "            'unstack',\n",
       "            'use_dropout',\n",
       "            'value',\n",
       "            'weights',\n",
       "            'zeros'}),\n",
       " 'attention_execution_decoder': frozenset({'TensorArray',\n",
       "            'attns',\n",
       "            'batch_size',\n",
       "            'cell_size',\n",
       "            'cell_type',\n",
       "            'concat',\n",
       "            'constant',\n",
       "            'decoder_inputs',\n",
       "            'dropout',\n",
       "            'embed',\n",
       "            'float32',\n",
       "            'format',\n",
       "            'get_state_size',\n",
       "            'initial_state',\n",
       "            'initial_symbol',\n",
       "            'layers',\n",
       "            'lstm_proj_size',\n",
       "            'name',\n",
       "            'outputs',\n",
       "            'random_normal_initializer',\n",
       "            'random_uniform_initializer',\n",
       "            'range',\n",
       "            'read',\n",
       "            'samples',\n",
       "            'shape',\n",
       "            'stack',\n",
       "            'states',\n",
       "            'time_steps',\n",
       "            'to_int64',\n",
       "            'transpose',\n",
       "            'unstack',\n",
       "            'use_dropout',\n",
       "            'value',\n",
       "            'weights',\n",
       "            'zeros'}),\n",
       " 'encoder_decoder': frozenset({'EOS_ID',\n",
       "            'append',\n",
       "            'attention_decoder',\n",
       "            'attention_weights',\n",
       "            'baseline_loss',\n",
       "            'batch_size',\n",
       "            'constant',\n",
       "            'dict',\n",
       "            'encoder_inputs',\n",
       "            'encoder_inputs_',\n",
       "            'expand_dims',\n",
       "            'get_weights',\n",
       "            'global_step',\n",
       "            'mask',\n",
       "            'matmul',\n",
       "            'monotonicity_decay',\n",
       "            'monotonicity_weight',\n",
       "            'monotonous',\n",
       "            'multi_encoder',\n",
       "            'non_monotonous',\n",
       "            'outputs',\n",
       "            'range',\n",
       "            'reduce_sum',\n",
       "            'reinforce_baseline',\n",
       "            'reshape',\n",
       "            'rewards',\n",
       "            'samples',\n",
       "            'sequence_loss',\n",
       "            'sequence_mask',\n",
       "            'source_length',\n",
       "            'sqrt',\n",
       "            'src_indices',\n",
       "            'src_len',\n",
       "            'src_mask',\n",
       "            'stop_gradient',\n",
       "            'target_length',\n",
       "            'tile',\n",
       "            'to_float',\n",
       "            'to_int32',\n",
       "            'trg_indices',\n",
       "            'trg_len',\n",
       "            'trg_mask',\n",
       "            'true_src_len',\n",
       "            'true_trg_len',\n",
       "            'use_baseline'}),\n",
       " 'reconstruction_encoder_decoder': frozenset({'EOS_ID',\n",
       "            'attention_decoder',\n",
       "            'attention_weights',\n",
       "            'attn_loss',\n",
       "            'batch_size',\n",
       "            'einsum',\n",
       "            'eye',\n",
       "            'get_weights',\n",
       "            'matmul',\n",
       "            'max_src_len',\n",
       "            'multi_encoder',\n",
       "            'norm',\n",
       "            'reconstructed_weights',\n",
       "            'sequence_loss',\n",
       "            'sequence_mask',\n",
       "            'src_mask',\n",
       "            'to_float'}),\n",
       " 'chained_encoder_decoder': frozenset({'BOS_ID',\n",
       "            'EOS_ID',\n",
       "            'append',\n",
       "            'attention_decoder',\n",
       "            'attns',\n",
       "            'b',\n",
       "            'chaining_non_linearity',\n",
       "            'chaining_stop_gradient',\n",
       "            'concat',\n",
       "            'decoder_outputs',\n",
       "            'dict',\n",
       "            'einsum',\n",
       "            'encoder_inputs',\n",
       "            'encoder_inputs_',\n",
       "            'get_variable',\n",
       "            'get_weights',\n",
       "            'multi_encoder',\n",
       "            'ones',\n",
       "            'other_inputs',\n",
       "            'reduce_sum',\n",
       "            'sequence_loss',\n",
       "            'states',\n",
       "            'stop_gradient',\n",
       "            'tanh',\n",
       "            'to_int32',\n",
       "            'value',\n",
       "            'w',\n",
       "            'weights',\n",
       "            'x'}),\n",
       " 'chained_encoder_decoder_execute': frozenset({'BOS_ID',\n",
       "            'EOS_ID',\n",
       "            'append',\n",
       "            'attention_decoder',\n",
       "            'attention_execution_decoder',\n",
       "            'attns',\n",
       "            'b',\n",
       "            'chaining_non_linearity',\n",
       "            'chaining_stop_gradient',\n",
       "            'concat',\n",
       "            'decoder_outputs',\n",
       "            'dict',\n",
       "            'einsum',\n",
       "            'encoder_inputs',\n",
       "            'encoder_inputs_',\n",
       "            'get_variable',\n",
       "            'get_weights',\n",
       "            'multi_encoder',\n",
       "            'ones',\n",
       "            'other_inputs',\n",
       "            'reduce_sum',\n",
       "            'sequence_loss',\n",
       "            'states',\n",
       "            'stop_gradient',\n",
       "            'tanh',\n",
       "            'to_int32',\n",
       "            'value',\n",
       "            'w',\n",
       "            'weights',\n",
       "            'x'}),\n",
       " 'softmax': frozenset({'exp', 'logits'}),\n",
       " 'sequence_loss': frozenset({'average_across_batch',\n",
       "            'average_across_timesteps',\n",
       "            'crossent',\n",
       "            'log_perp',\n",
       "            'logits',\n",
       "            'reduce_sum',\n",
       "            'reshape',\n",
       "            'sparse_softmax_cross_entropy_with_logits',\n",
       "            'stack',\n",
       "            'targets',\n",
       "            'weights'}),\n",
       " 'reinforce_baseline': frozenset({'baseline',\n",
       "            'decoder_states',\n",
       "            'dense',\n",
       "            'squeeze',\n",
       "            'stop_gradient'}),\n",
       " 'baseline_loss': frozenset({'average_across_batch',\n",
       "            'average_across_timesteps',\n",
       "            'cost',\n",
       "            'reduce_sum',\n",
       "            'rewards',\n",
       "            'weights'})}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_function_calls(example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cf75b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items = ast.parse(\"a.append(1)\").body\n",
    "expr = items[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0907f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_sample_files_df(python_files_df, n_files=100, repo_col=\"repo_name\"):\n",
    "    \"sample n_files from each repo\"\n",
    "    if n_files == None:\n",
    "        n_files = python_files_df.shape[0]\n",
    "    return (\n",
    "        python_files_df.dropna()\n",
    "        .groupby(repo_col)\n",
    "        .apply(lambda df: df.iloc[: min(df.shape[0], n_files)])\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c156c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = (\n",
    "    python_files_df.dropna()\n",
    "    .groupby(\"repo_name\")\n",
    "    .apply(lambda df: df.iloc[: min(df.shape[0], 100)])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e34ca119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'path', 'repo_name'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2054a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def clean_task_names(tasks):\n",
    "    return (\n",
    "        tasks.str.replace(\"2D \", \"\")\n",
    "        .str.replace(\"3D \", \"\")\n",
    "        .str.replace(\"4D \", \"\")\n",
    "        .str.replace(\"6D \", \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_task_edges(python_files_df):\n",
    "    task_exploded_df = python_files_df[[\"repo_name\", \"tasks\"]].explode(\"tasks\")\n",
    "    task_exploded_df[\"tasks\"] = clean_task_names(task_exploded_df[\"tasks\"])\n",
    "    return (\n",
    "        task_exploded_df.groupby(\"tasks\")\n",
    "        .apply(lambda df: {df[\"tasks\"].iloc[0]: frozenset(df[\"repo_name\"].values)})\n",
    "        .tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "add15c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'path', 'repo_name'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "888e954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def try_run(f):\n",
    "    def _maybe_failed_f(args):\n",
    "        try:\n",
    "            return f(args)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    return _maybe_failed_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03a5fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def get_upper_level_edges(upper_level_names, lower_level_edges):\n",
    "    return [\n",
    "        {\n",
    "            file_name: frozenset(\n",
    "                [encode_bytes(function) for function in lower_level_edges.keys()]\n",
    "            )\n",
    "        }\n",
    "        for (file_name, lower_level_edges) in zip(upper_level_names, lower_level_edges)\n",
    "        if type(lower_level_edges) is dict and len(lower_level_edges.keys()) > 0\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_records(function_edges):\n",
    "    return [\n",
    "        {\"calling_function\": calling_fn, \"called_function\": called_fn}\n",
    "        for calling_fn in function_edges.keys()\n",
    "        for called_fn in function_edges[calling_fn]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e83ca9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def encode_bytes(s):\n",
    "    return bytes(s, \"UTF-8\")\n",
    "\n",
    "\n",
    "class RepoDependencyGraphFetcher:\n",
    "    def clean_content(self, python_files_df, cleaned_column_name=\"clean_content\"):\n",
    "        python_files_df[cleaned_column_name] = python_files_df[\"content\"].str.replace(\n",
    "            \"429: Too Many Requests\", \"\"\n",
    "        )\n",
    "        python_files_df[cleaned_column_name] = python_files_df[\"clean_content\"].apply(\n",
    "            lambda s: np.nan if s == \"\" else s\n",
    "        )\n",
    "        return python_files_df\n",
    "\n",
    "    def get_repo_and_root_edges(\n",
    "        self, python_files_df, path_col=\"path\", add_repo_label=False\n",
    "    ):\n",
    "        filenames = (\n",
    "            python_files_df[path_col].apply(os.path.basename).str.replace(\".py\", \"\")\n",
    "        )\n",
    "\n",
    "        if add_repo_label:\n",
    "            filenames = python_files_df[\"repo_name\"] + \":\" + filenames\n",
    "        filenames = filenames.apply(encode_bytes)\n",
    "        repo_edges_dict = (\n",
    "            filenames.groupby(python_files_df[\"repo_name\"])\n",
    "            .agg(lambda args: frozenset(args))\n",
    "            .to_dict()\n",
    "        )\n",
    "        repo_edges = [{k: repo_edges_dict[k]} for k in repo_edges_dict]\n",
    "        root_edges = [\n",
    "            {\n",
    "                \"<ROOT>\": frozenset(\n",
    "                    repo for edges in repo_edges for repo in repo_edges_dict.keys()\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "        return root_edges, repo_edges\n",
    "\n",
    "    def get_filename_and_function_edges(\n",
    "        self, python_files_df, clean_content, add_repo_label=False, path_col=\"path\"\n",
    "    ):\n",
    "        if clean_content:\n",
    "            content_column = \"clean_content\"\n",
    "            python_files_df = self.clean_content(python_files_df, content_column)\n",
    "        else:\n",
    "            content_column = \"content\"\n",
    "\n",
    "        filenames = (\n",
    "            python_files_df[path_col].apply(os.path.basename).str.replace(\".py\", \"\")\n",
    "        )\n",
    "        if add_repo_label:\n",
    "            filenames = python_files_df[\"repo_name\"] + \":\" + filenames\n",
    "        filenames = filenames.apply(encode_bytes)\n",
    "        files = python_files_df[content_column]\n",
    "        function_edges = files.apply(try_run(get_function_calls)).dropna()\n",
    "        function_edges = function_edges.to_list()\n",
    "        filename_edges = get_upper_level_edges(filenames, function_edges)\n",
    "        return filename_edges, function_edges\n",
    "\n",
    "    def get_records(self, edges):\n",
    "        return [record for edge_group in edges for record in make_records(edge_group)]\n",
    "\n",
    "    def make_dependency_df(self, records, edge_type):\n",
    "        dependency_records_df = pd.DataFrame.from_records(records)\n",
    "        dependency_records_df.columns = [\"source\", \"destination\"]\n",
    "        dependency_records_df[\"edge_type\"] = edge_type\n",
    "        dependency_records_df = dependency_records_df[\n",
    "            (dependency_records_df[\"source\"].apply(len) > 0)\n",
    "            & (dependency_records_df[\"source\"] != \"null\")\n",
    "            & (dependency_records_df[\"destination\"].apply(len) > 0)\n",
    "            & (dependency_records_df[\"source\"] != dependency_records_df[\"destination\"])\n",
    "        ]\n",
    "        return dependency_records_df.drop_duplicates()\n",
    "\n",
    "    def get_dependency_df(\n",
    "        self,\n",
    "        python_files_df,\n",
    "        dep_type=\"repo\",\n",
    "        add_filename_repo_label=False,\n",
    "        clean_content=True,\n",
    "    ):\n",
    "        print(\"constructing edges\")\n",
    "        if dep_type == \"repo\":\n",
    "            root_edges, repo_edges = self.get_repo_and_root_edges(\n",
    "                python_files_df, add_repo_label=add_filename_repo_label\n",
    "            )\n",
    "            edge_groups = [root_edges, repo_edges]\n",
    "            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]\n",
    "            edge_types = [\"root-repo\", \"repo-file\"]\n",
    "        elif dep_type == \"function\":\n",
    "            filename_edges, function_edges = self.get_filename_and_function_edges(\n",
    "                python_files_df, clean_content, add_repo_label=add_filename_repo_label\n",
    "            )\n",
    "            edge_groups = [filename_edges, function_edges]\n",
    "            record_groups = [self.get_records(edge_group) for edge_group in edge_groups]\n",
    "            edge_types = [\"file-function\", \"function-function\"]\n",
    "        else:\n",
    "            raise NotImplementedError(\"unsupported dep_type: {}\".format(dep_type))\n",
    "        print(\"creating dependency dataframe\")\n",
    "        return pd.concat(\n",
    "            [\n",
    "                self.make_dependency_df(records, edge_type)\n",
    "                for edge_type, records in zip(edge_types, record_groups)\n",
    "            ]\n",
    "        ).drop_duplicates(subset=[\"source\", \"destination\"], keep=\"first\")\n",
    "\n",
    "    def prepare_dependency_records(\n",
    "        self, python_files_df, add_filename_repo_label=False\n",
    "    ):\n",
    "        repo_records_df = self.get_dependency_df(\n",
    "            python_files_df,\n",
    "            \"repo\",\n",
    "            clean_content=True,\n",
    "            add_filename_repo_label=add_filename_repo_label,\n",
    "        )\n",
    "        function_records_df = self.get_dependency_df(\n",
    "            python_files_df,\n",
    "            \"function\",\n",
    "            clean_content=True,\n",
    "            add_filename_repo_label=add_filename_repo_label,\n",
    "        )\n",
    "        dependency_records_df = pd.concat([repo_records_df, function_records_df])\n",
    "        dependency_records_df[\"source\"] = dependency_records_df[\"source\"].apply(\n",
    "            lambda s: s if type(s) is str else s.decode(\"utf-8\")\n",
    "        )\n",
    "        dependency_records_df[\"destination\"] = dependency_records_df[\n",
    "            \"destination\"\n",
    "        ].apply(lambda s: s if type(s) is str else s.decode(\"utf-8\"))\n",
    "        return dependency_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "29816370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_node_degrees(records_df):\n",
    "    outdegree = records_df[\"source\"].value_counts()\n",
    "    indegree = records_df[\"destination\"].value_counts()\n",
    "    degree = outdegree.add(indegree, fill_value=0)\n",
    "    return outdegree, indegree, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f653b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_aggregated_edge_type_df(records_df, edge_type):\n",
    "    sample_df = records_df[records_df[\"edge_type\"] == edge_type]\n",
    "    repos = sample_df.groupby(\"source\").groups.keys()\n",
    "    descriptions = sample_df.groupby(\"source\").apply(\n",
    "        lambda df: \" \".join(set(df[\"destination\"].sample(min(len(df), 1000))))\n",
    "    )\n",
    "    raw_descriptions_df = descriptions.reset_index()\n",
    "    descriptions_df = raw_descriptions_df.apply(\n",
    "        lambda item: item[\"source\"] + \" \" + item[0].replace(item[\"source\"] + \":\", \"\"),\n",
    "        axis=1,\n",
    "    )\n",
    "    descriptions_df.index = raw_descriptions_df[\"source\"]\n",
    "    return descriptions_df\n",
    "\n",
    "\n",
    "def get_descriptions(records_df):\n",
    "    repo_descriptions = get_aggregated_edge_type_df(records_df, \"repo-file\")\n",
    "    file_descriptions = get_aggregated_edge_type_df(records_df, \"file-function\")\n",
    "    file_descriptions.name = \"file_description\"\n",
    "    repo_descriptions.name = \"repo_description\"\n",
    "    return repo_descriptions, file_descriptions\n",
    "\n",
    "\n",
    "def get_description_records_df(records_df):\n",
    "    repo_descriptions, file_descriptions = get_descriptions(records_df)\n",
    "    return records_df.merge(\n",
    "        repo_descriptions, left_on=\"source\", right_index=True\n",
    "    ).merge(file_descriptions, left_on=\"destination\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4b84bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = python_files_df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "433b64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = RepoDependencyGraphFetcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "73ee6281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import pickle\\n\\nimport tensorflow as tf\\nimpo...</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import tensorflow as tf\\nimport math\\nfrom ten...</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import tensorflow as tf\\nfrom tensorflow.contr...</td>\n",
       "      <td>translate/conv_lstm.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import tensorflow as tf\\nfrom translate import...</td>\n",
       "      <td>translate/beam_search.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from tensorflow.python.ops import init_ops\\nfr...</td>\n",
       "      <td>translate/rnn.py</td>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td># flake8: noqa\\n# There's no way to ignore \"F4...</td>\n",
       "      <td>src/transformers/models/byt5/__init__.py</td>\n",
       "      <td>huggingface/transformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td># coding=utf-8\\n# Copyright 2019-present, the ...</td>\n",
       "      <td>src/transformers/models/retribert/configuratio...</td>\n",
       "      <td>huggingface/transformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td># flake8: noqa\\n# There's no way to ignore \"F4...</td>\n",
       "      <td>src/transformers/models/retribert/__init__.py</td>\n",
       "      <td>huggingface/transformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td># coding=utf-8\\n# Copyright 2018 The HuggingFa...</td>\n",
       "      <td>src/transformers/models/retribert/tokenization...</td>\n",
       "      <td>huggingface/transformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td># coding=utf-8\\n# Copyright 2019-present, the ...</td>\n",
       "      <td>src/transformers/models/retribert/modeling_ret...</td>\n",
       "      <td>huggingface/transformers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    import pickle\\n\\nimport tensorflow as tf\\nimpo...   \n",
       "1    import tensorflow as tf\\nimport math\\nfrom ten...   \n",
       "2    import tensorflow as tf\\nfrom tensorflow.contr...   \n",
       "3    import tensorflow as tf\\nfrom translate import...   \n",
       "4    from tensorflow.python.ops import init_ops\\nfr...   \n",
       "..                                                 ...   \n",
       "995  # flake8: noqa\\n# There's no way to ignore \"F4...   \n",
       "996  # coding=utf-8\\n# Copyright 2019-present, the ...   \n",
       "997  # flake8: noqa\\n# There's no way to ignore \"F4...   \n",
       "998  # coding=utf-8\\n# Copyright 2018 The HuggingFa...   \n",
       "999  # coding=utf-8\\n# Copyright 2019-present, the ...   \n",
       "\n",
       "                                                  path  \\\n",
       "0                            translate/import_graph.py   \n",
       "1                                  translate/models.py   \n",
       "2                               translate/conv_lstm.py   \n",
       "3                             translate/beam_search.py   \n",
       "4                                     translate/rnn.py   \n",
       "..                                                 ...   \n",
       "995           src/transformers/models/byt5/__init__.py   \n",
       "996  src/transformers/models/retribert/configuratio...   \n",
       "997      src/transformers/models/retribert/__init__.py   \n",
       "998  src/transformers/models/retribert/tokenization...   \n",
       "999  src/transformers/models/retribert/modeling_ret...   \n",
       "\n",
       "                    repo_name  \n",
       "0             trangvu/ape-npi  \n",
       "1             trangvu/ape-npi  \n",
       "2             trangvu/ape-npi  \n",
       "3             trangvu/ape-npi  \n",
       "4             trangvu/ape-npi  \n",
       "..                        ...  \n",
       "995  huggingface/transformers  \n",
       "996  huggingface/transformers  \n",
       "997  huggingface/transformers  \n",
       "998  huggingface/transformers  \n",
       "999  huggingface/transformers  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "12ac1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing edges\n",
      "creating dependency dataframe\n",
      "constructing edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-143-3a361432cfe4>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')\n",
      "<ipython-input-143-3a361432cfe4>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  python_files_df[cleaned_column_name] = python_files_df['content'].str.replace(\"429: Too Many Requests\", \"\")\n",
      "<ipython-input-143-3a361432cfe4>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  python_files_df[cleaned_column_name] = python_files_df['clean_content'].apply(lambda s: np.nan if s == \"\" else s)\n",
      "<ipython-input-143-3a361432cfe4>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dependency dataframe\n"
     ]
    }
   ],
   "source": [
    "records_df = wrapper.prepare_dependency_records(\n",
    "    sample_files, add_filename_repo_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6edbfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_file_destinations = records_df[records_df[\"edge_type\"] == \"repo-file\"][\n",
    "    \"destination\"\n",
    "]\n",
    "assert \"harryhan618/LaneNet:postprocess\" in labeled_file_destinations.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9679d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing edges\n",
      "creating dependency dataframe\n",
      "constructing edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-143-3a361432cfe4>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')\n",
      "<ipython-input-143-3a361432cfe4>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  python_files_df[cleaned_column_name] = python_files_df['content'].str.replace(\"429: Too Many Requests\", \"\")\n",
      "<ipython-input-143-3a361432cfe4>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  python_files_df[cleaned_column_name] = python_files_df['clean_content'].apply(lambda s: np.nan if s == \"\" else s)\n",
      "<ipython-input-143-3a361432cfe4>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  filenames = python_files_df[path_col].apply(os.path.basename).str.replace('.py', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dependency dataframe\n"
     ]
    }
   ],
   "source": [
    "records_df = wrapper.prepare_dependency_records(\n",
    "    sample_files, add_filename_repo_label=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cee843cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_file_destinations = records_df[records_df[\"edge_type\"] == \"repo-file\"][\n",
    "    \"destination\"\n",
    "]\n",
    "assert \"postprocess\" in labeled_file_destinations.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_records_df_with_labeled_files(records_df):\n",
    "    outdegree, indegree, degree = get_node_degrees(records_df)\n",
    "    labeled_files_dependency_records_df = records_df.copy()\n",
    "    labeled_files = labeled_files_dependency_records_df[\n",
    "        labeled_files_dependency_records_df[\"edge_type\"] == \"repo-file\"\n",
    "    ][\"destination\"]\n",
    "    labeled_files = (\n",
    "        labeled_files_dependency_records_df[\n",
    "            labeled_files_dependency_records_df[\"edge_type\"] == \"repo-file\"\n",
    "        ][\"source\"]\n",
    "        + \":\"\n",
    "        + labeled_files\n",
    "    )\n",
    "    labeled_files_dependency_records_df[\"destination\"][\n",
    "        labeled_files_dependency_records_df[\"edge_type\"] == \"repo-file\"\n",
    "    ] = labeled_files\n",
    "    return labeled_files_dependency_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41538f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df = pd.read_csv(\"output/dependency_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdegree, indegree, degree = get_node_degrees(dependency_records_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6518df61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"trangvu/ape-npi\" in python_files_df[\"repo_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dependency_records_df = repo_dependency_graph_fetcher.get_dependency_df(python_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bytes(s):\n",
    "    return bytes(s, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c63cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes(\"a\", \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "b\"a\".decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2548e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df.to_csv(\"data/raw_dependency_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = pd.concat(\n",
    "    [dependency_records_df[\"source\"], dependency_records_df[\"destination\"]]\n",
    ").value_counts()\n",
    "valid_nodes = total_counts[(total_counts > 1)].index\n",
    "valid_dependency_records_df = dependency_records_df[\n",
    "    dependency_records_df[\"source\"].isin(valid_nodes)\n",
    "    & dependency_records_df[\"destination\"].isin(valid_nodes)\n",
    "]\n",
    "valid_total_counts = pd.concat(\n",
    "    [valid_dependency_records_df[\"source\"], valid_dependency_records_df[\"destination\"]]\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffe857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b84a5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependency_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[\"edge_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d95a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_edges = dependency_records_df[\"source\"][\n",
    "    dependency_records_df[\"edge_type\"] == \"repo-file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd62673",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716dcbe7",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74861cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[dependency_records_df[\"source\"] == \"fairseq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[dependency_records_df[\"source\"] == \"wgan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff65e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[dependency_records_df[\"source\"] == \"LinearExplainer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[\"destination\"].value_counts().describe()  # plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[\"edge_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_records_df = pd.read_csv(\"data/raw_dependency_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507741a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a597519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd81ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df.to_csv(\"data/raw_dependency_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3160b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependency_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41322e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2190556",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dependency_records_df.to_csv(\"data/dependency_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dependency_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe98c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_dependency_records_df[valid_dependency_records_df[\"edge_type\"] == \"repo-file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce31b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = repo_dependency_graph_fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f60bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_edges, repo_edges = self.get_repo_and_root_edges(sample_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_groups = [root_edges, repo_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(root_edges[0][\"<ROOT>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_groups = [self.get_records(edge_group) for edge_group in edge_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec2f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26223af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71566bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ae59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79309032",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_df = get_sample_files_df(python_files_df, n_files=1000)\n",
    "repo_records_df = repo_dependency_fetcher.get_dependency_df(\n",
    "    sample_files_df, \"repo\", clean_content=True\n",
    ")\n",
    "function_records_df = repo_dependency_fetcher.get_dependency_df(\n",
    "    sample_files_df, \"function\", clean_content=True\n",
    ")\n",
    "dependency_records_df = pd.concat([repo_records_df, function_records_df])\n",
    "dependency_records_df[\"source\"] = dependency_records_df[\"source\"].apply(\n",
    "    lambda s: s if type(s) is str else s.decode(\"utf-8\")\n",
    ")\n",
    "dependency_records_df[\"destination\"] = dependency_records_df[\"destination\"].apply(\n",
    "    lambda s: s if type(s) is str else s.decode(\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b\"1\".decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df36ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df[\"destination\"] = dependency_records_df[\"destination\"].apply(\n",
    "    lambda s: s if type(s) is str else s.decode(\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df.to_csv(\"data/dependency_records.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

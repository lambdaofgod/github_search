{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ca2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb389bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4123905",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_t5_path = \"output/doc_id_generation_model/best_checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base-multi-sum\")#(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d879e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_path_summary(texts, n_beams=16, max_length=256, max_label_length=64, min_length=32):\n",
    "    inputs = tokenizer(texts, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "    summaries = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "                     attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "                     length_penalty=0.8, num_beams=n_beams, max_length=max_label_length, min_length=min_length)\n",
    "    return tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "import ast\n",
    "from github_search import seq2seq_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec00dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df = pd.read_csv(\"output/papers_with_readmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8829f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tasks = paperswithcode_df['tasks'].apply(lambda ts: \", \".join(ast.literal_eval(ts)))\n",
    "repo_tasks = pd.DataFrame({\"repo\": paperswithcode_df['repo'], \"tasks\": repo_tasks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df = repo_tasks.merge(files_df, on='repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776691e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_dataset = datasets.load_from_disk(\"output/seq2seq_hf_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eade5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_area_ds= datasets.Dataset.from_pandas(pd.read_csv(\"data/paperswithcode_tasks.csv\").dropna()[['area', 'task_description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_area_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd750172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plbart_str = \"uclanlp/plbart-single_task-en_python\"#uclanlp/plbart-python-en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25753b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(plbart_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa806178",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = transformers.AutoModelForSequenceClassification.from_pretrained(plbart_str, num_labels=len(set(paperswithcode_area_ds['area'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"task_description\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9233ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "cross_encoder = sentence_transformers.cross_encoder.CrossEncoder(plbart_str, max_length=128)\n",
    "files_sample = files_with_tasks_df.iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_contents = seq2seq_dataset['contents'][::32][:10]\n",
    "example_doc_idxs = seq2seq_dataset['doc_id'][::32][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(map(tuple, files_with_tasks_df[['tasks', 'repo', 'path', 'content']].itertuples(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd77419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import tqdm\n",
    "\n",
    "pairs_with_sims = []\n",
    "for i in tqdm.tqdm(range(1000)):\n",
    "    print()\n",
    "    pair = pairs[i]\n",
    "    file = pair[-1]\n",
    "    tasks = pair[0]\n",
    "    full_path = pair[1] + \"/\" + pair[2]\n",
    "    sim = cross_encoder.predict([[\"tasks: \" + tasks, file]]).tolist()[0]\n",
    "    pairs_with_sims.append((*pair, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_with_sims_df = pd.DataFrame(pairs_with_sims, columns=[\"tasks\", \"repo\", \"path\", \"contents\", \"similarity\"])\n",
    "pairs_with_sims_df['similarity'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_with_sims_df.groupby(\"repo\").agg([\"mean\"]).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d794d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_with_sims_df.groupby(\"repo\").agg([\"max\"]).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32864179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__, tokenizer = seq2seq_utils.get_seq2seq_model_with_tokenizer(\"Salesforce/codet5-base-multi-sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831284f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.RobertaTokenizerFast.from_pretrained(\"Salesforce/codet5-base-multi-sum\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3db7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = tokenizer(\"<PATH_TASK_SEP>\", add_special_tokens=False)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "inputs = tokenizer(example_contents, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "             attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "             length_penalty=0.8, num_beams=10, max_length=64, min_length=8)#, decoder_start_token_id=decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[7].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_paths = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in output_paths], columns=[\"path\", \"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b15c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in example_doc_idxs], columns=[\"path\", \"tasks\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ee5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49947e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec50fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = [content + \" predict docID: \" for (content, doc_id) in zip(example_contents, example_doc_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b18b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sru_contents = '''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "import sru\n",
    "\n",
    "\n",
    "rnn_class_type_mapping = {\"lstm\": nn.LSTM, \"sru\": sru.SRU}\n",
    "\n",
    "\n",
    "class SentenceRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    sentence_transformers RNN wrapper\n",
    "    \"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea269b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction_contents = '''\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "class IncrementalHyperbolicMDS:\n",
    "    def __init__(self, n_components, dtype=\"float16\"):\n",
    "        self.ipca = decomposition.IncrementalPCA(n_components=n_components)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def partial_fit(self, D):\n",
    "        Y = -np.cosh(D)\n",
    "        self.ipca.partial_fit(Y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe57bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_contents = '''\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import compose, feature_extraction, metrics\n",
    "from functools import reduce, partial\n",
    "import attr\n",
    "from typing import Union\n",
    "import umap\n",
    "import altair\n",
    "\n",
    "\n",
    "from game_recommender import steam_data\n",
    "'''# + \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mining_content = '''\n",
    "def get_wordnet_similarity(\n",
    "    word, another_word, similarity_method=\"resnik\", pos=None, ic=None\n",
    "):\n",
    "    if ic is None:\n",
    "        ic = wordnet_ic.ic(\"ic-semcor.dat\")\n",
    "    assert similarity_method in [\n",
    "        \"lin\",\n",
    "        \"jcn\",\n",
    "        \"resnik\",\n",
    "    ], \"Unsupported similarity method: \" + str(similarity_method)\n",
    "    word_synset = wn.synsets(word, pos)[0]\n",
    "    another_word_synset = wn.synsets(another_word, pos)[0]\n",
    "    if similarity_method == \"lin\":\n",
    "        return word_synset.lin_similarity(another_word_synset, ic)\n",
    "    elif similarity_method == \"jcn\":\n",
    "        return word_synset.jcn_similarity(another_word_synset, ic)\n",
    "    else:\n",
    "        return word_synset.res_similarity(another_word_synset, ic)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_content = '''\n",
    "import numpy as np\n",
    "import attr\n",
    "from toolz import partial\n",
    "from scarce_learn.zero_shot import zsl_base\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from scarce_learn.zero_shot import torch_util\n",
    "\n",
    "\n",
    "class DEVISELayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_class_features, margin, init_weights_std=0.1):\n",
    "        super(DEVISELayer, self).__init__()\n",
    "        init_weights = init_weights_std * torch.randn(n_features, n_class_features) \n",
    "        self.weights = nn.Parameter(data=init_weights.cuda())\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, X, y, label_embeddings):\n",
    "        loss = torch.Tensor([0]).cuda()\n",
    "        for i in range(X.shape[0]):\n",
    "            loss += self._devise_loss(X[i], y[i], label_embeddings)\n",
    "        return loss\n",
    "\n",
    "    def _devise_loss(self, embedding, label, label_embeddings):\n",
    "        indicator = torch.ones(label_embeddings.shape[0], dtype=bool)\n",
    "        indicator[label] = 0\n",
    "        per_class_loss = torch_util.similarity_based_hinge_loss(self.weights, embedding, label, label_embeddings)\n",
    "        return nn.ReLU()(self.margin + per_class_loss).sum()\n",
    "\n",
    "    def predict(self, X, label_embeddings):\n",
    "        class_similarities = torch_util.bilinear_feature_similarity(self.weights, X, label_embeddings)\n",
    "        return torch.argmax(class_similarities, axis=1)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_content = '''\n",
    "import attr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "try:\n",
    "    import numba\n",
    "except ImportError as e:\n",
    "    logging.warning(\n",
    "        \"numba not found, you'll not be able to use mlutil.evolutionary_algorithms.multiobjective\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bounded_gaussian_noise_mutation(x, n_mutants, lo=0, hi=1, sigma=1e-2):\n",
    "    noise = sigma * np.random.randn(n_mutants, x.shape[-1])\n",
    "    return np.clip(x + noise, lo, hi)\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class NSGAII:\n",
    "\n",
    "    optimized_function = attr.ib()\n",
    "    chromosome_size: int = attr.ib()\n",
    "    mutation_function = attr.ib(default=bounded_gaussian_noise_mutation)\n",
    "    random_initializer = attr.ib(default=np.random.rand)\n",
    "    population_bounds = attr.ib(default=(0, 1))\n",
    "    objective_names = attr.ib(default=(\"1st objective\", \"2nd objective\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content = '''\n",
    "import numpy as np\n",
    "import ot\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def get_stem_vectors(filtered_stems, keyed_vectors):\n",
    "    return np.vstack(\n",
    "        [\n",
    "            np.mean([keyed_vectors[w] for w in stem_list], axis=0)\n",
    "            for stem_list in filtered_stems\n",
    "            if len(stem_list) > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_word_vector_optimal_transport(\n",
    "    word_vectors1, word_vectors2, ot_method=ot.sinkhorn, reg=0.01, normalize_dists=True\n",
    "):\n",
    "    cost = cosine_distances(word_vectors1, word_vectors2)\n",
    "    height, width = cost.shape\n",
    "    a = np.ones(height)\n",
    "    b = np.ones(width)\n",
    "    if normalize_dists:\n",
    "        a = a / a.sum()\n",
    "        b = b / b.sum()\n",
    "    ot_matrix = ot_method(a, b, cost, reg=reg)\n",
    "    return ot_matrix, (ot_matrix * cost).sum()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6851712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d49acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_content = \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6655fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(path_content, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=5, max_length=128, min_length=32)[0].tolist(), top_k=0, top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ea1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"_nbdev.py\", \"haystack_search.py\", \"rss_feeds.py\", \"zero_shot_learning.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_series = \"lambdaofgod <REPO_NAME_SEP> pytorch_hackathon <REPO_PATH_SEP> pytorch_hackathon <PATH_TASK_SEP> \" + pd.Series(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbf822",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(paths_series[3], return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=10, max_length=128, min_length=16)[0].tolist(), #, top_p=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = seq2seq_dataset[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12852637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(examples, tokenize=False):\n",
    "    if tokenize:\n",
    "        inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n",
    "                       max_length=128, return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = {\"input_ids\": torch.tensor(examples[\"input_ids\"]).to(model.device), \"attention_mask\": torch.tensor(examples[\"attention_mask\"]).to(model.device)}\n",
    "    with torch.no_grad():\n",
    "        model_output = model.encoder(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset = seq2seq_dataset.train_test_split(test_size=10000)['test'].map(embed_text,\n",
    "    batched=True,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset[0].keys()#['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"metric learning\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"implements zero-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encoder(**tokenizer(query_text, return_tensors=\"pt\").to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10403f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = query_emb.last_hidden_state.mean(axis=1)[0].to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406af109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32223d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_emb = np.array(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9e7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.iloc[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019603d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample = files_with_tasks_df.iloc[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df722b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[embs_dataset[\"contents\"][i] for i in list(embs_dataset.search(\"embedding\", query=query_emb.astype(\"float32\")).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9159f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94208380",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(**(tokenizer(example_imports, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=20, max_length=128, min_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = model.encoder(**inputs.to(model.device)) #decoder_input_ids=torch.tensor([[decoder_start_token_id]]).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f80238",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = files_with_tasks_df['tasks'].str.split(',').explode().str.strip().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e590b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode((-lm_output.logits).argsort()[:,:,:5][0,0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7f285",
   "metadata": {},
   "source": [
    "# TODO wyławianie istotnych informacji z konfigów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_doc_ids = get_predicted_path_summary(example_contents, max_length=64, min_length=16, max_label_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tasks = [d for d in generated_doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tasks = [d for d in example_doc_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in zip(predicted_tasks, true_tasks, example_contents):\n",
    "    print(\"#\")\n",
    "    print(p[2])\n",
    "    print(p[0])\n",
    "    print(p[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator \n",
    "\n",
    "import sentence_transformers\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08766d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_t5_path = \"output/doc_id_generation_model/best_checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base-multi-sum\")#(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d97431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"output/doc_id_generation_model/best_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_path_summary(texts, n_beams=16, max_length=256, max_label_length=64, min_length=32):\n",
    "    inputs = tokenizer(texts, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "    summaries = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "                     attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "                     length_penalty=0.8, num_beams=n_beams, max_length=max_label_length, min_length=min_length)\n",
    "    return tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7071d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "import ast\n",
    "from github_search import seq2seq_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_df = pd.read_csv(\"output/papers_with_readmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_df = pd.read_feather(\"output/selected_python_files_imports.feather\")\n",
    "files_df = pd.read_feather(\"output/selected_python_files.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ace297",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tasks = paperswithcode_df['tasks'].apply(lambda ts: \", \".join(ast.literal_eval(ts)))\n",
    "repo_tasks = pd.DataFrame({\"repo\": paperswithcode_df['repo'], \"tasks\": repo_tasks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df = repo_tasks.merge(files_df, on='repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_dataset = datasets.load_from_disk(\"output/seq2seq_hf_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32372ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_area_ds= datasets.Dataset.from_pandas(pd.read_csv(\"data/paperswithcode_tasks.csv\").dropna()[['area', 'task_description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plbart_str = \"uclanlp/plbart-single_task-en_python\"#uclanlp/plbart-python-en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ade0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = sentence_transformers.cross_encoder.CrossEncoder(\"microsoft/codebert-base\", max_length=512, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08837256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df = files_with_tasks_df.iloc[::25].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc362be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df['tasks'].str.split(\", \").explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df['repo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPairDataPreprocessor:\n",
    "    \n",
    "    def __init__(self, first_text_columns, second_text_columns, add_first_text_columns_prompts=True, add_second_text_columns_prompts=False):\n",
    "        self.first_text_columns = first_text_columns\n",
    "        self.second_text_columns = second_text_columns \n",
    "        self.first_interpolation_str = self.get_interpolation_str(first_text_columns, add_first_text_columns_prompts)\n",
    "        self.second_interpolation_str = self.get_interpolation_str(second_text_columns, add_second_text_columns_prompts)\n",
    "    \n",
    "    def get_interpolation_str(self, columns, add_prompt):\n",
    "        if add_prompt:\n",
    "            return \"\\n\".join(f\"# {col}: \" + \"{}\" for col in columns)\n",
    "        else:\n",
    "            return \"\\n\".join([\"{}\" for __ in range(len(columns))])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        first_pretty_interpolation_str = \"\\t\" + self.first_interpolation_str.replace(\"\\n\", \"\\n\\t\")\n",
    "        second_pretty_interpolation_str = \"\\t\" + self.second_interpolation_str.replace(\"\\n\", \"\\n\\t\")\n",
    "        return (f\"{self.__class__.__name__}\\n\" +\n",
    "            f\"first text columns: {self.first_text_columns}\\n\" +  \n",
    "            f\"second text columns: {self.second_text_columns}\\n\" +  \n",
    "            f\"pattern:\\n {first_pretty_interpolation_str}\\n{second_pretty_interpolation_str}\"\n",
    "        )\n",
    "       \n",
    "    def prepare_input_examples(self, df, label):\n",
    "        return [\n",
    "            sentence_transformers.InputExample(\n",
    "                texts=[self.first_interpolation_str.format(*values),  self.second_interpolation_str.format(*second_values)],\n",
    "                label=label\n",
    "            )\n",
    "            for values, second_values in zip(zip(*[df[col] for col in self.first_text_columns]), zip(* [df[col] for col in self.second_text_columns]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea584ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_content_preprocessor = TextPairDataPreprocessor(first_text_columns=[\"repo\", \"path\", \"tasks\"], second_text_columns=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_content_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_tasks_df, 1)[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8833036",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_input_examples = repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_tasks_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81416fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_permuted_tasks_df = sample_files_with_tasks_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a097255",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_permuted_tasks_df['tasks'] = sample_files_with_tasks_df['tasks'].sample(len(sample_files_with_tasks_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_input_examples = repo_path_task_content_preprocessor.prepare_input_examples(sample_files_with_permuted_tasks_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a550112",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_examples = positive_input_examples + negative_input_examples\n",
    "input_example_labels = np.ones(len(input_examples))\n",
    "input_example_labels[len(positive_input_examples):] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_examples, test_input_examples = model_selection.train_test_split(input_examples, stratify=input_example_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c78a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = pd.Series([\" \".join(ie.texts) for ie in input_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04710400",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = files_with_tasks_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_input_examples, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec06091",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.fit(train_dataloader,\n",
    "  epochs=5,\n",
    "  evaluator=evaluator,\n",
    "  use_amp=True,\n",
    "  callback=lambda score, epoch, steps: print(\"epoch {} score: {}\".format(epoch, round(score, 3)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.save(\"output/sbert/cross_encoder_repo_path_task_10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620db84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(positive_input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7384307",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_scores = cross_encoder.predict([ex.texts for ex in test_input_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdd886",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_scores = np.array([ex.label for ex in test_input_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab783c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pd.DataFrame({\"test_score_residual\": test_true_scores - test_predicted_scores}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37902a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [ex.texts for ex in test_input_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2890861",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16357b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tasks = [ex[0].split(\"tasks: \")[-1] for ex in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d72171",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.concat([\n",
    "    pd.DataFrame.from_records(test_texts, columns=[\"text1\", \"text2\"]),\n",
    "    pd.Series(test_tasks, name=\"tasks\").str.split(\", \"),\n",
    "    pd.Series(test_predicted_scores, name=\"model_score\"),\n",
    "    pd.Series(test_true_scores, name=\"is_positive\"),\n",
    "],\n",
    "axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[test_results_df['is_positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf45c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[test_results_df['is_positive'] == 1].explode(\"tasks\").groupby(\"tasks\").agg([\"mean\", \"count\"]).sort_values((\"model_score\", \"mean\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[test_results_df['is_positive'] == 1].explode(\"tasks\").groupby(\"tasks\").agg([\"mean\", \"count\"])[(\"model_score\", \"mean\")].plot.hist()#, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938be501",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_preprocessor = TextPairDataPreprocessor(first_text_columns=[\"repo\", \"path\"], second_text_columns=[\"tasks\"])\n",
    "repo_path_content_task_preprocessor = TextPairDataPreprocessor(first_text_columns=[\"repo\", \"path\", \"content\"], second_text_columns=[\"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ddd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_content_task_predictions = cross_encoder.predict([ex.texts for ex in repo_path_content_task_preprocessor.prepare_input_examples(files_with_tasks_df[1::25], 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57053783",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_predictions = cross_encoder.predict([ex.texts for ex in repo_path_task_preprocessor.prepare_input_examples(files_with_tasks_df[1::25], 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ex.texts for ex in repo_path_task_preprocessor.prepare_input_examples(files_with_tasks_df[:1:25], 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fcca6",
   "metadata": {},
   "source": [
    "# repo + path  ~ tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f469011",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(repo_path_task_predictions).plot.hist()\n",
    "pd.Series(repo_path_task_predictions).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7529b",
   "metadata": {},
   "source": [
    "# repo + path + content ~ tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e64125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(repo_path_content_task_predictions).plot.hist()\n",
    "pd.Series(repo_path_content_task_predictions).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformers.SentenceTransformer(\"output/sbert/cross_encoder_repo_path_task_10k/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.predict([\n",
    "    [\"# tasks: neural networks \\n\", positive_input_pairs[0][1]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f379a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_without_content =[[f\"#repo: {repo}\\n #path: {path}\", f\"# tasks: {tasks}\"]\n",
    "    for (repo, tasks, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['tasks'],\n",
    "        sample_files_with_tasks_df['path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935235c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_without_content[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_task_scores = cross_encoder.predict([\n",
    "    [f\"#repo: {repo}\\n #path: {path}\", f\"#tasks: {tasks}\"]\n",
    "    for (repo, tasks, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['tasks'],\n",
    "        sample_files_with_tasks_df['path'])]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_content_scores = cross_encoder.predict([\n",
    "    [f\"#repo: {repo}\\n #path: {path}\", f\"#content: {content}\"]\n",
    "    for (repo, content, path) in zip(\n",
    "        sample_files_with_tasks_df['repo'],\n",
    "        sample_files_with_tasks_df['content'],\n",
    "        sample_files_with_tasks_df['path'])]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29faac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(repo_path_content_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(repo_path_task_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7c453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_with_tasks_df.sort_values(\"score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9571d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18902e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd65ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__, tokenizer = seq2seq_utils.get_seq2seq_model_with_tokenizer(\"Salesforce/codet5-base-multi-sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.RobertaTokenizerFast.from_pretrained(\"Salesforce/codet5-base-multi-sum\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e99fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = tokenizer(\"<PATH_TASK_SEP>\", add_special_tokens=False)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "inputs = tokenizer(example_contents, max_length=max_length,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "             attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "             length_penalty=0.8, num_beams=10, max_length=64, min_length=8)#, decoder_start_token_id=decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[7].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_paths = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in output_paths], columns=[\"path\", \"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df = pd.DataFrame.from_records([p.split(\"<PATH_TASK_SEP>\") for p in example_doc_idxs], columns=[\"path\", \"tasks\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d23456",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_ids_df['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = [content + \" predict docID: \" for (content, doc_id) in zip(example_contents, example_doc_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c08e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sru_contents = '''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "import sru\n",
    "\n",
    "\n",
    "rnn_class_type_mapping = {\"lstm\": nn.LSTM, \"sru\": sru.SRU}\n",
    "\n",
    "\n",
    "class SentenceRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    sentence_transformers RNN wrapper\n",
    "    \"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction_contents = '''\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "class IncrementalHyperbolicMDS:\n",
    "    def __init__(self, n_components, dtype=\"float16\"):\n",
    "        self.ipca = decomposition.IncrementalPCA(n_components=n_components)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def partial_fit(self, D):\n",
    "        Y = -np.cosh(D)\n",
    "        self.ipca.partial_fit(Y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b97b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_contents = '''\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import compose, feature_extraction, metrics\n",
    "from functools import reduce, partial\n",
    "import attr\n",
    "from typing import Union\n",
    "import umap\n",
    "import altair\n",
    "\n",
    "\n",
    "from game_recommender import steam_data\n",
    "'''# + \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d137fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mining_content = '''\n",
    "def get_wordnet_similarity(\n",
    "    word, another_word, similarity_method=\"resnik\", pos=None, ic=None\n",
    "):\n",
    "    if ic is None:\n",
    "        ic = wordnet_ic.ic(\"ic-semcor.dat\")\n",
    "    assert similarity_method in [\n",
    "        \"lin\",\n",
    "        \"jcn\",\n",
    "        \"resnik\",\n",
    "    ], \"Unsupported similarity method: \" + str(similarity_method)\n",
    "    word_synset = wn.synsets(word, pos)[0]\n",
    "    another_word_synset = wn.synsets(another_word, pos)[0]\n",
    "    if similarity_method == \"lin\":\n",
    "        return word_synset.lin_similarity(another_word_synset, ic)\n",
    "    elif similarity_method == \"jcn\":\n",
    "        return word_synset.jcn_similarity(another_word_synset, ic)\n",
    "    else:\n",
    "        return word_synset.res_similarity(another_word_synset, ic)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e981bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_content = '''\n",
    "import numpy as np\n",
    "import attr\n",
    "from toolz import partial\n",
    "from scarce_learn.zero_shot import zsl_base\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from scarce_learn.zero_shot import torch_util\n",
    "\n",
    "\n",
    "class DEVISELayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_class_features, margin, init_weights_std=0.1):\n",
    "        super(DEVISELayer, self).__init__()\n",
    "        init_weights = init_weights_std * torch.randn(n_features, n_class_features) \n",
    "        self.weights = nn.Parameter(data=init_weights.cuda())\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, X, y, label_embeddings):\n",
    "        loss = torch.Tensor([0]).cuda()\n",
    "        for i in range(X.shape[0]):\n",
    "            loss += self._devise_loss(X[i], y[i], label_embeddings)\n",
    "        return loss\n",
    "\n",
    "    def _devise_loss(self, embedding, label, label_embeddings):\n",
    "        indicator = torch.ones(label_embeddings.shape[0], dtype=bool)\n",
    "        indicator[label] = 0\n",
    "        per_class_loss = torch_util.similarity_based_hinge_loss(self.weights, embedding, label, label_embeddings)\n",
    "        return nn.ReLU()(self.margin + per_class_loss).sum()\n",
    "\n",
    "    def predict(self, X, label_embeddings):\n",
    "        class_similarities = torch_util.bilinear_feature_similarity(self.weights, X, label_embeddings)\n",
    "        return torch.argmax(class_similarities, axis=1)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_content = '''\n",
    "import attr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "try:\n",
    "    import numba\n",
    "except ImportError as e:\n",
    "    logging.warning(\n",
    "        \"numba not found, you'll not be able to use mlutil.evolutionary_algorithms.multiobjective\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bounded_gaussian_noise_mutation(x, n_mutants, lo=0, hi=1, sigma=1e-2):\n",
    "    noise = sigma * np.random.randn(n_mutants, x.shape[-1])\n",
    "    return np.clip(x + noise, lo, hi)\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class NSGAII:\n",
    "\n",
    "    optimized_function = attr.ib()\n",
    "    chromosome_size: int = attr.ib()\n",
    "    mutation_function = attr.ib(default=bounded_gaussian_noise_mutation)\n",
    "    random_initializer = attr.ib(default=np.random.rand)\n",
    "    population_bounds = attr.ib(default=(0, 1))\n",
    "    objective_names = attr.ib(default=(\"1st objective\", \"2nd objective\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content = '''\n",
    "import numpy as np\n",
    "import ot\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "\n",
    "def get_stem_vectors(filtered_stems, keyed_vectors):\n",
    "    return np.vstack(\n",
    "        [\n",
    "            np.mean([keyed_vectors[w] for w in stem_list], axis=0)\n",
    "            for stem_list in filtered_stems\n",
    "            if len(stem_list) > 0\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_word_vector_optimal_transport(\n",
    "    word_vectors1, word_vectors2, ot_method=ot.sinkhorn, reg=0.01, normalize_dists=True\n",
    "):\n",
    "    cost = cosine_distances(word_vectors1, word_vectors2)\n",
    "    height, width = cost.shape\n",
    "    a = np.ones(height)\n",
    "    b = np.ones(width)\n",
    "    if normalize_dists:\n",
    "        a = a / a.sum()\n",
    "        b = b / b.sum()\n",
    "    ot_matrix = ot_method(a, b, cost, reg=reg)\n",
    "    return ot_matrix, (ot_matrix * cost).sum()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e919cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_transport_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea169262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_content = \" predict docID: lambdaofgod <REPO_NAME_SEP> mlutil <REPO_PATH_SEP> mlutil/recommendation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73399dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(path_content, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=5, max_length=128, min_length=32)[0].tolist(), top_k=0, top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"_nbdev.py\", \"haystack_search.py\", \"rss_feeds.py\", \"zero_shot_learning.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_series = \"lambdaofgod <REPO_NAME_SEP> pytorch_hackathon <REPO_PATH_SEP> pytorch_hackathon <PATH_TASK_SEP> \" + pd.Series(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa45f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**(tokenizer(paths_series[3], return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=10, max_length=128, min_length=16)[0].tolist(), #, top_p=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad26a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = seq2seq_dataset[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00130ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dec369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(examples, tokenize=False):\n",
    "    if tokenize:\n",
    "        inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n",
    "                       max_length=128, return_tensors=\"pt\")\n",
    "    else:\n",
    "        inputs = {\"input_ids\": torch.tensor(examples[\"input_ids\"]).to(model.device), \"attention_mask\": torch.tensor(examples[\"attention_mask\"]).to(model.device)}\n",
    "    with torch.no_grad():\n",
    "        model_output = model.encoder(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "    return {\"embedding\": pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset = seq2seq_dataset.train_test_split(test_size=10000)['test'].map(embed_text,\n",
    "    batched=True,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset[0].keys()#['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"metric learning\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c085a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1af91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"implements zero-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encoder(**tokenizer(query_text, return_tensors=\"pt\").to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de078d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = query_emb.last_hidden_state.mean(axis=1)[0].to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d305d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_emb = np.array(embs_dataset[0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b109a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20df3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.iloc[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample = files_with_tasks_df.iloc[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb33d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab242e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36769309",
   "metadata": {},
   "outputs": [],
   "source": [
    "[embs_dataset[\"contents\"][i] for i in list(embs_dataset.search(\"embedding\", query=query_emb.astype(\"float32\")).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ff169",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(**(tokenizer(example_imports, return_tensors=\"pt\").to(model.device)),\n",
    "    num_beams=20, max_length=128, min_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc71881",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = model.encoder(**inputs.to(model.device)) #decoder_input_ids=torch.tensor([[decoder_start_token_id]]).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = files_with_tasks_df['tasks'].str.split(',').explode().str.strip().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5073813",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode((-lm_output.logits).argsort()[:,:,:5][0,0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56bb9d",
   "metadata": {},
   "source": [
    "# TODO wyławianie istotnych informacji z konfigów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b83c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_doc_ids = get_predicted_path_summary(example_contents, max_length=64, min_length=16, max_label_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38356509",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tasks = [d for d in generated_doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tasks = [d for d in example_doc_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in zip(predicted_tasks, true_tasks, example_contents):\n",
    "    print(\"#\")\n",
    "    print(p[2])\n",
    "    print(p[0])\n",
    "    print(p[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

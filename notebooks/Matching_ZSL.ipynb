{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp matching_zsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import ast\n",
    "import tqdm\n",
    "import json\n",
    "import attr\n",
    "from operator import itemgetter\n",
    "\n",
    "from scarce_learn import zero_shot\n",
    "from mlutil.feature_extraction import embeddings\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction, metrics, model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "from github_search import paperswithcode_tasks\n",
    "\n",
    "import mlutil\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from scarce_learn.zero_shot import devise_jax, devise_torch\n",
    "\n",
    "from github_search import github_readmes\n",
    "import concurrent.futures\n",
    "from github_search import python_call_graph\n",
    "import gensim\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upstream\n",
    "\n",
    "import_corpus_path = 'output/module_corpus.csv'\n",
    "word_vectors_filename = 'output/import2vec_module_vectors.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "import_corpus_df = pd.read_csv(import_corpus_path)\n",
    "per_repo_imports = import_corpus_df.groupby('repo')['imports'].agg(sum).apply(set)\n",
    "import_corpus_df['imports'] = import_corpus_df['imports'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 4.14 s, total: 3min 37s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "python_files_df = pd.read_csv('data/crawled_python_files.csv', encoding='latin-1')\n",
    "repo_names = python_files_df['repo_name']\n",
    "import_corpus_df = pd.read_csv(import_corpus_path)\n",
    "per_repo_imports = import_corpus_df.groupby('repo')['imports'].agg(sum).apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797972, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1749175, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   trangvu/ape-npi\n",
       "1                   trangvu/ape-npi\n",
       "2                   trangvu/ape-npi\n",
       "3                   trangvu/ape-npi\n",
       "4                   trangvu/ape-npi\n",
       "                     ...           \n",
       "1797967    vuanhtu1993/Keras-SRGANs\n",
       "1797968    vuanhtu1993/Keras-SRGANs\n",
       "1797969    vuanhtu1993/Keras-SRGANs\n",
       "1797970    vuanhtu1993/Keras-SRGANs\n",
       "1797971    vuanhtu1993/Keras-SRGANs\n",
       "Name: repo_name, Length: 1797972, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df['repo_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26999,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df['repo_name'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python_files_df['repo'] = python_files_df['repo_name'].str.split(\"/\").apply(itemgetter(1))  + '/' + python_files_df['repo_name']\n",
    "repo_names_tmp = python_files_df['repo_name']\n",
    "repo_names = repo_names_tmp.unique()\n",
    "python_files_df['repo_name'] = python_files_df['repo']\n",
    "python_files_df['repo'] = repo_names_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.01 ms, sys: 116 Âµs, total: 7.13 ms\n",
      "Wall time: 8.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import2vec = gensim.models.KeyedVectors.load(word_vectors_filename)\n",
    "import2vec_embedder = mlutil.feature_extraction.embeddings.AverageWordEmbeddingsVectorizer(import2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv('output/papers_with_imports.csv')\n",
    "paperswithcode_with_imports_df['tasks'] = paperswithcode_with_imports_df['tasks'].str.replace(\"2d \", \"\").str.replace(\"3d \", \"\").str.replace(\"4d \", \"\").str.replace(\"6d \", \"\").str.lower().apply(ast.literal_eval)\n",
    "paperswithcode_with_imports_df['imports'] = paperswithcode_with_imports_df['imports'].str.replace(\"set\\(\\)\", \"{}\").apply(ast.literal_eval)#str.replace(\"2d \", \"\").str.replace(\"3d \", \"\").str.replace(\"4d \", \"\").str.replace(\"6d \", \"\").str.lower().apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 23)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df['n_imports'] = paperswithcode_with_imports_df['imports'].apply(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df['n_imports_with_embeddings'] = paperswithcode_with_imports_df['imports'].apply(lambda imps: len([imp in import2vec.vocab.keys() for imp in imps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 279 ms, total: 33.8 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embeddings = mlutil.feature_extraction.embeddings.load_gensim_embedding_model('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "fasttext_model = fasttext.load_model(\"output/python_files_fasttext_dim200.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "        \n",
    "class LossCallback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:30:51: loading wv recursively from output/abstract_readme_w2v200.bin.wv.* with mmap=None\n",
      "INFO - 11:30:51: loading vectors from output/abstract_readme_w2v200.bin.wv.vectors.npy with mmap=None\n",
      "INFO - 11:30:51: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:30:51: loading vocabulary recursively from output/abstract_readme_w2v200.bin.vocabulary.* with mmap=None\n",
      "INFO - 11:30:51: loading trainables recursively from output/abstract_readme_w2v200.bin.trainables.* with mmap=None\n",
      "INFO - 11:30:51: loading syn1neg from output/abstract_readme_w2v200.bin.trainables.syn1neg.npy with mmap=None\n",
      "INFO - 11:30:51: setting ignored attribute cum_table to None\n",
      "INFO - 11:30:51: loaded output/abstract_readme_w2v200.bin\n"
     ]
    }
   ],
   "source": [
    "python_word_embeddings = gensim.models.Word2Vec.load('output/abstract_readme_w2v200.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@attr.s\n",
    "class RepoTaskData:\n",
    "    \n",
    "    tasks = attr.ib()\n",
    "    repos = attr.ib()\n",
    "    X = attr.ib()\n",
    "    all_tasks = attr.ib()\n",
    "    y = attr.ib()\n",
    "    \n",
    "    def split_tasks(area_grouped_tasks, test_size=0.2):\n",
    "        tasks_train, tasks_test = model_selection.train_test_split(area_grouped_tasks['task'], stratify=area_grouped_tasks['area'], test_size=test_size, random_state=0)\n",
    "        return tasks_train, tasks_test\n",
    "    \n",
    "    def create_split(tasks_train, all_tasks, paperswithcode_with_features_df, X_repr):\n",
    "        train_indicator = paperswithcode_with_features_df['most_common_task'].isin(tasks_train)\n",
    "        print(train_indicator.shape)\n",
    "        repos_train = paperswithcode_with_features_df['repo'][train_indicator]\n",
    "        repos_test = paperswithcode_with_features_df['repo'][~train_indicator]\n",
    "        X_repr = X_repr.apply(lambda x: \" \".join(x))\n",
    "        X_train = X_repr[train_indicator]\n",
    "        X_test = X_repr[~train_indicator]\n",
    "        all_tasks_train = all_tasks[train_indicator]\n",
    "        all_tasks_test = all_tasks[~train_indicator]\n",
    "        y_train = paperswithcode_with_features_df[train_indicator]['most_common_task'].str.lower()\n",
    "        y_test = paperswithcode_with_features_df[~train_indicator]['most_common_task'].str.lower()\n",
    "        \n",
    "        return (\n",
    "            RepoTaskData(tasks_train, repos_train, X_train, all_tasks_train, y_train),\n",
    "            RepoTaskData(tasks_test, repos_test, X_test, all_tasks_test, y_test)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_first_vocab_entry(vocab):\n",
    "    return list(itertools.islice(vocab.items(), 1))[0][0] \n",
    "\n",
    "\n",
    "class PairedKeyedVectors:\n",
    "    \n",
    "    @attr.s\n",
    "    class wv:\n",
    "        vocab = attr.ib()\n",
    "    \n",
    "    def __init__(self, kv1, kv2):\n",
    "        self.kv1 = kv1\n",
    "        self.kv2 = kv2\n",
    "        self.vocab = {**kv1.vocab, **kv2.vocab} \n",
    "        self.dim1 = len(kv1[get_first_vocab_entry(kv1.vocab)])\n",
    "        self.dim2 = len(kv2[get_first_vocab_entry(kv2.vocab)])\n",
    "        self.wv= PairedKeyedVectors.wv(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not item in self.kv1.vocab.keys():\n",
    "            return np.concatenate([np.zeros(self.dim1), self.kv2[item]])\n",
    "        elif not item in self.kv2.vocab.keys():\n",
    "            return np.concatenate([self.kv1[item], np.zeros(self.dim2)])\n",
    "        else:\n",
    "            return np.concatenate([self.kv1[item], self.kv2[item]])\n",
    "    \n",
    "\n",
    "\n",
    "@attr.s\n",
    "class RetrieverLearner:\n",
    "    \n",
    "    zs_learner: zero_shot.ZeroShotClassifier = attr.ib()\n",
    "    input_embedder: embeddings.EmbeddingVectorizer = attr.ib() \n",
    "    y_embedder: embeddings.EmbeddingVectorizer = attr.ib()\n",
    "    input_embedder_kwargs = attr.ib(default=dict())\n",
    "        \n",
    "    @staticmethod\n",
    "    def create(\n",
    "        zs_learner: zero_shot.ZeroShotClassifier,\n",
    "        input_embeddings: gensim.models.KeyedVectors,\n",
    "        target_embeddings: gensim.models.KeyedVectors,\n",
    "        input_embedding_method: embeddings.EmbeddingVectorizer,\n",
    "        y_embedding_method: embeddings.EmbeddingVectorizer,\n",
    "        input_embedder_kwargs=dict()\n",
    "    ):\n",
    "        input_embedder = input_embedding_method(input_embeddings, **input_embedder_kwargs) \n",
    "        y_embedder = y_embedding_method(target_embeddings)\n",
    "        return RetrieverLearner(zs_learner, input_embedder, y_embedder)\n",
    "    \n",
    "    def get_target_embeddings(self, y):\n",
    "        unique_y = pd.Series(y.unique())\n",
    "        y_embeddings = self.y_embedder.transform(unique_y)\n",
    "        return unique_y, y_embeddings\n",
    "    \n",
    "    def fit_learner(self, data, **kwargs):\n",
    "        self.input_embedder.fit(data.X)\n",
    "        X_embeddings = self.input_embedder.transform(data.X)\n",
    "        self.y_embedder.fit(data.y)\n",
    "        unique_y, y_embeddings = self.get_target_embeddings(data.y)\n",
    "        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])\n",
    "        self.zs_learner.fit(np.array(X_embeddings), np.array(input_y_idxs), np.array(y_embeddings), **kwargs)\n",
    "        \n",
    "    def predict_idxs(self, X, y_embeddings):\n",
    "        X_embeddings = self.input_embedder.transform(X)\n",
    "        return self.zs_learner.predict(X_embeddings, y_embeddings)\n",
    "    \n",
    "    def predict_topk(self, X, y_embeddings, target_names, k=5, similarity=metrics.pairwise.cosine_similarity):\n",
    "        X_embeddings = self.input_embedder.transform(X)\n",
    "        predictions = self.zs_learner.predict_raw(X_embeddings)\n",
    "        target_similarities = similarity(predictions, y_embeddings)\n",
    "        targets = [target_names[row[:k]] for row in (-target_similarities).argsort(axis=1)]\n",
    "        return targets\n",
    "        \n",
    "    def evaluate(self, data, metric):\n",
    "        unique_y, y_embeddings = self.get_target_embeddings(data.y)\n",
    "        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])\n",
    "        predicted_idxs = self.predict_idxs(data.X, y_embeddings)\n",
    "        return metric(input_y_idxs, predicted_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_accuracy(learner, X, y, y_names, k=10, similarity=metrics.pairwise.cosine_similarity):\n",
    "    input_embeddings = learner.input_embedder.transform(X)\n",
    "    y_embeddings = learner.y_embedder.transform(y_names)\n",
    "    predictions = learner.zs_learner.predict_raw(input_embeddings)\n",
    "    target_similarities = similarity(predictions, y_embeddings)\n",
    "    target_idxs = (-target_similarities).argsort(axis=1)\n",
    "    targets = [y_names.iloc[row[:k]] for row in target_idxs]\n",
    "\n",
    "    accuracies = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        true_tasks = set(all_tasks_test.iloc[i])\n",
    "        accuracies[i] = len(true_tasks.intersection(set(targets[i].values))) / min(len(true_tasks), k)\n",
    "    return accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "graph = pickle.load(open('output/call_igraph.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.get_vertex_dataframe().iloc[graph.neighborhood(vertices=[\"<ROOT>\"])[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get repos that are in graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_nodes = graph.get_vertex_dataframe()['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 939 ms, sys: 55.9 ms, total: 995 ms\n",
      "Wall time: 999 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paperswithcode_with_features_df = paperswithcode_with_imports_df[\n",
    "    paperswithcode_with_imports_df['repo'].isin(graph.get_vertex_dataframe()['name']) |\n",
    "    paperswithcode_with_imports_df['repo'].apply(lambda s: s.split(\"/\")[1]).isin(graph.get_vertex_dataframe()['name'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = paperswithcode_with_imports_df[paperswithcode_with_imports_df['repo'].isin(paperswithcode_with_features_df['repo'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_task_name(task_name):\n",
    "    return task_name.replace(\"2d \", \"\").replace(\"3d \", \"\").replace(\"4d \", \"\").replace(\"6d \", \"\").lower()\n",
    "\n",
    "paperswithcode_with_features_df['most_common_task'] = paperswithcode_with_features_df['most_common_task'].str.lower()\n",
    "tasks = paperswithcode_with_features_df['most_common_task'].str.lower()\n",
    "tasks = tasks.apply(clean_task_name)\n",
    "all_tasks = paperswithcode_with_features_df['tasks'].apply(lambda s: [clean_task_name(t) for t in s])\n",
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "semantic segmentation                     1520\n",
       "object detection                          1508\n",
       "image classification                      1387\n",
       "language modelling                         716\n",
       "representation learning                    634\n",
       "                                          ... \n",
       "scene text                                 100\n",
       "electron microscopy image segmentation     100\n",
       "cell segmentation                          100\n",
       "nuclear segmentation                        98\n",
       "scene understanding                         98\n",
       "Name: tasks, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks.explode().value_counts()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_area_grouped_tasks(paperswithcode_tasks_path='data/paperswithcode_tasks.csv'):\n",
    "    area_grouped_tasks = pd.read_csv('data/paperswithcode_tasks.csv')\n",
    "    area_grouped_tasks['task'] = area_grouped_tasks['task'].str.replace(\"-\", ' ')\n",
    "    area_grouped_tasks = area_grouped_tasks[area_grouped_tasks['task'].isin(tasks)]\n",
    "    area_counts = area_grouped_tasks['area'].value_counts()\n",
    "    area_grouped_tasks = area_grouped_tasks[area_grouped_tasks['area'].isin(area_counts.index[area_counts > 1])]\n",
    "    return area_grouped_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:30:58: Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - 11:30:58: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "area_grouped_tasks = get_area_grouped_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_train, tasks_test = RepoTaskData.split_tasks(area_grouped_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783             binarization\n",
       "209      activity recognition\n",
       "1182          active learning\n",
       "1731          image denoising\n",
       "493          video prediction\n",
       "                ...          \n",
       "1068      weather forecasting\n",
       "985     trajectory prediction\n",
       "344         image enhancement\n",
       "249            face detection\n",
       "1487                starcraft\n",
       "Name: task, Length: 76, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            dictionary learning\n",
       "1             sentiment analysis\n",
       "2                region proposal\n",
       "3               image generation\n",
       "4                       fairness\n",
       "                  ...           \n",
       "17383          anomaly detection\n",
       "17384             style transfer\n",
       "17385             style transfer\n",
       "17386             style transfer\n",
       "17387    representation learning\n",
       "Name: most_common_task, Length: 17388, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df['most_common_task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2252"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df['most_common_task'].isin(tasks_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_readme_summaries(upstream, product, keywords=True):\n",
    "    pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "    raw_readmes = list(pool.map(github_readmes.get_readme, paperswithcode_with_features_df['repo']))\n",
    "    readmes = pd.Series(raw_readmes).apply(github_readmes.try_decode)\n",
    "    return readmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readmes(df, keywords=True):\n",
    "    pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "    raw_readmes = list(pool.map(github_readmes.get_readme, df['repo']))\n",
    "    readmes = list(map(github_readmes.try_decode, raw_readmes))\n",
    "    return readmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'paper_url', 'arxiv_id', 'title', 'abstract', 'url_abs',\n",
       "       'url_pdf', 'proceeding', 'authors', 'tasks', 'date', 'methods',\n",
       "       'framework', 'mentioned_in_github', 'mentioned_in_paper',\n",
       "       'paper_arxiv_id', 'paper_title', 'paper_url_abs', 'paper_url_pdf',\n",
       "       'repo', 'repo_url', 'most_common_task', 'imports', 'n_imports',\n",
       "       'n_imports_with_embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 25)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 s, sys: 1.55 s, total: 7.6 s\n",
      "Wall time: 9min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readmes = get_readmes(paperswithcode_with_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_keywords(text):\n",
    "    return python_call_graph.try_run(gensim.summarization.keywords)(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 s, sys: 1.9 s, total: 5.47 s\n",
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readme_keywords = pd.Series(pool.map(try_keywords, readmes)).str.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['repo_description'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c81d76485dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdependency_records_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'repo-repo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrepo_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_root_dependency_records_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repo_description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo_description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdescribable_paperswithcode_with_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaperswithcode_with_features_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaperswithcode_with_features_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_descriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['repo_description'] not in index\""
     ]
    }
   ],
   "source": [
    "dependency_records_df = pd.read_csv('output/processed_dependency_records.csv').dropna()#.iloc[:1000000]\n",
    "non_root_dependency_records_df = dependency_records_df[\n",
    "    (dependency_records_df['source'] != \"<ROOT>\") &\n",
    "    (dependency_records_df['edge_type'] != 'repo-repo')\n",
    "]\n",
    "repo_descriptions = non_root_dependency_records_df[['source', 'repo_description']].groupby('source').apply(lambda df: df['repo_description'].iloc[0])\n",
    "\n",
    "describable_paperswithcode_with_features_df = paperswithcode_with_features_df[paperswithcode_with_features_df['repo'].isin(repo_descriptions.index)]\n",
    "describable_paperswithcode_with_imports_df = paperswithcode_with_imports_df[paperswithcode_with_imports_df['repo'].isin(repo_descriptions.index)]\n",
    "describable_repo_tasks = all_tasks[paperswithcode_with_imports_df['repo'].isin(repo_descriptions.index)]\n",
    "\n",
    "\n",
    "import_data_train, import_data_test = RepoTaskData.create_split(tasks_train, describable_repo_tasks, describable_paperswithcode_with_features_df, describable_paperswithcode_with_imports_df['imports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describable_paperswithcode_with_features_df = paperswithcode_with_features_df[paperswithcode_with_features_df['repo'].isin(repo_descriptions.index)]\n",
    "describable_paperswithcode_with_imports_df = paperswithcode_with_imports_df[paperswithcode_with_imports_df['repo'].isin(repo_descriptions.index)]\n",
    "describable_repo_tasks = all_tasks[paperswithcode_with_imports_df['repo'].isin(repo_descriptions.index)]\n",
    "\n",
    "\n",
    "import_data_train, import_data_test = RepoTaskData.create_split(tasks_train, describable_repo_tasks, describable_paperswithcode_with_features_df, describable_paperswithcode_with_imports_df['imports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describable_paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describable_paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embedder = mlutil.feature_extraction.embeddings.AverageWordEmbeddingsVectorizer(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_outgoing_edges(graph, node):\n",
    "    #idx = pd.Index(graph.names).get_loc(node)\n",
    "    #outgoing_edges_idx = np.where(graph.mat[idx].todense())[1]\n",
    "    return graph.get_vertex_dataframe().iloc[graph.successors(node)]['name']\n",
    "    #return graph.names[outgoing_edges_idx]\n",
    "\n",
    "\n",
    "def get_repo_functions(graph, repo):\n",
    "    return ' '.join(get_outgoing_edges(graph, repo).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_records = pd.read_csv('output/dependency_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def prepare_task_train_test_split(upstream, area_grouped_tasks_path, product):\n",
    "    area_grouped_tasks = get_area_grouped_tasks(area_grouped_tasks_path)\n",
    "    tasks_train, tasks_test = RepoTaskData.split_tasks(area_grouped_tasks)\n",
    "    tasks_train.to_csv(product['train'], index=None)\n",
    "    tasks_test.to_csv(product['test'], index=None)\n",
    "\n",
    "\n",
    "def prepare_graph_repo_task_data(upstream, product):\n",
    "    graph_data_train, graph_data_test = RepoTaskData.create_split(tasks_train, all_tasks, paperswithcode_with_features_df, paperswithcode_with_imports_df['imports'])\n",
    "    graph_data_train.X = graph_data_train.repos.apply(lambda x: get_repo_functions(graph, x))\n",
    "    graph_data_test.X = graph_data_test.repos.apply(lambda x: get_repo_functions(graph, x))\n",
    "    pickle.dump((graph_data_train, graph_data_test), open(str(product), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.exists(\"output/tmp_graph_data.pkl\"):\n",
    "    (graph_data_train, graph_data_test) = pickle.load(open(\"output/tmp_graph_data.pkl\", \"rb\"))\n",
    "else:\n",
    "    graph_data_train, graph_data_test = RepoTaskData.create_split(tasks_train, all_tasks, paperswithcode_with_features_df, paperswithcode_with_imports_df['imports'])\n",
    "    graph_data_train.X = graph_data_train.repos.apply(lambda x: get_repo_functions(graph, x))\n",
    "    graph_data_test.X = graph_data_test.repos.apply(lambda x: get_repo_functions(graph, x))\n",
    "    pickle.dump((graph_data_train, graph_data_test), open(\"output/tmp_graph_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train, graph_data_test = RepoTaskData.create_split(tasks_train, describable_repo_tasks, describable_paperswithcode_with_features_df, describable_paperswithcode_with_imports_df['imports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.X = pd.Series(repo_descriptions.loc[graph_data_train.repos].values, index=graph_data_train.repos.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_test.X = pd.Series(repo_descriptions.loc[graph_data_test.repos].values, index=graph_data_test.repos.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.repos.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outgoing_edges(graph, get_outgoing_edges(graph, graph_data_train.repos.iloc[0]).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_data_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.X.ilpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(graph_data_train.X)):\n",
    "    graph_data_train.X.iloc[i] = graph_data_train.X.iloc[i].replace(graph_data_train.repos.iloc[i], \"\")\n",
    "for i in range(len(graph_data_test.X)):\n",
    "    graph_data_test.X.iloc[i] = graph_data_test.X.iloc[i].replace(graph_data_test.repos.iloc[i], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.X = graph_data_train.X.str.replace(\":\", \" \")\n",
    "graph_data_train.X = graph_data_train.X.str.replace(\"<ROOT>\", \" \")\n",
    "graph_data_test.X = graph_data_test.X.str.replace(\":\", \" \")\n",
    "graph_data_test.X = graph_data_test.X.str.replace(\"<ROOT>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "    \n",
    "def get_retrieval_results(learner, data, k=10, similarity=metrics.pairwise.cosine_similarity):\n",
    "    y_names, __ = learner.get_target_embeddings(data.y)\n",
    "    input_embeddings = learner.input_embedder.transform(data.X)\n",
    "    y_embeddings = learner.y_embedder.transform(y_names)\n",
    "    predictions = learner.zs_learner.predict_raw(input_embeddings)\n",
    "    input_target_similarities = similarity(predictions, y_embeddings)\n",
    "\n",
    "    X_recalled = [\n",
    "        np.argsort(-input_target_similarities[:,y_idx])[:k]\n",
    "        for (y_idx, __) in enumerate(y_names)\n",
    "    ]\n",
    "    return X_recalled\n",
    "\n",
    "\n",
    "def get_retrieval_accuracies(learner, data, k=10, similarity=metrics.pairwise.cosine_similarity):\n",
    "    y_names, __ = learner.get_target_embeddings(data.y)\n",
    "    recalled_X = get_retrieval_results(learner, data, k=k, similarity=similarity)\n",
    "    recalled_X_actual_y = [data.y.iloc[idxs_recalled].explode() for idxs_recalled in recalled_X]\n",
    "    accurately_recalled = [\n",
    "        y_name in recalled_X_actual_y[y_idx].values \n",
    "        for (y_idx, y_name) in enumerate(y_names)\n",
    "    ]\n",
    "    return pd.Series(data=accurately_recalled, index=y_names)\n",
    "\n",
    "\n",
    "def get_retrieval_accuracy(learner, data, k=10, similarity=metrics.pairwise.cosine_similarity):\n",
    "    y_names, __ = learner.get_target_embeddings(data.y)\n",
    "    return np.mean(get_retrieval_accuracies(learner, data, k, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def run_learner_experiment(\n",
    "    retriever_learner,\n",
    "    data_train, data_test\n",
    "):\n",
    "    retriever_learner.fit_learner(data_train)\n",
    "    \n",
    "    accuracy_train = retriever_learner.evaluate(data_train, metrics.accuracy_score)\n",
    "    accuracy_test = retriever_learner.evaluate(data_test, metrics.accuracy_score)\n",
    "    top10_accuracy_train = get_retrieval_accuracy(retriever_learner, data_train, k=10)\n",
    "    top10_accuracy_test = get_retrieval_accuracy(retriever_learner, data_test, k=10)\n",
    "    \n",
    "    return dict(\n",
    "        accuracy_train=accuracy_train,\n",
    "        accuracy_test=accuracy_test,\n",
    "        top10_accuracy_train=top10_accuracy_train,\n",
    "        top10_accuracy_test=top10_accuracy_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_abstract = ~paperswithcode_with_imports_df['abstract'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_train[has_abstract]\n",
    "paperswithcode_with_features_df[has_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data_train, abstract_data_test = RepoTaskData.create_split(tasks_train[has_abstract], all_tasks[has_abstract], paperswithcode_with_features_df[has_abstract], paperswithcode_with_features_df[has_abstract]['abstract'].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scarce_learn.zero_shot import devise_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    python_word_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "abstract_learner.fit_learner(abstract_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(abstract_learner, abstract_data_train, abstract_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract model using fasttext trained on Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "abstract_fasttext_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    word_embeddings,\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(abstract_fasttext_learner, abstract_data_train, abstract_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model on READMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_readmes_df = pd.read_csv(\"output/papers_with_readmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_readmes_df['readme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df['readme'] = paperswithcode_with_readmes_df['readme'] \n",
    "paperswithcode_with_features_df['readme'] = paperswithcode_with_readmes_df['readme'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_readme = ~paperswithcode_with_imports_df['readme'].isna()\n",
    "\n",
    "readme_data_train, readme_data_test = RepoTaskData.create_split(tasks_train[has_readme], all_tasks[has_readme], paperswithcode_with_features_df[has_readme], paperswithcode_with_features_df[has_readme]['readme'].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 100),\n",
    "    python_word_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "readme_learner.fit_learner(readme_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(readme_learner, readme_data_train, readme_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext on READMEs - worse than word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "readme_fasttext_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    word_embeddings,\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(readme_fasttext_learner, readme_data_train, readme_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_keywords_data_train, readme_keywords_data_test = RepoTaskData.create_split(tasks_train[has_readme], all_tasks[has_readme], paperswithcode_with_features_df[has_readme], readme_keywords[has_readme].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "readme_keywords_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(10, 10),\n",
    "    word_embeddings,\n",
    "    word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "run_learner_experiment(readme_keywords_learner, readme_keywords_data_train, readme_keywords_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "import2vec_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(lmbda=100.0, gamma=10.0),\n",
    "    import2vec,\n",
    "    word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "run_learner_experiment(import2vec_learner, import_data_train, import_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRoNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prone_embeddings = gensim.models.KeyedVectors.load(\"data/prone_embeddings.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using repo embedding from node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prone_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100,10),\n",
    "    prone_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "run_learner_experiment(prone_learner, graph_data_train, graph_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregating vertex embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_kv_file =\"output/graphsage_embeddings_fasttext_dim200_epochs20_dim200_layers2.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_embeddings = gensim.models.KeyedVectors.load(graphsage_kv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(graphsage_embeddings.vocab)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(graphsage_embeddings.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = graph_data_train.X[150].strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token in vocab_list for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "graphsage_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    graphsage_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "run_learner_experiment(graphsage_learner, graph_data_train, graph_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_results = run_learner_experiment(abstract_learner, abstract_data_train, abstract_data_test)\n",
    "readme_results = run_learner_experiment(readme_learner, readme_data_train, readme_data_test)\n",
    "graphsage_results = run_learner_experiment(graphsage_learner, graph_data_train, graph_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_records([abstract_results, readme_results, graphsage_results])\n",
    "results_df['method'] = ['abstract', 'readme', 'graphsage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using GraphSAGE model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class LambdaTransformer:\n",
    "    \n",
    "    def __init__(self, transform_fn):\n",
    "        self.transform = transform_fn\n",
    "        \n",
    "    def fit(self, X, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search.pytorch_geometric_data import PygGraphWrapper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedder = embeddings.FastTextVectorizer(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dependency_graph_wrapper = PygGraphWrapper(fasttext_embedder.transform, non_root_dependency_records_df, \"source\", \"destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -ltr output/*graphsage*pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_model = torch.load(\"output/graphsage_model_20_dim200_layers2.pth\").cpu()\n",
    "graphsage_model.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_data_train, graphsage_data_test = RepoTaskData.create_split(tasks_train, all_tasks, paperswithcode_with_features_df, paperswithcode_with_imports_df.repo.apply(lambda s: [s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_data_train.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def make_records_df(sources, connected_vertices):\n",
    "    return pd.DataFrame.from_records(\n",
    "        [\n",
    "            {\"source\": src, \"destination\": dst, \"edge_type\": \"repo-file\"}\n",
    "            for (src, destinations) in zip(sources, connected_vertices)\n",
    "            for dst in destinations \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records_df = make_records_df(graphsage_data_train.repos, graph_data_train.X.fillna(\"\").str.split()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_vertex_embeddings(wrapper, vertex_subset, model):\n",
    "    features = (\n",
    "        model.full_forward(\n",
    "            wrapper.dataset.x, wrapper.dataset.edge_index\n",
    "        )\n",
    "        #.cpu()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    return features[wrapper.vertex_mapping.loc[vertex_subset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_records_df = make_records_df(graphsage_data_train.repos, graph_data_train.X.dropna().str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_graph_df = pd.concat([non_root_dependency_records_df, make_records_df(graphsage_data_train.repos, graph_data_train.X.dropna().str.split())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_graph_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "extended_dependency_graph_wrapper = PygGraphWrapper(embeddings.FastTextVectorizer(fasttext_model).transform, dep_graph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_dependency_graph_wrapper.dataset.x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "extended_dependency_graph_wrapper.get_vertex_embeddings(graphsage_data_train.X.iloc[0].split(), graphsage_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_learner = RetrieverLearner(\n",
    "    zero_shot.ESZSLearner(100,10),\n",
    "    LambdaTransformer(lambda x: extended_dependency_graph_wrapper.get_vertex_embeddings(x, graphsage_model)),\n",
    "    embeddings.FastTextVectorizer(fasttext_model)\n",
    ")\n",
    "graphsage_learner.fit_learner(graphsage_data_train)\n",
    "graphsage_learner.evaluate(graphsage_data_train, metric=metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(graphsage_learner, graphsage_data_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(graphsage_learner, graphsage_data_test, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Retriever:\n",
    "    \n",
    "    input_embedder = attr.ib()\n",
    "    query_embedder = attr.ib()\n",
    "    zs_learner = attr.ib()\n",
    "    embeddings_calculated = attr.ib(default=False)\n",
    "    \n",
    "    def set_embeddings(self, X_names, X):\n",
    "        self.X_embeddings = self.input_embedder.transform(X)\n",
    "        self.X = X\n",
    "        self.X_names = X_names\n",
    "        self.embeddings_calculated = True\n",
    "        \n",
    "    def retrieve_query_results(self, query, k=10, similarity=metrics.pairwise.cosine_similarity):\n",
    "        if not self.embeddings_calculated:\n",
    "            raise Exception(\"embeddings not calculated\")\n",
    "        input_embeddings = self.X_embeddings\n",
    "        y_embeddings = self.query_embedder.transform([query])\n",
    "        predictions = self.zs_learner.predict_raw(input_embeddings)\n",
    "        input_target_similarities = similarity(predictions, y_embeddings)\n",
    "        return self.X_names.iloc[np.argsort(-input_target_similarities[:,0])[:k]]\n",
    "\n",
    "    def from_retriever_learner(learner):\n",
    "        return Retriever(learner.input_embedder, learner.y_embedder, learner.zs_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(readme_data_test, open(\"output/readme_data_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(readme_learner, open(\"output/readme_learner.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_retriever = Retriever.from_retriever_learner(readme_learner)\n",
    "readme_retriever.set_embeddings(readme_data_train.repos, readme_data_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever = Retriever.from_retriever_learner(graphsage_learner)\n",
    "graphsage_retriever.set_embeddings(graphsage_data_train.repos, graphsage_data_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metric_learning_results = readme_retriever.retrieve_query_results(\"metric learning\", k=5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metric_learning_results_graphsage = graphsage_retriever.retrieve_query_results(\"metric learning\", k=5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_learning_results = readme_retriever.retrieve_query_results(\"distance learning\", k=5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_learning_results_graphsage = graphsage_retriever.retrieve_query_results(\"distance learning\", k=5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_learning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_learning_results_graphsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readme_retriever.retrieve_query_results(\"video text detection\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever.retrieve_query_results(\"video text detection\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_retriever.retrieve_query_results(\"context recommender systems\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever.retrieve_query_results(\"context recommender systems\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_retriever.retrieve_query_results(\"bayesian optimization\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever.retrieve_query_results(\"bayesian optimization\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_retriever.retrieve_query_results(\"evolutionary methods\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever.retrieve_query_results(\"evolutionary methods\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_retriever.retrieve_query_results(\"painting\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_retriever.retrieve_query_results(\"painting\", k=10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retrieve_query_results(graphsage_learner, graphsage_data_train.repos, graphsage_data_train.X, \"distance learning\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_query_results(graphsage_learner, graphsage_data_train.X, \"distance learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation of repo, import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data_train, paired_data_test = RepoTaskData.create_split(tasks_train, all_tasks, paperswithcode_with_features_df, paperswithcode_with_imports_df['imports'])\n",
    "paired_data_train.X = graph_data_train.X + \" \" + import_data_train.X\n",
    "paired_data_test.X = graph_data_test.X + \" \" + import_data_test.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data_train.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    PairedKeyedVectors(python_word_embeddings.wv, graphsage_embeddings),\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer\n",
    ")\n",
    "\n",
    "paired_learner.fit_learner(graph_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_learner.evaluate(graph_data_train, metric=metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(paired_learner, paired_data_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(paired_learner, paired_data_test, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (learner, learner_name, test) in zip(\n",
    "    [import2vec_learner, prone_learner, paired_learner],\n",
    "    ['import2vec', 'prone', 'both'],\n",
    "    [X_test, repo_graph_terms_test, X_paired_test]\n",
    "):\n",
    "    accs = []\n",
    "    for k in [1, 3, 5, 10, 20]:\n",
    "        rec = get_retrieval_accuracy(learner, test, y_test, test_task_idxs, k=k)\n",
    "        accs.append(rec)\n",
    "    results.append(pd.Series(name=learner_name, data=accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = [\"Accuracy@{}\".format(i) for i in [1, 3, 5, 10, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.round(3).to_markdown(open(\"metrics/zsl_results.md\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat metrics/zsl_results.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_distances = metrics.pairwise.cosine_distances(task_embeddings, task_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_embeddings = gensim.models.KeyedVectors.load('data/poincare5.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.wrappers.fasttext\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search import typical_file_parts\n",
    "from mlutil import prototype_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_lines_df = typical_file_parts.get_selected_lines_and_repos(python_files_df['repo_name'], python_files_df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting prototypical lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_selector = prototype_selection.PrototypeSelector(fasttext_avg_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fasttext_prototypes = json.load(open('data/fasttext_prototypes.json', 'r'))\n",
    "except:\n",
    "    fasttext_selector.fit_prototypes(selected_lines_df['line'], selected_lines_df['repo'])\n",
    "    fasttext_prototypes = fasttext_selector.prototypes\n",
    "    json.dump(fasttext_prototypes, open('data/fasttext_prototypes.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_vectorizer = embeddings.TransformerVectorizer('microsoft/codebert-base', batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_selector = prototype_selection.PrototypeSelector(codebert_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    codebert_prototypes = json.load(open('data/codebert_prototypes.json', 'r'))\n",
    "except:\n",
    "    codebert_selector.fit_prototypes(selected_lines_df['line'], selected_lines_df['repo'])\n",
    "    codebert_prototypes = codebert_selector.prototypes\n",
    "    json.dump(codebert_prototypes, open('data/codebert_prototypes.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_prototypes(vectorizer, prototypes):\n",
    "    prototype_aggregated_embeddings = {}\n",
    "    for key in prototypes.keys():\n",
    "        prototype_aggregated_embeddings[key] = np.mean(vectorizer.transform(prototypes[key]), axis=0)\n",
    "    return list(prototype_aggregated_embeddings.keys()), np.row_stack(prototype_aggregated_embeddings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_prototypes = {\n",
    "    repo: v\n",
    "    for (repo, v) in codebert_prototypes.items()\n",
    "    if repo in paperswithcode_with_imports_df['repo_name'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_prototypes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_prototypes = {\n",
    "    repo: v\n",
    "    for (repo, v) in fasttext_prototypes.items()\n",
    "    if repo in paperswithcode_with_imports_df['repo_name'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototypes(repo_name):\n",
    "    return pd.DataFrame({\"codebert\": codebert_prototypes[repo_name], \"fasttext\": fasttext_prototypes[repo_name]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_prototypes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prototypes(\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prototypes(\"mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prototypes(\"Recommenders-movielens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prototypes(\"mmdetection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_prototypes['mmdetection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_repos, codebert_prototype_embeddings = vectorize_prototypes(codebert_vectorizer, codebert_prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_repos, fasttext_prototype_embeddings = vectorize_prototypes(fasttext_avg_embedder, fasttext_prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fasttext_prototype_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_tasks_series = paperswithcode_with_imports_df['most_common_task']\n",
    "paperswithcode_tasks_series.index = paperswithcode_with_imports_df['repo_name']\n",
    "#paperswithcode_tasks_series = paperswithcode_tasks_series[paperswithcode_tasks_series.index.isin(fasttext_repos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_tasks = paperswithcode_tasks_series.loc[fasttext_repos]\n",
    "fasttext_tasks_embeddings = task_embedder.transform(fasttext_tasks)\n",
    "codebert_tasks = paperswithcode_tasks_series.loc[codebert_repos]\n",
    "codebert_tasks_embeddings = task_embedder.transform(codebert_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_prototype_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eszs_learner = zero_shot.ESZSLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_prototype_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(codebert_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eszs_learner.fit(codebert_prototype_embeddings, codebert_tasks, task_embeddings[:-1])\n",
    "eszs_learner.score(codebert_prototype_embeddings, codebert_tasks, task_embeddings[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eszs_learner.fit(fasttext_prototype_embeddings, fasttext_tasks, task_embeddings[:-1])\n",
    "eszs_learner.score(fasttext_prototype_embeddings, fasttext_tasks, task_embeddings[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(selected_lines_df['repo']))[3007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_lines_df = selected_lines_df[selected_lines_df['repo'] == 'auto_ml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del codebert_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_lines_df['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebert_selector.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_embeddings = fasttext_avg_embedder.transform(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names = \n",
    "repo_embeddings = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

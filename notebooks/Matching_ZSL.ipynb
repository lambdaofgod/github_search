{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp matching_zsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.local/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n",
      "/home/kuba/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/kuba/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import os\n",
    "import ast\n",
    "import tqdm\n",
    "import json\n",
    "import attr\n",
    "from operator import itemgetter\n",
    "from scipy.stats import hmean\n",
    "import logging\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction, metrics, model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from mlutil.feature_extraction import embeddings\n",
    "import mlutil\n",
    "from scarce_learn import zero_shot\n",
    "from scarce_learn.zero_shot import devise_jax, devise_torch\n",
    "from github_search import (\n",
    "    paperswithcode_tasks,\n",
    "    github_readmes,\n",
    "    python_call_graph,\n",
    "    data_utils,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search.pytorch_geometric_data import PygGraphWrapper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from github_search.papers_with_code.paperswithcode_tasks import clean_task_name\n",
    "import fasttext\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upstream\n",
    "\n",
    "import_corpus_path = \"output/module_corpus.csv\"\n",
    "word_vectors_filename = \"output/import2vec_module_vectors.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "import_corpus_df = pd.read_csv(import_corpus_path)\n",
    "per_repo_imports = import_corpus_df.groupby('repo')['imports'].agg(sum).apply(set)\n",
    "import_corpus_df['imports'] = import_corpus_df['imports'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "#python_files_df = pd.read_csv('data/crawled_python_files.csv', encoding='latin-1')\n",
    "#repo_names = python_files_df['repo_name']\n",
    "import_corpus_df = pd.read_csv(import_corpus_path)\n",
    "per_repo_imports = import_corpus_df.groupby('repo')['imports'].agg(sum).apply(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python_files_df['repo'] = python_files_df['repo_name'].str.split(\"/\").apply(itemgetter(1))  + '/' + python_files_df['repo_name']\n",
    "repo_names_tmp = python_files_df['repo_name']\n",
    "repo_names = repo_names_tmp.unique()\n",
    "python_files_df['repo_name'] = python_files_df['repo']\n",
    "python_files_df['repo'] = repo_names_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:44:01,320 - gensim.utils - INFO - loading Word2VecKeyedVectors object from output/import2vec_module_vectors.bin\n",
      "2022-10-17 22:44:01,329 - gensim.utils - INFO - setting ignored attribute vectors_norm to None\n",
      "2022-10-17 22:44:01,329 - gensim.utils - INFO - loaded output/import2vec_module_vectors.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.18 ms, sys: 3.3 ms, total: 10.5 ms\n",
      "Wall time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import2vec = gensim.models.KeyedVectors.load(word_vectors_filename)\n",
    "import2vec_embedder = (\n",
    "    mlutil.feature_extraction.embeddings.AverageWordEmbeddingsVectorizer(import2vec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"3d reconstruction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-cf3f273267ba>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  paperswithcode_with_imports_df[\"imports\"]\n"
     ]
    }
   ],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")\n",
    "paperswithcode_with_imports_df[\"tasks\"] = (\n",
    "    paperswithcode_with_imports_df[\"tasks\"]\n",
    "    .apply(clean_task_name)\n",
    "    .apply(ast.literal_eval)\n",
    ")\n",
    "paperswithcode_with_imports_df[\"imports\"] = (\n",
    "    paperswithcode_with_imports_df[\"imports\"]\n",
    "    .str.replace(\"set\\(\\)\", \"{}\")\n",
    "    .apply(ast.literal_eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38061, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df[\"n_imports\"] = paperswithcode_with_imports_df[\n",
    "    \"imports\"\n",
    "].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df[\n",
    "    \"n_imports_with_embeddings\"\n",
    "] = paperswithcode_with_imports_df[\"imports\"].apply(\n",
    "    lambda imps: len([imp in import2vec.vocab.keys() for imp in imps])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38061"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df[\"repo\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:44:03,941 - gensim.models.utils_any2vec - INFO - loading projection weights from /home/kuba/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2022-10-17 22:44:42,024 - gensim.models.utils_any2vec - INFO - loaded (400000, 300) matrix from /home/kuba/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.8 s, sys: 318 ms, total: 38.1 s\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_embeddings = mlutil.feature_extraction.embeddings.load_gensim_embedding_model(\n",
    "    \"glove-wiki-gigaword-300\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = fasttext.load_model(\"output/python_files_fasttext_dim200.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "class LossCallback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print(\"Loss after epoch {}: {}\".format(self.epoch, loss))\n",
    "        else:\n",
    "            print(\n",
    "                \"Loss after epoch {}: {}\".format(\n",
    "                    self.epoch, loss - self.loss_previous_step\n",
    "                )\n",
    "            )\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:45:04,550 - gensim.utils - INFO - loading Word2Vec object from output/abstract_readme_w2v200.bin\n",
      "2022-10-17 22:45:04,694 - gensim.utils - INFO - loading wv recursively from output/abstract_readme_w2v200.bin.wv.* with mmap=None\n",
      "2022-10-17 22:45:04,695 - gensim.utils - INFO - loading vectors from output/abstract_readme_w2v200.bin.wv.vectors.npy with mmap=None\n",
      "2022-10-17 22:45:04,764 - gensim.utils - INFO - setting ignored attribute vectors_norm to None\n",
      "2022-10-17 22:45:04,764 - gensim.utils - INFO - loading vocabulary recursively from output/abstract_readme_w2v200.bin.vocabulary.* with mmap=None\n",
      "2022-10-17 22:45:04,765 - gensim.utils - INFO - loading trainables recursively from output/abstract_readme_w2v200.bin.trainables.* with mmap=None\n",
      "2022-10-17 22:45:04,766 - gensim.utils - INFO - loading syn1neg from output/abstract_readme_w2v200.bin.trainables.syn1neg.npy with mmap=None\n",
      "2022-10-17 22:45:04,916 - gensim.utils - INFO - setting ignored attribute cum_table to None\n",
      "2022-10-17 22:45:04,917 - gensim.utils - INFO - loaded output/abstract_readme_w2v200.bin\n"
     ]
    }
   ],
   "source": [
    "python_word_embeddings = gensim.models.Word2Vec.load(\n",
    "    \"output/abstract_readme_w2v200.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class RepoTaskData:\n",
    "\n",
    "    tasks = attr.ib()\n",
    "    repos = attr.ib()\n",
    "    X = attr.ib()\n",
    "    all_tasks = attr.ib()\n",
    "    y = attr.ib()\n",
    "\n",
    "    def split_tasks(area_grouped_tasks, test_size=0.2):\n",
    "        tasks_train, tasks_test = model_selection.train_test_split(\n",
    "            area_grouped_tasks[\"task\"],\n",
    "            stratify=area_grouped_tasks[\"area\"],\n",
    "            test_size=test_size,\n",
    "            random_state=0,\n",
    "        )\n",
    "        return tasks_train, tasks_test\n",
    "\n",
    "    def create_split(\n",
    "        tasks_test,\n",
    "        all_tasks,\n",
    "        paperswithcode_with_features_df,\n",
    "        X_repr,\n",
    "        y_col=\"least_common_task\",\n",
    "    ):\n",
    "        train_indicator = paperswithcode_with_features_df[\"tasks\"].apply(\n",
    "            lambda ts: not (any([t in list(tasks_test) for t in ts]))\n",
    "        )\n",
    "        repos_train = paperswithcode_with_features_df[\"repo\"][train_indicator]\n",
    "        repos_test = paperswithcode_with_features_df[\"repo\"][~train_indicator]\n",
    "        X_repr = X_repr.apply(lambda x: \" \".join(x))\n",
    "        X_train = X_repr[train_indicator]\n",
    "        X_test = X_repr[~train_indicator]\n",
    "        all_tasks_train = all_tasks[train_indicator]\n",
    "        all_tasks_test = all_tasks[~train_indicator]\n",
    "        y_train = (\n",
    "            paperswithcode_with_features_df[train_indicator][y_col]\n",
    "            .str.lower()\n",
    "            .apply(clean_task_name)\n",
    "        )\n",
    "        y_test = (\n",
    "            paperswithcode_with_features_df[~train_indicator][y_col]\n",
    "            .str.lower()\n",
    "            .apply(clean_task_name)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            RepoTaskData(tasks_train, repos_train, X_train, all_tasks_train, y_train),\n",
    "            RepoTaskData(tasks_test, repos_test, X_test, all_tasks_test, y_test),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_first_vocab_entry(vocab):\n",
    "    return list(itertools.islice(vocab.items(), 1))[0][0]\n",
    "\n",
    "\n",
    "class PairedKeyedVectors:\n",
    "    @attr.s\n",
    "    class wv:\n",
    "        vocab = attr.ib()\n",
    "\n",
    "    def __init__(self, kv1, kv2):\n",
    "        self.kv1 = kv1\n",
    "        self.kv2 = kv2\n",
    "        self.vocab = {**kv1.vocab, **kv2.vocab}\n",
    "        self.dim1 = len(kv1[get_first_vocab_entry(kv1.vocab)])\n",
    "        self.dim2 = len(kv2[get_first_vocab_entry(kv2.vocab)])\n",
    "        self.wv = PairedKeyedVectors.wv(self.vocab)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not item in self.kv1.vocab.keys():\n",
    "            return np.concatenate([np.zeros(self.dim1), self.kv2[item]])\n",
    "        elif not item in self.kv2.vocab.keys():\n",
    "            return np.concatenate([self.kv1[item], np.zeros(self.dim2)])\n",
    "        else:\n",
    "            return np.concatenate([self.kv1[item], self.kv2[item]])\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class RetrieverLearner:\n",
    "\n",
    "    zs_learner: zero_shot.ZeroShotClassifier = attr.ib()\n",
    "    input_embedder: embeddings.EmbeddingVectorizer = attr.ib()\n",
    "    y_embedder: embeddings.EmbeddingVectorizer = attr.ib()\n",
    "    input_embedder_kwargs = attr.ib(default=dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        zs_learner: zero_shot.ZeroShotClassifier,\n",
    "        input_embeddings: gensim.models.KeyedVectors,\n",
    "        target_embeddings: gensim.models.KeyedVectors,\n",
    "        input_embedding_method: embeddings.EmbeddingVectorizer,\n",
    "        y_embedding_method: embeddings.EmbeddingVectorizer,\n",
    "        input_embedder_kwargs=dict(),\n",
    "    ):\n",
    "        input_embedder = input_embedding_method(\n",
    "            input_embeddings, **input_embedder_kwargs\n",
    "        )\n",
    "        y_embedder = y_embedding_method(target_embeddings)\n",
    "        return RetrieverLearner(zs_learner, input_embedder, y_embedder)\n",
    "\n",
    "    def get_target_embeddings(self, y):\n",
    "        unique_y = pd.Series(y.unique())\n",
    "        y_embeddings = self.y_embedder.transform(unique_y)\n",
    "        return unique_y, y_embeddings\n",
    "\n",
    "    def fit_learner(self, data, **kwargs):\n",
    "        self.input_embedder.fit(data.X)\n",
    "        X_embeddings = self.input_embedder.transform(data.X)\n",
    "        self.y_embedder.fit(data.y)\n",
    "        unique_y, y_embeddings = self.get_target_embeddings(data.y)\n",
    "        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])\n",
    "        self.zs_learner.fit(\n",
    "            np.array(X_embeddings),\n",
    "            np.array(input_y_idxs),\n",
    "            np.array(y_embeddings),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def predict_idxs(self, X, y_embeddings):\n",
    "        X_embeddings = self.input_embedder.transform(X)\n",
    "        return self.zs_learner.predict(X_embeddings, y_embeddings)\n",
    "\n",
    "    def predict_topk(\n",
    "        self,\n",
    "        X,\n",
    "        y_embeddings,\n",
    "        target_names,\n",
    "        k=5,\n",
    "        similarity=metrics.pairwise.cosine_similarity,\n",
    "    ):\n",
    "        X_embeddings = self.input_embedder.transform(X)\n",
    "        predictions = self.zs_learner.predict_raw(X_embeddings)\n",
    "        target_similarities = similarity(predictions, y_embeddings)\n",
    "        targets = [\n",
    "            target_names[row[:k]] for row in (-target_similarities).argsort(axis=1)\n",
    "        ]\n",
    "        return targets\n",
    "\n",
    "    def evaluate(self, data, metric):\n",
    "        unique_y, y_embeddings = self.get_target_embeddings(data.y)\n",
    "        input_y_idxs = data.y.apply(lambda t: unique_y[unique_y == t].index[0])\n",
    "        predicted_idxs = self.predict_idxs(data.X, y_embeddings)\n",
    "        return metric(input_y_idxs, predicted_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pickle.load(open(\"output/call_igraph.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53702"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.get_vertex_dataframe().iloc[graph.neighborhood(vertices=[\"<ROOT>\"])[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get repos that are in graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38061"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_imports_df[\"repo\"].isin(graph.get_vertex_dataframe()[\"name\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_nodes = graph.get_vertex_dataframe()[\"name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779471"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df = (\n",
    "    pd.read_csv(\"output/papers_with_readmes.csv\")\n",
    "    .dropna(subset=[\"least_common_task\"])\n",
    "    .dropna(subset=[\"readme\", \"abstract\"])\n",
    ")\n",
    "paperswithcode_with_tasks_df[\"tasks\"] = paperswithcode_with_tasks_df[\"tasks\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39701,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_tasks_df[\"readme\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4256560 output/papers_with_readmes.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l output/papers_with_readmes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_records_df = pd.read_feather(\"output/processed_dependency_records.feather\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependency_records_df.to_feather(\"output/processed_dependency_records.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def filter_smaller_tasks(paperswithcode_with_tasks_df, min_task_count=10):\n",
    "    task_counts = paperswithcode_with_tasks_df[\"least_common_task\"].value_counts()\n",
    "    return paperswithcode_with_tasks_df[\n",
    "        paperswithcode_with_tasks_df[\"least_common_task\"].isin(\n",
    "            task_counts[task_counts >= min_task_count].index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def prepare_paperswithcode_with_features_df(\n",
    "    paperswithcode_with_tasks_df, dependency_records_df, min_task_count\n",
    "):\n",
    "    paperswithcode_with_features_df = paperswithcode_with_tasks_df[\n",
    "        paperswithcode_with_tasks_df[\"repo\"].isin(graph.get_vertex_dataframe()[\"name\"])\n",
    "        | paperswithcode_with_tasks_df[\"repo\"]\n",
    "        .apply(lambda s: s.split(\"/\")[1])\n",
    "        .isin(graph.get_vertex_dataframe()[\"name\"])\n",
    "    ]\n",
    "    paperswithcode_with_features_df = paperswithcode_with_features_df.dropna(\n",
    "        subset=[\"readme\", \"abstract\"]\n",
    "    )\n",
    "    tasks = paperswithcode_with_features_df[\"least_common_task\"].str.lower()\n",
    "\n",
    "    per_repo_dependency_records = data_utils.get_repo_records(\n",
    "        paperswithcode_with_features_df[\"repo\"], dependency_records_df\n",
    "    )\n",
    "    per_repo_dependency_records = per_repo_dependency_records.reset_index()\n",
    "    per_repo_dependency_records.columns = [\"source\", \"dependency_records\"]\n",
    "    paperswithcode_with_features_df = paperswithcode_with_features_df.merge(\n",
    "        per_repo_dependency_records.reset_index(), left_on=\"repo\", right_on=\"source\"\n",
    "    )\n",
    "    all_tasks = paperswithcode_with_features_df[\"tasks\"]\n",
    "    is_valid_record = all_tasks.apply(len) > 0\n",
    "    paperswithcode_with_features_df = filter_smaller_tasks(\n",
    "        paperswithcode_with_features_df[is_valid_record], min_task_count\n",
    "    )\n",
    "    all_tasks = paperswithcode_with_features_df[\"tasks\"]\n",
    "    all_task_counts = all_tasks.explode().value_counts()\n",
    "    valid_tasks = all_task_counts[all_task_counts >= min_task_count].index\n",
    "    paperswithcode_with_features_df[\"tasks\"] = paperswithcode_with_features_df[\n",
    "        \"tasks\"\n",
    "    ].apply(lambda ts: [t for t in ts if t in valid_tasks])\n",
    "    return paperswithcode_with_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39701, 17)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_tasks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_features_df = prepare_paperswithcode_with_features_df(\n",
    "    paperswithcode_with_tasks_df, dependency_records_df, min_task_count=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = paperswithcode_with_imports_df[\n",
    "    paperswithcode_with_imports_df[\"repo\"].isin(paperswithcode_with_features_df[\"repo\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = paperswithcode_with_features_df[\n",
    "    \"tasks\"\n",
    "]  # .apply(lambda tasks: [t for t in tasks if t in valid_tasks.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = all_tasks.explode().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image classification             1757\n",
       "object detection                 1079\n",
       "language modelling                798\n",
       "domain adaptation                 760\n",
       "data augmentation                 694\n",
       "                                 ... \n",
       "automatic post editing             10\n",
       "color image denoising              10\n",
       "handwritten digit recognition      10\n",
       "synthetic data generation          10\n",
       "code summarization                 10\n",
       "Name: least_common_task, Length: 429, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df[\"least_common_task\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32690, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_grouped_tasks = paperswithcode_tasks.get_area_grouped_tasks()\n",
    "area_grouped_tasks[\"task\"] = area_grouped_tasks[\"task\"].apply(clean_task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_grouped_tasks = pd.read_csv(\"data/paperswithcode_tasks.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_area_grouped_tasks[\"task\"] = all_area_grouped_tasks[\"task\"].str.replace(\"-\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_train, tasks_test = RepoTaskData.split_tasks(all_area_grouped_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readme_summaries(upstream, product, keywords=True):\n",
    "    pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "    raw_readmes = list(\n",
    "        pool.map(github_readmes.get_readme, paperswithcode_with_features_df[\"repo\"])\n",
    "    )\n",
    "    readmes = pd.Series(raw_readmes).apply(github_readmes.try_decode)\n",
    "    return readmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readmes(df, keywords=True):\n",
    "    pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "    raw_readmes = list(pool.map(github_readmes.get_readme, df[\"repo\"]))\n",
    "    readmes = list(map(github_readmes.try_decode, raw_readmes))\n",
    "    return readmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'paper_url', 'paper_title',\n",
       "       'paper_arxiv_id', 'paper_url_abs', 'paper_url_pdf', 'repo_url',\n",
       "       'mentioned_in_paper', 'mentioned_in_github', 'framework', 'repo',\n",
       "       'title', 'abstract', 'tasks', 'least_common_task', 'readme', 'index',\n",
       "       'source', 'dependency_records'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32690, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperswithcode_with_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "readmes = get_readmes(paperswithcode_with_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_keywords(text):\n",
    "    return python_call_graph.try_run(gensim.summarization.keywords)(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f63026fffa0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embedder = mlutil.feature_extraction.embeddings.AverageWordEmbeddingsVectorizer(\n",
    "    word_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_outgoing_edges(graph, node):\n",
    "    # idx = pd.Index(graph.names).get_loc(node)\n",
    "    # outgoing_edges_idx = np.where(graph.mat[idx].todense())[1]\n",
    "    return graph.get_vertex_dataframe().iloc[graph.successors(node)][\"name\"]\n",
    "    # return graph.names[outgoing_edges_idx]\n",
    "\n",
    "\n",
    "def get_repo_functions(graph_records, repo):\n",
    "    return \" \".join(set(get_outgoing_edges(graph, repo).values)).replace(repo + \":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def prepare_task_train_test_split(upstream, product):\n",
    "    area_grouped_tasks = pd.read_csv(str(upstream[\"prepare_area_grouped_tasks\"]))\n",
    "    tasks_train, tasks_test = RepoTaskData.split_tasks(area_grouped_tasks)\n",
    "    tasks_train.to_csv(product[\"train\"], index=None)\n",
    "    tasks_test.to_csv(product[\"test\"], index=None)\n",
    "\n",
    "\n",
    "def prepare_graph_repo_task_data(upstream, product):\n",
    "    graph_data_train, graph_data_test = RepoTaskData.create_split(\n",
    "        tasks_train,\n",
    "        all_tasks,\n",
    "        paperswithcode_with_features_df,\n",
    "        paperswithcode_with_imports_df[\"imports\"],\n",
    "    )\n",
    "    graph_data_train.X = graph_data_train.repos.apply(\n",
    "        lambda x: get_repo_functions(graph, x)\n",
    "    )\n",
    "    graph_data_test.X = graph_data_test.repos.apply(\n",
    "        lambda x: get_repo_functions(graph, x)\n",
    "    )\n",
    "    pickle.dump((graph_data_train, graph_data_test), open(str(product), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_data_train, readme_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    paperswithcode_with_features_df[\"tasks\"],\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_features_df[\"readme\"].str.split(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_test.to_csv(\"output/test_tasks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6453655552156623"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_data_train.X.shape[0] / paperswithcode_with_features_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6453655552156623"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_data_train.X.shape[0] / paperswithcode_with_features_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train, graph_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    paperswithcode_with_features_df[\"tasks\"],\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_features_df[\"repo\"].apply(lambda t: [t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_valid_record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_valid_record' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(\"output/tmp_graph_data.pkl\"):\n",
    "    (graph_data_train, graph_data_test) = pickle.load(\n",
    "        open(\"output/tmp_graph_data.pkl\", \"rb\")\n",
    "    )\n",
    "else:\n",
    "    graph_data_train, graph_data_test = RepoTaskData.create_split(\n",
    "        tasks_test,\n",
    "        all_tasks[is_valid_record],\n",
    "        paperswithcode_with_features_df[is_valid_record],\n",
    "        paperswithcode_with_features_df[is_valid_record][\"readme\"].str.split(),\n",
    "    )\n",
    "\n",
    "    graph_records_train_X = pd.Series(\n",
    "        [\n",
    "            get_repo_functions(graph, x)\n",
    "            for x in tqdm.notebook.tqdm(graph_data_train.repos)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    graph_records_test_X = pd.Series(\n",
    "        [\n",
    "            get_repo_functions(graph, x)\n",
    "            for x in tqdm.notebook.tqdm(graph_data_test.repos)\n",
    "        ]\n",
    "    )\n",
    "    graph_data_train.X = graph_records_train_X\n",
    "    graph_data_test.X = graph_records_test_X\n",
    "    pickle.dump(\n",
    "        (graph_data_train, graph_data_test), open(\"output/tmp_graph_data.pkl\", \"wb\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(graph_data_train.X)):\n",
    "    graph_data_train.X.iloc[i] = graph_data_train.X.iloc[i].replace(\n",
    "        graph_data_train.repos.iloc[i], \"\"\n",
    "    )\n",
    "for i in range(len(graph_data_test.X)):\n",
    "    graph_data_test.X.iloc[i] = graph_data_test.X.iloc[i].replace(\n",
    "        graph_data_test.repos.iloc[i], \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_train.X = graph_data_train.X.str.replace(\":\", \" \")\n",
    "graph_data_train.X = graph_data_train.X.str.replace(\"<ROOT>\", \" \")\n",
    "graph_data_test.X = graph_data_test.X.str.replace(\":\", \" \")\n",
    "graph_data_test.X = graph_data_test.X.str.replace(\"<ROOT>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def maybe_get_ndarray_elem(arr, idx, default=-1):\n",
    "    if len(arr) <= idx:\n",
    "        return default\n",
    "    else:\n",
    "        return arr[idx]\n",
    "\n",
    "\n",
    "def get_retrieval_results(\n",
    "    learner, data, queried_tasks, k=10, similarity=metrics.pairwise.cosine_similarity\n",
    "):\n",
    "    if queried_tasks == \"all\":\n",
    "        tasks = data.all_tasks.explode().drop_duplicates()\n",
    "    elif queried_tasks == \"target\":\n",
    "        tasks = data.y.drop_duplicates()\n",
    "    else:\n",
    "        tasks = queried_tasks\n",
    "    y_names, __ = learner.get_target_embeddings(tasks)\n",
    "    input_embeddings = learner.input_embedder.transform(data.X)\n",
    "    y_embeddings = learner.y_embedder.transform(y_names)\n",
    "    predictions = learner.zs_learner.predict_raw(input_embeddings)\n",
    "    input_target_similarities = similarity(predictions, y_embeddings)\n",
    "\n",
    "    X_recalled = [\n",
    "        np.argsort(-input_target_similarities[:, y_idx])[:k]\n",
    "        for (y_idx, __) in enumerate(y_names)\n",
    "    ]\n",
    "    return y_names, X_recalled\n",
    "\n",
    "\n",
    "def get_retrieval_metrics(\n",
    "    learner,\n",
    "    data,\n",
    "    k=10,\n",
    "    similarity=metrics.pairwise.cosine_similarity,\n",
    "    queried_tasks=\"all\",\n",
    "):\n",
    "    y_names, retrieved_X = get_retrieval_results(\n",
    "        learner, data, k=k, similarity=similarity, queried_tasks=queried_tasks\n",
    "    )\n",
    "    retrieved_X_actual_labels = [\n",
    "        data.all_tasks.iloc[idxs_recalled].explode().values\n",
    "        for idxs_recalled in retrieved_X\n",
    "    ]\n",
    "    retrieved_idxs = [\n",
    "        np.where(retrieved_X_actual_labels[y_idx] == y_name)[0]\n",
    "        for (y_idx, y_name) in enumerate(y_names)\n",
    "    ]\n",
    "    num_recalled = [len(r) for r in retrieved_idxs]\n",
    "    pos_recalled = [maybe_get_ndarray_elem(r, 0) for r in retrieved_idxs]\n",
    "    accurately_recalled = [r > -1 for r in pos_recalled]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"retrieved_labels\": retrieved_X_actual_labels,\n",
    "            \"num_recalled\": num_recalled,\n",
    "            \"recalled\": accurately_recalled,\n",
    "            \"position\": pos_recalled,\n",
    "        },\n",
    "        index=y_names,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_retrieval_accuracy(\n",
    "    learner,\n",
    "    data,\n",
    "    k=10,\n",
    "    similarity=metrics.pairwise.cosine_similarity,\n",
    "    queried_tasks=None,\n",
    "):\n",
    "    return np.mean(\n",
    "        get_retrieval_metrics(learner, data, k, similarity, queried_tasks)[\"recalled\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_learner_experiment(\n",
    "    retriever_learner, data_train, data_test, queried_tasks=\"all\", fit_learner=True\n",
    "):\n",
    "    if fit_learner:\n",
    "        retriever_learner.fit_learner(data_train)\n",
    "\n",
    "    accuracy_train = retriever_learner.evaluate(data_train, metrics.accuracy_score)\n",
    "    accuracy_test = retriever_learner.evaluate(data_test, metrics.accuracy_score)\n",
    "    top10_accuracy_train = get_retrieval_accuracy(\n",
    "        retriever_learner, data_train, queried_tasks=queried_tasks, k=10\n",
    "    )\n",
    "    top10_accuracy_test = get_retrieval_accuracy(\n",
    "        retriever_learner, data_test, queried_tasks=queried_tasks, k=10\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        accuracy_train=accuracy_train,\n",
    "        accuracy_test=accuracy_test,\n",
    "        top10_accuracy_train=top10_accuracy_train,\n",
    "        top10_accuracy_test=top10_accuracy_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models as sbert_models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "word_embedding_model = sbert_models.Transformer(\n",
    "    model_name, max_seq_length=64, do_lower_case=True\n",
    ")\n",
    "pooling_model = sbert_models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(), \"cls\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:46:39,522 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: output/sbert/sru2x256_epoch125/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Path output/sbert/sru2x256_epoch125/ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e24494217cd8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/sbert/sru2x256_epoch125/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/forks/sentence-transformers/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m#Not a path, load from hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'\\\\'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Path {} not found\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbasic_transformer_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Path output/sbert/sru2x256_epoch125/ not found"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer(\"output/sbert/sru2x256_epoch125/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_data_train, abstract_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    paperswithcode_with_features_df[\"tasks\"],\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_features_df[\"abstract\"].str.split(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scarce_learn.zero_shot import devise_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_objective_impl(zslearner_kwargs, embedding_kwargs, data_train, data_test):\n",
    "    zs_learner = zero_shot.ESZSLearner(**zslearner_kwargs)\n",
    "    learner = RetrieverLearner.create(zs_learner, **embedding_kwargs)\n",
    "    exp_results = run_learner_experiment(\n",
    "        learner, abstract_data_train, abstract_data_test\n",
    "    )\n",
    "    print(exp_results)\n",
    "    return exp_results[\"top10_accuracy_test\"]\n",
    "\n",
    "\n",
    "def retrieval_objective(trial):\n",
    "    lmbda = trial.suggest_float(\"lmbda\", 1, 1000, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1, 1000, log=True)\n",
    "    zslearner_kwargs = dict(lmbda=lmbda, gamma=gamma)\n",
    "    embedding_kwargs = dict(\n",
    "        input_embeddings=python_word_embeddings,\n",
    "        target_embeddings=python_word_embeddings,\n",
    "        input_embedding_method=embeddings.AverageWordEmbeddingsVectorizer,\n",
    "        y_embedding_method=embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    )\n",
    "    return retriever_learner_trial_impl(\n",
    "        zslearner_kwargs, embedding_kwargs, abstract_data_train, abstract_data_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abstract_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    python_word_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    ")\n",
    "\n",
    "abstract_learner.fit_learner(abstract_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(\n",
    "    abstract_learner,\n",
    "    abstract_data_train,\n",
    "    abstract_data_test,\n",
    "    fit_learner=False,\n",
    "    queried_tasks=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(\n",
    "    abstract_learner,\n",
    "    abstract_data_train,\n",
    "    abstract_data_test,\n",
    "    fit_learner=False,\n",
    "    queried_tasks=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@attr.s\n",
    "class SBertModelWrapper:\n",
    "\n",
    "    model = attr.ib()\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.encode(X.values)\n",
    "\n",
    "\n",
    "sbert_wrapper = SBertModelWrapper(sbert_model)\n",
    "abstract_sbert_learner = RetrieverLearner(\n",
    "    zero_shot.ESZSLearner(100, 100),\n",
    "    embeddings.AverageWordEmbeddingsVectorizer(python_word_embeddings),\n",
    "    sbert_wrapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(\n",
    "    abstract_sbert_learner, abstract_data_train, abstract_data_test, queried_tasks=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract model using fasttext trained on Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "abstract_fasttext_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 100),\n",
    "    word_embeddings,\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(\n",
    "    abstract_fasttext_learner, abstract_data_train, abstract_data_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model on READMEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paperswithcode_with_readmes_df = pd.read_csv(\"output/papers_with_readmes.csv\")\n",
    "paperswithcode_with_imports_df['readme'] = paperswithcode_with_readmes_df['readme'] \n",
    "paperswithcode_with_features_df['readme'] = readmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_data_train, readme_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    all_tasks,\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_features_df[\"readme\"].str.split(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    python_word_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    ")\n",
    "\n",
    "readme_learner.fit_learner(readme_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_learner_experiment(\n",
    "    readme_learner, readme_data_train, readme_data_test, fit_learner=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_learner_experiment(\n",
    "    readme_learner,\n",
    "    readme_data_train,\n",
    "    readme_data_test,\n",
    "    fit_learner=False,\n",
    "    queried_tasks=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_sbert_learner = RetrieverLearner(\n",
    "    zero_shot.ESZSLearner(100, 100), sbert_wrapper, sbert_wrapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learner_experiment(readme_sbert_learner, readme_data_train, readme_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext on READMEs - worse than word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "readme_fasttext_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    word_embeddings,\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_learner_experiment(readme_fasttext_learner, readme_data_train, readme_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readme_keywords_data_train, readme_keywords_data_test = RepoTaskData.create_split(tasks_train[has_readme], all_tasks[has_readme], paperswithcode_with_features_df[has_readme], readme_keywords[has_readme].str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "readme_keywords_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(10, 10),\n",
    "    word_embeddings,\n",
    "    word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer\n",
    ")\n",
    "\n",
    "run_learner_experiment(readme_keywords_learner, readme_keywords_data_train, readme_keywords_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data_train, import_data_test = RepoTaskData.create_split(\n",
    "    tasks_test[is_valid_record],\n",
    "    all_tasks[is_valid_record],\n",
    "    paperswithcode_with_features_df[is_valid_record],\n",
    "    paperswithcode_with_features_df[is_valid_record][\"imports\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezslearner = zero_shot.ESZSLearner()\n",
    "import2vec_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(lmbda=100.0, gamma=10.0),\n",
    "    import2vec,\n",
    "    word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    ")\n",
    "\n",
    "run_learner_experiment(import2vec_learner, import_data_train, import_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRoNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prone_embeddings = gensim.models.KeyedVectors.load(\"data/prone_embeddings.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using repo embedding from node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prone_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    prone_embeddings,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    ")\n",
    "\n",
    "run_learner_experiment(prone_learner, graph_data_train, graph_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregating vertex embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search import pytorch_geometric_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"output/models/graphsage/graphsage_model_1_dim128_layers2_step100.pth\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"output/graphsage_input_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 1min 9s, total: 4min 10s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kv = pytorch_geometric_training.get_gnn_kv(\"graphsage\", data.vertex_names, model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epsilon2probability'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vertex_names[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = dependency_records_df.iloc[:100][\"destination\"].str.split(\":\").apply(lambda l: l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"<ROOT>\" in vs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          gnn-residual-correlation\n",
       "1                       torchdiffeq\n",
       "2                        latent_ode\n",
       "3                      odenet_mnist\n",
       "4                          ode_demo\n",
       "                     ...           \n",
       "3010404                 participate\n",
       "3010405                 coincidence\n",
       "3010406            uniform_quantize\n",
       "3010407                 linear_Q_fn\n",
       "3010408                 conv2d_Q_fn\n",
       "Length: 3010409, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_map = pd.Series(index=range(len(data.vertex_names)), data=data.vertex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.subgraph("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[\"conv2d_Q_fn\"] - kv[\"torchdiffeq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH# DEBUGGING GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_kv_file = (\n",
    "    \"output/graphsage_embeddings_fasttext_dim200_epochs20_dim200_layers2.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_embeddings = gensim.models.KeyedVectors.load(graphsage_kv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using GraphSAGE model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class LambdaTransformer:\n",
    "    def __init__(self, transform_fn):\n",
    "        self.transform = transform_fn\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        return self\n",
    "\n",
    "\n",
    "class PyGGraphModelTransformer:\n",
    "    def __init__(self, model, dependency_graph_wrapper):\n",
    "        self.model = model\n",
    "        self.dependency_graph_wrapper = dependency_graph_wrapper\n",
    "\n",
    "    def transform(self, x):\n",
    "        return self.dependency_graph_wrapper.get_vertex_embeddings(x, self.model)\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search.pytorch_geometric_data import PygGraphWrapper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_model = torch.load(\"output/graphsage_model_100_dim200_layers2.pth\").cpu()\n",
    "graphsage_model.eval()  # = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_graph_wrapper = data_utils.make_extended_dependency_wrapper(\n",
    "    repos=pd.concat([readme_data_train.repos, readme_data_test.repos]),\n",
    "    dependency_records_df=dependency_records_df[\n",
    "        dependency_records_df[\"edge_type\"] == \"repo-file\"\n",
    "    ],\n",
    "    fasttext_model=fasttext_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_vertex_embeddings(wrapper, vertex_subset, model):\n",
    "    features = (\n",
    "        model.full_forward(wrapper.dataset.x, wrapper.dataset.edge_index)\n",
    "        .cpu()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    return features[wrapper.vertex_mapping.loc[vertex_subset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_data_test.repos.isin(dependency_graph_wrapper.records_df[\"source\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def make_pyggraph_retriever_learner(\n",
    "    zs_learner, dependency_graph_wrapper, model, y_embedder\n",
    "):\n",
    "\n",
    "    lambda_transformer = PyGGraphModelTransformer(model, dependency_graph_wrapper)\n",
    "    return RetrieverLearner(zs_learner, lambda_transformer, y_embedder)\n",
    "\n",
    "\n",
    "def save_pyggraph_retriever_learner(pyggraph_retriever_learner, directory):\n",
    "    p = pathlib.Path(directory)\n",
    "    p.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_data_train, graphsage_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    all_tasks,\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_features_df[\"repo\"].apply(lambda s: [s]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphsage_vectorizer = embeddings.AverageWordEmbeddingsVectorizer(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f10df163fa0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv[\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_vectors = graphsage_vectorizer.transform(graphsage_data_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.59876251e-05, -3.57627869e-06, -6.05583191e-05, -2.85506248e-05,\n",
       "       -5.10215759e-05,  7.53402710e-05,  3.64780426e-05,  6.43730164e-05,\n",
       "       -2.26497650e-05, -5.33461571e-05,  5.87701797e-05, -7.58171082e-05,\n",
       "       -8.61100852e-05, -1.34706497e-04, -9.53674316e-06, -7.60555267e-05,\n",
       "       -2.24411488e-05, -5.17368317e-05, -2.71797180e-05,  9.53674316e-06,\n",
       "       -3.09944153e-05,  1.75237656e-04,  2.69412994e-05, -5.48362732e-05,\n",
       "       -6.29425049e-05, -2.38418579e-05, -8.05854797e-05,  4.34815884e-05,\n",
       "        1.14917755e-04, -3.48091125e-05,  1.12056732e-05, -5.14984131e-05,\n",
       "       -5.69820404e-05,  3.43322754e-05, -2.98023224e-05, -2.02655792e-05,\n",
       "        6.24656677e-05,  3.36170197e-05, -2.46763229e-05, -2.34842300e-05,\n",
       "        4.94718552e-06,  9.87052917e-05, -3.98159027e-05, -3.91006470e-05,\n",
       "        6.51478767e-05, -4.95910645e-05,  1.21593475e-05, -5.30481339e-05,\n",
       "       -6.65187836e-05, -3.10540199e-05,  6.67572021e-06, -5.34057617e-05,\n",
       "        8.91089439e-06, -1.00612640e-04, -5.37037849e-05, -8.84532928e-05,\n",
       "       -1.78813934e-07,  9.36985016e-05, -8.46385956e-05, -6.19888306e-06,\n",
       "       -1.71661377e-05,  1.16825104e-05, -1.04904175e-05, -3.69846821e-05,\n",
       "        4.43458557e-05, -7.58171082e-05,  1.00135803e-04,  1.82390213e-05,\n",
       "       -8.96453857e-05,  3.05175781e-05, -9.77516174e-06,  2.89678574e-05,\n",
       "        5.24520874e-05,  2.14576721e-05,  1.59740448e-05,  3.31401825e-05,\n",
       "        2.12192535e-05, -3.26633453e-05,  2.88486481e-05,  3.45706940e-05,\n",
       "       -1.97291374e-05,  7.73072243e-05,  5.66244125e-05, -3.09944153e-05,\n",
       "        1.00851059e-04, -7.15255737e-06, -1.21593475e-05,  1.64031982e-04,\n",
       "       -5.79357147e-05,  9.79900360e-05, -6.31809235e-05,  3.74317169e-05,\n",
       "       -1.29938126e-05,  1.22904778e-04,  6.29425049e-05, -2.75373459e-05,\n",
       "       -5.76227903e-05,  8.34465027e-07, -1.76429749e-05, -2.95639038e-05,\n",
       "        1.37865543e-04, -1.62124634e-05, -5.81741333e-05, -4.07695770e-05,\n",
       "        4.75645065e-05,  1.03473663e-04, -1.13248825e-04, -4.23192978e-06,\n",
       "        6.24656677e-05, -2.02655792e-05,  3.26633453e-05,  4.88758087e-06,\n",
       "       -1.31130219e-04, -7.24792480e-05, -1.03712082e-05, -5.53131104e-05,\n",
       "        4.45246696e-05, -3.69548798e-05,  1.36375427e-04, -8.25524330e-06,\n",
       "       -1.06096268e-05, -1.16825104e-05, -7.12871552e-05,  5.96046448e-05,\n",
       "       -4.29153442e-05, -4.86373901e-05, -4.57763672e-05,  4.64916229e-05])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    kv,\n",
    "    python_word_embeddings,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphsage_learner = make_pyggraph_retriever_learner(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    dependency_graph_wrapper,\n",
    "    graphsage_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer(python_word_embeddings),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 15.1 s, total: 27.7 s\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graphsage_results = run_learner_experiment(\n",
    "    graphsage_learner, graphsage_data_train, graphsage_data_test, fit_learner=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_train': 0.05991373181020998,\n",
       " 'accuracy_test': 0.01475028034158544,\n",
       " 'top10_accuracy_train': 0.13004484304932734,\n",
       " 'top10_accuracy_test': 0.0683111954459203}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphsage_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "abstract_results = run_learner_experiment(\n",
    "    abstract_learner, abstract_data_train, abstract_data_test, fit_learner=False\n",
    ")\n",
    "readme_results = run_learner_experiment(\n",
    "    readme_learner, readme_data_train, readme_data_test, fit_learner=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import2vec_results = run_learner_experiment(\n",
    "    import2vec_learner, import_data_train, import_data_test, fit_learner=False\n",
    ")\n",
    "prone_results = run_learner_experiment(\n",
    "    prone_learner, graph_data_train, graph_data_test, fit_learner=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_records(\n",
    "    [abstract_results, readme_results, graphsage_results]\n",
    ")\n",
    "results_df[\"method\"] = [\"abstract\", \"readme\", \"graphsage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.round(3).to_csv(\"output/retrieval_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"method\", \"top10_accuracy_train\", \"top10_accuracy_test\"]].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_query_level_results(retriever_learner, data_test, k=10):\n",
    "\n",
    "    accuracy_test = retriever_learner.evaluate(data_test, metrics.accuracy_score)\n",
    "    results = get_retrieval_metrics(retriever_learner, data_test, k=k)\n",
    "    results[\"position\"] = results[\"position\"].replace(-1, np.inf)\n",
    "\n",
    "    return accuracy_test, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_idx_or_inf(xs, a):\n",
    "    idxs = np.where(xs == a)[0].astype(int)\n",
    "    if len(idxs) == 0:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return idxs[0]\n",
    "\n",
    "\n",
    "def get_areas(area_grouped_tasks, tasks):\n",
    "    return tasks.apply(\n",
    "        lambda ts: area_grouped_tasks[\"area\"][\n",
    "            area_grouped_tasks[\"task\"].isin(ts)\n",
    "        ].unique()\n",
    "    )\n",
    "\n",
    "\n",
    "erroneous_area_tasks = []\n",
    "\n",
    "\n",
    "def analyze_query_level_results(\n",
    "    query_level_results, area_grouped_tasks, erroneous_area_tasks=erroneous_area_tasks\n",
    "):\n",
    "    retrieval_results_with_area_test = area_grouped_tasks.merge(\n",
    "        query_level_results, left_on=\"task\", right_index=True\n",
    "    )\n",
    "    for tasks in retrieval_results_with_area_test[\"retrieved_labels\"].values:\n",
    "        for task in tasks:\n",
    "            try:\n",
    "                partial(get_areas, area_grouped_tasks)(pd.Series([[task]]))\n",
    "            except:\n",
    "                erroneous_area_tasks.append(task)\n",
    "    retrieved_areas = get_areas(\n",
    "        area_grouped_tasks, retrieval_results_with_area_test[\"retrieved_labels\"]\n",
    "    )  # apply(partial(get_areas, area_grouped_tasks))\n",
    "    retrieval_results_with_area_test[\"retrieved_areas\"] = retrieved_areas\n",
    "    is_area_retrieved = retrieval_results_with_area_test.apply(\n",
    "        lambda row: row[\"area\"] in row[\"retrieved_areas\"][:10], axis=1\n",
    "    )\n",
    "    num_area_retrieved = retrieval_results_with_area_test.apply(\n",
    "        lambda row: len(\n",
    "            np.where(row[\"area\"] == np.array(row[\"retrieved_areas\"])[:10])[0]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    area_idx = retrieval_results_with_area_test.apply(\n",
    "        lambda row: get_idx_or_inf(np.array(row[\"retrieved_areas\"]), row[\"area\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    retrieval_results_with_area_test[\"area_recalled\"] = is_area_retrieved\n",
    "    retrieval_results_with_area_test[\"area_recalled_position\"] = area_idx\n",
    "    retrieval_results_with_area_test[\"num_area_recalled\"] = num_area_retrieved\n",
    "    query_level_results = retrieval_results_with_area_test.groupby(\"area\").agg(\n",
    "        {\n",
    "            \"recalled\": \"mean\",\n",
    "            \"num_recalled\": \"mean\",\n",
    "            # \"position\": [\"median\", \"mean\"],\n",
    "            \"area_recalled\": \"mean\",\n",
    "            \"num_area_recalled\": \"mean\",\n",
    "            \"area_recalled_position\": [\"median\"],\n",
    "        }\n",
    "    )\n",
    "    query_level_results[\"count\"] = retrieval_results_with_area_test[\n",
    "        \"area\"\n",
    "    ].value_counts()  # groupby('area').agg('count')\n",
    "    return query_level_results\n",
    "\n",
    "\n",
    "def get_analyzed_query_level_results(retriever_learner, data_test, area_grouped_tasks):\n",
    "    detailed_results_all = get_query_level_results(retriever_learner, data_test)\n",
    "    retrieval_results_with_area_test = analyze_query_level_results(\n",
    "        query_level_results, area_grouped_tasks\n",
    "    )\n",
    "    return retrieval_results_with_area_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "readme_accuracy, raw_readme_area_results = get_query_level_results(\n",
    "    readme_learner, readme_data_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_accuracy, raw_graphsage_area_results = get_query_level_results(\n",
    "    graphsage_learner, graphsage_data_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_test.y.value_counts()[:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_area_results = analyze_query_level_results(\n",
    "    raw_readme_area_results, area_grouped_tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_area_results.round(2).sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_tasks = graphsage_data_train.all_tasks.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_area_results.round(2).sort_values(\"count\", ascending=False).to_latex(\n",
    "    open(\"output/readme_area_results.tex\", \"w\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graphsage_area_results = analyze_query_level_results(\n",
    "    raw_graphsage_area_results, area_grouped_tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_area_results.round(2).sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_area_results.round(2).sort_values(\"count\", ascending=False).to_latex(\n",
    "    open(\"output/graphsage_area_results.tex\", \"w\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_area_results.sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks_all = graph_data_train.all_tasks.explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph_data_train.all_tasks.explode().value_counts() < 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graphsage_data_test.all_tasks.explode().value_counts() < 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_data_train.y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_data_test.y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(graph_data_test.y.unique()).intersection(train_tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_data_test.y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/graphsage_area_results.tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(readme_data_test, open(\"output/readme_data_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(graphsage_data_test, open(\"output/graphsage_data_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(graphsage_learner, open(\"output/graphsage_learner.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(readme_learner, open(\"output/readme_learner.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation of repo, import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data_train, paired_data_test = RepoTaskData.create_split(\n",
    "    tasks_test,\n",
    "    all_tasks,\n",
    "    paperswithcode_with_features_df,\n",
    "    paperswithcode_with_imports_df[\"imports\"],\n",
    ")\n",
    "paired_data_train.X = graph_data_train.X + \" \" + import_data_train.X\n",
    "paired_data_test.X = graph_data_test.X + \" \" + import_data_test.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data_train.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_learner = RetrieverLearner.create(\n",
    "    zero_shot.ESZSLearner(100, 10),\n",
    "    PairedKeyedVectors(python_word_embeddings.wv, graphsage_embeddings),\n",
    "    fasttext_model,\n",
    "    embeddings.AverageWordEmbeddingsVectorizer,\n",
    "    embeddings.FastTextVectorizer,\n",
    ")\n",
    "\n",
    "paired_learner.fit_learner(graph_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_learner.evaluate(graph_data_train, metric=metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(paired_learner, paired_data_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_retrieval_accuracy(paired_learner, paired_data_test, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (learner, learner_name, test) in zip(\n",
    "    [import2vec_learner, prone_learner, paired_learner],\n",
    "    [\"import2vec\", \"prone\", \"both\"],\n",
    "    [X_test, repo_graph_terms_test, X_paired_test],\n",
    "):\n",
    "    accs = []\n",
    "    for k in [1, 3, 5, 10, 20]:\n",
    "        rec = get_retrieval_accuracy(learner, test, y_test, test_task_idxs, k=k)\n",
    "        accs.append(rec)\n",
    "    results.append(pd.Series(name=learner_name, data=accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = [\"Accuracy@{}\".format(i) for i in [1, 3, 5, 10, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.round(3).to_markdown(open(\"metrics/zsl_results.md\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat metrics/zsl_results.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_distances = metrics.pairwise.cosine_distances(task_embeddings, task_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_embeddings = gensim.models.KeyedVectors.load(\"data/poincare5.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.wrappers.fasttext\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github_search import typical_file_parts\n",
    "from mlutil import prototype_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_lines_df = typical_file_parts.get_selected_lines_and_repos(\n",
    "    python_files_df[\"repo_name\"], python_files_df[\"content\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

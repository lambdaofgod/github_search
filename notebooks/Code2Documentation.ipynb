{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0453ac27-94b3-4050-bbab-8eee9f020f94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dspy\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3600749-172a-4cb3-9871-83a67266a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wiring DSPy to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e096dbb-fe28-4b79-a7c7-464892e3a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "resource = Resource(attributes={})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba75b7-cc54-468b-a553-71cc38e2ee40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b243530-3870-4b1c-953c-9834f1e919e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_df = pd.read_json(\"../output/paperswithcode_with_readmes.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f03f31-8d07-4603-a322-d21781152c93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['repo', 'paper_urls', 'paper_titles', 'titles', 'arxiv_ids', 'authors',\n",
       "       'tasks', 'readme'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a03c49e-c677-452b-8c4d-ec47cf7b5255",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# SincNet\n",
      "SincNet is a neural architecture for processing **raw audio samples**. It is a novel Convolutional Neural Network (CNN) that encourages the first convolutional layer to discover more **meaningful filters**. SincNet is based on parametrized sinc functions, which implement band-pass filters.\n",
      "\n",
      "In contrast to standard CNNs, that learn all elements of each filter, only low and high cutoff frequencies are directly learned from data with the proposed method. This offers a very compact and efficient way to derive a **customized filter bank** specifically tuned for the desired application. \n",
      "\n",
      "This project releases a collection of codes and utilities to perform speaker identification with SincNet.\n",
      "An example of speaker identification with the TIMIT database is provided. If you are interested in **SincNet applied to speech recognition you can take a look into the PyTorch-Kaldi github repository (https://github.com/mravanelli/pytorch-kaldi).** \n",
      "\n",
      "<img src=\"https://github.com/mravanelli/SincNet/blob/master/SincNet.png\" width=\"400\" img align=\"right\">\n",
      "\n",
      "[Take a look into our video introduction to SincNet](https://www.youtube.com/watch?v=mXQBObRGUgk&feature=youtu.be)\n",
      "\n",
      "## Cite us\n",
      "If you use this code or part of it, please cite us!\n",
      "\n",
      "*Mirco Ravanelli, Yoshua Bengio, âSpeaker Recognition from raw waveform with SincNetâ* [Arxiv](http://arxiv.org/abs/1808.00158)\n",
      "\n",
      "\n",
      "## Prerequisites\n",
      "- Linux\n",
      "- Python 3.6/2.7\n",
      "- pytorch 1.0\n",
      "- pysoundfile (``` conda install -c conda-forge pysoundfile```)\n",
      "- We also suggest using the anaconda environment.\n",
      "\n",
      "## Updates\n",
      "Feb, 16 2019:\n",
      "- We replaced the old \"sinc_conv\"  with \"SincConv_fast\". The latter is 50% faster.\n",
      "- In the near future, we plan to support SincNet based speaker-id within the [PyTorch-Kaldi project](https://github.com/mravanelli/pytorch-kaldi) (the current version of the project only supports SincNEt for speech recognition experiments). This will allow users to perform speaker recognition experiments in a faster and much more flexible environment. The current repository will anyway remain as a showcase. \n",
      "\n",
      "## How to run a TIMIT experiment\n",
      "Even though the code can be easily adapted to any speech dataset, in the following part of the documentation we provide an example based on the popular TIMIT dataset.\n",
      "\n",
      "**1. Run TIMIT data preparation.**\n",
      "\n",
      "This step is necessary to store a version of TIMIT in which start and end silences are removed and the amplitude of each speech utterance is normalized. To do it, run the following code:\n",
      "\n",
      "``\n",
      "python TIMIT_preparation.py $TIMIT_FOLDER $OUTPUT_FOLDER data_lists/TIMIT_all.scp\n",
      "``\n",
      "\n",
      "where:\n",
      "- *$TIMIT_FOLDER* is the folder of the original TIMIT corpus\n",
      "- *$OUTPUT_FOLDER* is the folder in which the normalized TIMIT will be stored\n",
      "- *data_lists/TIMIT_all.scp* is the list of the TIMIT files used for training/test the speaker id system.\n",
      "\n",
      "**2. Run the speaker id experiment.**\n",
      "\n",
      "- Modify the *[data]* section of *cfg/SincNet_TIMIT.cfg* file according to your paths. In particular, modify the *data_folder* with the *$OUTPUT_FOLDER* specified during the TIMIT preparation. The other parameters of the config file belong to the following sections:\n",
      " 1. *[windowing]*, that defines how each sentence is split into smaller chunks.\n",
      " 2. *[cnn]*,  that specifies the characteristics of the CNN architecture.\n",
      " 3. *[dnn]*,  that specifies the characteristics of the fully-connected DNN architecture following the CNN layers.\n",
      " 4. *[class]*, that specify the softmax classification part.\n",
      " 5. *[optimization]*, that reports the main hyperparameters used to train the architecture.\n",
      "\n",
      "- Once setup the cfg file, you can run the speaker id experiments using the following command:\n",
      "\n",
      "``\n",
      "python speaker_id.py --cfg=cfg/SincNet_TIMIT.cfg\n",
      "``\n",
      "\n",
      "The network might take several hours to converge (depending on the speed of your GPU card). In our case, using an *nvidia TITAN X*, the full training took about 24 hours. If you use the code within a cluster is crucial to copy the normalized dataset into the local node, since the current version of the code requires frequent accesses to the stored wav files. Note that several possible optimizations to improve the code speed are not implemented in this version since are out of the scope of this work.\n",
      "\n",
      "\n",
      "**3. Results.**\n",
      "\n",
      "The results are saved into the *output_folder* specified in the cfg file. In this folder, you can find a file (*res.res*) summarizing training and test error rates. The model *model_raw.pkl* is the SincNet model saved after the last iteration. \n",
      "Using the cfg file specified above, we obtain the following results:\n",
      "```\n",
      "epoch 0, loss_tr=5.542032 err_tr=0.984189 loss_te=4.996982 err_te=0.969038 err_te_snt=0.919913\n",
      "epoch 8, loss_tr=1.693487 err_tr=0.434424 loss_te=2.735717 err_te=0.612260 err_te_snt=0.069264\n",
      "epoch 16, loss_tr=0.861834 err_tr=0.229424 loss_te=2.465258 err_te=0.520276 err_te_snt=0.038240\n",
      "epoch 24, loss_tr=0.528619 err_tr=0.144375 loss_te=2.948707 err_te=0.534053 err_te_snt=0.062049\n",
      "epoch 32, loss_tr=0.362914 err_tr=0.100518 loss_te=2.530276 err_te=0.469060 err_te_snt=0.015152\n",
      "epoch 40, loss_tr=0.267921 err_tr=0.076445 loss_te=2.761606 err_te=0.464799 err_te_snt=0.023088\n",
      "epoch 48, loss_tr=0.215479 err_tr=0.061406 loss_te=2.737486 err_te=0.453493 err_te_snt=0.010823\n",
      "epoch 56, loss_tr=0.173690 err_tr=0.050732 loss_te=2.812427 err_te=0.443322 err_te_snt=0.011544\n",
      "epoch 64, loss_tr=0.145256 err_tr=0.043594 loss_te=2.917569 err_te=0.438507 err_te_snt=0.009380\n",
      "epoch 72, loss_tr=0.128894 err_tr=0.038486 loss_te=3.009008 err_te=0.438005 err_te_snt=0.019481\n",
      "....\n",
      "epoch 320, loss_tr=0.033052 err_tr=0.009639 loss_te=4.076542 err_te=0.416710 err_te_snt=0.006494\n",
      "epoch 328, loss_tr=0.033344 err_tr=0.010117 loss_te=3.928874 err_te=0.415024 err_te_snt=0.007215\n",
      "epoch 336, loss_tr=0.033228 err_tr=0.010166 loss_te=4.030224 err_te=0.410034 err_te_snt=0.005051\n",
      "epoch 344, loss_tr=0.033313 err_tr=0.010166 loss_te=4.402949 err_te=0.428691 err_te_snt=0.009380\n",
      "epoch 352, loss_tr=0.031828 err_tr=0.009238 loss_te=4.080747 err_te=0.414066 err_te_snt=0.006494\n",
      "epoch 360, loss_tr=0.033095 err_tr=0.009600 loss_te=4.254683 err_te=0.419954 err_te_snt=0.005772\n",
      "``` \n",
      "The converge is initially very fast (see the first 30 epochs). After that the performance improvement decreases and oscillations into the sentence error rate performance appear. Despite these oscillations an average improvement trend can be observed for the subsequent epochs. In this experiment, we stopped our training  at epoch 360.\n",
      "The fields of the res.res file have the following meaning:\n",
      "- loss_tr: is the average training loss (i.e., cross-entropy function) computed at every frame.\n",
      "- err_tr: is the classification error (measured at frame level) of the training data. Note that we split the speech signals into chunks of 200ms with 10ms overlap. The error is averaged for all the chunks of the training dataset.\n",
      "- loss_te is the average test loss (i.e., cross-entropy function) computed at every frame.\n",
      "- err_te: is the classification error (measured at frame level) of the test data.\n",
      "- err_te_snt: is the classification error (measured at sentence level) of the test data. Note that we split the speech signal into chunks of 200ms with 10ms overlap. For each chunk, our SincNet performs a prediction over the set of speakers. To compute this classification error rate we averaged the predictions and, for each sentence, we voted for the speaker with the highest average probability.\n",
      "\n",
      "[You can find our trained model for TIMIT here.](https://bitbucket.org/mravanelli/sincnet_models/)\n",
      "\n",
      "## Where SincNet is implemented?\n",
      "To take a look into the SincNet implementation you should open the file *dnn_models.py* and read the classes *SincNet*, *sinc_conv* and the function *sinc*.\n",
      "\n",
      "## How to use SincNet with a different dataset?\n",
      "In this repository, we used the TIMIT dataset as a tutorial to show how SincNet works. \n",
      "With the current version of the code, you can easily use a different corpus. To do it you should provide in input the corpora-specific input files (in wav format) and your own labels. You should thus modify the paths into the *.scp files you find in the data_lists folder. \n",
      "\n",
      "To assign to each sentence the right label, you also have to modify the dictionary \"*TIMIT_labels.npy*\". \n",
      "The labels are specified within a python dictionary that contains sentence ids as keys (e.g., \"*si1027*\") and speaker_ids as values. Each speaker_id is an integer, ranging from 0 to N_spks-1. In the TIMIT dataset, you can easily retrieve the speaker id from the path (e.g., *train/dr1/fcjf0/si1027.wav* is the sentence_id \"*si1027*\" uttered by the speaker \"*fcjf0*\"). For other datasets, you should be able to retrieve in such a way this dictionary containing pairs of speakers and sentence ids.\n",
      "\n",
      "You should then modify the config file (*cfg/SincNet_TIMIT.cfg*) according to your new paths. Remember also to change the field \"*class_lay=462*\" according to the number of speakers N_spks you have in your dataset.\n",
      "\n",
      "**The version of the Librispeech dataset used in the paper is available upon request**. In our work, we have used only 12-15 seconds of training material for each speaker and we processed the original librispeech sentences in order to perform amplitude normalization. Moreover, we used a simple energy-based VAD to avoid silences at the beginning and end of each sentence as well as to split in multiple chunks the sentences that contain longer silence\n",
      "\n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "[1] Mirco Ravanelli, Yoshua Bengio, âSpeaker Recognition from raw waveform with SincNetâ [Arxiv](http://arxiv.org/abs/1808.00158)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "example_repo = readme_df.iloc[idx][\"repo\"]\n",
    "print(readme_df.iloc[idx][\"readme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e19c862-167b-4338-9d82-fc6abbb94a9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dependency_records_df = pd.read_json(\"../output/dependency_records/repo_dependencies_articlerank.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2707bcf1-034e-4581-a27b-3357778d2cf9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156449 entries, 0 to 156448\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   repo       156449 non-null  object\n",
      " 1   edge_type  156449 non-null  object\n",
      " 2   nodes      156449 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "dependency_records_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287c718f-3b4c-4b84-af5d-75eb68b1ef78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "python_files_df = pd.read_parquet(\"../output/repo_selected_files.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160bd503-5b5b-45b3-86b8-feae33190ac2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo_name\n",
       "shlizee/Audeo                               10\n",
       "aishikchakraborty/LexSub                    10\n",
       "natsumeS/analysis                           10\n",
       "shermanhung/U-Net                           10\n",
       "mtanti/mtanti-phd                           10\n",
       "anonymous1100/Distributional-Discrepancy    10\n",
       "vumaasha/atlas                              10\n",
       "cyberjam/darknet_submit                     10\n",
       "wszlong/sb-nmt                              10\n",
       "Jeffrey-Ede/adaptive-scans                  10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[\"repo_name\"].value_counts().iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3c5f06-a3ac-4069-9038-4b83cd183e33",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'008karan/SincNet_demo'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7ec5ec-e8e6-4009-ae17-f5af7ad00cf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12260      compute_d_vector.py\n",
       "31929               data_io.py\n",
       "47820            speaker_id.py\n",
       "115220           similarity.py\n",
       "127410            inference.py\n",
       "189566           dnn_models.py\n",
       "258739    TIMIT_preparation.py\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_repo_files_df = python_files_df[python_files_df[\"repo_name\"] == example_repo]\n",
    "example_repo_files_df[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7102199-a0c1-4c3c-8bf2-0b0277cc873b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7 entries, 12260 to 258739\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   content    7 non-null      object\n",
      " 1   path       7 non-null      object\n",
      " 2   repo_name  7 non-null      object\n",
      " 3   tasks      7 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 280.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "example_repo_files_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6f80f4-8289-401c-9207-571458820488",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# compute_d_vector.py\n",
      "# Mirco Ravanelli \n",
      "# Mila - University of Montreal \n",
      "\n",
      "# Feb 2019\n",
      "\n",
      "# Description: \n",
      "# This code computes d-vectors using a pre-trained model\n",
      " \n",
      "\n",
      "import os\n",
      "import soundfile as sf\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "import numpy as np\n",
      "from dnn_models import MLP\n",
      "from dnn_models import SincNet as CNN \n",
      "from data_io import ReadList,read_conf_inp,str_to_bool\n",
      "import sys\n",
      "\n",
      "# Model to use for computing the d-vectors\n",
      "model_file=\"SincNet_demo/sincnet_models/SincNet_TIMIT/model_raw.pkl\" # This is the model to use for computing the d-vectors (it should be pre-trained using the speaker-id DNN)\n",
      "cfg_file='SincNet_demo/cfg/SincNet_TIMIT.cfg' # Config file of the speaker-id experiment used to generate the model\n",
      "#te_lst='data_lists/TIMIT_test.scp' # List of the wav files to process\n",
      "te_lst='SincNet_demo/test.scp'\n",
      "out_dict_file='SincNet_demo/d_vect_dr1_fcjf00.npy' # output dictionary containing the a sentence id as key as the d-vector as value\n",
      "data_folder='SincNet_demo/timit'\n",
      "\n",
      "avoid_small_en_fr=True\n",
      "energy_th = 0.1  # Avoid frames with an energy that is 1/10 over the average energy\n",
      "\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "#device = None\n",
      "\n",
      "# Reading cfg file\n",
      "options=read_conf_inp(cfg_file)\n",
      "\n",
      "\n",
      "#[data]\n",
      "pt_file=options.pt_file\n",
      "output_folder=options.output_folder\n",
      "\n",
      "#[windowing]\n",
      "fs=int(options.fs)\n",
      "cw_len=int(options.cw_len)\n",
      "cw_shift=int(options.cw_shift)\n",
      "\n",
      "#[cnn]\n",
      "cnn_N_filt=list(map(int, options.cnn_N_filt.split(',')))\n",
      "cnn_len_filt=list(map(int, options.cnn_len_filt.split(',')))\n",
      "cnn_max_pool_len=list(map(int, options.cnn_max_pool_len.split(',')))\n",
      "cnn_use_laynorm_inp=str_to_bool(options.cnn_use_laynorm_inp)\n",
      "cnn_use_batchnorm_inp=str_to_bool(options.cnn_use_batchnorm_inp)\n",
      "cnn_use_laynorm=list(map(str_to_bool, options.cnn_use_laynorm.split(',')))\n",
      "cnn_use_batchnorm=list(map(str_to_bool, options.cnn_use_batchnorm.split(',')))\n",
      "cnn_act=list(map(str, options.cnn_act.split(',')))\n",
      "cnn_drop=list(map(float, options.cnn_drop.split(',')))\n",
      "\n",
      "\n",
      "#[dnn]\n",
      "fc_lay=list(map(int, options.fc_lay.split(',')))\n",
      "fc_drop=list(map(float, options.fc_drop.split(',')))\n",
      "fc_use_laynorm_inp=str_to_bool(options.fc_use_laynorm_inp)\n",
      "fc_use_batchnorm_inp=str_to_bool(options.fc_use_batchnorm_inp)\n",
      "fc_use_batchnorm=list(map(str_to_bool, options.fc_use_batchnorm.split(',')))\n",
      "fc_use_laynorm=list(map(str_to_bool, options.fc_use_laynorm.split(',')))\n",
      "fc_act=list(map(str, options.fc_act.split(',')))\n",
      "\n",
      "#[class]\n",
      "class_lay=list(map(int, options.class_lay.split(',')))\n",
      "class_drop=list(map(float, options.class_drop.split(',')))\n",
      "class_use_laynorm_inp=str_to_bool(options.class_use_laynorm_inp)\n",
      "class_use_batchnorm_inp=str_to_bool(options.class_use_batchnorm_inp)\n",
      "class_use_batchnorm=list(map(str_to_bool, options.class_use_batchnorm.split(',')))\n",
      "class_use_laynorm=list(map(str_to_bool, options.class_use_laynorm.split(',')))\n",
      "class_act=list(map(str, options.class_act.split(',')))\n",
      "\n",
      "\n",
      "wav_lst_te=ReadList(te_lst)\n",
      "snt_te=len(wav_lst_te)\n",
      "\n",
      "\n",
      "# Folder creation\n",
      "try:\n",
      "    os.stat(output_folder)\n",
      "except:\n",
      "    os.mkdir(output_folder) \n",
      "    \n",
      "    \n",
      "# loss function\n",
      "cost = nn.NLLLoss()\n",
      "\n",
      "  \n",
      "# Converting context and shift in samples\n",
      "wlen=int(fs*cw_len/1000.00)\n",
      "wshift=int(fs*cw_shift/1000.00)\n",
      "\n",
      "# Batch_dev\n",
      "Batch_dev=128\n",
      "\n",
      "\n",
      "# Feature extractor CNN\n",
      "CNN_arch = {'input_dim': wlen,\n",
      "          'fs': fs,\n",
      "          'cnn_N_filt': cnn_N_filt,\n",
      "          'cnn_len_filt': cnn_len_filt,\n",
      "          'cnn_max_pool_len':cnn_max_pool_len,\n",
      "          'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
      "          'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
      "          'cnn_use_laynorm':cnn_use_laynorm,\n",
      "          'cnn_use_batchnorm':cnn_use_batchnorm,\n",
      "          'cnn_act': cnn_act,\n",
      "          'cnn_drop':cnn_drop,          \n",
      "          }\n",
      "\n",
      "CNN_net=CNN(CNN_arch)\n",
      "CNN_net.to(device)\n",
      "\n",
      "\n",
      "\n",
      "DNN1_arch = {'input_dim': CNN_net.out_dim,\n",
      "          'fc_lay': fc_lay,\n",
      "          'fc_drop': fc_drop, \n",
      "          'fc_use_batchnorm': fc_use_batchnorm,\n",
      "          'fc_use_laynorm': fc_use_laynorm,\n",
      "          'fc_use_laynorm_inp': fc_use_laynorm_inp,\n",
      "          'fc_use_batchnorm_inp':fc_use_batchnorm_inp,\n",
      "          'fc_act': fc_act,\n",
      "          }\n",
      "\n",
      "DNN1_net=MLP(DNN1_arch)\n",
      "DNN1_net.to(device)\n",
      "\n",
      "\n",
      "DNN2_arch = {'input_dim':fc_lay[-1] ,\n",
      "          'fc_lay': class_lay,\n",
      "          'fc_drop': class_drop, \n",
      "          'fc_use_batchnorm': class_use_batchnorm,\n",
      "          'fc_use_laynorm': class_use_laynorm,\n",
      "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
      "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
      "          'fc_act': class_act,\n",
      "          }\n",
      "\n",
      "\n",
      "DNN2_net=MLP(DNN2_arch)\n",
      "DNN2_net.to(device)\n",
      "\n",
      "\n",
      "checkpoint_load = torch.load(model_file)\n",
      "CNN_net.load_state_dict(checkpoint_load['CNN_model_par'])\n",
      "DNN1_net.load_state_dict(checkpoint_load['DNN1_model_par'])\n",
      "DNN2_net.load_state_dict(checkpoint_load['DNN2_model_par'])\n",
      "\n",
      "\n",
      "\n",
      "CNN_net.eval()\n",
      "DNN1_net.eval()\n",
      "DNN2_net.eval()\n",
      "test_flag=1 \n",
      "\n",
      "\n",
      "d_vector_dim=fc_lay[-1]\n",
      "d_vect_dict={}\n",
      "\n",
      "   \n",
      "with torch.no_grad(): \n",
      "    \n",
      "    for i in range(snt_te):\n",
      "           \n",
      "         [signal, fs] = sf.read(data_folder+'/'+wav_lst_te[i])\n",
      "         \n",
      "         # Amplitude normalization\n",
      "         signal=signal/np.max(np.abs(signal))\n",
      "        \n",
      "         signal=torch.from_numpy(signal).float().to(device).contiguous()\n",
      "        \n",
      "         if avoid_small_en_fr: \n",
      "             # computing energy on each frame:\n",
      "             beg_samp=0\n",
      "             end_samp=wlen\n",
      "    \n",
      "             N_fr=int((signal.shape[0]-wlen)/(wshift))\n",
      "             Batch_dev=N_fr\n",
      "             en_arr=torch.zeros(N_fr).float().contiguous().to(device)\n",
      "             count_fr=0\n",
      "             count_fr_tot=0\n",
      "             while end_samp<signal.shape[0]:\n",
      "                en_arr[count_fr]=torch.sum(signal[beg_samp:end_samp].pow(2))\n",
      "                beg_samp=beg_samp+wshift\n",
      "                end_samp=beg_samp+wlen\n",
      "                count_fr=count_fr+1\n",
      "                count_fr_tot=count_fr_tot+1\n",
      "                if count_fr==N_fr:\n",
      "                    break\n",
      "    \n",
      "             en_arr_bin=en_arr>torch.mean(en_arr)*0.1\n",
      "             en_arr_bin.to(device)\n",
      "             n_vect_elem=torch.sum(en_arr_bin)\n",
      "    \n",
      "             if n_vect_elem<10:\n",
      "                 print('only few elements used to compute d-vectors')\n",
      "                 sys.exit(0)\n",
      "\n",
      "\n",
      "\n",
      "         # split signals into chunks\n",
      "         beg_samp=0\n",
      "         end_samp=wlen\n",
      "         \n",
      "         N_fr=int((signal.shape[0]-wlen)/(wshift))\n",
      "         \n",
      "        \n",
      "         sig_arr=torch.zeros([Batch_dev,wlen]).float().to(device).contiguous()\n",
      "         dvects=Variable(torch.zeros(N_fr,d_vector_dim).float().to(device).contiguous())\n",
      "         count_fr=0\n",
      "         count_fr_tot=0\n",
      "         while end_samp<signal.shape[0]:\n",
      "             sig_arr[count_fr,:]=signal[beg_samp:end_samp]\n",
      "             beg_samp=beg_samp+wshift\n",
      "             end_samp=beg_samp+wlen\n",
      "             count_fr=count_fr+1\n",
      "             count_fr_tot=count_fr_tot+1\n",
      "             if count_fr==Batch_dev:\n",
      "                 inp=Variable(sig_arr)\n",
      "                 dvects[count_fr_tot-Batch_dev:count_fr_tot,:]=DNN1_net(CNN_net(inp))\n",
      "                 count_fr=0\n",
      "                 sig_arr=torch.zeros([Batch_dev,wlen]).float().to(device).contiguous()\n",
      "           \n",
      "         if count_fr>0:\n",
      "          inp=Variable(sig_arr[0:count_fr])\n",
      "          dvects[count_fr_tot-count_fr:count_fr_tot,:]=DNN1_net(CNN_net(inp))\n",
      "        \n",
      "         if avoid_small_en_fr:\n",
      "             dvects=dvects.index_select(0, (en_arr_bin==1).nonzero().view(-1))\n",
      "         \n",
      "         # averaging and normalizing all the d-vectors\n",
      "         d_vect_out=torch.mean(dvects/dvects.norm(p=2, dim=1).view(-1,1),dim=0)\n",
      "         \n",
      "         # checks for nan\n",
      "         nan_sum=torch.sum(torch.isnan(d_vect_out))\n",
      "\n",
      "         if nan_sum>0:\n",
      "             print((wav_lst_te[i]))\n",
      "             sys.exit(0)\n",
      "\n",
      "         \n",
      "         # saving the d-vector in a numpy dictionary\n",
      "         dict_key=wav_lst_te[i].split('/')[-2]+'/'+wav_lst_te[i].split('/')[-1]\n",
      "         d_vect_dict[dict_key]=d_vect_out.cpu().numpy()\n",
      "         print(dict_key)\n",
      "d_vect_dict={\"41\":sum(d_vect_dict.values())}\n",
      "# Save the dictionary\n",
      "np.save(out_dict_file, d_vect_dict)\n"
     ]
    }
   ],
   "source": [
    "print(example_repo_files_df[\"content\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8a6774fb-1846-453d-a7e9-0b3183f7289e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "codellama = dspy.OllamaLocal(model=\"codellama\",model_type='text',\n",
    "                                max_tokens=512,\n",
    "                                temperature=0,\n",
    "                                top_p=0.9, frequency_penalty=1.17)\n",
    "\n",
    "mistral = dspy.OllamaLocal(model=\"mistral\",model_type='text',\n",
    "                                max_tokens=512,\n",
    "                                temperature=0,\n",
    "                                top_p=0.9, frequency_penalty=1.17)\n",
    "\n",
    "#len(example_repo_files_df[\"content\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b726120-73e0-4082-b94c-44451d8e60f1",
   "metadata": {},
   "source": [
    "## Multiple OLlama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "112ff901-7410-4586-8c2e-0f8dad87d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/hpack/huffman_table.py:4062: RuntimeWarning: coroutine 'RepoCodeSummarizer._summarize_files_async' was never awaited\n",
      "  (31, HUFFMAN_EMIT_SYMBOL, 245),\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e9fb9943-7e54-44ec-a060-4ea81a166f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dsp.modules.ollama.OllamaLocal at 0x75bf5a30aef0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codellama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "90c1b68c-4659-454f-ab29-d8a8fbea2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OLLAMA_HOST=127.0.0.1:1143{i}', 'ollama', 'serve', '{ollama_model_name}']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"OLLAMA_HOST=127.0.0.1:1143{i} ollama serve {ollama_model_name}\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ef632ee2-ae08-4eef-b840-24f9ceb01bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "argv first element cannot be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[275], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         os\u001b[38;5;241m.\u001b[39mspawnl(os\u001b[38;5;241m.\u001b[39mP_NOWAIT, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOLLAMA_HOST=127.0.0.1:1143\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ollama serve; ollama run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mollama_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite factorial in Python\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#yield proc\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ollama_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mrun_ollama_subprocesses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_model_name\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[275], line 12\u001b[0m, in \u001b[0;36mrun_ollama_subprocesses\u001b[0;34m(ollama_model_name, port_suffix_range)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_ollama_subprocesses\u001b[39m(ollama_model_name, port_suffix_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m port_suffix_range:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m#proc_env = dict(os.environ) | {'OLLAMA_HOST': f\"127.0.0.1:1143{i}\"}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m#proc = subprocess.Popen(['ollama', 'serve'], env=proc_env)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#proc.communicate(['ollama', 'run', ollama_model_name, \"write factorial in Python\"])\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawnl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP_NOWAIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOLLAMA_HOST=127.0.0.1:1143\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m ollama serve; ollama run \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mollama_model_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite factorial in Python\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:929\u001b[0m, in \u001b[0;36mspawnl\u001b[0;34m(mode, file, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawnl\u001b[39m(mode, file, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    923\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"spawnl(mode, file, *args) -> integer\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03mExecute file with arguments from args in a subprocess.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03mIf mode == P_NOWAIT return the pid of the process.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03mIf mode == P_WAIT return the process's exit code if it exits normally;\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03motherwise return -SIG, where SIG is the signal that killed it. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 929\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspawnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:880\u001b[0m, in \u001b[0;36mspawnv\u001b[0;34m(mode, file, args)\u001b[0m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawnv\u001b[39m(mode, file, args):\n\u001b[1;32m    874\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"spawnv(mode, file, args) -> integer\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03mExecute file with arguments from args in a subprocess.\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03mIf mode == P_NOWAIT return the pid of the process.\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03mIf mode == P_WAIT return the process's exit code if it exits normally;\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03motherwise return -SIG, where SIG is the signal that killed it. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_spawnvef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:851\u001b[0m, in \u001b[0;36m_spawnvef\u001b[0;34m(mode, file, args, env, func)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margv must be a tuple or a list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margv first element cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    852\u001b[0m pid \u001b[38;5;241m=\u001b[39m fork()\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pid:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Child\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: argv first element cannot be empty"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "ollama_model_name = \"codellama\"\n",
    "\n",
    "\n",
    "def run_ollama_subprocesses(ollama_model_name, port_suffix_range=range(2)):\n",
    "    for i in port_suffix_range:\n",
    "        #proc_env = dict(os.environ) | {'OLLAMA_HOST': f\"127.0.0.1:1143{i}\"}\n",
    "        #proc = subprocess.Popen(['ollama', 'serve'], env=proc_env)\n",
    "        #proc.communicate(['ollama', 'run', ollama_model_name, \"write factorial in Python\"])\n",
    "        os.spawnl(os.P_NOWAIT, f'OLLAMA_HOST=127.0.0.1:1143{i} ollama serve; ollama run {ollama_model_name} \"write factorial in Python\"')\n",
    "        #yield proc\n",
    "\n",
    "ollama_processes = list(run_ollama_subprocesses(ollama_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032e1fb-f15e-4e8f-9c6f-3713e7eff19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7615728-9c4a-4889-9371-e03288936bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "22cef435-2c04-44ae-80a8-0c911c9ed4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269499"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "774e89a1-fb97-47f4-8da6-c5330f2f2c32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "\n",
    "class FileSummary(dspy.Signature):\n",
    "    code = dspy.InputField(desc=\"Python code\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Summary of the code given guiding question\")\n",
    "\n",
    "\n",
    "class RepoSummary(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"Python file summaries\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Repository summary\")\n",
    "\n",
    "    \n",
    "def fetch_code(repo_name, python_files_df=python_files_df, n=5):\n",
    "    repo_files_df = python_files_df[python_files_df[\"repo_name\"] == repo_name].iloc[:n]\n",
    "    return repo_files_df\n",
    "\n",
    "\n",
    "REPO_SUMMARIZER_QUESTION_TEMPLATE = \"For a repository named '{}' describe the functionalities of the following code:\"\n",
    "\n",
    "class RepoCodeSummarizer(dspy.Module):\n",
    "    def __init__(self, fetch_code_fn, prompt_template=REPO_SUMMARIZER_QUESTION_TEMPLATE, verbose=True):\n",
    "        super().__init__()\n",
    "        self.fetch_code = fetch_code\n",
    "        self.summarize_file = dspy.ChainOfThought(FileSummary)\n",
    "        self.summarize_repo = dspy.ChainOfThought(RepoSummary)\n",
    "        self.prompt_template = prompt_template\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    async def _summarize_file(self, lm, repo_name, code, i):\n",
    "        if self.verbose:\n",
    "            print(f\"running lm no. {i}\")\n",
    "        with dspy.context(lm=lm):\n",
    "            file_summary = self.summarize_file(question=self.prompt_template.format(repo_name), code=code)\n",
    "        return file_summary\n",
    "\n",
    "    async def _summarize_files_async(self, lms, repo_name, code_file_contents):\n",
    "        lms = [\n",
    "            lm \n",
    "            for lm_list in itertools.repeat(lms, len(code_file_contents))\n",
    "            for lm in lm_list\n",
    "        ]\n",
    "        async_tasks = [\n",
    "            self._summarize_file(lm, repo_name, code, i) for (i, (lm, code)) in enumerate(zip(lms, code_file_contents))\n",
    "        ]\n",
    "                               \n",
    "        return await asyncio.gather(*async_tasks)\n",
    "    \n",
    "    def _summarize_files(self, lms, repo_name, code_file_contents):\n",
    "\n",
    "        loop = asyncio.new_event_loop()\n",
    "\n",
    "        return loop.run_until_complete(self._summarize_files_async(lms, repo_name, code_file_contents))\n",
    "    \n",
    "    def forward(self, repo_name, lms):\n",
    "        code_files = self.fetch_code(repo_name)\n",
    "        summaries = []\n",
    "        #for code in code_files[\"content\"].to_list():\n",
    "        #    file_summary = self.summarize_file(question=self.prompt_template.format(repo_name), code=code)\n",
    "        #    summaries.append(file_summary)\n",
    "        #\n",
    "        summaries = self._summarize_files(lms, repo_name, code_files[\"content\"].to_list())\n",
    "        \n",
    "        summaries_context = \"\\n\".join([f\"{filename} summary:\\n {summary}\" for filename, summary in zip(code_files[\"path\"], summaries)])\n",
    "\n",
    "        repo_summary_question = f\"\"\"\n",
    "        Given the following summaries of '{repo_name}' files write repository README.\n",
    "        Focus on the functionalities and features.\n",
    "        There is no need to describe the dependencies and setup.\n",
    "        The README should provide answers to the following questions:\n",
    "        - what machine learning problem does this repository tackle?\n",
    "        - what kind of data does it use?\n",
    "        Base your answer only on the information from context.\n",
    "        \"\"\"\n",
    "        repo_summary = self.summarize_repo(\n",
    "            question=repo_summary_question.strip(),\n",
    "            context=summaries\n",
    "        )\n",
    "            \n",
    "        return dspy.Prediction(**repo_summary, context_history=summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "51ae6edb-835f-42ab-bb47-6e7511bcdd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[\"repo_name\"].isin(readme_df[\"repo\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058663f-3787-4642-afc4-858944b5a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "lms = [\n",
    "    dspy.OllamaLocal(model=\"codellama\",model_type='text',\n",
    "                                max_tokens=512,\n",
    "                                temperature=0,\n",
    "                                base_url=f'http://localhost:1143{i}',\n",
    "                                top_p=0.9, frequency_penalty=1.17)\n",
    "    for i in [0, 1, 4]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc354eb-a7f8-430b-be21-80af84afd471",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#dspy.configure(lm=codellama)\n",
    "\n",
    "#example_code = example_repo_files_df[\"content\"].iloc[0]\n",
    "\n",
    "repo_summarizer = RepoCodeSummarizer(fetch_code)\n",
    "#code_summarizer(code=[example_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c1f54-fd77-4c7a-876f-250277faea06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "example_repo = python_files_df[\"repo_name\"].unique()[1]\n",
    "\n",
    "print(example_repo)\n",
    "fetch_code(example_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13597d4a-ed50-447b-ad21-b56fcc5db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readme_df[readme_df[\"repo\"] == example_repo].iloc[0][\"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787bbe1-294b-4015-836c-37b8101646ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readme_df[readme_df[\"repo\"] == example_repo].iloc[0][\"readme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec88fe5-ee83-4966-a3ce-cb88f6a99602",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "repo_summarizer_answer = repo_summarizer(example_repo, lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "508eea89-c7f8-40e0-beb9-cc201d4f1e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x75bfab03ed70 state=finished raised NameError>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_summarizer_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2474fbe2-7267-4d61-beb5-f74b37d5d3a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository \"semantic-foreground-inpainting\" tackles the problem of semantic segmentation with foreground inpainting using PyTorch library. It uses pretrained weights from VGG16 network that was trained on ImageNet dataset to perform this task.\n"
     ]
    }
   ],
   "source": [
    "print(repo_summarizer_answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6a0de042-33a4-40e6-ad83-0690bcbf1561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316.98333333333335"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * len(python_files_df[\"repo_name\"].unique()) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2166585-7341-4512-802f-387c6c5e7125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adc6ed-634a-4485-b348-8c13bc15574f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095ff96-2a36-4bf8-a68f-36140a280398",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "dspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "name": "Code2Documentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

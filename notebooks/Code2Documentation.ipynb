{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0453ac27-94b3-4050-bbab-8eee9f020f94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/github-search-hM2r__Rf-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dspy\n",
    "import re\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3600749-172a-4cb3-9871-83a67266a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wiring DSPy to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b243530-3870-4b1c-953c-9834f1e919e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_df = pd.read_json(\"../output/paperswithcode_with_readmes.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f26473e-f765-4044-a1ad-ef61e2c35f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "readme_df[\"tasks\"] = readme_df[\"tasks\"].apply(ast.literal_eval)#.explode().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f03f31-8d07-4603-a322-d21781152c93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    73326.000000\n",
       "mean         2.959455\n",
       "std          2.744033\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max        101.000000\n",
       "Name: tasks, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_df[\"tasks\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a03c49e-c677-452b-8c4d-ec47cf7b5255",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# SincNet\n",
      "SincNet is a neural architecture for processing **raw audio samples**. It is a novel Convolutional Neural Network (CNN) that encourages the first convolutional layer to discover more **meaningful filters**. SincNet is based on parametrized sinc functions, which implement band-pass filters.\n",
      "\n",
      "In contrast to standard CNNs, that learn all elements of each filter, only low and high cutoff frequencies are directly learned from data with the proposed method. This offers a very compact and efficient way to derive a **customized filter bank** specifically tuned for the desired application. \n",
      "\n",
      "This project releases a collection of codes and utilities to perform speaker identification with SincNet.\n",
      "An example of speaker identification with the TIMIT database is provided. If you are interested in **SincNet applied to speech recognition you can take a look into the PyTorch-Kaldi github repository (https://github.com/mravanelli/pytorch-kaldi).** \n",
      "\n",
      "<img src=\"https://github.com/mravanelli/SincNet/blob/master/SincNet.png\" width=\"400\" img align=\"right\">\n",
      "\n",
      "[Take a look into our video introduction to SincNet](https://www.youtube.com/watch?v=mXQBObRGUgk&feature=youtu.be)\n",
      "\n",
      "## Cite us\n",
      "If you use this code or part of it, please cite us!\n",
      "\n",
      "*Mirco Ravanelli, Yoshua Bengio, âSpeaker Recognition from raw waveform with SincNetâ* [Arxiv](http://arxiv.org/abs/1808.00158)\n",
      "\n",
      "\n",
      "## Prerequisites\n",
      "- Linux\n",
      "- Python 3.6/2.7\n",
      "- pytorch 1.0\n",
      "- pysoundfile (``` conda install -c conda-forge pysoundfile```)\n",
      "- We also suggest using the anaconda environment.\n",
      "\n",
      "## Updates\n",
      "Feb, 16 2019:\n",
      "- We replaced the old \"sinc_conv\"  with \"SincConv_fast\". The latter is 50% faster.\n",
      "- In the near future, we plan to support SincNet based speaker-id within the [PyTorch-Kaldi project](https://github.com/mravanelli/pytorch-kaldi) (the current version of the project only supports SincNEt for speech recognition experiments). This will allow users to perform speaker recognition experiments in a faster and much more flexible environment. The current repository will anyway remain as a showcase. \n",
      "\n",
      "## How to run a TIMIT experiment\n",
      "Even though the code can be easily adapted to any speech dataset, in the following part of the documentation we provide an example based on the popular TIMIT dataset.\n",
      "\n",
      "**1. Run TIMIT data preparation.**\n",
      "\n",
      "This step is necessary to store a version of TIMIT in which start and end silences are removed and the amplitude of each speech utterance is normalized. To do it, run the following code:\n",
      "\n",
      "``\n",
      "python TIMIT_preparation.py $TIMIT_FOLDER $OUTPUT_FOLDER data_lists/TIMIT_all.scp\n",
      "``\n",
      "\n",
      "where:\n",
      "- *$TIMIT_FOLDER* is the folder of the original TIMIT corpus\n",
      "- *$OUTPUT_FOLDER* is the folder in which the normalized TIMIT will be stored\n",
      "- *data_lists/TIMIT_all.scp* is the list of the TIMIT files used for training/test the speaker id system.\n",
      "\n",
      "**2. Run the speaker id experiment.**\n",
      "\n",
      "- Modify the *[data]* section of *cfg/SincNet_TIMIT.cfg* file according to your paths. In particular, modify the *data_folder* with the *$OUTPUT_FOLDER* specified during the TIMIT preparation. The other parameters of the config file belong to the following sections:\n",
      " 1. *[windowing]*, that defines how each sentence is split into smaller chunks.\n",
      " 2. *[cnn]*,  that specifies the characteristics of the CNN architecture.\n",
      " 3. *[dnn]*,  that specifies the characteristics of the fully-connected DNN architecture following the CNN layers.\n",
      " 4. *[class]*, that specify the softmax classification part.\n",
      " 5. *[optimization]*, that reports the main hyperparameters used to train the architecture.\n",
      "\n",
      "- Once setup the cfg file, you can run the speaker id experiments using the following command:\n",
      "\n",
      "``\n",
      "python speaker_id.py --cfg=cfg/SincNet_TIMIT.cfg\n",
      "``\n",
      "\n",
      "The network might take several hours to converge (depending on the speed of your GPU card). In our case, using an *nvidia TITAN X*, the full training took about 24 hours. If you use the code within a cluster is crucial to copy the normalized dataset into the local node, since the current version of the code requires frequent accesses to the stored wav files. Note that several possible optimizations to improve the code speed are not implemented in this version since are out of the scope of this work.\n",
      "\n",
      "\n",
      "**3. Results.**\n",
      "\n",
      "The results are saved into the *output_folder* specified in the cfg file. In this folder, you can find a file (*res.res*) summarizing training and test error rates. The model *model_raw.pkl* is the SincNet model saved after the last iteration. \n",
      "Using the cfg file specified above, we obtain the following results:\n",
      "```\n",
      "epoch 0, loss_tr=5.542032 err_tr=0.984189 loss_te=4.996982 err_te=0.969038 err_te_snt=0.919913\n",
      "epoch 8, loss_tr=1.693487 err_tr=0.434424 loss_te=2.735717 err_te=0.612260 err_te_snt=0.069264\n",
      "epoch 16, loss_tr=0.861834 err_tr=0.229424 loss_te=2.465258 err_te=0.520276 err_te_snt=0.038240\n",
      "epoch 24, loss_tr=0.528619 err_tr=0.144375 loss_te=2.948707 err_te=0.534053 err_te_snt=0.062049\n",
      "epoch 32, loss_tr=0.362914 err_tr=0.100518 loss_te=2.530276 err_te=0.469060 err_te_snt=0.015152\n",
      "epoch 40, loss_tr=0.267921 err_tr=0.076445 loss_te=2.761606 err_te=0.464799 err_te_snt=0.023088\n",
      "epoch 48, loss_tr=0.215479 err_tr=0.061406 loss_te=2.737486 err_te=0.453493 err_te_snt=0.010823\n",
      "epoch 56, loss_tr=0.173690 err_tr=0.050732 loss_te=2.812427 err_te=0.443322 err_te_snt=0.011544\n",
      "epoch 64, loss_tr=0.145256 err_tr=0.043594 loss_te=2.917569 err_te=0.438507 err_te_snt=0.009380\n",
      "epoch 72, loss_tr=0.128894 err_tr=0.038486 loss_te=3.009008 err_te=0.438005 err_te_snt=0.019481\n",
      "....\n",
      "epoch 320, loss_tr=0.033052 err_tr=0.009639 loss_te=4.076542 err_te=0.416710 err_te_snt=0.006494\n",
      "epoch 328, loss_tr=0.033344 err_tr=0.010117 loss_te=3.928874 err_te=0.415024 err_te_snt=0.007215\n",
      "epoch 336, loss_tr=0.033228 err_tr=0.010166 loss_te=4.030224 err_te=0.410034 err_te_snt=0.005051\n",
      "epoch 344, loss_tr=0.033313 err_tr=0.010166 loss_te=4.402949 err_te=0.428691 err_te_snt=0.009380\n",
      "epoch 352, loss_tr=0.031828 err_tr=0.009238 loss_te=4.080747 err_te=0.414066 err_te_snt=0.006494\n",
      "epoch 360, loss_tr=0.033095 err_tr=0.009600 loss_te=4.254683 err_te=0.419954 err_te_snt=0.005772\n",
      "``` \n",
      "The converge is initially very fast (see the first 30 epochs). After that the performance improvement decreases and oscillations into the sentence error rate performance appear. Despite these oscillations an average improvement trend can be observed for the subsequent epochs. In this experiment, we stopped our training  at epoch 360.\n",
      "The fields of the res.res file have the following meaning:\n",
      "- loss_tr: is the average training loss (i.e., cross-entropy function) computed at every frame.\n",
      "- err_tr: is the classification error (measured at frame level) of the training data. Note that we split the speech signals into chunks of 200ms with 10ms overlap. The error is averaged for all the chunks of the training dataset.\n",
      "- loss_te is the average test loss (i.e., cross-entropy function) computed at every frame.\n",
      "- err_te: is the classification error (measured at frame level) of the test data.\n",
      "- err_te_snt: is the classification error (measured at sentence level) of the test data. Note that we split the speech signal into chunks of 200ms with 10ms overlap. For each chunk, our SincNet performs a prediction over the set of speakers. To compute this classification error rate we averaged the predictions and, for each sentence, we voted for the speaker with the highest average probability.\n",
      "\n",
      "[You can find our trained model for TIMIT here.](https://bitbucket.org/mravanelli/sincnet_models/)\n",
      "\n",
      "## Where SincNet is implemented?\n",
      "To take a look into the SincNet implementation you should open the file *dnn_models.py* and read the classes *SincNet*, *sinc_conv* and the function *sinc*.\n",
      "\n",
      "## How to use SincNet with a different dataset?\n",
      "In this repository, we used the TIMIT dataset as a tutorial to show how SincNet works. \n",
      "With the current version of the code, you can easily use a different corpus. To do it you should provide in input the corpora-specific input files (in wav format) and your own labels. You should thus modify the paths into the *.scp files you find in the data_lists folder. \n",
      "\n",
      "To assign to each sentence the right label, you also have to modify the dictionary \"*TIMIT_labels.npy*\". \n",
      "The labels are specified within a python dictionary that contains sentence ids as keys (e.g., \"*si1027*\") and speaker_ids as values. Each speaker_id is an integer, ranging from 0 to N_spks-1. In the TIMIT dataset, you can easily retrieve the speaker id from the path (e.g., *train/dr1/fcjf0/si1027.wav* is the sentence_id \"*si1027*\" uttered by the speaker \"*fcjf0*\"). For other datasets, you should be able to retrieve in such a way this dictionary containing pairs of speakers and sentence ids.\n",
      "\n",
      "You should then modify the config file (*cfg/SincNet_TIMIT.cfg*) according to your new paths. Remember also to change the field \"*class_lay=462*\" according to the number of speakers N_spks you have in your dataset.\n",
      "\n",
      "**The version of the Librispeech dataset used in the paper is available upon request**. In our work, we have used only 12-15 seconds of training material for each speaker and we processed the original librispeech sentences in order to perform amplitude normalization. Moreover, we used a simple energy-based VAD to avoid silences at the beginning and end of each sentence as well as to split in multiple chunks the sentences that contain longer silence\n",
      "\n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "[1] Mirco Ravanelli, Yoshua Bengio, âSpeaker Recognition from raw waveform with SincNetâ [Arxiv](http://arxiv.org/abs/1808.00158)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "example_repo = readme_df.iloc[idx][\"repo\"]\n",
    "print(readme_df.iloc[idx][\"readme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287c718f-3b4c-4b84-af5d-75eb68b1ef78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "python_files_df = pd.read_feather(\"../output/selected_python_code.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "160bd503-5b5b-45b3-86b8-feae33190ac2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 297731 entries, 0 to 297730\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   content        297731 non-null  object\n",
      " 1   path           297731 non-null  object\n",
      " 2   repo_name      297731 non-null  object\n",
      " 3   tasks          297731 non-null  object\n",
      " 4   selected_code  270093 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "python_files_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f3c5f06-a3ac-4069-9038-4b83cd183e33",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'008karan/SincNet_demo'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac7ec5ec-e8e6-4009-ae17-f5af7ad00cf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from github_search.lms.code2documentation import Code2Documentation, run_code2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b690ab0c-c7f2-4b8e-a6ec-307c7e8e9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_base_url=\"http://localhost:11430\"\n",
    "small_lm_base_url=\"http://localhost:11431\"\n",
    "lm_model_name = \"codellama\"\n",
    "ollama_lm = dspy.OllamaLocal(\n",
    "            model=lm_model_name,\n",
    "            base_url=lm_base_url,\n",
    "            num_ctx=4096,\n",
    "            max_tokens=1024,\n",
    "            top_k=100,\n",
    ")\n",
    "small_ollama_lm = dspy.OllamaLocal(\n",
    "    model=lm_model_name,\n",
    "    base_url=small_lm_base_url,\n",
    "    num_ctx=1024,\n",
    "    max_tokens=256,\n",
    "    top_k=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7102199-a0c1-4c3c-8bf2-0b0277cc873b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rationale</th>\n",
       "      <th>answer</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>context_history</th>\n",
       "      <th>filenames</th>\n",
       "      <th>n_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This repository tackles the problem of speaker...</td>\n",
       "      <td>This repository tackles the problem of speaker...</td>\n",
       "      <td>008karan/SincNet_demo</td>\n",
       "      <td>1. `sincnet.py`: This file contains a PyTorch ...</td>\n",
       "      <td>[compute_d_vector.py, data_io.py, speaker_id.p...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           rationale  \\\n",
       "0  This repository tackles the problem of speaker...   \n",
       "\n",
       "                                              answer              repo_name  \\\n",
       "0  This repository tackles the problem of speaker...  008karan/SincNet_demo   \n",
       "\n",
       "                                     context_history  \\\n",
       "0  1. `sincnet.py`: This file contains a PyTorch ...   \n",
       "\n",
       "                                           filenames  n_files  \n",
       "0  [compute_d_vector.py, data_io.py, speaker_id.p...        7  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_code2doc(python_files_df[python_files_df[\"repo_name\"] == example_repo], [small_ollama_lm, ollama_lm], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a6774fb-1846-453d-a7e9-0b3183f7289e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b726120-73e0-4082-b94c-44451d8e60f1",
   "metadata": {},
   "source": [
    "## Multiple OLlama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ae6edb-835f-42ab-bb47-6e7511bcdd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[\"repo_name\"].isin(readme_df[\"repo\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5058663f-3787-4642-afc4-858944b5a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_lms = [\n",
    "    dspy.OllamaLocal(model=\"codellama\", base_url=f\"http://localhost:1143{i}\", num_ctx=2048, max_tokens=128, timeout_s=180)\n",
    "    for i in range(2)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffa50a-e752-40eb-9ef9-9d80ae455342",
   "metadata": {},
   "source": [
    "## Cohere\n",
    "\n",
    "There is a bug in DSPy version from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "380e4491-6e57-4c19-b24a-e3b2d1082bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_dspy.dspy_lm_modules import Claude, Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87683c17-d282-4315-81e2-4ad3b4aa6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_haiku_name = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "with open(\"/home/kuba/.keys/anthropic_key.txt\") as f:\n",
    "    api_key = f.read().strip()\n",
    "    anthropic_lms = [\n",
    "        Claude(model=\"claude-3-haiku-20240307\", api_key=api_key, api_base=\"https://api.anthropic.com\", max_tokens=256)\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    del api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e590d67-54f8-4c8f-be98-1a86edf01603",
   "metadata": {},
   "outputs": [],
   "source": [
    "lms = ollama_lms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aad415ca-ba06-4e62-8cf7-251c5d1d1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lm(lm, prompt=\"print fibonacci function in Python\"):\n",
    "    return lm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd0de71c-9159-4c06-a9eb-d2531f7eccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.0,\n",
       " 'max_tokens': 256,\n",
       " 'top_p': 1.0,\n",
       " 'top_k': 1,\n",
       " 'n': 1,\n",
       " 'model': 'claude-3-haiku-20240307'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anthropic_lms[0].kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "420851fb-b63a-45ec-8b0b-4e41730cf74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here\\'s a Python function that prints the Fibonacci sequence up to a given number of terms:\\n\\n```python\\ndef fibonacci(n):\\n    \"\"\"\\n    Prints the Fibonacci sequence up to the nth term.\\n    \\n    Args:\\n        n (int): The number of terms to print in the Fibonacci sequence.\\n    \"\"\"\\n    a, b = 0, 1\\n    print(a)\\n    print(b)\\n    \\n    for i in range(2, n):\\n        c = a + b\\n        print(c)\\n        a, b = b, c\\n```\\n\\nTo use this function, simply call it with the desired number of terms:\\n\\n```python\\nfibonacci(10)\\n```\\n\\nThis will output the first 10 Fibonacci numbers:\\n\\n```\\n0\\n1\\n1\\n2\\n3\\n5\\n8\\n13\\n21\\n34\\n```\\n\\nHere\\'s how the `fibonacci()` function works:\\n\\n1. The function takes a single argument `n`, which represents the number of terms to print in the Fibonacci sequence.\\n2. The initial values of the Fibonacci sequence are set to `a = 0` and `b']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_lm(anthropic_lms[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68f415-a88d-4fa6-b82f-5ab5c7c973e3",
   "metadata": {},
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Optional\n",
    "\n",
    "import backoff\n",
    "\n",
    "from dsp.modules.lm import LM\n",
    "\n",
    "try:\n",
    "    import anthropic\n",
    "    anthropic_rate_limit = anthropic.RateLimitError\n",
    "except ImportError:\n",
    "    anthropic_rate_limit = Exception\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "BASE_URL = \"https://api.anthropic.com/v1/messages\"\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/.\"\"\"\n",
    "    print(\n",
    "        \"Backing off {wait:0.1f} seconds after {tries} tries \"\n",
    "        \"calling function {target} with kwargs \"\n",
    "        \"{kwargs}\".format(**details),\n",
    "    )\n",
    "\n",
    "\n",
    "def giveup_hdlr(details):\n",
    "    \"\"\"Wrapper function that decides when to give up on retry.\"\"\"\n",
    "    if \"rate limits\" in details.message:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "class Claude(LM):\n",
    "    \"\"\"Wrapper around anthropic's API. Supports both the Anthropic and Azure APIs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"claude-3-opus-20240229\",\n",
    "        api_key: Optional[str] = None,\n",
    "        api_base: Optional[str] = None,\n",
    "        only_completed = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(model)\n",
    "        try:\n",
    "            from anthropic import Anthropic\n",
    "        except ImportError as err:\n",
    "            raise ImportError(\n",
    "                \"Claude requires `pip install anthropic`.\") from err\n",
    "\n",
    "        self.provider = \"anthropic\"\n",
    "        self.api_key = api_key = os.environ.get(\n",
    "            \"ANTHROPIC_API_KEY\") if api_key is None else api_key\n",
    "        self.api_base = BASE_URL if api_base is None else api_base\n",
    "        self.kwargs = {\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0.0),\n",
    "            \"max_tokens\": min(kwargs.get(\"max_tokens\", 4096), 4096),\n",
    "            \"top_p\": kwargs.get(\"top_p\", 1.0),\n",
    "            \"top_k\": kwargs.get(\"top_k\", 1),\n",
    "            \"n\": kwargs.pop(\"n\", kwargs.pop(\"num_generations\", 1)),\n",
    "            **kwargs,\n",
    "        }\n",
    "        self.kwargs[\"model\"] = model\n",
    "        self.history: list[dict[str, Any]] = []\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.only_completed = only_completed\n",
    "\n",
    "    def log_usage(self, response):\n",
    "        \"\"\"Log the total tokens from the Anthropic API response.\"\"\"\n",
    "        usage_data = response.usage\n",
    "        if usage_data:\n",
    "            total_tokens = usage_data.input_tokens + usage_data.output_tokens\n",
    "            logger.info(f'{total_tokens}')\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        # caching mechanism requires hashable kwargs\n",
    "        kwargs[\"messages\"] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        kwargs.pop(\"n\")\n",
    "        response = self.client.messages.create(**kwargs)\n",
    "        history = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"kwargs\": kwargs,\n",
    "            \"raw_kwargs\": raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "        return response\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (anthropic_rate_limit),\n",
    "        max_time=1000,\n",
    "        max_tries=8,\n",
    "        on_backoff=backoff_hdlr,\n",
    "        giveup=giveup_hdlr,\n",
    "    )\n",
    "    def request(self, prompt: str, **kwargs):\n",
    "        \"\"\"Handles retrieval of completions from Anthropic whilst handling API errors.\"\"\"\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "    def __call__(self, prompt, return_sorted=False, **kwargs):\n",
    "        \"\"\"Retrieves completions from Anthropic.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to Anthropic\n",
    "            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.\n",
    "            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: list of completion choices\n",
    "        \"\"\"\n",
    "        only_completed = self.only_completed\n",
    "        #assert only_completed, \"for now\"\n",
    "        #assert return_sorted is False, \"for now\"\n",
    "        # per eg here: https://docs.anthropic.com/claude/reference/messages-examples\n",
    "        # max tokens can be used as a proxy to return smaller responses\n",
    "        # so this cannot be a proper indicator for incomplete response unless it isnt the user-intent.\n",
    "        n = kwargs.pop(\"n\", 1)\n",
    "        completions = []\n",
    "        for _ in range(n):\n",
    "            response = self.request(prompt, **kwargs)\n",
    "            # TODO: Log llm usage instead of hardcoded openai usage\n",
    "            # if dsp.settings.log_openai_usage:\n",
    "            #     self.log_usage(response)\n",
    "            print(response)\n",
    "            if only_completed and response.stop_reason == \"max_tokens\":\n",
    "                continue\n",
    "            completions = [c.text for c in response.content]\n",
    "        return completions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a5ab7a3-d704-4b54-99dc-0d7227404e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#list(ThreadPoolExecutor(max_workers=len(lms)).map(warmup_lm, lms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b0cd2f0-f2b4-4dd9-95e4-481f9939b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=lms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "0cc354eb-a7f8-430b-be21-80af84afd471",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#dspy.configure(lm=codellama)\n",
    "\n",
    "#example_code = example_repo_files_df[\"content\"].iloc[0]\n",
    "\n",
    "repo_summarizer = RepoCodeSummarizer(fetch_code)\n",
    "#code_summarizer(code=[example_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f3c1f54-fd77-4c7a-876f-250277faea06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chenyang-Lu/semantic-foreground-inpainting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import torch\\nimport math\\nimport numbers\\nimp...</td>\n",
       "      <td>ours_model.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>import pandas as pd\\nimport os\\nimport torch\\n...</td>\n",
       "      <td>cs_data_loader.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23591</th>\n",
       "      <td>from collections import OrderedDict\\nimport ma...</td>\n",
       "      <td>ours_extractors.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96348</th>\n",
       "      <td>import torch\\nimport torch.nn as nn\\nimport to...</td>\n",
       "      <td>ours_test.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154317</th>\n",
       "      <td>import torch\\nimport torch.functional as F\\nim...</td>\n",
       "      <td>util.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content                path  \\\n",
       "1       import torch\\nimport math\\nimport numbers\\nimp...       ours_model.py   \n",
       "7721    import pandas as pd\\nimport os\\nimport torch\\n...   cs_data_loader.py   \n",
       "23591   from collections import OrderedDict\\nimport ma...  ours_extractors.py   \n",
       "96348   import torch\\nimport torch.nn as nn\\nimport to...        ours_test.py   \n",
       "154317  import torch\\nimport torch.functional as F\\nim...             util.py   \n",
       "\n",
       "                                         repo_name  \\\n",
       "1       Chenyang-Lu/semantic-foreground-inpainting   \n",
       "7721    Chenyang-Lu/semantic-foreground-inpainting   \n",
       "23591   Chenyang-Lu/semantic-foreground-inpainting   \n",
       "96348   Chenyang-Lu/semantic-foreground-inpainting   \n",
       "154317  Chenyang-Lu/semantic-foreground-inpainting   \n",
       "\n",
       "                                               tasks  \n",
       "1       [scene understanding, semantic segmentation]  \n",
       "7721    [scene understanding, semantic segmentation]  \n",
       "23591   [scene understanding, semantic segmentation]  \n",
       "96348   [scene understanding, semantic segmentation]  \n",
       "154317  [scene understanding, semantic segmentation]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_repo = python_files_df[\"repo_name\"].unique()[1]\n",
    "\n",
    "print(example_repo)\n",
    "fetch_code(example_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13597d4a-ed50-447b-ad21-b56fcc5db4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene understanding', 'semantic segmentation']\n"
     ]
    }
   ],
   "source": [
    "print(readme_df[readme_df[\"repo\"] == example_repo].iloc[0][\"tasks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6787bbe1-294b-4015-836c-37b8101646ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# semantic-foreground-inpainting\n",
      "\n",
      "Codes and data of paper ''Semantic Foreground Inpainting from Weak Supervision'', IEEE Robotics and Automation Letters.\n",
      "\n",
      "Inplementation of the main experiment on the Cityscapes dataset. For KITTI, manual annotations are provided.\n",
      "\n",
      "## Datasets\n",
      "Two manually annotated datasets for testing are released:\n",
      "\n",
      "1. dataset/Cityscapes/gt_manual/gt_manual.zip\n",
      "\n",
      "2. dataset/KITTI/gt_manual/gt_manual.zip\n",
      "\n",
      "\n",
      "## Citation\n",
      "\n",
      "```\n",
      "@article{Lu2020ral,\n",
      "author = {Lu, Chenyang and Dubbelman, Gijs},\n",
      "journal = {IEEE Robotics and Automation Letters},\n",
      "title = {{Semantic Foreground Inpainting from Weak Supervision}},\n",
      "year = {2020}\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(readme_df[readme_df[\"repo\"] == example_repo].iloc[0][\"readme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9ec88fe5-ee83-4966-a3ce-cb88f6a99602",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 1/100 [00:19<31:42, 19.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 2/100 [00:38<31:08, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 3/100 [00:55<29:39, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 4/100 [01:13<29:05, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% 5/100 [01:30<27:47, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6% 6/100 [01:47<27:24, 17.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% 7/100 [02:04<26:56, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% 8/100 [02:23<27:26, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9% 9/100 [02:42<27:50, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 10/100 [03:02<28:13, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11% 11/100 [03:19<26:49, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% 12/100 [03:38<26:53, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% 13/100 [03:57<27:13, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2running lm no. 3\n",
      "\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14% 14/100 [04:15<26:29, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15% 15/100 [04:32<25:35, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16% 16/100 [04:55<27:10, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17% 17/100 [05:13<26:16, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18% 18/100 [05:29<24:59, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19% 19/100 [05:50<25:39, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 20/100 [06:08<24:52, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21% 21/100 [06:29<25:22, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% 22/100 [06:48<25:06, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23% 23/100 [07:06<24:04, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24% 24/100 [07:25<23:56, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% 25/100 [07:44<23:48, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26% 26/100 [08:03<23:33, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% 27/100 [08:24<23:43, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2running lm no. 3\n",
      "\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% 28/100 [08:45<23:51, 19.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29% 29/100 [09:04<23:27, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 30/100 [09:22<22:19, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31% 31/100 [09:42<22:19, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32% 32/100 [10:02<22:11, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 33/100 [10:20<21:28, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34% 34/100 [10:39<20:56, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35% 35/100 [10:59<20:51, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36% 36/100 [11:18<20:37, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37% 37/100 [11:40<20:57, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38% 38/100 [11:56<19:40, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39% 39/100 [12:16<19:26, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% 40/100 [12:35<19:14, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41% 41/100 [12:53<18:29, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42% 42/100 [13:13<18:23, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43% 43/100 [13:28<17:01, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44% 44/100 [13:47<17:09, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2running lm no. 3\n",
      "\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45% 45/100 [14:05<16:36, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46% 46/100 [14:20<15:22, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47% 47/100 [14:36<14:51, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48% 48/100 [14:57<15:40, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49% 49/100 [15:17<15:52, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% 50/100 [15:33<14:55, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51% 51/100 [15:52<14:59, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52% 52/100 [16:11<14:50, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53% 53/100 [16:29<14:11, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54% 54/100 [16:48<14:11, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55% 55/100 [17:07<14:01, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56% 56/100 [17:26<13:44, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57% 57/100 [17:46<13:44, 19.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58% 58/100 [18:07<13:47, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59% 59/100 [18:27<13:33, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% 60/100 [18:46<13:02, 19.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61% 61/100 [19:03<12:09, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2running lm no. 3\n",
      "\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62% 62/100 [19:21<11:41, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63% 63/100 [19:40<11:26, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64% 64/100 [19:58<11:08, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65% 65/100 [20:18<11:07, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66% 66/100 [20:38<10:55, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67% 67/100 [20:58<10:43, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68% 68/100 [21:19<10:36, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69% 69/100 [21:39<10:14, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% 70/100 [21:57<09:45, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71% 71/100 [22:16<09:16, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72% 72/100 [22:35<09:01, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73% 73/100 [22:56<08:55, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74% 74/100 [23:15<08:24, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75% 75/100 [23:36<08:14, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76% 76/100 [23:56<07:56, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77% 77/100 [24:13<07:18, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78% 78/100 [24:30<06:46, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79% 79/100 [24:49<06:31, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% 80/100 [25:08<06:17, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81% 81/100 [25:27<05:59, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82% 82/100 [25:45<05:36, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83% 83/100 [26:05<05:20, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84% 84/100 [26:27<05:17, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85% 85/100 [26:48<05:04, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86% 86/100 [27:07<04:39, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87% 87/100 [27:24<04:06, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88% 88/100 [27:42<03:45, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89% 89/100 [28:01<03:25, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% 90/100 [28:21<03:12, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91% 91/100 [28:40<02:50, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92% 92/100 [28:58<02:29, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93% 93/100 [29:14<02:06, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94% 94/100 [29:34<01:50, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0running lm no. 1\n",
      "\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95% 95/100 [29:51<01:31, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96% 96/100 [30:12<01:15, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% 97/100 [30:32<00:57, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98% 98/100 [30:48<00:36, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% 99/100 [31:08<00:18, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [31:26<00:00, 18.86s/it]\n"
     ]
    }
   ],
   "source": [
    "repo_summarizer_answers = []\n",
    "\n",
    "\n",
    "for repo_name in tqdm.tqdm(python_files_df[\"repo_name\"].unique()[:100]):\n",
    "    repo_summarizer_answers.append(repo_summarizer(repo_name, lms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "46e5d2e1-83c7-43c9-a984-cd3803bfa5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 1273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict(repo_summarizer_answers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c65d71-4a07-488d-b01d-c6f93f813a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff82eaa2-61ac-4511-9907-aba595d075ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/phoenix/datetime_utils.py:26: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(timestamps):\n",
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/phoenix/datetime_utils.py:26: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(timestamps):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Trace dataset saved to under ID: 353a22a7-b529-4d9d-a4ec-75f442aa3eb7\n",
      "📂 Trace dataset path: /home/kuba/Projects/torch_example/phoenix/sample_2k/trace_dataset-353a22a7-b529-4d9d-a4ec-75f442aa3eb7.parquet\n"
     ]
    }
   ],
   "source": [
    "directory = str(Path('~/Projects/torch_example/phoenix/sample_2k').expanduser())\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "my_traces = px.Client().get_trace_dataset().save(directory=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178862f4-60db-46bd-bd5f-3232d1c08fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d46648be-2743-4bf2-b988-41cb772c364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "braincorp/ASC\n",
      "\n",
      "['visual tracking']\n",
      "##################################################\n",
      "\n",
      "The 'braincorp/ASC' repository appears to be a machine learning library for image processing and computer vision tasks. It provides functionalities such as image sparsification, frame reading, and sparse coding, which suggest that it tackles various machine learning problems related to image processing and computer vision, such as image compression, denoising, and feature extraction. The data used by the repository is likely images or videos, as the code deals with frame reading and sparse coding of video frames.\n",
      "\n",
      "##################################################\n",
      "Chenyang-Lu/semantic-foreground-inpainting\n",
      "\n",
      "['scene understanding', 'semantic segmentation']\n",
      "##################################################\n",
      "\n",
      "This repository tackles the problem of semantic foreground inpainting using PyTorch. It uses a pre-trained ResNet18 backbone and adds a PSP module on top of it to extract features at multiple scales. The model also includes a hard thresholding layer to filter out low-confidence predictions.\n",
      "\n",
      "The repository contains several functions and classes related to semantic foreground inpainting, including `count_parameters`, which counts the number of parameters in a PyTorch model, and `random_rectangle_mask_gene`, which generates a random rectangle mask with a given size\n",
      "\n",
      "##################################################\n",
      "passalis/sef\n",
      "\n",
      "['dimensionality reduction', 'supervised dimensionality reduction']\n",
      "##################################################\n",
      "\n",
      "The provided code appears to be part of a machine learning project that tackles various functionalities related to the SEF (Self-Explaining Features) method. The code imports several libraries, including NumPy and Scikit-learn, which are commonly used for data manipulation and statistical analysis. It also imports the SEF library, which is not a standard library in Python but rather a custom library developed by the author of the repository. This suggests that the code is using the SEF method to perform various tasks related to machine learning, such as loading data, learning a new space, and performing domain randomization on a\n",
      "\n",
      "##################################################\n",
      "gvtulder/elasticdeform\n",
      "\n",
      "['medical image segmentation', 'semantic segmentation', 'multi tissue nucleus segmentation', 'skin cancer segmentation', 'retinal vessel segmentation', 'data augmentation', 'video polyp segmentation', '3d instance segmentation', 'cell segmentation', 'pancreas segmentation', 'machine translation', 'thermal image segmentation']\n",
      "##################################################\n",
      "\n",
      "This repository, 'gvtulder/elasticdeform', appears to be a collection of files related to an elastic deformation library for N-D images. The code is written in Python and uses the `torch` library, which suggests that it is designed for PyTorch. Based on the summaries provided, we can infer that this repository tackles the problem of elastic deformations on N-D images. The code provides functions for performing elastic deformations on grids and computing gradients with respect to the deformation parameters. This suggests that the library is designed to perform elastic\n",
      "\n",
      "##################################################\n",
      "dadangewp/SemEval2017-RumourEval\n",
      "\n",
      "['stance classification', 'misinformation', 'classification', 'general classification', 'rumour detection']\n",
      "##################################################\n",
      "\n",
      "The given code defines several classes and their methods in Python, including `LIWCAffect`, `DAL`, `LIWCFuture`, `LIWCInhib`, and `LIWCCogmech`. These classes are used to analyze text data and extract sentiment information based on the LIWC-AFFECT, DAL, LIWC-FUTURE, LIWC-INHIBITORY, and LIWC-COGMECH lexicons. The machine learning problem that this repository tackles is related to natural language processing (NLP) and sentiment analysis.\n",
      "\n",
      "The\n",
      "\n",
      "##################################################\n",
      "EmoryMLIP/DOvsOD_NeuralODEs\n",
      "\n",
      "['time series', 'time series regression', 'image classification']\n",
      "##################################################\n",
      "\n",
      "This repository tackles the problem of predicting the change in a time series from an initial state to a final state using a Neural Ordinary Differential Equations (Neural ODE) model. The code defines a neural ODE model and trains it using the Neural ODEs algorithm, which is a type of neural network that can learn the underlying dynamics of a system by optimizing the parameters of the ODE.\n",
      "\n",
      "The data used in this repository is likely to be a time series of some kind, possibly related to the change in a physical or biological system over time. The code defines several functions\n",
      "\n",
      "##################################################\n",
      "Jorge-Mendes/darknet-google-colab\n",
      "\n",
      "['real time object detection', 'one stage anchor free oriented object detection', 'data augmentation', 'object detection', 'image classification']\n",
      "##################################################\n",
      "\n",
      "This repository tackles the problem of object detection in videos using the Darknet library. The code defines a function called `YOLO()`, which is the main entry point of the program and performs various tasks related to object detection, such as loading the Darknet model, reading input video frames, and visualizing the results.\n",
      "\n",
      "The repository uses data from the Open Images dataset, which contains images with annotated objects for training and testing the object detection model. The code also defines several variables that are used throughout the program, including `configPath`, `weightsPath`, and `dataPath`, which are used to load\n",
      "\n",
      "##################################################\n",
      "CN-TU/reinforcement-learning-for-per-flow-buffer-sizing\n",
      "\n",
      "['reinforcement learning', 'fairness', 'online learning']\n",
      "##################################################\n",
      "\n",
      "The given code is a Python script that uses the `bake` library to automate the build process for a software project. The script defines several functions, each corresponding to a specific task, such as building, cleaning, and deploying the software. The repository 'CN-TU/reinforcement-learning-for-per-flow-buffer-sizing' tackles the problem of reinforcement learning for per-flow buffer sizing. The code uses several external libraries, including `optparse`, `xml.dom`, `shlex`, and `util`.\n",
      "\n",
      "The repository README\n",
      "\n",
      "##################################################\n",
      "kinglintianxia/KittiSeg\n",
      "\n",
      "['general classification', 'semantic segmentation', 'autonomous driving']\n",
      "##################################################\n",
      "\n",
      "This repository tackles the problem of semantic segmentation on the KITTI dataset using a trained neural network model. It uses the images and labels from the KITTI dataset as its training and testing data. The code loads the hyperparameters and modules from the log directory specified in the command line arguments, builds an input placeholder for the image and label data, and saves the weights of the model using a saver object.\n",
      "\n",
      "##################################################\n",
      "samadeusfp/prescriptiveProcessMonitoring\n",
      "\n",
      "['predictive process monitoring']\n",
      "##################################################\n",
      "\n",
      "The given code is a Python script that performs various tasks related to prescriptive process monitoring. The repository tackles the machine learning problem of predicting process performance based on historical data. The code uses a dataset of process data, which includes numerical and categorical variables, to train and evaluate a machine learning model for predicting process performance.\n",
      "\n",
      "The code also defines several functions, such as `calculate_cost` and `evaluate_model_cost`, which are likely to be used in the training and evaluation process. The script appears to be designed for a specific dataset, as it reads data from a CSV file using\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for repo_name, answer in zip(python_files_df[\"repo_name\"].unique()[:10], repo_summarizer_answers):\n",
    "    print(\"#\" * 50)\n",
    "    print(repo_name)\n",
    "    print()\n",
    "    print(readme_df[readme_df[\"repo\"] == repo_name][\"tasks\"].iloc[0])\n",
    "    print(\"#\" * 50)\n",
    "    print()\n",
    "    print(answer[\"answer\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff2c84fb-e6fb-49f6-aade-c8ce2f22213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The given code is a Python script that performs various tasks related to prescriptive process monitoring',\n",
       " 'The repository tackles the machine learning problem of predicting process performance based on historical data',\n",
       " 'The code uses a dataset of process data, which includes numerical and categorical variables, to train and evaluate a machine learning model for predicting process performance.\\n\\nThe code also defines several functions, such as `calculate_cost` and `evaluate_model_cost`, which are likely to be used in the training and evaluation process',\n",
       " 'The script appears to be designed for a specific dataset, as it reads data from a CSV file using']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[\"answer\"].split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "508eea89-c7f8-40e0-beb9-cc201d4f1e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This repository tackles the problem of semantic foreground inpainting on 3D medical images with their corresponding segmentations. The repository provides a PyTorch implementation of a semantic segmentation model that can perform foreground inpainting using a pre-trained ResNet18 backbone with a PSP module on top of it.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_summarizer_answer[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0de042-33a4-40e6-ad83-0690bcbf1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5 * len(python_files_df[\"repo_name\"].unique()) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeaec79-8df8-473d-b920-0970a9b6fd3c",
   "metadata": {},
   "source": [
    "## How many tokens per repo do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f2166585-7341-4512-802f-387c6c5e7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "51adc6ed-634a-4485-b348-8c13bc15574f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4c61ef8-4e68-4287-abd9-8853897411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample = python_files_df[python_files_df[\"repo_name\"].isin(python_files_df[\"repo_name\"].unique())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d095ff96-2a36-4bf8-a68f-36140a280398",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 56s, sys: 12.1 s, total: 5min 8s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_tokens = [\n",
    "    len(toks) for toks in tokenizer.encode_batch(files_sample[\"content\"], allowed_special={'<|endoftext|>'}, num_threads=16)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79ea44ef-0c1a-4e60-b6ef-76bb60e70f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sample[\"n_tokens\"] = file_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f255d63-e821-4ee4-9e2f-b60b6d8ae80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_repo = files_sample.groupby(\"repo_name\")[\"n_tokens\"].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81983f29-05fa-4a08-a6eb-51d702e5741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      38038.000\n",
       "mean        2074.254\n",
       "std        10616.494\n",
       "min            8.000\n",
       "25%          984.400\n",
       "50%         1473.400\n",
       "75%         2238.692\n",
       "max      1599139.100\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_repo.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d2cbd62a-b2a7-467f-bd20-e3deb17e19b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50781731.95793651"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_repo[tokens_per_repo < tokens_per_repo.quantile(0.90)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a061d6e1-b827-4d0e-a2ae-1a1550a68741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.78173195793651"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_repo[tokens_per_repo < tokens_per_repo.quantile(0.90)].sum() / 10 ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b75fd66c-ba84-47c6-93f6-00568fb41abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38038"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(python_files_df[\"repo_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba56493-f055-46ab-b17e-bd1150d09783",
   "metadata": {},
   "source": [
    "## Filtering code files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "77c46ce6-a130-47e6-b192-e62541b1d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from comment_parser import comment_parser\n",
    "import re\n",
    "\n",
    "\n",
    "class PythonCodeSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract(self, code) -> List[dict]:\n",
    "        pass\n",
    "\n",
    "    def extract_str(self, code):\n",
    "        return [selection[\"text\"] for selection in self.extract(code)]\n",
    "\n",
    "    def select_code(self, code):\n",
    "        matches = self.extract(code)\n",
    "        selected_code = \"\\n...\\n\".join([m[\"text\"] for m in merge_matches(matches)])\n",
    "        return selected_code\n",
    "\n",
    "    @classmethod\n",
    "    def _merge_matches(cls, matches):\n",
    "        merged_matches = []\n",
    "        tmp_match = matches[0]\n",
    "        for match in matches[1:]:\n",
    "            if match[\"match_type\"] == tmp_match[\"match_type\"] and match[\"line_start\"] == tmp_match[\"line_end\"] + 1:\n",
    "                tmp_match = {\n",
    "                    \"text\": tmp_match[\"text\"] + \"\\n\" + match[\"text\"],\n",
    "                    \"line_start\": tmp_match[\"line_start\"],\n",
    "                    \"line_end\": match[\"line_end\"],\n",
    "                    \"match_type\": tmp_match[\"match_type\"]\n",
    "                }\n",
    "            else:\n",
    "                merged_matches.append(tmp_match)\n",
    "                tmp_match = match\n",
    "        return merged_matches\n",
    "\n",
    "class CombinedSelector(PythonCodeSelector, BaseModel):\n",
    "    selectors: List[PythonCodeSelector]\n",
    "    \n",
    "    def extract(self, code):\n",
    "        extracted_parts = []\n",
    "        for selector in self.selectors:\n",
    "            extracted_parts += selector.extract(code)\n",
    "        return sorted(extracted_parts, key=lambda r: r[\"line_start\"])\n",
    "\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed=True\n",
    "\n",
    "\n",
    "class CommentSelector(PythonCodeSelector):\n",
    "\n",
    "    def extract(self, code):\n",
    "        comments = comment_parser.python_parser.extract_comments(code)\n",
    "        return [{\"text\": \"#\" + c.text(), \"line_start\": c.line_number(), \"line_end\": c.line_number(), \"match_type\": \"comment\"} for c in comments]\n",
    "\n",
    "    \n",
    "\n",
    "class SignatureSelector(PythonCodeSelector, BaseModel):\n",
    "    \n",
    "    pattern: re.Pattern = re.compile(\"(\\s+ def|class) (.*:$)\", re.MULTILINE)\n",
    "    \n",
    "    def extract(self, code):\n",
    "        re_newline = re.compile(r'\\n')\n",
    "        matches = []\n",
    "        for match in self.pattern.finditer(code):\n",
    "            start = match.start()\n",
    "            line_start = code.count(\"\\n\", 0, match.start())\n",
    "            line_offset = code.count(\"\\n\", start, match.end()) + 1\n",
    "            s = match.group()\n",
    "            matches.append({\"text\": s, \"line_start\": line_start, \"line_end\": line_start + line_offset, \"match_type\": \"signature\"})\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "5a98e26e-85a1-40bd-8a98-4c58af97c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = CombinedSelector(selectors=[CommentSelector(), SignatureSelector()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "047e7967-7516-40cd-a2d6-8a349d54914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 297731/297731 [13:13<00:00, 375.33it/s]  \n"
     ]
    }
   ],
   "source": [
    "selected_python_code_contents = []\n",
    "\n",
    "for code in tqdm.tqdm(python_files_df[\"content\"]):\n",
    "    try:\n",
    "        selected_python_code_contents.append(selector.select_code(code))\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        selected_python_code_contents.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "576d8116-688f-497d-8b88-ea14154093cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df[\"selected_code\"] = selected_python_code_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "id": "02bebef1-d40c-46c5-b465-6f2f85f09aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 297731 entries, 0 to 297730\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   content        297731 non-null  object\n",
      " 1   path           297731 non-null  object\n",
      " 2   repo_name      297731 non-null  object\n",
      " 3   tasks          297731 non-null  object\n",
      " 4   selected_code  270093 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "python_files_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "c16163d4-2260-4fa7-9fa4-6e88bab4a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df.to_json(\"../output/generated_readmes/python_files_with_selected_code.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "id": "8c1e988f-f0f2-42e4-8621-34b9f6bdcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files_df.to_feather(\"../output/generated_readmes/python_files_with_selected_code.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "a611bbf6-da5d-42c9-8cbc-0f194de970d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>tasks</th>\n",
       "      <th>selected_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># ============================================...</td>\n",
       "      <td>ASC/frame_reader.py</td>\n",
       "      <td>braincorp/ASC</td>\n",
       "      <td>[visual tracking]</td>\n",
       "      <td># ============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import torch\\nimport math\\nimport numbers\\nimp...</td>\n",
       "      <td>ours_model.py</td>\n",
       "      <td>Chenyang-Lu/semantic-foreground-inpainting</td>\n",
       "      <td>[scene understanding, semantic segmentation]</td>\n",
       "      <td>class PSPModule(nn.Module):\\n...\\n\\n    def __...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># License: MIT License https://github.com/pass...</td>\n",
       "      <td>examples/linear_outofsample_mutiple.py</td>\n",
       "      <td>passalis/sef</td>\n",
       "      <td>[dimensionality reduction, supervised dimensio...</td>\n",
       "      <td># License: MIT License https://github.com/pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import numpy\\nfrom . import torch\\nimport elas...</td>\n",
       "      <td>elasticdeform/torch.py</td>\n",
       "      <td>gvtulder/elasticdeform</td>\n",
       "      <td>[medical image segmentation, semantic segmenta...</td>\n",
       "      <td>class ElasticDeform(torch.autograd.Function):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Tue A...</td>\n",
       "      <td>affective/linguisticResourceLIWCAffect.py</td>\n",
       "      <td>dadangewp/SemEval2017-RumourEval</td>\n",
       "      <td>[stance classification, misinformation, classi...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n...\\nclass LIWCAffect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297726</th>\n",
       "      <td>\"\"\"Dataset\"\"\"\\n\\n\\n\\nimport codecs\\n\\nimport c...</td>\n",
       "      <td>finetune/dataset.py</td>\n",
       "      <td>mkavim/finetune_bert</td>\n",
       "      <td>[linear probe classification, language modelli...</td>\n",
       "      <td>class Dataset(object):\\n...\\n\\n\\n    def __ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297727</th>\n",
       "      <td>import os\\nimport json\\nimport isodate\\nfrom a...</td>\n",
       "      <td>ClassesAndUtil/Video.py</td>\n",
       "      <td>ucnet01/UCNet_Implementation</td>\n",
       "      <td>[general classification]</td>\n",
       "      <td># base_dir = os.path.join('..','YouTube-Spam-D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297728</th>\n",
       "      <td>import tensorflow as tf\\nimport tensorflow.con...</td>\n",
       "      <td>ops.py</td>\n",
       "      <td>taki0112/SPADE-Tensorflow</td>\n",
       "      <td>[image to image translation, image generation]</td>\n",
       "      <td>##############################################...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297729</th>\n",
       "      <td>import lasagne\\nfrom theano import sparse\\nimp...</td>\n",
       "      <td>layers.py</td>\n",
       "      <td>kimiyoung/planetoid</td>\n",
       "      <td>[entity extraction using gan, node classificat...</td>\n",
       "      <td>class DenseLayer(lasagne.layers.Layer):\\n...\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297730</th>\n",
       "      <td>import numpy as np\\nimport matplotlib.pyplot a...</td>\n",
       "      <td>v0_1/data_loader.py</td>\n",
       "      <td>ourownstory/AR-Net</td>\n",
       "      <td>[time series]</td>\n",
       "      <td># Plot generated process.\\n...\\nclass LocalDat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297731 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  \\\n",
       "0       # ============================================...   \n",
       "1       import torch\\nimport math\\nimport numbers\\nimp...   \n",
       "2       # License: MIT License https://github.com/pass...   \n",
       "3       import numpy\\nfrom . import torch\\nimport elas...   \n",
       "4       # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Tue A...   \n",
       "...                                                   ...   \n",
       "297726  \"\"\"Dataset\"\"\"\\n\\n\\n\\nimport codecs\\n\\nimport c...   \n",
       "297727  import os\\nimport json\\nimport isodate\\nfrom a...   \n",
       "297728  import tensorflow as tf\\nimport tensorflow.con...   \n",
       "297729  import lasagne\\nfrom theano import sparse\\nimp...   \n",
       "297730  import numpy as np\\nimport matplotlib.pyplot a...   \n",
       "\n",
       "                                             path  \\\n",
       "0                             ASC/frame_reader.py   \n",
       "1                                   ours_model.py   \n",
       "2          examples/linear_outofsample_mutiple.py   \n",
       "3                          elasticdeform/torch.py   \n",
       "4       affective/linguisticResourceLIWCAffect.py   \n",
       "...                                           ...   \n",
       "297726                        finetune/dataset.py   \n",
       "297727                    ClassesAndUtil/Video.py   \n",
       "297728                                     ops.py   \n",
       "297729                                  layers.py   \n",
       "297730                        v0_1/data_loader.py   \n",
       "\n",
       "                                         repo_name  \\\n",
       "0                                    braincorp/ASC   \n",
       "1       Chenyang-Lu/semantic-foreground-inpainting   \n",
       "2                                     passalis/sef   \n",
       "3                           gvtulder/elasticdeform   \n",
       "4                 dadangewp/SemEval2017-RumourEval   \n",
       "...                                            ...   \n",
       "297726                        mkavim/finetune_bert   \n",
       "297727                ucnet01/UCNet_Implementation   \n",
       "297728                   taki0112/SPADE-Tensorflow   \n",
       "297729                         kimiyoung/planetoid   \n",
       "297730                          ourownstory/AR-Net   \n",
       "\n",
       "                                                    tasks  \\\n",
       "0                                       [visual tracking]   \n",
       "1            [scene understanding, semantic segmentation]   \n",
       "2       [dimensionality reduction, supervised dimensio...   \n",
       "3       [medical image segmentation, semantic segmenta...   \n",
       "4       [stance classification, misinformation, classi...   \n",
       "...                                                   ...   \n",
       "297726  [linear probe classification, language modelli...   \n",
       "297727                           [general classification]   \n",
       "297728     [image to image translation, image generation]   \n",
       "297729  [entity extraction using gan, node classificat...   \n",
       "297730                                      [time series]   \n",
       "\n",
       "                                            selected_code  \n",
       "0       # ============================================...  \n",
       "1       class PSPModule(nn.Module):\\n...\\n\\n    def __...  \n",
       "2       # License: MIT License https://github.com/pass...  \n",
       "3       class ElasticDeform(torch.autograd.Function):\\...  \n",
       "4       # -*- coding: utf-8 -*-\\n...\\nclass LIWCAffect...  \n",
       "...                                                   ...  \n",
       "297726  class Dataset(object):\\n...\\n\\n\\n    def __ini...  \n",
       "297727  # base_dir = os.path.join('..','YouTube-Spam-D...  \n",
       "297728  ##############################################...  \n",
       "297729  class DenseLayer(lasagne.layers.Layer):\\n...\\n...  \n",
       "297730  # Plot generated process.\\n...\\nclass LocalDat...  \n",
       "\n",
       "[297731 rows x 5 columns]"
      ]
     },
     "execution_count": 1200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_feather(\"../output/generated_readmes/python_files_with_selected_code.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c4783-13f7-423a-8337-d31bb5fa67f7",
   "metadata": {},
   "source": [
    "## Summaries based on selected code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "0f96b334-9d0e-46f6-a023-c293860e9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_selected_code(repo_name, n=5):\n",
    "    selected_python_files =  python_files_df[python_files_df[\"repo_name\"] == repo_name].iloc[:5]\n",
    "    return selected_python_files[\"path\"], selected_python_files[\"selected_code\"]\n",
    "\n",
    "\n",
    "repo_selected_code_summarizer = RepoCodeSummarizer(fetch_selected_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "31d1d782-ee68-4b11-9b9a-f0747257acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n",
      "CPU times: user 88 ms, sys: 0 ns, total: 88 ms\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "example_repo_summary = repo_selected_code_summarizer(example_repo, lms=lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "a2ed96bb-fdd9-4bf2-8848-e1838db6a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 1/5 [00:13<00:52, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% 2/5 [00:28<00:44, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% 3/5 [00:41<00:27, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% 4/5 [00:57<00:14, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5/5 [01:13<00:00, 14.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for repo_name in tqdm.tqdm(python_files_df[\"repo_name\"].unique()[:5]):\n",
    "    example_repo_summary = repo_selected_code_summarizer(repo_name, lms=lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ea4cdfa4-9df7-4ddc-b706-5ec84fff2906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 1/10 [00:19<02:53, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 2/10 [00:38<02:34, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2running lm no. 3\n",
      "\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 3/10 [00:57<02:13, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% 4/10 [01:15<01:52, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% 5/10 [01:33<01:31, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% 6/10 [01:51<01:12, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% 7/10 [02:08<00:53, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% 8/10 [02:27<00:36, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% 9/10 [02:47<00:18, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running lm no. 0\n",
      "running lm no. 1\n",
      "running lm no. 2\n",
      "running lm no. 3\n",
      "running lm no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [03:07<00:00, 18.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for repo_name in tqdm.tqdm(python_files_df[\"repo_name\"].unique()[:10]):\n",
    "    example_repo_summary = repo_summarizer(repo_name, lms=lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "b283fdc2-bd2a-4e13-afc6-a0c47afe51f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['braincorp/ASC', 'Chenyang-Lu/semantic-foreground-inpainting',\n",
       "       'passalis/sef', 'gvtulder/elasticdeform',\n",
       "       'dadangewp/SemEval2017-RumourEval', 'EmoryMLIP/DOvsOD_NeuralODEs',\n",
       "       'Jorge-Mendes/darknet-google-colab',\n",
       "       'CN-TU/reinforcement-learning-for-per-flow-buffer-sizing',\n",
       "       'kinglintianxia/KittiSeg',\n",
       "       'samadeusfp/prescriptiveProcessMonitoring'], dtype=object)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[\"repo_name\"].unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "6da7a288-1da0-4bee-9669-fcdd622fcc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 s, sys: 7.64 ms, total: 27.4 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected_code_n_tokens = python_files_df[\"selected_code\"].dropna().apply(lambda selected_code: len(tokenizer.encode(selected_code, allowed_special={'<|endoftext|>'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "7d6bbc71-b901-4a71-ab68-fedf93d8aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    270093.000000\n",
       "mean        263.895784\n",
       "std         703.452079\n",
       "min           0.000000\n",
       "25%          43.000000\n",
       "50%         126.000000\n",
       "75%         293.000000\n",
       "max      132601.000000\n",
       "Name: selected_code, dtype: float64"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_code_n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "bd05b510-b7c8-4f25-b693-82f6ac86582e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38038.000000\n",
       "mean         0.243348\n",
       "std          0.270368\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.200000\n",
       "75%          0.400000\n",
       "max          1.000000\n",
       "Name: selected_code, dtype: float64"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([python_files_df[\"repo_name\"], python_files_df[\"selected_code\"].isna()],axis=1).groupby(\"repo_name\")[\"selected_code\"].mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "d72cccef-3950-46ba-a7a9-17c359b5f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class InputExample(object):\n",
      "...\n",
      "\n",
      "\n",
      "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
      "...\n",
      "class InputFeatures(object):\n",
      "...\n",
      "\n",
      "\n",
      "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
      "...\n",
      "# This is a simple heuristic which will always truncate the longer sequence\n",
      "# one token at a time. This makes more sense than truncating an equal percent\n",
      "# of tokens from each, since if one sequence is very short then each token\n",
      "# that's truncated likely contains more information than a longer sequence.\n",
      "...\n",
      "# Modifies `tokens_a` and `tokens_b` in place so that the total\n",
      "# length is less than the specified length.\n",
      "# Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
      "...\n",
      "# Account for [CLS] and [SEP] with \"- 2\"\n",
      "...\n",
      "# The convention in BERT is:\n",
      "# (a) For sequence pairs:\n",
      "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
      "#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
      "# (b) For single sequences:\n",
      "#  tokens:   [CLS] the dog is hairy . [SEP]\n",
      "#  type_ids:   0   0   0   0  0     0   0\n",
      "#\n",
      "# Where \"type_ids\" are used to indicate whether this is the first\n",
      "# sequence or the second sequence. The embedding vectors for `type=0` and\n",
      "# `type=1` were learned during pre-training and are added to the wordpiece\n",
      "# embedding vector (and position vector). This is not *strictly* necessary\n",
      "# since the [SEP] token unambiguously separates the sequences, but it makes\n",
      "# it easier for the model to learn the concept of sequences.\n",
      "#\n",
      "# For classification tasks, the first vector (corresponding to [CLS]) is\n",
      "# used as as the \"sentence vector\". Note that this only makes sense because\n",
      "# the entire model is fine-tuned.\n",
      "...\n",
      "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
      "# tokens are attended to.\n",
      "...\n",
      "# Zero-pad up to the sequence length.\n",
      "...\n",
      "class DataProcessor(object):\n",
      "...\n",
      "\n",
      "\n",
      "    def get_train_examples(self, filename, size=-1):\n",
      "\n",
      "\n",
      "    def get_dev_examples(self, filename, size=-1):\n",
      "\n",
      "\n",
      "    def get_test_examples(self, filename, size=-1):\n",
      "\n",
      "\n",
      "    def get_labels(self):\n",
      "...\n",
      "class TextProcessor(DataProcessor):\n",
      "...\n",
      "\n",
      "    def __init__(self, data_dir, label_dir):\n",
      "...\n",
      "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
      "...\n",
      "#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
      "...\n",
      "\n",
      "\n",
      "    def get_labels(self, filename=\"labels.csv\"):\n",
      "...\n",
      "\n",
      "\n",
      "    def _create_examples(self, df, set_type, text_col, label_col):\n",
      "...\n",
      "class MultiLabelTextProcessor(TextProcessor):\n",
      "...\n",
      "\n",
      "    def _create_examples(self, df, set_type, text_col, label_col):\n",
      "...\n",
      "\n",
      "        def _get_labels(row, label_col):\n",
      "...\n",
      "# create one hot vector of labels\n",
      "...\n",
      "# cast with string in case labels are integers\n",
      "...\n",
      "class LRFinderDataset(Dataset):\n",
      "...\n",
      "\n",
      "    def __init__(self, data_dir, filename, text_col, label_col):\n",
      "...\n",
      "\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "...\n",
      "\n",
      "\n",
      "    def __len__(self):\n",
      "...\n",
      "class BertDataBunch(object):\n",
      "...\n",
      "# just in case someone passes string instead of Path\n",
      "...\n",
      "# instantiate the new tokeniser object using the tokeniser name\n",
      "...\n",
      "# Train DataLoader\n",
      "...\n",
      "# Validation DataLoader\n",
      "...\n",
      "# no grads necessary, hence double val batch size\n",
      "...\n",
      "# Test set loader for predictions\n",
      "...\n",
      "\n",
      "\n",
      "    def get_dl_from_texts(self, texts):\n",
      "...\n",
      "\n",
      "\n",
      "    def save(self, filename=\"databunch.pkl\"):\n",
      "...\n",
      "# test is not supposed to be a file - just a list of texts\n",
      "...\n",
      "# Create tokenized and numericalized features\n",
      "...\n",
      "# xlnet has a cls token at the end\n",
      "...\n",
      "# pad on the left for xlnet\n",
      "...\n",
      "# Create folder if it doesn't exist\n",
      "...\n",
      "# Convert to Tensors and build dataset\n"
     ]
    }
   ],
   "source": [
    "example_selected_code = \"\\n...\\n\".join([m[\"text\"] for m in merge_matches(example_matches)])\n",
    "print(example_selected_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "584cfc78-d2bb-48e2-b9d8-c4d0095addbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(example_selected_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "806d00c0-cfef-469b-b6ff-e5033c6b1ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class InputExample(object):\n",
      "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
      "\n",
      "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
      "        \"\"\"Constructs a InputExample.\n",
      "\n",
      "        Args:\n",
      "            guid: Unique id for the example.\n",
      "            text_a: string. The untokenized text of the first sequence. For single\n",
      "            sequence tasks, only this sequence must be specified.\n",
      "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
      "            Only must be specified for sequence pair tasks.\n",
      "            labels: (Optional) [string]. The label of the example. This should be\n",
      "            specified for train and dev examples, but not for test examples.\n",
      "        \"\"\"\n",
      "        self.guid = guid\n",
      "        self.text_a = text_a\n",
      "        self.text_b = text_b\n",
      "        if isinstance(label, list):\n",
      "            self.label = label\n",
      "        elif label:\n",
      "            self.label = str(label)\n",
      "        else:\n",
      "            self.label = None\n"
     ]
    }
   ],
   "source": [
    "for elem in ast.walk(ast.parse(example_python_code)):\n",
    "    if type(elem) == ast.ClassDef:\n",
    "        print(ast.unparse(elem))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "2afd3ccb-9560-4fab-84d3-708dd033063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'A single training/test example for simple sequence classification.'\""
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.unparse(elem.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "5f35d99a-f318-4adb-b8e2-9b280d4225e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_elem = elem.body[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "93d8a473-59c2-4446-a5c1-d4bea3b7c708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Expr at 0x7601c20dc9a0>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_elem.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "cdb559a8-494b-4016-9943-787592cf2946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Constructs a InputExample.\\\\n\\\\n        Args:\\\\n            guid: Unique id for the example.\\\\n            text_a: string. The untokenized text of the first sequence. For single\\\\n            sequence tasks, only this sequence must be specified.\\\\n            text_b: (Optional) string. The untokenized text of the second sequence.\\\\n            Only must be specified for sequence pair tasks.\\\\n            labels: (Optional) [string]. The label of the example. This should be\\\\n            specified for train and dev examples, but not for test examples.\\\\n        '\""
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.unparse(elem.body[1].body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d9db7121-234e-406a-a027-00fc4e76dfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def __init__(self, guid, text_a, text_b=None, label=None):\\n    \"\"\"Constructs a InputExample.\\n\\n        Args:\\n            guid: Unique id for the example.\\n            text_a: string. The untokenized text of the first sequence. For single\\n            sequence tasks, only this sequence must be specified.\\n            text_b: (Optional) string. The untokenized text of the second sequence.\\n            Only must be specified for sequence pair tasks.\\n            labels: (Optional) [string]. The label of the example. This should be\\n            specified for train and dev examples, but not for test examples.\\n        \"\"\"\\n    self.guid = guid\\n    self.text_a = text_a\\n    self.text_b = text_b\\n    if isinstance(label, list):\\n        self.label = label\\n    elif label:\\n        self.label = str(label)\\n    else:\\n        self.label = None'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.unparse(elem.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfafe3-184d-4ea0-af3f-4d98db29eafe",
   "metadata": {},
   "source": [
    "## summarizer with code file concatenation\n",
    "\n",
    "The first version is pretty slow most likely due to the fact of calling the LM multiple times.\n",
    "\n",
    "Now that the code was selected the context should be able to handle several concatenated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "da9e8cac-2b80-4297-ada9-b6851b23a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def override_lm_params(**kwargs):\n",
    "    lm = dspy.settings.lm\n",
    "    old_kwargs = {param_name: lm.kwargs[param_name] for param_name in kwargs.keys()}\n",
    "    try:\n",
    "        for param_name, param_value in kwargs.items():\n",
    "            lm.kwargs[param_name] = param_value\n",
    "        yield\n",
    "    finally:\n",
    "        for param_name, param_value in old_kwargs.items():\n",
    "            lm.kwargs[param_name] = param_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "511f7f9a-0e70-480d-82e2-ad938ea1c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiFileSummary(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"Python code\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Summary of the code given guiding question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "8e7b7664-0734-4d41-bf3c-f814d99603a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    file_summary_question_template = \"\"\"\n",
    "    given the code extracted from Python files of {} repository,\n",
    "    separated with ``` describe what each file implements in 3 sentences.\n",
    "    Focus on machine learning models and data.\"\"\"\n",
    "\n",
    "    repo_summary_question_template = \"\"\"\n",
    "    Using summaries of '{}' files from Context, write repository README.\n",
    "    Focus on the functionalities and features.\n",
    "    There is no need to describe the dependencies and setup.\n",
    "    The README should provide answers to the following questions:\n",
    "    - what machine learning problem does this repository tackle?\n",
    "    - what kind of data does it use?\n",
    "    Base your answer only on the information from context.\n",
    "    \"\"\".strip()\n",
    "\n",
    "\n",
    "class RepoCodeSummarizer(dspy.Module):\n",
    "    \n",
    "    def __init__(self, fetch_code_fn, repo_summary_question_template=Prompts.repo_summary_question_template, file_summary_question_template=Prompts.file_summary_question_template, verbose=True):\n",
    "        super().__init__()\n",
    "        self.fetch_code = fetch_code_fn\n",
    "        self.summarize_files = dspy.Predict(MultiFileSummary)\n",
    "        self.summarize_repo = dspy.ChainOfThought(RepoSummary)\n",
    "        self.file_summary_question_template = file_summary_question_template\n",
    "        self.repo_summary_question_template = repo_summary_question_template\n",
    "\n",
    "    def _create_multi_file_input(self, paths, code_file_contents):\n",
    "        return \"\\n\\n\".join([\n",
    "            self._create_single_file_part(path, code)\n",
    "            for path, code in zip(paths, code_file_contents)\n",
    "        ])\n",
    "\n",
    "    def _create_single_file_part(self, path, code):\n",
    "        return f\"file {path}\\n```\\n{code}\\n```\"\n",
    "    \n",
    "    def _summarize_files(self, repo_name):\n",
    "        paths, code_file_contents = self.fetch_code(repo_name)\n",
    "        file_summarization_input = self._create_multi_file_input(paths, code_file_contents)\n",
    "        return self.summarize_files(question=self.file_summary_question_template.format(repo_name), context=file_summarization_input)\n",
    "    \n",
    "    def forward(self, repo_name, file_summarizer_lm_kwargs={\"num_predict\": 1024}, repo_summarizer_lm_kwargs={\"num_ctx\": 1024, \"num_predict\": 256}):\n",
    "\n",
    "        with override_lm_params(**file_summarizer_lm_kwargs):\n",
    "            summaries = self._summarize_files(repo_name)[\"answer\"]\n",
    "\n",
    "        with override_lm_params(**repo_summarizer_lm_kwargs):\n",
    "            repo_summary = self.summarize_repo(\n",
    "                question=self.repo_summary_question_template.format(repo_name),\n",
    "                context=summaries\n",
    "            )\n",
    "            \n",
    "        return dspy.Prediction(**repo_summary, context_history=summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "7fd64403-0c75-4a35-abe7-30363a3cffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_lm = dspy.OllamaLocal(model=\"codellama\", base_url=f\"http://localhost:11430\", num_ctx=4096, max_tokens=1024, timeout_s=180, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "a9a81e77-8a9d-47c9-abef-e5681d5163c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=ollama_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "a73560ba-37f8-4e5b-9bd7-2177ddd2764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_summarizer = RepoCodeSummarizer(fetch_selected_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "d1473d51-5841-4036-af18-22ceff467989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11034                           JunYeopLee/capsule-networks\n",
       "61606                                JiwonCocoder/matching1\n",
       "52976               DeepPathology/TUPAC16_AlternativeLabels\n",
       "238475                                         kmaninis/COB\n",
       "12237                                      dair-iitd/imojie\n",
       "112884                             koshian2/OctConv-TFKeras\n",
       "14479                                     hellopipu/TC-MGAN\n",
       "32069                                 PhIMaL/DeePyMoD_torch\n",
       "100694                mktoid/made-thousand-facial-landmarks\n",
       "12180                               HarikrishnanNB/ChaosNet\n",
       "111050                                JohanZYe/IWAE-pytorch\n",
       "33050                                   NathanGavenski/IUPE\n",
       "45757                               malllabiisc/NeuralDater\n",
       "38859                                    Cyanogenoid/fspool\n",
       "21033                            tollymune/CycleGAN-PyTorch\n",
       "11331                              Slowpuncher24/mlhiphy_v2\n",
       "80521                                           ysig/GraKeL\n",
       "14949                         marcopodda/fragment-based-dgm\n",
       "16746                                       griegler/octnet\n",
       "46720                                  a-taniguchi/SpCoSLAM\n",
       "24727                                      saviaga/faceswap\n",
       "3911                                   sgadgil6/cnslab_fmri\n",
       "7426                                tsc2017/Inception-Score\n",
       "26037                     FengleiFan/SparseShortcutTopology\n",
       "52445                                           xijiz/cfgen\n",
       "47330                                           hankook/SLA\n",
       "7645                                  TuSimple/TuSimple-DUC\n",
       "17520                  yz93/Learn-to-Interpret-Atari-Agents\n",
       "6643                                          adjidieng/ETM\n",
       "23026     hyunbool/frequent_itemset_mining_using_languag...\n",
       "3730                 gbouritsas/graph-substructure-networks\n",
       "75972                    amaarquadri/PerfectInformationGame\n",
       "1744      thfylsty/ICPR2020_A_Dual_branch_Network_for_In...\n",
       "5598                            ProGamerGov/neural-style-pt\n",
       "41775                                     DeepSceneSeg/SSMA\n",
       "6645                                         juanmanpr/mfas\n",
       "1061                                       xialeiliu/GFR-IL\n",
       "18186                          prstrive/CondConv-tensorflow\n",
       "26470                                   Khurramjaved96/mrcl\n",
       "34544                              ShichaoSun/math_seq2tree\n",
       "31599                            fromme0528/pytorch-WaveGAN\n",
       "26497                 Networks-Learning/strategic-decisions\n",
       "73122                                    aradhanacha/aif360\n",
       "1895                               DeLightCMU/MAL-inference\n",
       "16264      willtop/Spatial_DeepLearning_Wireless_Scheduling\n",
       "22024                                 agermanidis/OpenGPT-2\n",
       "7557                      gomiss/action-balance-exploration\n",
       "28900                                   rhgao/co-separation\n",
       "65113            scut-aitcm/Competitive-Inner-Imaging-SENet\n",
       "123136                      kaiwang960112/Self-Cure-Network\n",
       "Name: repo_name, dtype: object"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_repos = python_files_df[\"repo_name\"].drop_duplicates().sample(1000)\n",
    "sampled_repos.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "31f6a008-8653-4833-88a2-c46ee7440312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tasks\n",
       "general classification                143\n",
       "image classification                   92\n",
       "semantic segmentation                  81\n",
       "translation                            80\n",
       "reinforcement learning                 77\n",
       "                                     ... \n",
       "scene generation                        1\n",
       "data visualization                      1\n",
       "deception detection                     1\n",
       "foveation                               1\n",
       "human object interaction detection      1\n",
       "Name: count, Length: 638, dtype: int64"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_df[readme_df[\"repo\"].isin(sampled_repos)][\"tasks\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce3bec-9eb1-46b5-ac59-2378265d8061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "e9fbf6f0-46a8-4705-8870-a361bfcca057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tasks\n",
       "general classification        7530\n",
       "image classification          6215\n",
       "semantic segmentation         5691\n",
       "object detection              5645\n",
       "translation                   4428\n",
       "reinforcement learning        4265\n",
       "classification                3350\n",
       "language modelling            3299\n",
       "representation learning       3089\n",
       "question answering            2831\n",
       "machine translation           2684\n",
       "transfer learning             2583\n",
       "image generation              2258\n",
       "sentiment analysis            2172\n",
       "frame                         2157\n",
       "data augmentation             2106\n",
       "time series                   1782\n",
       "text classification           1675\n",
       "domain adaptation             1525\n",
       "super resolution              1517\n",
       "pose estimation               1450\n",
       "natural language inference    1449\n",
       "real time object detection    1446\n",
       "instance segmentation         1425\n",
       "decision making               1310\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_df[\"tasks\"].explode().value_counts().iloc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "603e0b32-985f-43f8-a9d1-8a51b418971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_repos_df = pd.read_json(\"../output/sampled_repos.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "2fd9e3d5-00cc-4f80-a9ad-1fb17227e783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935"
      ]
     },
     "execution_count": 1266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sampled_repos_df[\"query_tasks\"].explode().value_counts() > 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "id": "c844cd3c-b4be-4334-9f79-5ab5796b6cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2540.000000\n",
       "mean        2.270866\n",
       "std         1.549961\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max        26.000000\n",
       "Name: query_tasks, dtype: float64"
      ]
     },
     "execution_count": 1267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_repos_df[\"query_tasks\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "f0efaeed-0c82-4663-93db-5c2d19d869c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.000000\n",
       "mean      28.840000\n",
       "std       25.841309\n",
       "min       10.000000\n",
       "25%       16.000000\n",
       "50%       21.000000\n",
       "75%       32.000000\n",
       "max      242.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 1268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_repos_df[\"query_tasks\"].explode().value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "id": "dc8fb698-4829-447e-a00a-bb3b6fd74e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_tasks\n",
       "classification                                 242\n",
       "named entity recognition                       143\n",
       "data augmentation                              131\n",
       "image generation                               114\n",
       "super resolution                                94\n",
       "                                              ... \n",
       "viewpoint estimation                            10\n",
       "traveling salesman problem                      10\n",
       "joint multilingual sentence representations     10\n",
       "language identification                         10\n",
       "point cloud completion                          10\n",
       "Name: count, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 1269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_repos_df[\"query_tasks\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "id": "4518d9a3-ef5a-4767-8e4b-718c26833dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>paper_urls</th>\n",
       "      <th>paper_titles</th>\n",
       "      <th>titles</th>\n",
       "      <th>arxiv_ids</th>\n",
       "      <th>authors</th>\n",
       "      <th>tasks</th>\n",
       "      <th>readme</th>\n",
       "      <th>query_tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0h-n0/thdbonas</td>\n",
       "      <td>['https://paperswithcode.com/paper/deep-bayesi...</td>\n",
       "      <td>['Deep Bayesian Optimization on Attributed Gra...</td>\n",
       "      <td>['Deep Bayesian Optimization on Attributed Gra...</td>\n",
       "      <td>['1905.13403', '1905.06159', '1502.05700']</td>\n",
       "      <td>[\"['Jiaxu Cui', 'Bo Yang', 'Xia Hu']\", \"['Lizh...</td>\n",
       "      <td>[neural architecture search, gaussian processe...</td>\n",
       "      <td>[![Github CI/CD](https://github.com/0h-n0/thdb...</td>\n",
       "      <td>[gaussian processes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xSameer/ast</td>\n",
       "      <td>['https://paperswithcode.com/paper/pre-trainin...</td>\n",
       "      <td>['Pre-training on high-resource speech recogni...</td>\n",
       "      <td>['Pre-training on high-resource speech recogni...</td>\n",
       "      <td>['1809.01431']</td>\n",
       "      <td>[\"['Sameer Bansal', 'Herman Kamper', 'Karen Li...</td>\n",
       "      <td>[speech recognition, translation, automatic sp...</td>\n",
       "      <td># NOTE!\\n\\nThis repository is currently being ...</td>\n",
       "      <td>[speech to text translation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0zgur0/STAR_Network</td>\n",
       "      <td>['https://paperswithcode.com/paper/gating-revi...</td>\n",
       "      <td>['Gating Revisited: Deep Multi-layer RNNs That...</td>\n",
       "      <td>['Gating Revisited: Deep Multi-layer RNNs That...</td>\n",
       "      <td>['1911.11033']</td>\n",
       "      <td>['[\\'Mehmet Ozgur Turkoglu\\', \"Stefano D\\'Aron...</td>\n",
       "      <td>[sequential image classification, action recog...</td>\n",
       "      <td># STAckable Recurrent (STAR) Network\\n[PAMI21]...</td>\n",
       "      <td>[sequential image classification]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1044197988/TF.Keras-Commonly-used-models</td>\n",
       "      <td>['https://paperswithcode.com/paper/u-net-convo...</td>\n",
       "      <td>['U-Net: Convolutional Networks for Biomedical...</td>\n",
       "      <td>['U-Net: Convolutional Networks for Biomedical...</td>\n",
       "      <td>['1505.04597', '1807.10165', '1804.03999', '18...</td>\n",
       "      <td>[\"['Olaf Ronneberger', 'Philipp Fischer', 'Tho...</td>\n",
       "      <td>[medical image segmentation, brain tumor segme...</td>\n",
       "      <td># TF.Keras-å¸¸ç¨åå·\\n\\n__èªå·±æ´ççä¸...</td>\n",
       "      <td>[brain tumor segmentation, lung nodule segment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12wang3/mllp</td>\n",
       "      <td>['https://paperswithcode.com/paper/scalable-ru...</td>\n",
       "      <td>['Scalable Rule-Based Representation Learning ...</td>\n",
       "      <td>['Scalable Rule-Based Representation Learning ...</td>\n",
       "      <td>['2109.15103', '1912.04695']</td>\n",
       "      <td>[\"['Zhuo Wang', 'Wei zhang', 'Ning Liu', 'Jian...</td>\n",
       "      <td>[classification, general classification, repre...</td>\n",
       "      <td># Our new work\\nFor better scalability and cla...</td>\n",
       "      <td>[classification, binarization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>zuzuba/CISR_NeurIPS20</td>\n",
       "      <td>['https://paperswithcode.com/paper/airsim-high...</td>\n",
       "      <td>['AirSim: High-Fidelity Visual and Physical Si...</td>\n",
       "      <td>['AirSim: High-Fidelity Visual and Physical Si...</td>\n",
       "      <td>['1705.05065', '2006.12136']</td>\n",
       "      <td>[\"['Shital Shah', 'Debadeepta Dey', 'Chris Lov...</td>\n",
       "      <td>[autonomous driving, autonomous vehicles, safe...</td>\n",
       "      <td># Curriculum Induction for Safe Reinforcement ...</td>\n",
       "      <td>[autonomous vehicles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>zyang-16/MCNS</td>\n",
       "      <td>['https://paperswithcode.com/paper/understandi...</td>\n",
       "      <td>['Understanding Negative Sampling in Graph Rep...</td>\n",
       "      <td>['Understanding Negative Sampling in Graph Rep...</td>\n",
       "      <td>['2005.09863']</td>\n",
       "      <td>[\"['Zhen Yang', 'Ming Ding', 'Chang Zhou', 'Ho...</td>\n",
       "      <td>[node classification, representation learning,...</td>\n",
       "      <td># MCNS\\n\\n### __[Arxiv](https://arxiv.org/abs/...</td>\n",
       "      <td>[node classification, graph representation lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>zyning/signalSeparation</td>\n",
       "      <td>['https://paperswithcode.com/paper/u-net-convo...</td>\n",
       "      <td>['U-Net: Convolutional Networks for Biomedical...</td>\n",
       "      <td>['U-Net: Convolutional Networks for Biomedical...</td>\n",
       "      <td>['1505.04597']</td>\n",
       "      <td>[\"['Olaf Ronneberger', 'Philipp Fischer', 'Tho...</td>\n",
       "      <td>[medical image segmentation, semantic segmenta...</td>\n",
       "      <td># Spectrum Sensing\\n\\n\\nForked from [U-Net](ht...</td>\n",
       "      <td>[cell segmentation, thermal image segmentation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>zzangjinsun/NLSPN_ECCV20</td>\n",
       "      <td>['https://paperswithcode.com/paper/deformable-...</td>\n",
       "      <td>['Deformable ConvNets v2: More Deformable, Bet...</td>\n",
       "      <td>['Deformable ConvNets v2: More Deformable, Bet...</td>\n",
       "      <td>['1811.11168', '2007.10042']</td>\n",
       "      <td>[\"['Xizhou Zhu', 'Han Hu', 'Stephen Lin', 'Jif...</td>\n",
       "      <td>[semantic segmentation, depth estimation, ster...</td>\n",
       "      <td>Non-Local Spatial Propagation Network for Dept...</td>\n",
       "      <td>[depth completion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>zzl-pointcloud/Data_Augmentation_Zoo_for_Objec...</td>\n",
       "      <td>['https://paperswithcode.com/paper/gridmask-da...</td>\n",
       "      <td>['GridMask Data Augmentation', 'Learning Data ...</td>\n",
       "      <td>['GridMask Data Augmentation', 'Learning Data ...</td>\n",
       "      <td>['2001.04086', '1906.11172', '1902.07296']</td>\n",
       "      <td>[\"['Pengguang Chen', 'Shu Liu', 'Hengshuang Zh...</td>\n",
       "      <td>[small object detection, image augmentation, s...</td>\n",
       "      <td># Data_Augmentation_Zoo_for_Object_Detection\\n...</td>\n",
       "      <td>[image augmentation, data augmentation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2540 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   repo  \\\n",
       "0                                        0h-n0/thdbonas   \n",
       "1                                          0xSameer/ast   \n",
       "2                                   0zgur0/STAR_Network   \n",
       "3              1044197988/TF.Keras-Commonly-used-models   \n",
       "4                                          12wang3/mllp   \n",
       "...                                                 ...   \n",
       "2535                              zuzuba/CISR_NeurIPS20   \n",
       "2536                                      zyang-16/MCNS   \n",
       "2537                            zyning/signalSeparation   \n",
       "2538                           zzangjinsun/NLSPN_ECCV20   \n",
       "2539  zzl-pointcloud/Data_Augmentation_Zoo_for_Objec...   \n",
       "\n",
       "                                             paper_urls  \\\n",
       "0     ['https://paperswithcode.com/paper/deep-bayesi...   \n",
       "1     ['https://paperswithcode.com/paper/pre-trainin...   \n",
       "2     ['https://paperswithcode.com/paper/gating-revi...   \n",
       "3     ['https://paperswithcode.com/paper/u-net-convo...   \n",
       "4     ['https://paperswithcode.com/paper/scalable-ru...   \n",
       "...                                                 ...   \n",
       "2535  ['https://paperswithcode.com/paper/airsim-high...   \n",
       "2536  ['https://paperswithcode.com/paper/understandi...   \n",
       "2537  ['https://paperswithcode.com/paper/u-net-convo...   \n",
       "2538  ['https://paperswithcode.com/paper/deformable-...   \n",
       "2539  ['https://paperswithcode.com/paper/gridmask-da...   \n",
       "\n",
       "                                           paper_titles  \\\n",
       "0     ['Deep Bayesian Optimization on Attributed Gra...   \n",
       "1     ['Pre-training on high-resource speech recogni...   \n",
       "2     ['Gating Revisited: Deep Multi-layer RNNs That...   \n",
       "3     ['U-Net: Convolutional Networks for Biomedical...   \n",
       "4     ['Scalable Rule-Based Representation Learning ...   \n",
       "...                                                 ...   \n",
       "2535  ['AirSim: High-Fidelity Visual and Physical Si...   \n",
       "2536  ['Understanding Negative Sampling in Graph Rep...   \n",
       "2537  ['U-Net: Convolutional Networks for Biomedical...   \n",
       "2538  ['Deformable ConvNets v2: More Deformable, Bet...   \n",
       "2539  ['GridMask Data Augmentation', 'Learning Data ...   \n",
       "\n",
       "                                                 titles  \\\n",
       "0     ['Deep Bayesian Optimization on Attributed Gra...   \n",
       "1     ['Pre-training on high-resource speech recogni...   \n",
       "2     ['Gating Revisited: Deep Multi-layer RNNs That...   \n",
       "3     ['U-Net: Convolutional Networks for Biomedical...   \n",
       "4     ['Scalable Rule-Based Representation Learning ...   \n",
       "...                                                 ...   \n",
       "2535  ['AirSim: High-Fidelity Visual and Physical Si...   \n",
       "2536  ['Understanding Negative Sampling in Graph Rep...   \n",
       "2537  ['U-Net: Convolutional Networks for Biomedical...   \n",
       "2538  ['Deformable ConvNets v2: More Deformable, Bet...   \n",
       "2539  ['GridMask Data Augmentation', 'Learning Data ...   \n",
       "\n",
       "                                              arxiv_ids  \\\n",
       "0            ['1905.13403', '1905.06159', '1502.05700']   \n",
       "1                                        ['1809.01431']   \n",
       "2                                        ['1911.11033']   \n",
       "3     ['1505.04597', '1807.10165', '1804.03999', '18...   \n",
       "4                          ['2109.15103', '1912.04695']   \n",
       "...                                                 ...   \n",
       "2535                       ['1705.05065', '2006.12136']   \n",
       "2536                                     ['2005.09863']   \n",
       "2537                                     ['1505.04597']   \n",
       "2538                       ['1811.11168', '2007.10042']   \n",
       "2539         ['2001.04086', '1906.11172', '1902.07296']   \n",
       "\n",
       "                                                authors  \\\n",
       "0     [\"['Jiaxu Cui', 'Bo Yang', 'Xia Hu']\", \"['Lizh...   \n",
       "1     [\"['Sameer Bansal', 'Herman Kamper', 'Karen Li...   \n",
       "2     ['[\\'Mehmet Ozgur Turkoglu\\', \"Stefano D\\'Aron...   \n",
       "3     [\"['Olaf Ronneberger', 'Philipp Fischer', 'Tho...   \n",
       "4     [\"['Zhuo Wang', 'Wei zhang', 'Ning Liu', 'Jian...   \n",
       "...                                                 ...   \n",
       "2535  [\"['Shital Shah', 'Debadeepta Dey', 'Chris Lov...   \n",
       "2536  [\"['Zhen Yang', 'Ming Ding', 'Chang Zhou', 'Ho...   \n",
       "2537  [\"['Olaf Ronneberger', 'Philipp Fischer', 'Tho...   \n",
       "2538  [\"['Xizhou Zhu', 'Han Hu', 'Stephen Lin', 'Jif...   \n",
       "2539  [\"['Pengguang Chen', 'Shu Liu', 'Hengshuang Zh...   \n",
       "\n",
       "                                                  tasks  \\\n",
       "0     [neural architecture search, gaussian processe...   \n",
       "1     [speech recognition, translation, automatic sp...   \n",
       "2     [sequential image classification, action recog...   \n",
       "3     [medical image segmentation, brain tumor segme...   \n",
       "4     [classification, general classification, repre...   \n",
       "...                                                 ...   \n",
       "2535  [autonomous driving, autonomous vehicles, safe...   \n",
       "2536  [node classification, representation learning,...   \n",
       "2537  [medical image segmentation, semantic segmenta...   \n",
       "2538  [semantic segmentation, depth estimation, ster...   \n",
       "2539  [small object detection, image augmentation, s...   \n",
       "\n",
       "                                                 readme  \\\n",
       "0     [![Github CI/CD](https://github.com/0h-n0/thdb...   \n",
       "1     # NOTE!\\n\\nThis repository is currently being ...   \n",
       "2     # STAckable Recurrent (STAR) Network\\n[PAMI21]...   \n",
       "3     # TF.Keras-å¸¸ç¨åå·\\n\\n__èªå·±æ´ççä¸...   \n",
       "4     # Our new work\\nFor better scalability and cla...   \n",
       "...                                                 ...   \n",
       "2535  # Curriculum Induction for Safe Reinforcement ...   \n",
       "2536  # MCNS\\n\\n### __[Arxiv](https://arxiv.org/abs/...   \n",
       "2537  # Spectrum Sensing\\n\\n\\nForked from [U-Net](ht...   \n",
       "2538  Non-Local Spatial Propagation Network for Dept...   \n",
       "2539  # Data_Augmentation_Zoo_for_Object_Detection\\n...   \n",
       "\n",
       "                                            query_tasks  \n",
       "0                                  [gaussian processes]  \n",
       "1                          [speech to text translation]  \n",
       "2                     [sequential image classification]  \n",
       "3     [brain tumor segmentation, lung nodule segment...  \n",
       "4                        [classification, binarization]  \n",
       "...                                                 ...  \n",
       "2535                              [autonomous vehicles]  \n",
       "2536  [node classification, graph representation lea...  \n",
       "2537    [cell segmentation, thermal image segmentation]  \n",
       "2538                                 [depth completion]  \n",
       "2539            [image augmentation, data augmentation]  \n",
       "\n",
       "[2540 rows x 9 columns]"
      ]
     },
     "execution_count": 1270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_repos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "686145c1-6433-40ca-970b-57012a196cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    903.000000\n",
       "mean       7.974529\n",
       "std        2.958024\n",
       "min        1.000000\n",
       "25%        6.000000\n",
       "50%       10.000000\n",
       "75%       10.000000\n",
       "max       10.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 1174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files_df[python_files_df[\"repo_name\"].isin(sampled_repos_df[\"repo\"])][\"repo_name\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "d55b8347-3a18-49ca-9bc5-d0accea9de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 903/903 [2:27:34<00:00,  9.81s/it]  \n"
     ]
    }
   ],
   "source": [
    "repo_summary_answers = []\n",
    "\n",
    "for repo_name in tqdm.tqdm(sampled_repos_df[\"repo\"]):\n",
    "    repo_summary_answers.append(repo_summarizer(repo_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "cce19e01-66c0-4eda-9bf6-2891a991af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/phoenix/datetime_utils.py:26: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(timestamps):\n",
      "/home/kuba/.cache/pypoetry/virtualenvs/llms-dspy-cWHDaHg3-py3.10/lib/python3.10/site-packages/phoenix/datetime_utils.py:26: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(timestamps):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Trace dataset saved to under ID: 875fe8b2-2350-46a6-a147-0ebefabecbc4\n",
      "📂 Trace dataset path: /home/kuba/Projects/torch_example/phoenix/trace_dataset-875fe8b2-2350-46a6-a147-0ebefabecbc4.parquet\n"
     ]
    }
   ],
   "source": [
    "directory = str(Path('~/Projects/torch_example/phoenix').expanduser())\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "my_traces = px.Client().get_trace_dataset().save(directory=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "7ab349a4-0262-4c5a-b35e-eb42de95a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo_name, answer in zip(sampled_repos_df[\"repo\"], repo_summary_answers):\n",
    "    answer[\"repo_name\"] = repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "fddd2f1d-01cb-460d-8914-808110780597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale='This repository tackles the task of image captioning, which is a machine learning problem that involves generating natural language descriptions for images. The data used in this repository is the MSCOCO dataset, which contains over 300,000 images with corresponding captions.\\n\\nThe repository provides code for building and training an image captioning model using TensorFlow and Keras. It includes a configuration file that specifies the hyperparameters for training, such as the number of input shards, the image format, and the vocabulary size. The repository also includes code for building the inference graph, creating the vocabulary, loading the model from checkpoint, and preparing the caption generator.\\n\\nThe repository also includes unit tests for the ShowAndTellModel class, which checks the number of parameters in the model, the output shapes, and the accuracy of the model on a test set. Additionally, it defines the Vocabulary class that creates the vocabulary dictionary and the ImageDecoder class that decodes JPEG images to run sanity checks.\\n\\nOverall, this repository provides a comprehensive solution for building and training an image captioning model using Tensor',\n",
       "    answer='This repository tackles the task of image captioning, which is a machine learning problem that involves generating natural language descriptions for images. The data used in this repository is the MSCOCO dataset, which contains over 300,000 images with corresponding captions. The repository provides code for building and training an image captioning model using TensorFlow and Keras. It includes a configuration file that specifies the hyperparameters for training, such as the number of input shards, the image format, and the vocabulary size. The repository also includes code for building the inference graph, creating the vocabulary, loading the model from checkpoint, and preparing the caption generator. The repository also includes unit tests for the ShowAndTellModel class, which checks the number of parameters in the model, the output shapes, and the accuracy of the model on a test set. Additionally, it defines the Vocabulary class that creates the vocabulary dictionary and the ImageDecoder class that decodes JPEG images to run sanity checks. Overall, this repository provides a comprehensive solution for building and training an image captioning model using TensorFlow and Keras.',\n",
       "    context_history=\"* `im2txt/configuration.py`: This file contains the configuration for the Show and Tell model, including the number of input shards, the image format, and the vocabulary size. It also defines the TrainingConfig class that specifies the number of examples per epoch, the optimizer, learning rate, and other hyperparameters for training.\\n* `im2txt/run_inference.py`: This file contains code for building the inference graph, creating the vocabulary, loading the model from checkpoint, and preparing the caption generator. It also defines the ShowAndTellModel class that builds the input pipeline, creates the vocabulary, and loads the model from checkpoint.\\n* `im2txt/show_and_tell_model_test.py`: This file contains unit tests for the ShowAndTellModel class, including tests for the number of parameters in the model, the output shapes, and the accuracy of the model on a test set. It also defines the ShowAndTellModelTest class that inherits from tf.test.TestCase and provides methods to count the number of parameters in the model, check the output shapes, and evaluate the model's accuracy on a test set.\\n* `im2txt/data/build_mscoco_data.py`: This file contains code for building the MSCOCO data, including generating a sharded version of the file name, breaking up each image into multiple entities for each caption, and filtering uncommon words and sorting by descending count. It also defines the Vocabulary class that creates the vocabulary dictionary and the ImageDecoder class that decodes JPEG images to run sanity checks.\\n* `im2txt/show_and_tell_model.py`: This file contains code for building the Show and Tell model, including building the input pipeline, creating the vocabulary, loading the model from checkpoint, and running the batch of sequence embeddings through the LSTM. It also defines the ShowAndTellModel class that inherits from tf.keras.layers.Layer and provides methods to build the input pipeline, create the vocabulary, load the model from checkpoint, and run the batch of sequence embeddings through the LSTM.\",\n",
       "    repo_name='21-projects-for-deep-learning/image2text'\n",
       ")"
      ]
     },
     "execution_count": 1184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_summary_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "bde5e27a-953c-445e-9dff-fad8c6e50a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records([dict(r) for r in repo_summary_answers]).to_json(\"../output/generated_readmes/dspy_generated_readme_samples.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "id": "685bf2a7-09b2-4ef4-a62a-d9afc6e27fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_summary_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2502e34-10e2-42bb-b71f-0fd141fd0759",
   "metadata": {},
   "source": [
    "## Viewing generation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "e7b3c172-d19a-49da-9478-532039d9e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "def highlight_substring_matches(text, matched_substrings, highlight_color=Fore.RED):\n",
    "    callback = lambda pat: highlight_color + pat.group() + Fore.RESET\n",
    "    pattern = re.compile(\"|\".join([f\"({t})\" for t in matched_substrings]))\n",
    "    return re.sub(pattern, callback, text)\n",
    "\n",
    "\n",
    "class RepoTextTaskHighlighter(BaseModel):\n",
    "    repo_metadata_df: pd.DataFrame\n",
    "    repo_name_col: str = \"repo\"\n",
    "\n",
    "    def highlight_repo_tasks(self, repo_name, text, highlight_color=Fore.RED):\n",
    "        repo_tasks = self.get_repo_tasks(repo_name)\n",
    "        return highlight_substring_matches(text, repo_tasks, highlight_color)\n",
    "        \n",
    "    def get_repo_tasks(self, repo_name):\n",
    "        return self.repo_metadata_df[self.repo_metadata_df[self.repo_name_col] == repo_name].iloc[0][\"tasks\"]\n",
    "\n",
    "    def get_repo_text_from_metadata(self, repo_name, field_name=\"readme\"):\n",
    "        return self.repo_metadata_df[self.repo_metadata_df[self.repo_name_col] == repo_name].iloc[0][field_name]\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "24847ece-3c86-4455-8ca1-184a503bcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_task_highlighter = RepoTextTaskHighlighter(repo_metadata_df=readme_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "09a39178-750d-4225-8ab9-552f60ed7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This repository, 'jacobgil/pytorch-grad-cam', appears to tackle the problem of visualizing the attention of a PyTorch model on an image. The repository contains several files that implement different methods for computing and visualizing the attention of a model, including Activation Maximization (AM), Guided Backpropagation (GBP), EigenCAM, and Grad-CAM. The data used in this repository appears to be images, as the files contain code related to image processing and manipulation. The repository also contains code for training and evaluating PyTorch models, which suggests that the data is likely to be used for machine learning tasks such as image classification or object detection. Overall, this repository seems to provide a collection of tools and techniques for visualizing the attention of PyTorch models on images, which could be useful for understanding how these models are making predictions and identifying areas of interest in the input data.\n"
     ]
    }
   ],
   "source": [
    "print(repo_task_highlighter.highlight_repo_tasks(selected_repo_name, answer_multifile[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "122a5868-8ba2-4737-b6aa-8af47f82cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This repository, 'jacobgil/pytorch-grad-cam', appears to tackle the problem of visualizing the attention of a PyTorch model on an image. The repository contains several files that implement different methods for computing and visualizing the attention of a model, including Activation Maximization (AM), Guided Backpropagation (GBP), EigenCAM, and Grad-CAM.\n",
      "\n",
      "The data used in this repository appears to be images, as the files contain code related to image processing and manipulation. The repository also contains code for training and evaluating PyTorch models, which suggests that the data is likely to be used for machine learning tasks such as image classification or object detection.\n",
      "\n",
      "Overall, this repository seems to provide a collection of tools and techniques for visualizing the attention of PyTorch models on images, which could be useful for understanding how these models are making predictions and identifying areas of interest in the input data.\n"
     ]
    }
   ],
   "source": [
    "print(repo_task_highlighter.highlight_repo_tasks(selected_repo_name, answer_multifile[\"rationale\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "457b46a4-7b2d-493c-bbd3-f2a49dfa4adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This repository, 'jacobgil/pytorch-grad-cam', appears to tackle the problem of visualizing the attention of a PyTorch model on an image. The repository contains several files that implement different methods for computing and visualizing the attention of a model, including Activation Maximization (AM), Guided Backpropagation (GBP), EigenCAM, and Grad-CAM. The data used in this repository appears to be images, as the files contain code related to image processing and manipulation. The repository also contains code for training and evaluating PyTorch models, which suggests that the data is likely to be used for machine learning tasks such as image classification or object detection. Overall, this repository seems to provide a collection of tools and techniques for visualizing the attention of PyTorch models on images, which could be useful for understanding how these models are making predictions and identifying areas of interest in the input data.\""
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_multifile[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "3974ff65-92cf-4d68-a40b-aa3697b7997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image generation',\n",
       " 'decision making',\n",
       " 'knowledge distillation',\n",
       " 'interpretable machine learning',\n",
       " 'fairness',\n",
       " 'weakly supervised object localization',\n",
       " '3d action recognition',\n",
       " 'action recognition',\n",
       " 'adversarial attack',\n",
       " 'temporal action localization',\n",
       " 'object localization']"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_task_highlighter.get_repo_tasks(selected_repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "01ed4e58-d964-42ea-8524-c9a3347ee0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
      "![Build Status](https://github.com/jacobgil/pytorch-grad-cam/workflows/Tests/badge.svg)\n",
      "[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=Monthly%20Downloads)](https://pepy.tech/project/grad-cam)\n",
      "[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=total&units=international_system&left_color=black&right_color=blue&left_text=Total%20Downloads)](https://pepy.tech/project/grad-cam)\n",
      "\n",
      "# Advanced AI explainability for PyTorch\n",
      "\n",
      "`pip install grad-cam`\n",
      "\n",
      "Documentation with advanced tutorials: [https://jacobgil.github.io/pytorch-gradcam-book](https://jacobgil.github.io/pytorch-gradcam-book)\n",
      "\n",
      "\n",
      "This is a package with state of the art methods for Explainable AI for computer vision.\n",
      "This can be used for diagnosing model predictions, either in production or while\n",
      "developing models.\n",
      "The aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.\n",
      "\n",
      "â­ Comprehensive collection of Pixel Attribution methods for Computer Vision.\n",
      "\n",
      "â­ Tested on many Common CNN Networks and Vision Transformers.\n",
      "\n",
      "â­ Advanced use cases: Works with Classification, Object Detection, Semantic Segmentation, Embedding-similarity and more.\n",
      "\n",
      "â­ Includes smoothing methods to make the CAMs look nice.\n",
      "\n",
      "â­ High performance: full support for batches of images in all methods.\n",
      "\n",
      "â­ Includes metrics for checking if you can trust the explanations, and tuning them for best performance.\n",
      "\n",
      "\n",
      "![visualization](https://github.com/jacobgil/jacobgil.github.io/blob/master/assets/cam_dog.gif?raw=true\n",
      ")\n",
      "\n",
      "| Method              | What it does                                                                                                                |\n",
      "|---------------------|-----------------------------------------------------------------------------------------------------------------------------|\n",
      "| GradCAM             | Weight the 2D activations by the average gradient                                                                           |\n",
      "| HiResCAM            | Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models |\n",
      "| GradCAMElementWise  | Like GradCAM but element-wise multiply the activations with the gradients then apply a ReLU operation before summing        |\n",
      "| GradCAM++           | Like GradCAM but uses second order gradients                                                                                |\n",
      "| XGradCAM            | Like GradCAM but scale the gradients by the normalized activations                                                          |\n",
      "| AblationCAM         | Zero out activations and measure how the output drops (this repository includes a fast batched implementation)              |\n",
      "| ScoreCAM            | Perbutate the image by the scaled activations and measure how the output drops                                              |\n",
      "| EigenCAM            | Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)        |\n",
      "| EigenGradCAM        | Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner |\n",
      "| LayerCAM            | Spatially weight the activations by positive gradients. Works better especially in lower layers                             |\n",
      "| FullGrad            | Computes the gradients of the biases from all over the network, and then sums them                                          |\n",
      "| Deep Feature Factorizations           | Non Negative Matrix Factorization on the 2D activations                                                 |\n",
      "\n",
      "## Visual Examples\n",
      "\n",
      "| What makes the network think the image label is 'pug, pug-dog' | What makes the network think the image label is 'tabby, tabby cat' | Combining Grad-CAM with Guided Backpropagation for the 'pug, pug-dog' class |\n",
      "| ---------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------|\n",
      " <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog.jpg?raw=true\" width=\"256\" height=\"256\"> | <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cat.jpg?raw=true\" width=\"256\" height=\"256\"> | <img src=\"https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true\" width=\"256\" height=\"256\"> |\n",
      "\n",
      "## Object Detection and Semantic Segmentation\n",
      "| Object Detection | Semantic Segmentation |\n",
      "| -----------------|-----------------------|\n",
      "| <img src=\"./examples/both_detection.png\" width=\"256\" height=\"256\"> | <img src=\"./examples/cars_segmentation.png\" width=\"256\" height=\"200\"> |\n",
      "\n",
      "## Explaining similarity to other images / embeddings\n",
      "<img src=\"./examples/embeddings.png\">\n",
      "\n",
      "## Deep Feature Factorization\n",
      "<img src=\"./examples/dff1.png\">\n",
      "<img src=\"./examples/dff2.png\">\n",
      "\n",
      "## Classification\n",
      "\n",
      "#### Resnet50:\n",
      "| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n",
      "| ---------|-------|----------|------------|------------|\n",
      "| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/resnet50_dog_gradcam_cam.jpg)     |  ![](./examples/resnet50_dog_ablationcam_cam.jpg)   |![](./examples/resnet50_dog_scorecam_cam.jpg)   |\n",
      "| Cat    | ![](./examples/dog_cat.jfif?raw=true) | ![](./examples/resnet50_cat_gradcam_cam.jpg?raw=true)     |  ![](./examples/resnet50_cat_ablationcam_cam.jpg?raw=true)   |![](./examples/resnet50_cat_scorecam_cam.jpg)   |\n",
      "\n",
      "#### Vision Transfomer (Deit Tiny):\n",
      "| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n",
      "| ---------|-------|----------|------------|------------|\n",
      "| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/vit_dog_gradcam_cam.jpg)     |  ![](./examples/vit_dog_ablationcam_cam.jpg)   |![](./examples/vit_dog_scorecam_cam.jpg)   |\n",
      "| Cat    | ![](./examples/dog_cat.jfif) | ![](./examples/vit_cat_gradcam_cam.jpg)     |  ![](./examples/vit_cat_ablationcam_cam.jpg)   |![](./examples/vit_cat_scorecam_cam.jpg)   |\n",
      "\n",
      "#### Swin Transfomer (Tiny window:7 patch:4 input-size:224):\n",
      "| Category  | Image | GradCAM  |  AblationCAM |  ScoreCAM |\n",
      "| ---------|-------|----------|------------|------------|\n",
      "| Dog    | ![](./examples/dog_cat.jfif) | ![](./examples/swinT_dog_gradcam_cam.jpg)     |  ![](./examples/swinT_dog_ablationcam_cam.jpg)   |![](./examples/swinT_dog_scorecam_cam.jpg)   |\n",
      "| Cat    | ![](./examples/dog_cat.jfif) | ![](./examples/swinT_cat_gradcam_cam.jpg)     |  ![](./examples/swinT_cat_ablationcam_cam.jpg)   |![](./examples/swinT_cat_scorecam_cam.jpg)   |\n",
      "\n",
      "\n",
      "# Metrics and Evaluation for XAI\n",
      "\n",
      "<img src=\"./examples/metrics.png\">\n",
      "<img src=\"./examples/road.png\">\n",
      "\n",
      "\n",
      "----------\n",
      "# Choosing the Target Layer\n",
      "You need to choose the target layer to compute CAM for.\n",
      "Some common choices are:\n",
      "- FasterRCNN: model.backbone\n",
      "- Resnet18 and 50: model.layer4[-1]\n",
      "- VGG and densenet161: model.features[-1]\n",
      "- mnasnet1_0: model.layers[-1]\n",
      "- ViT: model.blocks[-1].norm1\n",
      "- SwinT: model.layers[-1].blocks[-1].norm1\n",
      "\n",
      "If you pass a list with several layers, the CAM will be averaged accross them.\n",
      "This can be useful if you're not sure what layer will perform best.\n",
      "\n",
      "----------\n",
      "\n",
      "# Using from code as a library\n",
      "\n",
      "```python\n",
      "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
      "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
      "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
      "from torchvision.models import resnet50\n",
      "\n",
      "model = resnet50(pretrained=True)\n",
      "target_layers = [model.layer4[-1]]\n",
      "input_tensor = # Create an input tensor image for your model..\n",
      "# Note: input_tensor can be a batch tensor with several images!\n",
      "\n",
      "# Construct the CAM object once, and then re-use it on many images:\n",
      "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda)\n",
      "\n",
      "# You can also use it within a with statement, to make sure it is freed,\n",
      "# In case you need to re-create it inside an outer loop:\n",
      "# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n",
      "#   ...\n",
      "\n",
      "# We have to specify the target we want to generate\n",
      "# the Class Activation Maps for.\n",
      "# If targets is None, the highest scoring category\n",
      "# will be used for every image in the batch.\n",
      "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
      "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
      "\n",
      "targets = [ClassifierOutputTarget(281)]\n",
      "\n",
      "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
      "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
      "\n",
      "# In this example grayscale_cam has only one image in the batch:\n",
      "grayscale_cam = grayscale_cam[0, :]\n",
      "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
      "\n",
      "# You can also get the model outputs without having to re-inference\n",
      "model_outputs = cam.outputs\n",
      "```\n",
      "\n",
      "----------\n",
      "\n",
      "# Metrics and evaluating the explanations\n",
      "\n",
      "```python\n",
      "from pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget\n",
      "from pytorch_grad_cam.metrics.cam_mult_image import CamMultImageConfidenceChange\n",
      "# Create the metric target, often the confidence drop in a score of some category\n",
      "metric_target = ClassifierOutputSoftmaxTarget(281)\n",
      "scores, batch_visualizations = CamMultImageConfidenceChange()(input_tensor, \n",
      "  inverse_cams, targets, model, return_visualization=True)\n",
      "visualization = deprocess_image(batch_visualizations[0, :])\n",
      "\n",
      "# State of the art metric: Remove and Debias\n",
      "from pytorch_grad_cam.metrics.road import ROADMostRelevantFirst, ROADLeastRelevantFirst\n",
      "cam_metric = ROADMostRelevantFirst(percentile=75)\n",
      "scores, perturbation_visualizations = cam_metric(input_tensor, \n",
      "  grayscale_cams, targets, model, return_visualization=True)\n",
      "\n",
      "# You can also average accross different percentiles, and combine\n",
      "# (LeastRelevantFirst - MostRelevantFirst) / 2\n",
      "from pytorch_grad_cam.metrics.road import ROADMostRelevantFirstAverage,\n",
      "                                          ROADLeastRelevantFirstAverage,\n",
      "                                          ROADCombined\n",
      "cam_metric = ROADCombined(percentiles=[20, 40, 60, 80])\n",
      "scores = cam_metric(input_tensor, grayscale_cams, targets, model)\n",
      "```\n",
      "----------\n",
      "\n",
      "\n",
      "# Advanced use cases and tutorials:\n",
      "\n",
      "You can use this package for \"custom\" deep learning models, for example Object Detection or Semantic Segmentation.\n",
      "\n",
      "\n",
      "You will have to define objects that you can then pass to the CAM algorithms:\n",
      "1. A reshape_transform, that aggregates the layer outputs into 2D tensors that will be displayed.\n",
      "2. Model Targets, that define what target do you want to compute the visualizations for, for example a specific category, or a list of bounding boxes.\n",
      "\n",
      "Here you can find detailed examples of how to use this for various custom use cases like object detection:\n",
      "\n",
      "These point to the new documentation jupter-book for fast rendering.\n",
      "The jupyter notebooks themselves can be found under the tutorials folder in the git repository.\n",
      "\n",
      "- [Notebook tutorial: XAI Recipes for the HuggingFace ð¤ Image Classification Models](<https://jacobgil.github.io/pytorch-gradcam-book/HuggingFace.html>)\n",
      "\n",
      "- [Notebook tutorial: Deep Feature Factorizations for better model explainability](<https://jacobgil.github.io/pytorch-gradcam-book/Deep%20Feature%20Factorizations.html>)\n",
      "\n",
      "- [Notebook tutorial: Class Activation Maps for Object Detection with Faster-RCNN](<https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html>)\n",
      "\n",
      "- [Notebook tutorial: Class Activation Maps for YOLO5](<https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html>)\n",
      "\n",
      "- [Notebook tutorial: Class Activation Maps for Semantic Segmentation](<https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html>)\n",
      "\n",
      "- [Notebook tutorial: Adapting pixel attribution methods for embedding outputs from models](<https://jacobgil.github.io/pytorch-gradcam-book/Pixel%20Attribution%20for%20embeddings.html>)\n",
      "\n",
      "- [Notebook tutorial: May the best explanation win. CAM Metrics and Tuning](<https://jacobgil.github.io/pytorch-gradcam-book/CAM%20Metrics%20And%20Tuning%20Tutorial.html>)\n",
      "\n",
      "- [How it works with Vision/SwinT transformers](tutorials/vision_transformers.md)\n",
      "\n",
      "\n",
      "----------\n",
      "\n",
      "# Smoothing to get nice looking CAMs\n",
      "\n",
      "To reduce noise in the CAMs, and make it fit better on the objects,\n",
      "two smoothing methods are supported:\n",
      "\n",
      "- `aug_smooth=True`\n",
      "\n",
      "  Test time augmentation: increases the run time by x6.\n",
      "\n",
      "  Applies a combination of horizontal flips, and mutiplying the image\n",
      "  by [1.0, 1.1, 0.9].\n",
      "\n",
      "  This has the effect of better centering the CAM around the objects.\n",
      "\n",
      "\n",
      "- `eigen_smooth=True`\n",
      "\n",
      "  First principle component of `activations*weights`\n",
      "\n",
      "  This has the effect of removing a lot of noise.\n",
      "\n",
      "\n",
      "|AblationCAM | aug smooth | eigen smooth | aug+eigen smooth|\n",
      "|------------|------------|--------------|--------------------|\n",
      "![](./examples/nosmooth.jpg) | ![](./examples/augsmooth.jpg) | ![](./examples/eigensmooth.jpg) | ![](./examples/eigenaug.jpg) | \n",
      "\n",
      "----------\n",
      "\n",
      "# Running the example script:\n",
      "\n",
      "Usage: `python cam.py --image-path <path_to_image> --method <method> --output-dir <output_dir_path> `\n",
      "\n",
      "\n",
      "To use with CUDA:\n",
      "`python cam.py --image-path <path_to_image> --use-cuda  --output-dir <output_dir_path> `\n",
      "\n",
      "----------\n",
      "\n",
      "You can choose between:\n",
      "\n",
      "`GradCAM` , `HiResCAM`, `ScoreCAM`, `GradCAMPlusPlus`, `AblationCAM`, `XGradCAM` , `LayerCAM`, `FullGrad` and `EigenCAM`.\n",
      "\n",
      "Some methods like ScoreCAM and AblationCAM require a large number of forward passes,\n",
      "and have a batched implementation.\n",
      "\n",
      "You can control the batch size with\n",
      "`cam.batch_size = `\n",
      "\n",
      "----------\n",
      "\n",
      "## Citation\n",
      "If you use this for research, please cite. Here is an example BibTeX entry:\n",
      "\n",
      "```\n",
      "@misc{jacobgilpytorchcam,\n",
      "  title={PyTorch library for CAM methods},\n",
      "  author={Jacob Gildenblat and contributors},\n",
      "  year={2021},\n",
      "  publisher={GitHub},\n",
      "  howpublished={\\url{https://github.com/jacobgil/pytorch-grad-cam}},\n",
      "}\n",
      "```\n",
      "\n",
      "----------\n",
      "\n",
      "# References\n",
      "https://arxiv.org/abs/1610.02391 <br>\n",
      "`Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n",
      "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra`\n",
      "\n",
      "https://arxiv.org/abs/2011.08891 <br>\n",
      "`Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks\n",
      "Rachel L. Draelos, Lawrence Carin`\n",
      "\n",
      "https://arxiv.org/abs/1710.11063 <br>\n",
      "`Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\n",
      "Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n",
      "\n",
      "https://arxiv.org/abs/1910.01279 <br>\n",
      "`Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\n",
      "Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n",
      "\n",
      "https://ieeexplore.ieee.org/abstract/document/9093360/ <br>\n",
      "`Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization.\n",
      "Saurabh Desai and Harish G Ramaswamy. In WACV, pages 972â980, 2020`\n",
      "\n",
      "https://arxiv.org/abs/2008.02312 <br>\n",
      "`Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\n",
      "Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n",
      "\n",
      "https://arxiv.org/abs/2008.00299 <br>\n",
      "`Eigen-CAM: Class Activation Map using Principal Components\n",
      "Mohammed Bany Muhammad, Mohammed Yeasin`\n",
      "\n",
      "http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf <br>\n",
      "`LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\n",
      "Peng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n",
      "\n",
      "https://arxiv.org/abs/1905.00780 <br>\n",
      "`Full-Gradient Representation for Neural Network Visualization\n",
      "Suraj Srinivas, Francois Fleuret`\n",
      "\n",
      "https://arxiv.org/abs/1806.10206 <br>\n",
      "`Deep Feature Factorization For Concept Discovery\n",
      "Edo Collins, Radhakrishna Achanta, Sabine SÃ¼sstrunk`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(repo_task_highlighter.highlight_repo_tasks(selected_repo_name, repo_task_highlighter.get_repo_text_from_metadata(selected_repo_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387e8bf-2b6d-4aac-9407-f3e56dd096d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github_search",
   "language": "python",
   "name": "github_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "name": "Code2Documentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

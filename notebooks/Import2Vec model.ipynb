{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa790d11d70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_import_corpus = pd.read_csv('data/module_import_corpus.csv').iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_import_corpus = module_import_corpus.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "ImportDataset allows for sampling `context`, `target` pairs and negative examples as described in Import2Vec paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportDataset:\n",
    "    \n",
    "    def __init__(self, import_corpus, min_import_frequency=10):\n",
    "        self._vectorizer = feature_extraction.text.CountVectorizer(min_df=min_import_frequency, binary=True)\n",
    "        occurrence_matrix = self._vectorizer.fit_transform(import_corpus)\n",
    "        n_imports = np.array((occurrence_matrix.sum(axis=1) > 1)).reshape(-1)\n",
    "        valid_indices = np.where(n_imports)[0]\n",
    "        occurrence_matrix = occurrence_matrix[valid_indices,:]\n",
    "        self._occurrence_matrix = occurrence_matrix\n",
    "        self.corpus_size = occurrence_matrix.shape[0] \n",
    "        self.vocabulary_size = occurrence_matrix.shape[1] \n",
    "    \n",
    "    def sample_imports(self, n_positive_imports, n_negative_imports=None):\n",
    "        if n_negative_imports is None:\n",
    "            n_negative_imports = n_positive_imports \n",
    "            \n",
    "        positive_import_contexts, positive_import_targets = self._sample_positive_or_negative_imports(n_positive_imports, positive=True)\n",
    "        negative_import_contexts, negative_import_targets = self._sample_positive_or_negative_imports(n_negative_imports, positive=False)\n",
    "        positive_import_contexts, positive_import_targets = torch.tensor(positive_import_contexts), torch.tensor(positive_import_targets) \n",
    "        negative_import_contexts, negative_import_targets = torch.tensor(negative_import_contexts), torch.tensor(negative_import_targets)\n",
    "        predictions = torch.cat(\n",
    "            (\n",
    "                torch.ones(n_positive_imports),\n",
    "                torch.zeros(n_positive_imports)\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "        contexts = torch.cat((positive_import_contexts, negative_import_contexts), axis=0)\n",
    "        targets = torch.cat((positive_import_targets, negative_import_targets), axis=0)\n",
    "        return contexts, targets, predictions\n",
    "        \n",
    "    def _sample_positive_or_negative_imports(self, n_imports, positive):\n",
    "        file_indices_sample = np.random.choice(range(self.corpus_size), size=n_imports)\n",
    "        context_indices = []\n",
    "        target_indices = []\n",
    "        for idx in file_indices_sample:\n",
    "            sample_row = np.array(self._occurrence_matrix[idx].todense())[0]\n",
    "            import_indices = np.where(sample_row)[0]\n",
    "            context_index = np.random.choice(import_indices, size=1)[0]\n",
    "            if positive:\n",
    "                sample_row[context_index] = 0\n",
    "                possible_target_indices = np.where(sample_row)[0]\n",
    "            else:\n",
    "                possible_target_indices = np.where(sample_row == 0)[0] \n",
    "            target_index = np.random.choice(possible_target_indices)\n",
    "            context_indices.append(context_index)\n",
    "            target_indices.append(target_index)\n",
    "        return context_indices, target_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_import_corpus = [\n",
    "    'pandas numpy',\n",
    "    'numpy seaborn',\n",
    "    'pandas tensorflow',\n",
    "    'tensorflow seaborn',\n",
    "    'pandas seaborn'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_import_dataset = ImportDataset(small_import_corpus, min_import_frequency=1)\n",
    "assert small_import_dataset.corpus_size == 5\n",
    "assert small_import_dataset.vocabulary_size == 4\n",
    "assert small_import_dataset._occurrence_matrix.sum() == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Import2VecModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Import2VecModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, context, target):\n",
    "        context_embeddings = self.embeddings(context)\n",
    "        target_embeddings = self.embeddings(target)\n",
    "        similarities = (context_embeddings * target_embeddings).sum(axis=1) \n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_import_corpus = module_import_corpus[module_import_corpus.str.split().apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_module_import_corpus = module_import_corpus[module_import_corpus.str.split().apply(set).apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "import_dataset = ImportDataset(list(valid_module_import_corpus))\n",
    "import2vec = Import2VecModeler(import_dataset.vocabulary_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 5000 \n",
    "n_positive_imports = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23238283910828894"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_positive_imports * n_iterations / import_dataset.corpus_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive_imports * n_iterations / import_dataset.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3204/5000 [08:09<04:36,  6.49it/s]"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(import2vec.parameters(), lr=0.001)\n",
    "\n",
    "for iteration in tqdm.tqdm(range(n_iterations)):\n",
    "    total_loss = 0\n",
    "    (context, target, pred) = import_dataset.sample_imports(n_positive_imports=n_positive_imports)\n",
    "    import2vec.zero_grad()\n",
    "    log_probs = import2vec(context, target)\n",
    "\n",
    "    loss = loss_function(log_probs, pred)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.replace_with_rules(s, rules)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_with_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file.csv  nlp_github_repos.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mortardata/mortar-etl-redshift</td>\n",
       "      <td>README.md</td>\n",
       "      <td># Mortar ETL Pipeline for Redshift\\n\\nA custom...</td>\n",
       "      <td>[{'name': 'PigLatin', 'bytes': '4890'}, {'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsubery/sidekiq-unique-jobs</td>\n",
       "      <td>README.md</td>\n",
       "      <td># SidekiqUniqueJobs [![Build Status](https://t...</td>\n",
       "      <td>[{'name': 'Ruby', 'bytes': '41751'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ovaskevich/svids</td>\n",
       "      <td>README.md</td>\n",
       "      <td># svids\\nIntrusion Detection System - An appli...</td>\n",
       "      <td>[{'name': 'C', 'bytes': '3755'}, {'name': 'Mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kfirg77/kDebuger</td>\n",
       "      <td>README.md.orig</td>\n",
       "      <td>kDebuger\\n========\\n\\nShow information about t...</td>\n",
       "      <td>[{'name': 'CSS', 'bytes': '27336'}, {'name': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wdi-hk-sep-2014/PepperLunchClone</td>\n",
       "      <td>README.rdoc</td>\n",
       "      <td>== README\\n\\nThis README would normally docume...</td>\n",
       "      <td>[{'name': 'CSS', 'bytes': '2444'}, {'name': 'C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          repo_name            path  \\\n",
       "0    mortardata/mortar-etl-redshift       README.md   \n",
       "1       tsubery/sidekiq-unique-jobs       README.md   \n",
       "2                  ovaskevich/svids       README.md   \n",
       "3                  kfirg77/kDebuger  README.md.orig   \n",
       "4  wdi-hk-sep-2014/PepperLunchClone     README.rdoc   \n",
       "\n",
       "                                             content  \\\n",
       "0  # Mortar ETL Pipeline for Redshift\\n\\nA custom...   \n",
       "1  # SidekiqUniqueJobs [![Build Status](https://t...   \n",
       "2  # svids\\nIntrusion Detection System - An appli...   \n",
       "3  kDebuger\\n========\\n\\nShow information about t...   \n",
       "4  == README\\n\\nThis README would normally docume...   \n",
       "\n",
       "                                            language  \n",
       "0  [{'name': 'PigLatin', 'bytes': '4890'}, {'name...  \n",
       "1               [{'name': 'Ruby', 'bytes': '41751'}]  \n",
       "2  [{'name': 'C', 'bytes': '3755'}, {'name': 'Mak...  \n",
       "3  [{'name': 'CSS', 'bytes': '27336'}, {'name': '...  \n",
       "4  [{'name': 'CSS', 'bytes': '2444'}, {'name': 'C...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_json('../data/nlp_github_repos.json', lines=True)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = raw_df.copy()\n",
    "test_df = test_df[~test_df['content'].isna()]\n",
    "test_df = test_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search/preprocessing.py:54: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 54 of the file /home/kuba/Projects/github_search/preprocessing.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  return BeautifulSoup(html).get_text()\n"
     ]
    }
   ],
   "source": [
    "test_df['cleaned_content']  = test_df['content'].apply(markdown_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mortar ETL Pipeline for Redshift\n",
      "\n",
      "A customizable ETL pipeline for building an Amazon Redshift data warehouse by [Mortar Data](http://www.mortardata.com).\n",
      "\n",
      "## Getting Started / Tutorials\n",
      "\n",
      "This project contains a complete, runnable example of the Mortar ETL pipeline on example data, as well as a template project for easily getting started with your own data.\n",
      "\n",
      "For a complete tutorial and explanation of how the ETL pipeline works, see the [Build an Amazon Redshift Data Warehouse tutorial](http://help.mortardata.com/data_apps/redshift_data_warehouse).\n",
      "\n",
      "## Asking Questions\n",
      "\n",
      "If you have any questions about this project, please post to the [Mortar Q&A Forum](https://answers.mortardata.com/) to ask Mortar's engineers and data scientists.\n",
      "\n",
      "## Common Problems\n",
      "\n",
      "The mortar-etl-redshift project has a dependency on the PostgresSQL Python library [Psycopg2](http://initd.org/psycopg/).  This library requires your system to be able to compile C Python extensions against the libpq library.  If your system is not set up for that you will see an error like:\n",
      "\n",
      "    ...\n",
      "    Installing user defined python dependencies... failed\n",
      "     !\n",
      "     !    Unable to setup a python environment with your dependencies, see dependency_install.log for more details\n",
      "\n",
      "To fix this error see, follow the steps at [Psyopg Library](http://help.mortardata.com/data_apps/redshift_data_warehouse/setup_your_project#toc_2PsycopgLibrary).\n",
      "\n",
      "\n",
      "**************************************************\n",
      "# SidekiqUniqueJobs [![Build Status](https://travis-ci.org/mhenrixon/sidekiq-unique-jobs.png?branch=master)](https://travis-ci.org/mhenrixon/sidekiq-unique-jobs) [![Code Climate](https://codeclimate.com/github/mhenrixon/sidekiq-unique-jobs.png)](https://codeclimate.com/github/mhenrixon/sidekiq-unique-jobs)\n",
      "\n",
      "The missing unique jobs for sidekiq\n",
      "\n",
      "## Requirements\n",
      "\n",
      "See https://github.com/mperham/sidekiq#requirements for what is required. Starting from 3.0.13 only sidekiq 3 is supported and support for MRI 1.9 is dropped (it might work but won't be worked on)\n",
      "\n",
      "## Installation\n",
      "\n",
      "Add this line to your application's Gemfile:\n",
      "\n",
      "    gem 'sidekiq-unique-jobs'\n",
      "\n",
      "And then execute:\n",
      "\n",
      "    $ bundle\n",
      "\n",
      "Or install it yourself as:\n",
      "\n",
      "    $ gem install sidekiq-unique-jobs\n",
      "\n",
      "## Usage\n",
      "\n",
      "All that is required is that you specifically set the sidekiq option for *unique* to true like below:\n",
      "\n",
      "```ruby\n",
      "sidekiq_options unique: true\n",
      "```\n",
      "\n",
      "For jobs scheduled in the future it is possible to set for how long the job\n",
      "should be unique. The job will be unique for the number of seconds configured (default 30 minutes)\n",
      "or until the job has been completed. Thus, the job will be unique for the shorter of the two.  Note that Sidekiq versions before 3.0 will remove job keys after an hour, which means jobs can remain unique for at most an hour.\n",
      "\n",
      "*If you want the unique job to stick around even after it has been successfully\n",
      "processed then just set the unique_unlock_order to anything except `:before_yield` or `:after_yield` (`unique_unlock_order = :never`)\n",
      "\n",
      "You can also control the expiration length of the uniqueness check. If you want to enforce uniqueness over a longer period than the default of 30 minutes then you can pass the number of seconds you want to use to the sidekiq options:\n",
      "\n",
      "```ruby\n",
      "sidekiq_options unique: true, unique_job_expiration: 120 * 60 # 2 hours\n",
      "```\n",
      "\n",
      "Requiring the gem in your gemfile should be sufficient to enable unique jobs.\n",
      "\n",
      "### Usage with ActiveJob\n",
      "\n",
      "```ruby\n",
      "Sidekiq.default_worker_options = {\n",
      "   'unique' => true,\n",
      "   'unique_args' => proc do |args|\n",
      "     [args.first.except('job_id')]\n",
      "   end\n",
      "}\n",
      "SidekiqUniqueJobs.config.unique_args_enabled = true\n",
      "```\n",
      "\n",
      "\n",
      "### Finer Control over Uniqueness\n",
      "\n",
      "Sometimes it is desired to have a finer control over which arguments are used in determining uniqueness of the job, and others may be _transient_. For this use-case, you need to set `SidekiqUniqueJobs.config.unique_args_enabled` to true in an initializer, and then defined either `unique_args` method, or a ruby proc.\n",
      "\n",
      "The unique_args method need to return an array of values to use for uniqueness check.\n",
      "\n",
      "```ruby\n",
      "SidekiqUniqueJobs.config.unique_args_enabled = true\n",
      "```\n",
      "\n",
      "The method or the proc can return a modified version of args without the transient arguments included, as shown below:\n",
      "\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_args: :unique_args\n",
      "\n",
      "  def self.unique_args(name, id, options)\n",
      "    [ name, options[:type] ]\n",
      "  end\n",
      "\n",
      "  ...\n",
      "\n",
      "end\n",
      "\n",
      "class UniqueJobWithFilterProc\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_args: ->(args) { [ args.first ] }\n",
      "\n",
      "  ...\n",
      "\n",
      "end\n",
      "```\n",
      "\n",
      "Note that objects passed into workers are converted to JSON *after* running through client middleware. In server middleware, the JSON is passed directly to the worker `#perform` method. So, you may run into issues where the arguments are different when enqueuing than they are when performing. Your `unique_args` method may need to account for this.\n",
      "\n",
      "### Unlock Ordering\n",
      "\n",
      "By default the server middleware will release the worker lock after yielding to the next middleware or worker. Alternatively, this can be changed by passing the `unique_unlock_order` option:\n",
      "\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_unlock_order: :before_yield\n",
      "\n",
      "  ...\n",
      "\n",
      "end\n",
      "```\n",
      "\n",
      "### Logging\n",
      "\n",
      "To see logging in sidekiq when duplicate payload has been filtered out you can enable on a per worker basis using the sidekiq options.  The default value is false\n",
      "\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  log_duplicate_payload: true\n",
      "\n",
      "  ...\n",
      "\n",
      "end\n",
      "```\n",
      "\n",
      "### Testing\n",
      "\n",
      "SidekiqUniqueJobs uses mock_redis for inline testing. Due to complaints about having that as a runtime dependency it was made a development dependency so if you are relying on inline testing you will have to add `gem 'mock_redis'` to your Gemfile.\n",
      "\n",
      "## Contributing\n",
      "\n",
      "1. Fork it\n",
      "2. Create your feature branch (`git checkout -b my-new-feature`)\n",
      "3. Commit your changes (`git commit -am 'Add some feature'`)\n",
      "4. Push to the branch (`git push origin my-new-feature`)\n",
      "5. Create new Pull Request\n",
      "\n",
      "## Contributors\n",
      "\n",
      "- https://github.com/salrepe\n",
      "- https://github.com/rickenharp\n",
      "- https://github.com/sax\n",
      "- https://github.com/eduardosasso\n",
      "- https://github.com/KensoDev\n",
      "- https://github.com/adstage-david\n",
      "- https://github.com/jprincipe\n",
      "- https://github.com/crberube\n",
      "- https://github.com/simonoff\n",
      "\n",
      "**************************************************\n",
      "# svids\n",
      "Intrusion Detection System - An application of HMMs for intrusion detection, created for a purpose.\n",
      "\n",
      "**************************************************\n",
      "kDebuger\n",
      "========\n",
      "\n",
      "Show information about the current page\n",
      "\n",
      "Usage: \n",
      "========\n",
      "\n",
      "You can use this plugin to manually log data. \n",
      "\n",
      "$tmp=array('Arr A'=>1);\n",
      "<br>\n",
      "kDebuger::log($tmp);\n",
      "\n",
      "<br>\n",
      "<br>\n",
      "\n",
      "<<<<<<< HEAD\n",
      "$tmp=array('Arr B'=>2);\n",
      "<br>\n",
      "kDebuger::log($tmp);    \n",
      "\n",
      "<br>\n",
      "<br>\n",
      "=======\n",
      "  You can use this plugin to manually log data. \n",
      "  \n",
      "  $tmp=array('Arr A'=>1);\n",
      "  kDebuger::log($tmp);\n",
      ">>>>>>> 728ecc978f524db01cc37727ab351fe6ddc24c70\n",
      "\n",
      "kDebuger::log('A');\n",
      "<br>\n",
      "kDebuger::log('B');\n",
      "<br>\n",
      "kDebuger::log('C');\n",
      "\n",
      "<br>\n",
      "<br>\n",
      "\n",
      "<<<<<<< HEAD\n",
      "kDebuger plugin print array in slider ander title call Filter\n",
      "=======\n",
      "  kDebuger plugin print array in slider ander\n",
      "  title call Filter\n",
      ">>>>>>> 728ecc978f524db01cc37727ab351fe6ddc24c70\n",
      "\n",
      "**************************************************\n",
      "== README\n",
      "\n",
      "This README would normally document whatever steps are necessary to get the\n",
      "application up and running.\n",
      "\n",
      "Things you may want to cover:\n",
      "\n",
      "* Ruby version\n",
      "\n",
      "* System dependencies\n",
      "\n",
      "* Configuration\n",
      "\n",
      "* Database creation\n",
      "\n",
      "* Database initialization\n",
      "\n",
      "* How to run the test suite\n",
      "\n",
      "* Services (job queues, cache servers, search engines, etc.)\n",
      "\n",
      "* Deployment instructions\n",
      "\n",
      "* ...\n",
      "\n",
      "\n",
      "Please feel free to use a different markup language if you do not plan to run\n",
      "<tt>rake doc:app</tt>.\n",
      "\n",
      "**************************************************\n",
      "# prueba-para-mostrarle-a-popi\n",
      "Esto es una prueba para mostrarle github a popi\n",
      "Mas texto!!\n",
      "\n",
      "Otro mas  \n",
      "\n",
      "**************************************************\n",
      "dairugger\n",
      "=========\n",
      "\n",
      "A golang interface to [voltron][0]. It's called dairugger because golang is\n",
      "stupid and makes you name projects before you can work on them, and the voltron\n",
      "wikipedia page wasn't very forthcoming with good names.\n",
      "\n",
      "Plus it kinda sounds like debugger.\n",
      "\n",
      "[0]: https://github.com/snare/voltron\n",
      "\n",
      "**************************************************\n",
      "# lua-cutil\n",
      "Lua CUtil is a util library for Lua\n",
      "\n",
      "## Build\n",
      "make sure lua was installed in your linux, and the following command 'make install' is just match lua 5.3, so lua 5.3 is suggested.\n",
      "```plain\n",
      "$ make all\n",
      "$ make install\n",
      "```\n",
      "\n",
      "the command 'make install' is short for the following commands: \n",
      "```plain\n",
      "$ cp cutil.so /usr/local/lib/lua/5.3\n",
      "$ chmod 755 /usr/local/lib/lua/5.3/cutil.so\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## Test\n",
      "there is an example in the 'tests' dir.\n",
      "```plain\n",
      "$ /usr/local/bin/lua\n",
      "Lua 5.3.2  Copyright (C) 1994-2015 Lua.org, PUC-Rio\n",
      "> dofile(\"tests/sample.lua\")\n",
      "lua version:    ssfoo明天a你好foo23333\n",
      "cutil version:  ssfoo明天a你好foo23333\n",
      "efficiency comparison: \n",
      "test lua version:       5.53\n",
      "test cutil version:     0.06\n",
      "```\n",
      "\n",
      "**************************************************\n",
      "# dbmapper\n",
      "\n",
      "A library for Dart developers. It is awesome.\n",
      "\n",
      "## Usage\n",
      "\n",
      "A simple usage example:\n",
      "\n",
      "    import 'package:dbmapper/dbmapper.dart';\n",
      "\n",
      "    main() {\n",
      "      var awesome = new Awesome();\n",
      "    }\n",
      "\n",
      "## Features and bugs\n",
      "\n",
      "Please file feature requests and bugs at the [issue tracker][tracker].\n",
      "\n",
      "[tracker]: http://example.com/issues/replaceme\n",
      "\n",
      "**************************************************\n",
      "check_tl_health Nagios Plugin README\n",
      "---------------------\n",
      "\n",
      "This plugin checks the hardware health and various interface metrics\n",
      "of tape libraries\n",
      "\n",
      "Copyright (C) Gerhard Lausser, gerhard.lausser@consol.de\n",
      "\n",
      "This program is free software; you can redistribute it and/or\n",
      "modify it under the terms of the GNU General Public License\n",
      "as published by the Free Software Foundation; either version 2\n",
      "of the License, or (at your option) any later version.\n",
      "\n",
      "This program is distributed in the hope that it will be useful,\n",
      "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      "GNU General Public License for more details.\n",
      "\n",
      "You should have received a copy of the GNU General Public License\n",
      "along with this program; if not, write to the Free Software\n",
      "Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
      "\n",
      "\n",
      "* For instructions on installing this plugin for use with Nagios,\n",
      "  see below. In addition, generic instructions for the GNU toolchain\n",
      "  can be found in the INSTALL file.\n",
      "\n",
      "* For major changes between releases, read the CHANGES file.\n",
      "\n",
      "* For information on detailed changes that have been made,\n",
      "  read the Changelog file.\n",
      "\n",
      "* This plugin is self-documenting.  All plugins that comply with\n",
      "  the basic guidelines for development will provide detailed help when\n",
      "  invoked with the '-h' or '--help' options.\n",
      "\n",
      "You can check for the latest plugin at:\n",
      "  http://labs.consol.de/nagios/check_tl_health\n",
      "\n",
      "Send mail to gerhard.lausser@consol.de for assistance.  \n",
      "Please include the OS type and version that you are using.\n",
      "Also, run the plugin with the '-vvv' option and provide the resulting \n",
      "version information.  Of course, there may be additional diagnostic information\n",
      "required as well. Use good judgment.\n",
      "\n",
      "\n",
      "How to \"compile\" the check_tl_health script.\n",
      "--------------------------------------------------------\n",
      "\n",
      "1) Run the configure script to initialize variables and create a Makefile, etc.\n",
      "\n",
      "\t./configure --prefix=BASEDIRECTORY --with-nagios-user=SOMEUSER --with-nagios-group=SOMEGROUP --with-perl=PATH_TO_PERL\n",
      "\n",
      "   a) Replace BASEDIRECTORY with the path of the directory under which Nagios\n",
      "      is installed (default is '/usr/local/nagios')\n",
      "   b) Replace SOMEUSER with the name of a user on your system that will be\n",
      "      assigned permissions to the installed plugins (default is 'nagios')\n",
      "   c) Replace SOMEGRP with the name of a group on your system that will be\n",
      "      assigned permissions to the installed plugins (default is 'nagios')\n",
      "   d) Replace PATH_TO_PERL with the path where a perl binary can be found.\n",
      "      Besides the system wide perl you might have installed a private perl\n",
      "      just for the nagios plugins (default is the perl in your path).\n",
      "\n",
      "\n",
      "2) \"Compile\" the plugin with the following command:\n",
      "\n",
      "\tmake\n",
      "\n",
      "    This will produce a \"check_tl_health\" script. You will also find\n",
      "    a \"check_tl_health.pl\" which you better ignore. It is the base for\n",
      "    the compilation filled with placeholders. These will be replaced during\n",
      "    the make process.\n",
      "\n",
      "\n",
      "3) Install the compiled plugin script with the following command:\n",
      "\n",
      "\tmake install\n",
      "\n",
      "   The installation procedure will attempt to place the plugin in a \n",
      "   'libexec/' subdirectory in the base directory you specified with\n",
      "   the --prefix argument to the configure script.\n",
      "\n",
      "\n",
      "4) Verify that your configuration files for Nagios contains\n",
      "   the correct paths to the new plugin.\n",
      "\n",
      "\n",
      "\n",
      "Command line parameters\n",
      "-----------------------\n",
      "\n",
      "You'll find the command line parameters on the website mentioned above.\n",
      "If it does not exist or is not up to date, this means that i hadn't the\n",
      "time to write/update it yet. I am aware of it, so please don't send me\n",
      "mails. Just accept it.\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "Gerhard Lausser <gerhard.lausser@consol.de>\n",
      "\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for r in test_df['content']:\n",
    "    print(r)\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mortar ETL Pipeline for Redshift\n",
      "A customizable ETL pipeline for building an Amazon Redshift data warehouse by Mortar Data.\n",
      "Getting Started / Tutorials\n",
      "This project contains a complete, runnable example of the Mortar ETL pipeline on example data, as well as a template project for easily getting started with your own data.\n",
      "For a complete tutorial and explanation of how the ETL pipeline works, see the Build an Amazon Redshift Data Warehouse tutorial.\n",
      "Asking Questions\n",
      "If you have any questions about this project, please post to the Mortar Q&A Forum to ask Mortar's engineers and data scientists.\n",
      "Common Problems\n",
      "The mortar-etl-redshift project has a dependency on the PostgresSQL Python library Psycopgxxnumber.  This library requires your system to be able to compile C Python extensions against the libpq library.  If your system is not set up for that you will see an error like:\n",
      "...\n",
      "Installing user defined python dependencies... failed\n",
      " !\n",
      " !    Unable to setup a python environment with your dependencies, see dependency_install.log for more details\n",
      "\n",
      "To fix this error see, follow the steps at Psyopg Library.\n",
      "**************************************************\n",
      "SidekiqUniqueJobs  \n",
      "The missing unique jobs for sidekiq\n",
      "Requirements\n",
      "See xxurlxxhashtagrequirements for what is required. Starting from xxnumber.xxnumber.xxnumber only sidekiq xxnumber is supported and support for MRI xxnumber.xxnumber is dropped (it might work but won't be worked on)\n",
      "Installation\n",
      "Add this line to your application's Gemfile:\n",
      "gem 'sidekiq-unique-jobs'\n",
      "\n",
      "And then execute:\n",
      "$ bundle\n",
      "\n",
      "Or install it yourself as:\n",
      "$ gem install sidekiq-unique-jobs\n",
      "\n",
      "Usage\n",
      "All that is required is that you specifically set the sidekiq option for unique to true like below:\n",
      "ruby\n",
      "sidekiq_options unique: true\n",
      "For jobs scheduled in the future it is possible to set for how long the job\n",
      "should be unique. The job will be unique for the number of seconds configured (default xxnumber minutes)\n",
      "or until the job has been completed. Thus, the job will be unique for the shorter of the two.  Note that Sidekiq versions before xxnumber.xxnumber will remove job keys after an hour, which means jobs can remain unique for at most an hour.\n",
      "xxstarIf you want the unique job to stick around even after it has been successfully\n",
      "processed then just set the unique_unlock_order to anything except :before_yield or :after_yield (unique_unlock_order xxequals :never)\n",
      "You can also control the expiration length of the uniqueness check. If you want to enforce uniqueness over a longer period than the default of xxnumber minutes then you can pass the number of seconds you want to use to the sidekiq options:\n",
      "ruby\n",
      "sidekiq_options unique: true, unique_job_expiration: xxnumber xxstar xxnumber xxhashtag xxnumber hours\n",
      "Requiring the gem in your gemfile should be sufficient to enable unique jobs.\n",
      "Usage with ActiveJob\n",
      "ruby\n",
      "Sidekiq.default_worker_options xxequals {\n",
      "   'unique' xxequals> true,\n",
      "   'unique_args' xxequals> proc do |args|\n",
      "     [args.first.except('job_id')]\n",
      "   end\n",
      "}\n",
      "SidekiqUniqueJobs.config.unique_args_enabled xxequals true\n",
      "Finer Control over Uniqueness\n",
      "Sometimes it is desired to have a finer control over which arguments are used in determining uniqueness of the job, and others may be transient. For this use-case, you need to set SidekiqUniqueJobs.config.unique_args_enabled to true in an initializer, and then defined either unique_args method, or a ruby proc.\n",
      "The unique_args method need to return an array of values to use for uniqueness check.\n",
      "ruby\n",
      "SidekiqUniqueJobs.config.unique_args_enabled xxequals true\n",
      "The method or the proc can return a modified version of args without the transient arguments included, as shown below:\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_args: :unique_args\n",
      "def self.unique_args(name, id, options)\n",
      "    [ name, options[:type] ]\n",
      "  end\n",
      "...\n",
      "end\n",
      "class UniqueJobWithFilterProc\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_args: ->(args) { [ args.first ] }\n",
      "...\n",
      "end\n",
      "```\n",
      "Note that objects passed into workers are converted to JSON after running through client middleware. In server middleware, the JSON is passed directly to the worker xxhashtagperform method. So, you may run into issues where the arguments are different when enqueuing than they are when performing. Your unique_args method may need to account for this.\n",
      "Unlock Ordering\n",
      "By default the server middleware will release the worker lock after yielding to the next middleware or worker. Alternatively, this can be changed by passing the unique_unlock_order option:\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  unique_unlock_order: :before_yield\n",
      "...\n",
      "end\n",
      "```\n",
      "Logging\n",
      "To see logging in sidekiq when duplicate payload has been filtered out you can enable on a per worker basis using the sidekiq options.  The default value is false\n",
      "```ruby\n",
      "class UniqueJobWithFilterMethod\n",
      "  include Sidekiq::Worker\n",
      "  sidekiq_options unique: true,\n",
      "                  log_duplicate_payload: true\n",
      "...\n",
      "end\n",
      "```\n",
      "Testing\n",
      "SidekiqUniqueJobs uses mock_redis for inline testing. Due to complaints about having that as a runtime dependency it was made a development dependency so if you are relying on inline testing you will have to add gem 'mock_redis' to your Gemfile.\n",
      "Contributing\n",
      "\n",
      "Fork it\n",
      "Create your feature branch (git checkout -b my-new-feature)\n",
      "Commit your changes (git commit -am 'Add some feature')\n",
      "Push to the branch (git push origin my-new-feature)\n",
      "Create new Pull Request\n",
      "\n",
      "Contributors\n",
      "\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "xxurl\n",
      "\n",
      "**************************************************\n",
      "svids\n",
      "Intrusion Detection System - An application of HMMs for intrusion detection, created for a purpose.\n",
      "**************************************************\n",
      "kDebuger\n",
      "Show information about the current page\n",
      "Usage:\n",
      "You can use this plugin to manually log data. \n",
      "$tmpxxequalsarray('Arr A'xxequals>xxnumber);\n",
      "\n",
      "kDebuger::log($tmp);\n",
      "\n",
      "\n",
      "<<<<<<< HEAD\n",
      "$tmpxxequalsarray('Arr B'xxequals>xxnumber);\n",
      "\n",
      "kDebuger::log($tmp);    \n",
      "\n",
      "\n",
      "xxequals\n",
      "  You can use this plugin to manually log data. \n",
      "$tmpxxequalsarray('Arr A'xxequals>xxnumber);\n",
      "  kDebuger::log($tmp);\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxnumbereccxxnumberfxxnumberdbxxnumberccxxnumberabxxnumberfexxnumberddcxxnumbercxxnumber\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kDebuger::log('A');\n",
      "\n",
      "kDebuger::log('B');\n",
      "\n",
      "kDebuger::log('C');\n",
      "\n",
      "\n",
      "<<<<<<< HEAD\n",
      "kDebuger plugin print array in slider ander title call Filter\n",
      "xxequals\n",
      "  kDebuger plugin print array in slider ander\n",
      "  title call Filter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxnumbereccxxnumberfxxnumberdbxxnumberccxxnumberabxxnumberfexxnumberddcxxnumbercxxnumber\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**************************************************\n",
      "xxequals README\n",
      "This README would normally document whatever steps are necessary to get the\n",
      "application up and running.\n",
      "Things you may want to cover:\n",
      "\n",
      "\n",
      "Ruby version\n",
      "\n",
      "\n",
      "System dependencies\n",
      "\n",
      "\n",
      "Configuration\n",
      "\n",
      "\n",
      "Database creation\n",
      "\n",
      "\n",
      "Database initialization\n",
      "\n",
      "\n",
      "How to run the test suite\n",
      "\n",
      "\n",
      "Services (job queues, cache servers, search engines, etc.)\n",
      "\n",
      "\n",
      "Deployment instructions\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Please feel free to use a different markup language if you do not plan to run\n",
      "rake doc:app.\n",
      "**************************************************\n",
      "prueba-para-mostrarle-a-popi\n",
      "Esto es una prueba para mostrarle github a popi\n",
      "Mas texto!!\n",
      "Otro mas  \n",
      "**************************************************\n",
      "dairugger\n",
      "A golang interface to voltron. It's called dairugger because golang is\n",
      "stupid and makes you name projects before you can work on them, and the voltron\n",
      "wikipedia page wasn't very forthcoming with good names.\n",
      "Plus it kinda sounds like debugger.\n",
      "**************************************************\n",
      "lua-cutil\n",
      "Lua CUtil is a util library for Lua\n",
      "Build\n",
      "make sure lua was installed in your linux, and the following command 'make install' is just match lua xxnumber.xxnumber, so lua xxnumber.xxnumber is suggested.\n",
      "plain\n",
      "$ make all\n",
      "$ make install\n",
      "the command 'make install' is short for the following commands: \n",
      "plain\n",
      "$ cp cutil.so /usr/local/lib/lua/xxnumber.xxnumber\n",
      "$ chmod xxnumber /usr/local/lib/lua/xxnumber.xxnumber/cutil.so\n",
      "Test\n",
      "there is an example in the 'tests' dir.\n",
      "```plain\n",
      "$ /usr/local/bin/lua\n",
      "Lua xxnumber.xxnumber.xxnumber  Copyright (C) xxnumber-xxnumber Lua.org, PUC-Rio\n",
      "\n",
      "dofile(\"tests/sample.lua\")\n",
      "lua version:    ssfoo明天a你好fooxxnumber\n",
      "cutil version:  ssfoo明天a你好fooxxnumber\n",
      "efficiency comparison: \n",
      "test lua version:       xxnumber.xxnumber\n",
      "test cutil version:     xxnumber.xxnumber\n",
      "```\n",
      "\n",
      "**************************************************\n",
      "dbmapper\n",
      "A library for Dart developers. It is awesome.\n",
      "Usage\n",
      "A simple usage example:\n",
      "import 'package:dbmapper/dbmapper.dart';\n",
      "\n",
      "main() {\n",
      "  var awesome xxequals new Awesome();\n",
      "}\n",
      "\n",
      "Features and bugs\n",
      "Please file feature requests and bugs at the issue tracker.\n",
      "**************************************************\n",
      "check_tl_health Nagios Plugin README\n",
      "This plugin checks the hardware health and various interface metrics\n",
      "of tape libraries\n",
      "Copyright (C) Gerhard Lausser, gerhard.lausser@consol.de\n",
      "This program is free software; you can redistribute it and/or\n",
      "modify it under the terms of the GNU General Public License\n",
      "as published by the Free Software Foundation; either version xxnumber\n",
      "of the License, or (at your option) any later version.\n",
      "This program is distributed in the hope that it will be useful,\n",
      "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      "GNU General Public License for more details.\n",
      "You should have received a copy of the GNU General Public License\n",
      "along with this program; if not, write to the Free Software\n",
      "Foundation, Inc., xxnumber Franklin Street, Fifth Floor, Boston, MA  xxnumber-xxnumber, USA.\n",
      "\n",
      "\n",
      "For instructions on installing this plugin for use with Nagios,\n",
      "  see below. In addition, generic instructions for the GNU toolchain\n",
      "  can be found in the INSTALL file.\n",
      "\n",
      "\n",
      "For major changes between releases, read the CHANGES file.\n",
      "\n",
      "\n",
      "For information on detailed changes that have been made,\n",
      "  read the Changelog file.\n",
      "\n",
      "\n",
      "This plugin is self-documenting.  All plugins that comply with\n",
      "  the basic guidelines for development will provide detailed help when\n",
      "  invoked with the '-h' or '--help' options.\n",
      "\n",
      "\n",
      "You can check for the latest plugin at:\n",
      "  xxurl\n",
      "Send mail to gerhard.lausser@consol.de for assistance.\n",
      "Please include the OS type and version that you are using.\n",
      "Also, run the plugin with the '-vvv' option and provide the resulting \n",
      "version information.  Of course, there may be additional diagnostic information\n",
      "required as well. Use good judgment.\n",
      "How to \"compile\" the check_tl_health script.\n",
      "xxnumber) Run the configure script to initialize variables and create a Makefile, etc.\n",
      "./configure --prefixxxequalsBASEDIRECTORY --with-nagios-userxxequalsSOMEUSER --with-nagios-groupxxequalsSOMEGROUP --with-perlxxequalsPATH_TO_PERL\n",
      "\n",
      "a) Replace BASEDIRECTORY with the path of the directory under which Nagios\n",
      "      is installed (default is '/usr/local/nagios')\n",
      "   b) Replace SOMEUSER with the name of a user on your system that will be\n",
      "      assigned permissions to the installed plugins (default is 'nagios')\n",
      "   c) Replace SOMEGRP with the name of a group on your system that will be\n",
      "      assigned permissions to the installed plugins (default is 'nagios')\n",
      "   d) Replace PATH_TO_PERL with the path where a perl binary can be found.\n",
      "      Besides the system wide perl you might have installed a private perl\n",
      "      just for the nagios plugins (default is the perl in your path).\n",
      "xxnumber) \"Compile\" the plugin with the following command:\n",
      "make\n",
      "\n",
      "This will produce a \"check_tl_health\" script. You will also find\n",
      "a \"check_tl_health.pl\" which you better ignore. It is the base for\n",
      "the compilation filled with placeholders. These will be replaced during\n",
      "the make process.\n",
      "\n",
      "xxnumber) Install the compiled plugin script with the following command:\n",
      "make install\n",
      "\n",
      "The installation procedure will attempt to place the plugin in a \n",
      "   'libexec/' subdirectory in the base directory you specified with\n",
      "   the --prefix argument to the configure script.\n",
      "xxnumber) Verify that your configuration files for Nagios contains\n",
      "   the correct paths to the new plugin.\n",
      "Command line parameters\n",
      "You'll find the command line parameters on the website mentioned above.\n",
      "If it does not exist or is not up to date, this means that i hadn't the\n",
      "time to write/update it yet. I am aware of it, so please don't send me\n",
      "mails. Just accept it.\n",
      "--\n",
      "Gerhard Lausser &xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;&xxhashtagxxnumber;\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for r in test_df['cleaned_content']:\n",
    "    print(r)\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE BE DRAGONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_tokenizer = fastai.text.Tokenizer(\n",
    "    tok_func=tok_fn,\n",
    "    pre_rules=[],\n",
    "    post_rules=[],\n",
    "    special_cases=[],\n",
    "    n_cpus=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['languages'] = raw_df['language'].apply(lambda ds: [d['name'] for d in ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_langs = ['python', 'r', 'matlab', 'julia', 'c++', 'java', 'scala']\n",
    "df = raw_df[raw_df['languages'].apply(lambda langs: any([lang.lower() in selected_langs for lang in langs]))]\n",
    "df = df.drop(['language'], axis=1)\n",
    "df = df[(~df['content'].isna())]\n",
    "df = df[df['content'].str.split().apply(len) > 25]\n",
    "df = df[(df['content'].apply(itemgetter(0)) != '<') & (df['content'].apply(itemgetter(-1)) != '>')]\n",
    "df['content'] = df['content'].str.replace(r'#+', 'xxhashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.replace(r'[0-9]+', 'xxnumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples =  10 * 10 ** 3\n",
    "lm_df = df[['repo_name', 'languages', 'content']][:n_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df = lm_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df[['content']].to_csv('github_repos_lm_text_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df.index = pd.RangeIndex(len(lm_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:45<00:00, 219.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "extracted_content = pd.Series([tokenize_markdown(md_string) for md_string in tqdm.tqdm(lm_df['content'])])\n",
    "lm_df['text'] = extracted_content.apply(' '.join)\n",
    "lm_df = lm_df[(~lm_df['text'].isna()) & (lm_df['text'].apply(len) > 0)]\n",
    "lm_df[['text']].to_csv('github_repos_lm_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to FastAI api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 257 ms, total: 1.4 s\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_lm = fastai.text.TextLMDataBunch.from_csv(\n",
    "    '', 'github_repos_lm_text.csv'\n",
    ")\n",
    "data_lm.save('data_lm_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = fastai.text.load_data('', 'data_lm_export.pkl', bs=bs, bptt=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (7999 items)\n",
       "x: LMTextList\n",
       "xxbos xxhashtag xxup xxunk - xxmaj edison - xxmaj python i found a lot of the existing code for xxmaj intel xxmaj edison and xxmaj sparkfun xxmaj edison xxmaj blocks to be confusing . xxmaj this repo attempts to provide simplified get - it - working python code for : xxhashtag xxmaj sparkfun xxmaj edison xxmaj blocks xxnumberdof xxup imu xxhashtag xxmaj sparkfun xxmaj edison xxmaj blocks xxup adc xxmaj none of these would happen without the community at large , so please see each file for license and credit .,xxbos xxmaj unbound xxup readme . / configure & & make & & make install xxmaj you can use libevent if you want . libevent is useful when using many ( xxnumber ) outgoing ports . xxmaj by default max xxnumber ports are opened at the same time and the builtin alternative is equally capable and a little faster . xxmaj more detailed xxup readme , xxup readme . svn , xxup readme . tests in doc directory manual pages can be found in doc directory , and are installed , unbound ( xxnumber ) . * example configuration file doc / example . conf,xxbos xxhashtag xxup api xxmaj versioning examples xxmaj this repository contains example xxmaj flask apps that are meant to visualize the difference between path - and header - based xxup api versioning . xxhashtag xxmaj path - based versioning xxmaj app available in through_header package uses path - based xxup api versioning . xxmaj it consists of two blueprints - api_vxxnumber and api_vxxnumber . xxmaj each one of them is mounted under its own xxup url prefix . xxmaj effectively , both xxup api versions are separate . xxmaj being separate , both blueprints can use different databases and other resources ( e . g . remote services ) to serve xxup api clients . xxmaj it ' s also possible to easily extract one of them into completely new app . xxmaj routing uses standard mechanisms ( xxmaj blueprint . route decorator ) which means that this scheme of xxup api separation requires no changes to the underlying framework . xxmaj disabling one version of the xxup api is as simple as commenting out two lines of code ( e . g . xxunk / app . py lines xxnumber and xxnumber ) . xxhashtag xxmaj header - based versioning xxmaj app available in through_header package uses simple header - based xxup api versioning . xxmaj it consists of two blueprints - api . xxmaj it ' s xxunk under / api xxup url prefix . xxup api version discovery is done by the blueprint ' s xxunk callback . xxmaj both xxup api versions share the same code base and view functions must alter their behavior ( e . g . response format ) according to xxup api version being requested,xxbos xxhashtag gaze xxmaj view images xxmaj dependencies : - sdlxxnumber - sdlxxnumber_image - and sdlxxnumber_ttf xxmaj list images you want to look at as command line arguments . xxmaj use j and k to go to the next and previous image , respectively . xxmaj use mouse wheel to zoom . xxmaj click and drag to move image around . xxup todo some code organization ! xxmaj window resizing xxmaj exceptions xxmaj show path of the displayed xxunk xxmaj rotate image xxmaj mouse wheel for other platforms than xxmaj linux,xxbos xxhashtag xxnumber - xxmaj fall - xxmaj hadoop xxnumber xxmaj fall xxmaj semester xxmaj hadoop and xxmaj spark xxmaj workshop by xxmaj min xxmaj li xxmaj the files here are all the code from the xxmaj hadoop and xxmaj spark workshop held on xxnumber / xxnumber / xxnumber . xxmaj the data and xxmaj hadoop and xxmaj spark programs are not included here . xxmaj they are packaged with the virtual machine which can be downloaded from http : / / python . acis . xxunk . edu / xxnumber - xxmaj fal - xxmaj hadoop /\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (2001 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj project move to sourceforge and xxmaj bitbucket and xxmaj github | _ | xxmaj mirror | xxmaj wiki | xxmaj downloads | xxmaj issues | xxunk : xxrep 5 - | : xxrep 4 - :| : xxrep 9 - :| : xxrep 6 - :| | sf | https : / / sourceforge . net / p / xxunk | xxmaj wiki | xxmaj downloads | xxmaj tickets | | bb | https : / / bitbucket . org / xxunk / xxunk | xxmaj wiki | xxmaj downloads | xxmaj issues | | gh | https : / / github . com / xxunk / xxunk | | | xxmaj issues | | gc | https : / / code . google . com / p / xxunk | old | old | old | xxunk xxmaj tools to work with android . dex and java . class files xxnumber . dex - reader / writer : xxmaj read / write the xxmaj dalvik xxmaj executable ( . dex ) file . xxmaj it has a light weight xxup api similar with xxup asm . xxnumber . xxunk - xxunk : xxmaj convert . dex file to . class files ( zipped as jar ) xxnumber . xxunk / baksmali : disassemble dex to xxunk files and assemble dex from xxunk files . different implementation to xxunk / baksmali , same syntax , but we support escape in type desc \" xxmaj xxunk / xxunk \\ t \\ uxxnumber ; \" xxnumber . other tools : xxunk - decrypt - string xxhashtag xxmaj need help ? send email to xxunk @ googlegroups . com or post on issue trackers list above . xxhashtag xxmaj license xxmaj apache xxnumber . xxnumber,xxbos xxmaj license : gplvxxnumber . xxmaj see xxup copying for details . xxmaj poolers original xxup readme : xxmaj downloads : https : / / sourceforge . net / projects / cpuminer / files / xxmaj git tree : https : / / github . com / pooler / cpuminer xxmaj dependencies : libcurl http : / / curl . haxx . se / libcurl / jansson http : / / www . digip . org / jansson / ( jansson is included in - tree ) xxmaj basic * nix build instructions : . / autogen . sh xxhashtag only needed if building from git repo . / nomacro . pl xxhashtag only needed if building on xxmaj mac xxup os x or with xxmaj clang . / configure xxup cflags = \" - xxmaj oxxnumber \" make xxnumber xxmaj notes for xxup aix users : xxnumber * xxmaj to build a xxnumber - bit binary , export xxup object_mode = xxnumber * xxup gnu - style long options are not supported , but are accessible via configuration file xxmaj basic xxmaj windows build instructions , using mingw : xxmaj install mingw and the xxup msys xxmaj developer xxmaj tool xxmaj kit ( http : / / www . mingw . org / ) * xxmaj make sure you have mstcpip . h in mingw \\ include xxmaj if using mingw - wxxnumber , install pthreads - wxxnumber xxmaj install libcurl devel ( http : / / curl . haxx . se / download . html ) * xxmaj make sure you have libcurl . mxxnumber in mingw \\ share \\ aclocal * xxmaj make sure you have curl - config in mingw \\ bin xxmaj in the xxup msys shell , run : . / autogen . sh xxhashtag only,xxbos xxunk interface xxmaj command execute ( ) xxmaj example classes that would implement the xxmaj command interface : xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxmaj description : xxmaj the xxmaj command interface defines a contract for all commands with one method to execute the command . xxmaj implementations of this interface will be specific commands , each corresponding to the text commands that the model will recognize . xxmaj once the model has generated a collection of xxmaj command objects , they will be passed to the command evaluator , which will run through all commands in the list . xxmaj some commands , such as those representing loops and defined methods , will hold a reference to a list of other commands , which could itself hold other nested structures as well . xxmaj simple commands such as move and rotate will need to be created with only a parameter value . xxmaj the command evaluator may also store lists of commands as \" programs \" so the model won ' t need to re - parse recent programs - they can execute straight from the xxmaj command objects . xxmaj separating the command implementation from the parsing allows for greater flexibility between parsing commands and executing them - so the model doesn ' t need to know how commands are executed . xxmaj parser aggregates text inputs into a list of strings to be parsed creates command objects xxmaj error xxmaj message xxmaj generations xxmaj the error message generator will only have two interactions : xxnumber ) xxmaj parser xxmaj the model will send the possible text inputs from,xxbos open - xxunk is an encoder and decoder for the xxup xxunk format , as described in xxup rfc xxnumber : xxmaj the xxup xxunk xxmaj generic xxmaj xxunk and xxmaj compression xxmaj data xxmaj format ( http : / / www . ietf . org / rfc / rfcxxnumber . txt ) a library with a simple xxup api is included , as well as a command - line executable that can apply the encoder and decoder to source , target , and delta files . xxmaj for further details , please refer to : http : / / code . google . com / p / open - xxunk / wiki / xxunk xxmaj see xxup install for ( generic ) installation instructions for c + + : basically . / configure & & make & & make install xxmaj this should compile the unit tests as well as \" xxunk \" , a simple command - line utility to run the encoder and decoder . xxmaj typical usage of xxunk is as follows ( the \" < \" and \" > \" are file redirect operations , not optional arguments ): xxunk encode - dictionary file . dict < xxunk > xxunk xxunk decode - dictionary file . dict < xxunk > xxunk xxmaj to see the command - line syntax of xxunk , use \" xxunk -- help \" or just \" xxunk \" . xxmaj to call the encoder from c + + code , assuming that dictionary , target , and delta are all std : : string objects : xxhashtaginclude / / xxmaj read this file for interface details [ ... ] xxunk : : xxunk encoder ( dictionary . data ( ) ,,xxbos xxmaj xxunk is a simple xxunk network with a simple graphical output . i used this code for a workshop i ran in xxnumber : xxmaj introduction to xxmaj computational xxmaj cognitive xxmaj modelling . xxmaj it is written with an emphasis on readability and clarity for people who have never programmed before . xxmaj feel free to use it for xxunk or other purposes . xxmaj if you do use it , contact me . i ' d love to hear what you do with it .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm"
   ]
  }
 ],
 "metadata": {
  "jekyll": {
   "keywords": "fastai",
   "summary": "Application to NLP, including ULMFiT fine-tuning",
   "title": "text"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

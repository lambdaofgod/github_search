{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2061fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp file_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144470b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import ast\n",
    "import tqdm\n",
    "import json\n",
    "import attr\n",
    "from operator import itemgetter\n",
    "\n",
    "from scarce_learn import zero_shot\n",
    "from mlutil.feature_extraction import embeddings\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction, metrics, model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "from github_search import paperswithcode_tasks\n",
    "\n",
    "import mlutil\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from scarce_learn.zero_shot import devise_jax, devise_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "324de450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d92557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn import model_selection\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import tqdm\n",
    "from transformers import RobertaForCausalLM, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bc6488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3e57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_functions_df = pd.read_csv(\n",
    "    \"output/python_functions.csv\", index_col=\"Unnamed: 0\", nrows=500000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94b5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = all_functions_df.iloc[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253a220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = functions_df[\n",
    "    ~functions_df[\"repo_name\"].isin(\n",
    "        [\n",
    "            \"tensorflow/models\",\n",
    "            \"google-research/google-research\",\n",
    "            \"tensorflow/tensor2tensor\",\n",
    "            \"yumoh/catboost_iter\",\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06e597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_functions_count = functions_df[\"repo_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11861bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = functions_df[\n",
    "    functions_df[\"repo_name\"].isin(\n",
    "        repo_functions_count[repo_functions_count > 10].index\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d605e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2bbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")\n",
    "paperswithcode_with_imports_df[\"tasks\"] = paperswithcode_with_imports_df[\"tasks\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39549feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>function_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>ImportGraph</td>\n",
       "      <td>class ImportGraph():\\n    '  Importing and run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>load_checkpoint</td>\n",
       "      <td>def load_checkpoint(sess, checkpoint_dir, file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>auto_reuse</td>\n",
       "      <td>def auto_reuse(fun):\\n    \"\\n    Wrapper that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>CellWrapper</td>\n",
       "      <td>class CellWrapper(RNNCell):\\n    '\\n    Wrappe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>multi_encoder</td>\n",
       "      <td>def multi_encoder(encoder_inputs, encoders, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_modeling_deit.py</td>\n",
       "      <td>DeiTModelIntegrationTest</td>\n",
       "      <td>@require_vision\\nclass DeiTModelIntegrationTes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_feature_extraction_detr.py</td>\n",
       "      <td>DetrFeatureExtractionTester</td>\n",
       "      <td>class DetrFeatureExtractionTester(unittest.Tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_feature_extraction_detr.py</td>\n",
       "      <td>DetrFeatureExtractionTest</td>\n",
       "      <td>@require_torch\\n@require_vision\\nclass DetrFea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_processor_clip.py</td>\n",
       "      <td>CLIPProcessorTest</td>\n",
       "      <td>@require_vision\\nclass CLIPProcessorTest(unitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_modeling_tf_distilbert.py</td>\n",
       "      <td>TFDistilBertModelTester</td>\n",
       "      <td>class TFDistilBertModelTester():\\n\\n    def __...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156212 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       repo_name                                   path  \\\n",
       "0                trangvu/ape-npi              translate/import_graph.py   \n",
       "1                trangvu/ape-npi              translate/import_graph.py   \n",
       "2                trangvu/ape-npi                    translate/models.py   \n",
       "3                trangvu/ape-npi                    translate/models.py   \n",
       "4                trangvu/ape-npi                    translate/models.py   \n",
       "...                          ...                                    ...   \n",
       "499995  huggingface/transformers            tests/test_modeling_deit.py   \n",
       "499996  huggingface/transformers  tests/test_feature_extraction_detr.py   \n",
       "499997  huggingface/transformers  tests/test_feature_extraction_detr.py   \n",
       "499998  huggingface/transformers           tests/test_processor_clip.py   \n",
       "499999  huggingface/transformers   tests/test_modeling_tf_distilbert.py   \n",
       "\n",
       "                      function_name  \\\n",
       "0                       ImportGraph   \n",
       "1                   load_checkpoint   \n",
       "2                        auto_reuse   \n",
       "3                       CellWrapper   \n",
       "4                     multi_encoder   \n",
       "...                             ...   \n",
       "499995     DeiTModelIntegrationTest   \n",
       "499996  DetrFeatureExtractionTester   \n",
       "499997    DetrFeatureExtractionTest   \n",
       "499998            CLIPProcessorTest   \n",
       "499999      TFDistilBertModelTester   \n",
       "\n",
       "                                            function_code  \n",
       "0       class ImportGraph():\\n    '  Importing and run...  \n",
       "1       def load_checkpoint(sess, checkpoint_dir, file...  \n",
       "2       def auto_reuse(fun):\\n    \"\\n    Wrapper that ...  \n",
       "3       class CellWrapper(RNNCell):\\n    '\\n    Wrappe...  \n",
       "4       def multi_encoder(encoder_inputs, encoders, en...  \n",
       "...                                                   ...  \n",
       "499995  @require_vision\\nclass DeiTModelIntegrationTes...  \n",
       "499996  class DetrFeatureExtractionTester(unittest.Tes...  \n",
       "499997  @require_torch\\n@require_vision\\nclass DetrFea...  \n",
       "499998  @require_vision\\nclass CLIPProcessorTest(unitt...  \n",
       "499999  class TFDistilBertModelTester():\\n\\n    def __...  \n",
       "\n",
       "[156212 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76503237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      23\n",
       "15      22\n",
       "12      21\n",
       "14      18\n",
       "21      18\n",
       "        ..\n",
       "1200     1\n",
       "743      1\n",
       "406      1\n",
       "531      1\n",
       "528      1\n",
       "Name: path, Length: 246, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_df.groupby(\"repo_name\").agg(\"count\")[\"path\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e108121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers.models\n",
    "\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/codebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216e2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/kuba/.cache/torch/sentence_transformers/microsoft_codebert-base. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "sentence_transformer_model = sentence_transformers.SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484993c",
   "metadata": {},
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Tell pytorch to run this model on the GPU.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name,truncation = True)\n",
    "model = RobertaModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True,\n",
    ")\n",
    "model = model.to(device).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd543e05",
   "metadata": {},
   "source": [
    "\n",
    "path_tokenizer = RobertaTokenizer.from_pretrained(model_name,truncation = True)\n",
    "path_model = RobertaForMaskedLM.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True,\n",
    ").roberta\n",
    "path_model = model.to(device).half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c7ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592b3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_grouped = functions_df.groupby(function_paths).apply(\n",
    "    lambda df: list(df[\"function_code\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "413e1203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1jsingh/rl_navigation agents/bst.py                                    1\n",
       "1jsingh/rl_navigation agents/dqn_agent.py                              1\n",
       "1jsingh/rl_navigation agents/model.py                                  1\n",
       "5yearsKim/Conditional-Normalizing-Flow data/dataset.py                 1\n",
       "5yearsKim/Conditional-Normalizing-Flow inference.py                    1\n",
       "                                                                      ..\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow data/create_synth90k_tfrecord.py     1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/create_crnn_ctc_tfrecord.py    1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_crnn_ctc.py               1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/inference_crnn_ctc.py          1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/train_crnn_ctc.py              1\n",
       "Length: 24053, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_grouped.groupby(functions_grouped.index).agg(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4447df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(functions_grouped.iloc[:2], start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b46993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef135b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trangvu/ape-npi translate/import_graph.py    [class ImportGraph():\\n    '  Importing and ru...\n",
       "trangvu/ape-npi translate/import_graph.py    [class ImportGraph():\\n    '  Importing and ru...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_grouped[function_paths.iloc[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb6879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tasks = paperswithcode_with_imports_df[[\"repo\", \"tasks\"]].groupby(\"repo\").agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d89a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_dataset(repo_tasks, functions_df):\n",
    "    functions_df = functions_df.merge(repo_tasks, left_on=\"repo_name\", right_on=\"repo\")\n",
    "    function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]\n",
    "    functions_groupby = functions_df.groupby(function_paths.values)\n",
    "    functions_grouped = functions_groupby.apply(\n",
    "        lambda df: frozenset(df[\"function_code\"])\n",
    "    )\n",
    "    tasks_grouped = functions_groupby.apply(lambda df: list(df[\"tasks\"]))\n",
    "    data_df = (\n",
    "        pd.DataFrame({\"tasks\": tasks_grouped, \"functions\": functions_grouped})\n",
    "        .explode(\"tasks\")\n",
    "        .explode(\"tasks\")\n",
    "        .reset_index()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    data_df[\"functions\"] = data_df[\"functions\"].apply(list)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3addd639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tasks</th>\n",
       "      <th>functions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1jsingh/rl_navigation agents/bst.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_normal.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_minkowski.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_mdm.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_matrices.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_02122019.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_09132018_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_10142018_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/fcn_scoring_layer_02052019_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lincaiming/py-faster-rcnn-update caffe-fast-rcnn/python/classify.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tasks  functions\n",
       "index                                                               \n",
       "1jsingh/rl_navigation agents/bst.py                     1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "...                                                   ...        ...\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_02...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_09...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_10...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/fcn_scoring_...     13         13\n",
       "lincaiming/py-faster-rcnn-update caffe-fast-rcn...     13         13\n",
       "\n",
       "[18400 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_dataset(repo_tasks, functions_df).groupby(\"index\").agg(\"count\").sort_values(\n",
    "    \"functions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ca3279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices_iterator(data_df, batch_size=8, shuffle=True):\n",
    "    sample_index = data_df.index.values\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    for i in range(0, len(sample_index), batch_size):\n",
    "        yield sample_index[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "262494ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_file_dataset(repo_tasks, functions_df)\n",
    "sample_index = data_df.index.values\n",
    "np.random.shuffle(sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a282230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tasks</th>\n",
       "      <th>functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112781</th>\n",
       "      <td>1jsingh/rl_navigation agents/bst.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class FixedSize_BinarySearchTree(object):\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>1jsingh/rl_navigation agents/dqn_agent.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class ReplayBuffer():\\n\\n    def __init__(sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248440</th>\n",
       "      <td>1jsingh/rl_navigation agents/model.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class Qnetwork(nn.Module):\\n\\n    def __init_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65105</th>\n",
       "      <td>5yearsKim/Conditional-Normalizing-Flow data/da...</td>\n",
       "      <td>Colorization</td>\n",
       "      <td>[class ImgDatasets(torch.utils.data.Dataset):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65621</th>\n",
       "      <td>5yearsKim/Conditional-Normalizing-Flow data/da...</td>\n",
       "      <td>Image Generation</td>\n",
       "      <td>[class ImgDatasets(torch.utils.data.Dataset):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218204</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_c...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24874</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...</td>\n",
       "      <td>Scene Text</td>\n",
       "      <td>[def _int_to_string(value, char_map_dict=None)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112880</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _int_to_string(value, char_map_dict=None)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108265</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...</td>\n",
       "      <td>Scene Text</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177723</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43986 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    index  \\\n",
       "112781                1jsingh/rl_navigation agents/bst.py   \n",
       "9923            1jsingh/rl_navigation agents/dqn_agent.py   \n",
       "248440              1jsingh/rl_navigation agents/model.py   \n",
       "65105   5yearsKim/Conditional-Normalizing-Flow data/da...   \n",
       "65621   5yearsKim/Conditional-Normalizing-Flow data/da...   \n",
       "...                                                   ...   \n",
       "218204  ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_c...   \n",
       "24874   ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...   \n",
       "112880  ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...   \n",
       "108265  ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...   \n",
       "177723  ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...   \n",
       "\n",
       "                         tasks  \\\n",
       "112781             Atari Games   \n",
       "9923               Atari Games   \n",
       "248440             Atari Games   \n",
       "65105             Colorization   \n",
       "65621         Image Generation   \n",
       "...                        ...   \n",
       "218204  Scene Text Recognition   \n",
       "24874               Scene Text   \n",
       "112880  Scene Text Recognition   \n",
       "108265              Scene Text   \n",
       "177723  Scene Text Recognition   \n",
       "\n",
       "                                                functions  \n",
       "112781  [class FixedSize_BinarySearchTree(object):\\n\\n...  \n",
       "9923    [class ReplayBuffer():\\n\\n    def __init__(sel...  \n",
       "248440  [class Qnetwork(nn.Module):\\n\\n    def __init_...  \n",
       "65105   [class ImgDatasets(torch.utils.data.Dataset):\\...  \n",
       "65621   [class ImgDatasets(torch.utils.data.Dataset):\\...  \n",
       "...                                                   ...  \n",
       "218204  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "24874   [def _int_to_string(value, char_map_dict=None)...  \n",
       "112880  [def _int_to_string(value, char_map_dict=None)...  \n",
       "108265  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "177723  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "\n",
       "[43986 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4750096",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_iter = get_sample_indices_iterator(data_df, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a53e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_inputs(batch):\n",
    "    indices = list(np.cumsum(batch[\"functions\"].apply(len).values))\n",
    "    file_function_idxs_start = np.array([0] + indices[:-1])\n",
    "    file_function_idxs_end = indices\n",
    "    reshaped_batch = batch.explode(\"functions\")\n",
    "\n",
    "    return (\n",
    "        batch[\"tasks\"].values,\n",
    "        batch[\"index\"].values,\n",
    "        reshaped_batch[\"functions\"].values,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f6f1bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_sample_indices_iterator at 0x7f48bb10a2e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817407d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \"\"\"\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3615\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3616\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7692d2ad11db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_function_idxs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_function_idxs_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "batch = data_df.iloc[next(iter(index_iter))]\n",
    "(\n",
    "    batch_tasks,\n",
    "    batch_paths,\n",
    "    batch_functions,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ") = prepare_model_inputs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17127ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_function_idxs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_function_idxs_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfc8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f95bf11",
   "metadata": {},
   "source": [
    "# Idea: use thinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, tokenizer, texts, max_length=512):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, max_length=max_length)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"][:, :max_length]\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, :max_length]\n",
    "    inputs = inputs.to(model.device)\n",
    "    return model(**inputs).last_hidden_state.mean(axis=1)\n",
    "\n",
    "\n",
    "# path_embeddings = get_embeddings(model, tokenizer, paths_batch)#.last_hidden_state.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_embeddings = get_embeddings(model, tokenizer, functions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeeb2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions_embeddings_reshaped = function_embeddings.repeat((5,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zrÃ³b dla jednego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a678f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_file_functions = function_embeddings[file_function_idxs_start[0]:file_function_idxs_end[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23233f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embeddings.shape\n",
    "# first_file_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionSelector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossAttentionSelector, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        path_embeddings,\n",
    "        function_embeddings,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        attn = F.sigmoid(path_embeddings @ function_embeddings.T)\n",
    "        attn_mask = torch.ones_like(attn).cuda().half()\n",
    "        for i in range(len(file_function_idxs_start)):\n",
    "            attn_mask[i, : file_function_idxs_start[i]] = 0\n",
    "            attn_mask[i, file_function_idxs_end[i] :] = 0\n",
    "        return (attn_mask * attn) @ function_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = CrossAttentionSelector().cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0001fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathBasedFileEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_features, hidden_size, dropout_prob=0.5):\n",
    "        super(PathBasedFileEmbedder, self).__init__()\n",
    "        self.cross_attention_pooler = CrossAttentionSelector()\n",
    "        self.path_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.function_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        path_embeddings,\n",
    "        function_embeddings,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        path_nl_embeddings = F.leaky_relu(self.bn1(self.path_dense(path_embeddings)))\n",
    "        function_nl_embeddings = F.leaky_relu(\n",
    "            self.bn2(self.function_dense(function_embeddings))\n",
    "        )\n",
    "        file_embeddings = self.cross_attention_pooler(\n",
    "            path_nl_embeddings,\n",
    "            function_nl_embeddings,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        return (path_nl_embeddings, F.leaky_relu(self.bn3(file_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e71700",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_embedder = PathBasedFileEmbedder(768, 20).cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings = sentence_transformer_model.encode(\n",
    "    batch_paths, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()\n",
    "function_embeddings = sentence_transformer_model.encode(\n",
    "    batch_functions, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()\n",
    "task_embeddings = sentence_transformer_model.encode(\n",
    "    batch_tasks, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2703b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector(\n",
    "    path_embeddings,\n",
    "    function_embeddings,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1794757",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(path_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81e1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleSimilarityHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(TripleSimilarityHead, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, u, v, reference):\n",
    "        # imilarity_features = torch.column_stack(\n",
    "        #    [u, v, torch.abs(u-reference), torch.abs(v-reference)]\n",
    "        # )\n",
    "        # hidden = F.tanh(self.bn(self.dense(similarity_features)))\n",
    "        return torch.column_stack(\n",
    "            [\n",
    "                torch.cosine_similarity(u, reference),\n",
    "                torch.cosine_similarity(v, reference),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add19ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(5))\n",
    "\n",
    "a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateSimilarityScorer(nn.Module):\n",
    "    def __init__(\n",
    "        self, sentence_transformer, embedding_features, hidden_size, dropout_prob=0.5\n",
    "    ):\n",
    "        super(AggregateSimilarityScorer, self).__init__()\n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        self.file_embedder = PathBasedFileEmbedder(embedding_features, hidden_size)\n",
    "        self.task_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.similarity_head = TripleSimilarityHead(hidden_size)\n",
    "        self.bn_task = nn.BatchNorm1d(hidden_size)\n",
    "        self.transformer_batch_size = 512\n",
    "\n",
    "    def get_trainable_params(self):\n",
    "        return itertools.chain(\n",
    "            self.file_embedder.parameters(),\n",
    "            self.task_dense.parameters(),\n",
    "            self.similarity_head.parameters(),\n",
    "            self.bn_task.parameters(),\n",
    "        )\n",
    "\n",
    "    def embed_tasks(self, tasks):\n",
    "        with torch.no_grad():\n",
    "            base_embeddings = self.sentence_transformer.encode(\n",
    "                tasks, convert_to_numpy=False, convert_to_tensor=True\n",
    "            ).half()\n",
    "        task_embeddings = F.leaky_relu(self.bn_task(self.task_dense(base_embeddings)))\n",
    "        return task_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tasks,\n",
    "        tasks_negative,\n",
    "        paths,\n",
    "        functions,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        all_inputs = list(tasks) + list(paths) + list(functions) + list(tasks_negative)\n",
    "        embeddings = self.sentence_transformer.encode(\n",
    "            all_inputs,\n",
    "            convert_to_numpy=False,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=self.transformer_batch_size,\n",
    "        ).half()\n",
    "        base_task_embeddings = embeddings[: len(tasks)]\n",
    "        base_path_embeddings = embeddings[len(tasks) : len(tasks) + len(paths)]\n",
    "        function_embeddings = embeddings[len(tasks) + len(paths) : -len(tasks_negative)]\n",
    "        base_negative_task_embeddings = embeddings[-len(tasks_negative) :]\n",
    "        task_embeddings = F.leaky_relu(\n",
    "            self.bn_task(self.task_dense(base_task_embeddings))\n",
    "        )\n",
    "        negative_task_embeddings = F.leaky_relu(\n",
    "            self.bn_task(self.task_dense(base_negative_task_embeddings))\n",
    "        )\n",
    "        path_embeddings, file_embeddings = self.file_embedder(\n",
    "            base_path_embeddings,\n",
    "            function_embeddings,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        positive_similarity = self.similarity_head(\n",
    "            path_embeddings, file_embeddings, task_embeddings\n",
    "        )\n",
    "        negative_similarity = self.similarity_head(\n",
    "            path_embeddings, file_embeddings, negative_task_embeddings\n",
    "        )\n",
    "        return positive_similarity, negative_similarity\n",
    "\n",
    "    def embed_files(\n",
    "        self, paths, functions, file_function_idxs_start, file_function_idxs_end\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            path_embeddings = self.sentence_transformer.encode(\n",
    "                paths,\n",
    "                convert_to_numpy=False,\n",
    "                convert_to_tensor=True,\n",
    "                batch_size=self.transformer_batch_size,\n",
    "            ).half()\n",
    "            function_embeddings = self.sentence_transformer.encode(\n",
    "                functions,\n",
    "                convert_to_numpy=False,\n",
    "                convert_to_tensor=True,\n",
    "                batch_size=self.transformer_batch_size,\n",
    "            ).half()\n",
    "            path_embeddings, file_embeddings = self.file_embedder(\n",
    "                path_embeddings,\n",
    "                function_embeddings,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            )\n",
    "        return path_embeddings, file_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273dcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embeddings.shape#* functions_embeddings_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd42749",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = sentence_transformer_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = (\n",
    "    AggregateSimilarityScorer(\n",
    "        sentence_transformer_model,\n",
    "        embedding_features=embedding_features,\n",
    "        hidden_size=512,\n",
    "    )\n",
    "    .cuda()\n",
    "    .half()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_batch = [\"classification\"] + [\"domain adaptation\"] * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9abdd",
   "metadata": {},
   "source": [
    "base_path_embeddings = get_embeddings(model, tokenizer, paths_batch)\n",
    "function_embeddings = get_embeddings(model, tokenizer, functions_batch)\n",
    "base_task_embeddings = get_embeddings(model, tokenizer, tasks_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a38bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be610d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tasks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71af065",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82862ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = similarity_scorer(\n",
    "    batch_tasks,\n",
    "    random_tasks,\n",
    "    batch_paths,\n",
    "    batch_functions,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ")\n",
    "# negative_scores = similarity_scorer(random_tasks, batch_paths, batch_functions, file_function_idxs_start, file_function_idxs_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5339d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468c238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483a1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.logsigmoid(x)\n",
    "            minus_logprobs = torch.nn.functional.logsigmoid(-x)\n",
    "\n",
    "            target_smoothed = target * (1 - self.smoothing) + self.smoothing / 2\n",
    "            smoothed_loss = -logprobs * target_smoothed - minus_logprobs * (\n",
    "                1 - target_smoothed\n",
    "            )\n",
    "            return smoothed_loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc288340",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.tensor([-100, 100]).float().reshape(2, 1)  # ('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015aaeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0, 1]).reshape(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32da209",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af187220",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing(log_probs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperswithcode\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_area_grouped_tasks():\n",
    "    client = paperswithcode.PapersWithCodeClient()\n",
    "    areas = client.area_list().results\n",
    "    s = 0\n",
    "\n",
    "    area_grouped_tasks = {}\n",
    "\n",
    "    for a in areas:\n",
    "        area_tasks = [\n",
    "            t.id for t in client.area_task_list(a.id, items_per_page=1000).results\n",
    "        ]\n",
    "        area_grouped_tasks[a.id] = area_tasks\n",
    "        n_tasks_per_area = len(area_tasks)\n",
    "        print(a.id, \":\", n_tasks_per_area)\n",
    "        s += n_tasks_per_area\n",
    "    tasks_df = pd.DataFrame(\n",
    "        {\"area\": area_grouped_tasks.keys(), \"task\": area_grouped_tasks.values()}\n",
    "    ).explode(\"task\")\n",
    "    tasks_df[\"task\"] = tasks_df[\"task\"].str.split(\"-\").apply(\" \".join)\n",
    "    return tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f742aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "area_grouped_tasks = get_area_grouped_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef56e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"tasks\"] = data_df[\"tasks\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = data_df[\"tasks\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = [area_grouped_tasks[area_grouped_tasks[\"task\"] == t] for t in unique_tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef890ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_area(task):\n",
    "    maybe_area = area_grouped_tasks[area_grouped_tasks[\"task\"] == task][\"area\"]\n",
    "    if len(maybe_area) == 0:\n",
    "        return area_grouped_tasks[\"area\"].sample(1).iloc[0]\n",
    "    else:\n",
    "        return maybe_area.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_non_matching_task(task):\n",
    "    task_area = get_task_area(task)\n",
    "    return (\n",
    "        area_grouped_tasks[area_grouped_tasks[\"area\"] != task_area]\n",
    "        .sample(1)\n",
    "        .iloc[0][\"task\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_non_matching_task(\"atari games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_tasks = dict(\n",
    "    [(t, [get_random_non_matching_task(t) for __ in range(10)]) for t in unique_tasks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(non_matching_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(non_matching_tasks[\"atari games\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d08de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = (\n",
    "    AggregateSimilarityScorer(\n",
    "        sentence_transformer_model,\n",
    "        embedding_features=embedding_features,\n",
    "        hidden_size=512,\n",
    "    )\n",
    "    .cuda()\n",
    "    .half()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = similarity_scorer.get_trainable_params()\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.5, nesterov=True)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b00abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = torch.load(\"output/similarity_scorer.pt\")  # .state_dict()\n",
    "similarity_scorer.transformer_batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cfbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "??prepare_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42831f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = prepare_model_inputs(batch)\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff565497",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34418b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings, file_embeddings = similarity_scorer.embed_files(*inputs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beafe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_outputs(model, input, batch_size=256):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(int(np.ceil(len(input) / batch_size))):\n",
    "            batch_outputs = model(input[i * batch_size : (i + 1) * batch_size])\n",
    "            outputs.append(batch_outputs.cpu().numpy())\n",
    "\n",
    "    return np.row_stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path_file_features(similarity_scorer, data_df, batch_size=128):\n",
    "    index_iterator = get_sample_indices_iterator(\n",
    "        data_df, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    all_path_embeddings = []\n",
    "    all_file_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for indices in tqdm.tqdm(\n",
    "            index_iterator, total=np.ceil(len(data_df) / batch_size)\n",
    "        ):\n",
    "            batch = data_df.iloc[indices]\n",
    "            (\n",
    "                __,\n",
    "                batch_paths,\n",
    "                batch_functions,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            ) = prepare_model_inputs(batch)\n",
    "            path_embeddings, file_embeddings = similarity_scorer.embed_files(\n",
    "                batch_paths,\n",
    "                batch_functions,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            )\n",
    "            all_path_embeddings.append(path_embeddings.cpu().numpy())\n",
    "            all_file_embeddings.append(file_embeddings.cpu().numpy())\n",
    "    return np.row_stack(all_path_embeddings), np.row_stack(all_file_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b36f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df[\"tasks\"] = sample_data_df[\"tasks\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ffe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df = data_df.iloc[:1000].reset_index(drop=True)\n",
    "from sklearn import metrics\n",
    "\n",
    "task_embeddings = get_batched_outputs(\n",
    "    similarity_scorer.embed_tasks, area_grouped_tasks[\"task\"].values\n",
    ")\n",
    "\n",
    "matching_task_embeddings = get_batched_outputs(\n",
    "    similarity_scorer.embed_tasks, sample_data_df[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc59017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_features, file_features = prepare_path_file_features(\n",
    "    similarity_scorer, sample_data_df, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de649ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_path_task_similarity = np.diag(\n",
    "    metrics.pairwise.cosine_similarity(path_features, matching_task_embeddings)\n",
    ")\n",
    "matching_file_task_similarity = np.diag(\n",
    "    metrics.pairwise.cosine_similarity(file_features, matching_task_embeddings)\n",
    ")\n",
    "\n",
    "path_task_similarity = metrics.pairwise.cosine_similarity(\n",
    "    path_features, task_embeddings\n",
    ")\n",
    "file_task_similarity = metrics.pairwise.cosine_similarity(\n",
    "    file_features, task_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(matching_path_task_similarity).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(path_task_similarity.reshape(-1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(matching_file_task_similarity + matching_path_task_similarity).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e529ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((path_task_similarity + file_task_similarity).reshape(-1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_task_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    (path_task_similarity / matching_path_task_similarity[:, np.newaxis]).reshape(-1)\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5662ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    (file_task_similarity / matching_file_task_similarity[:, np.newaxis]).reshape(-1)\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f78285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(matching_path_task_similarity.reshape(-1), label=\"matching\")\n",
    "sns.histplot(path_task_similarity[:30].reshape(-1), label=\"other\", color=\"red\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(matching_file_task_similarity.reshape(-1), label=\"matching\", color=\"red\")\n",
    "plt.show()\n",
    "sns.histplot(file_task_similarity.reshape(-1), label=\"other\", alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5219e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    (matching_path_task_similarity + matching_file_task_similarity),\n",
    "    label=\"matching\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.show()\n",
    "sns.histplot(\n",
    "    (path_task_similarity + file_task_similarity).reshape(-1), label=\"other\", alpha=0.5\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ea502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla pojedynczego negatywnego taska problem jest za Å‚atwy i siÄ™ nie generalizuje - trzeba zrobiÄ‡Â jakiÅ›Â ranking czy coÅ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87089f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlosses = livelossplot.PlotLosses(from_step=1)\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(\"epoch\", epoch)\n",
    "    index_iterator = get_sample_indices_iterator(data_df, batch_size=batch_size)\n",
    "    non_matching_tasks = dict(\n",
    "        [\n",
    "            (t, [get_random_non_matching_task(t) for __ in range(10)])\n",
    "            for t in unique_tasks\n",
    "        ]\n",
    "    )\n",
    "    n_batches = np.ceil(len(data_df) / batch_size)\n",
    "    for i, batch_indices in tqdm.tqdm(enumerate(index_iterator), total=n_batches):\n",
    "        batch = data_df.iloc[batch_indices]\n",
    "        (\n",
    "            batch_tasks,\n",
    "            batch_paths,\n",
    "            batch_functions,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        ) = prepare_model_inputs(batch)\n",
    "        random_tasks = [random.choice(non_matching_tasks[t]) for t in batch_tasks]\n",
    "        scores, negative_scores = similarity_scorer(\n",
    "            batch_tasks,\n",
    "            random_tasks,\n",
    "            batch_paths,\n",
    "            batch_functions,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        scores = scores.sum(dim=-1, keepdim=True)\n",
    "        negative_scores = negative_scores.sum(dim=-1, keepdim=True)\n",
    "        all_scores = torch.cat([scores, negative_scores])\n",
    "        label = torch.ones(len(batch) * 2, 1).cuda().half()\n",
    "        label[batch_size:] = 0\n",
    "\n",
    "        loss = label_smoothing(all_scores, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (\n",
    "            ((scores > negative_scores)).float().mean().item()\n",
    "        )  # detach().cpu().numpy().mean()\n",
    "        plotlosses.update({\"loss\": loss.item(), \"accuracy\": acc})\n",
    "        if i % 5 == 0:\n",
    "            plotlosses.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8edf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30515870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(similarity_scorer, \"output/similarity_scorer.pt\")#.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks = (\n",
    "    repo_tasks.sample(10).explode(\"tasks\").sample(len(batch_tasks))[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40988a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed466ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fadb8e",
   "metadata": {},
   "source": [
    "batch size mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36142b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(functions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings = get_embedding(model, tokenizer, functions_batch).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Sigmoid()(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

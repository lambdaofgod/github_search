{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2061fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp file_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144470b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import ast\n",
    "import tqdm\n",
    "import json\n",
    "import attr\n",
    "from operator import itemgetter\n",
    "\n",
    "from scarce_learn import zero_shot\n",
    "from mlutil.feature_extraction import embeddings\n",
    "import itertools\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction, metrics, model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "from github_search import paperswithcode_tasks\n",
    "\n",
    "import mlutil\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from scarce_learn.zero_shot import devise_jax, devise_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "324de450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d92557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn import model_selection\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import tqdm\n",
    "from transformers import RobertaForCausalLM, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bc6488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3e57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_functions_df = pd.read_csv(\n",
    "    \"output/python_functions.csv\", index_col=\"Unnamed: 0\", nrows=500000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94b5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = all_functions_df.iloc[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253a220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = functions_df[\n",
    "    ~functions_df[\"repo_name\"].isin(\n",
    "        [\n",
    "            \"tensorflow/models\",\n",
    "            \"google-research/google-research\",\n",
    "            \"tensorflow/tensor2tensor\",\n",
    "            \"yumoh/catboost_iter\",\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06e597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_functions_count = functions_df[\"repo_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11861bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_df = functions_df[\n",
    "    functions_df[\"repo_name\"].isin(\n",
    "        repo_functions_count[repo_functions_count > 10].index\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d605e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2bbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperswithcode_with_imports_df = pd.read_csv(\"output/papers_with_imports.csv\")\n",
    "paperswithcode_with_imports_df[\"tasks\"] = paperswithcode_with_imports_df[\"tasks\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39549feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>function_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>ImportGraph</td>\n",
       "      <td>class ImportGraph():\\n    '  Importing and run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/import_graph.py</td>\n",
       "      <td>load_checkpoint</td>\n",
       "      <td>def load_checkpoint(sess, checkpoint_dir, file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>auto_reuse</td>\n",
       "      <td>def auto_reuse(fun):\\n    \"\\n    Wrapper that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>CellWrapper</td>\n",
       "      <td>class CellWrapper(RNNCell):\\n    '\\n    Wrappe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trangvu/ape-npi</td>\n",
       "      <td>translate/models.py</td>\n",
       "      <td>multi_encoder</td>\n",
       "      <td>def multi_encoder(encoder_inputs, encoders, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_modeling_deit.py</td>\n",
       "      <td>DeiTModelIntegrationTest</td>\n",
       "      <td>@require_vision\\nclass DeiTModelIntegrationTes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_feature_extraction_detr.py</td>\n",
       "      <td>DetrFeatureExtractionTester</td>\n",
       "      <td>class DetrFeatureExtractionTester(unittest.Tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_feature_extraction_detr.py</td>\n",
       "      <td>DetrFeatureExtractionTest</td>\n",
       "      <td>@require_torch\\n@require_vision\\nclass DetrFea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_processor_clip.py</td>\n",
       "      <td>CLIPProcessorTest</td>\n",
       "      <td>@require_vision\\nclass CLIPProcessorTest(unitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>huggingface/transformers</td>\n",
       "      <td>tests/test_modeling_tf_distilbert.py</td>\n",
       "      <td>TFDistilBertModelTester</td>\n",
       "      <td>class TFDistilBertModelTester():\\n\\n    def __...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156212 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       repo_name                                   path  \\\n",
       "0                trangvu/ape-npi              translate/import_graph.py   \n",
       "1                trangvu/ape-npi              translate/import_graph.py   \n",
       "2                trangvu/ape-npi                    translate/models.py   \n",
       "3                trangvu/ape-npi                    translate/models.py   \n",
       "4                trangvu/ape-npi                    translate/models.py   \n",
       "...                          ...                                    ...   \n",
       "499995  huggingface/transformers            tests/test_modeling_deit.py   \n",
       "499996  huggingface/transformers  tests/test_feature_extraction_detr.py   \n",
       "499997  huggingface/transformers  tests/test_feature_extraction_detr.py   \n",
       "499998  huggingface/transformers           tests/test_processor_clip.py   \n",
       "499999  huggingface/transformers   tests/test_modeling_tf_distilbert.py   \n",
       "\n",
       "                      function_name  \\\n",
       "0                       ImportGraph   \n",
       "1                   load_checkpoint   \n",
       "2                        auto_reuse   \n",
       "3                       CellWrapper   \n",
       "4                     multi_encoder   \n",
       "...                             ...   \n",
       "499995     DeiTModelIntegrationTest   \n",
       "499996  DetrFeatureExtractionTester   \n",
       "499997    DetrFeatureExtractionTest   \n",
       "499998            CLIPProcessorTest   \n",
       "499999      TFDistilBertModelTester   \n",
       "\n",
       "                                            function_code  \n",
       "0       class ImportGraph():\\n    '  Importing and run...  \n",
       "1       def load_checkpoint(sess, checkpoint_dir, file...  \n",
       "2       def auto_reuse(fun):\\n    \"\\n    Wrapper that ...  \n",
       "3       class CellWrapper(RNNCell):\\n    '\\n    Wrappe...  \n",
       "4       def multi_encoder(encoder_inputs, encoders, en...  \n",
       "...                                                   ...  \n",
       "499995  @require_vision\\nclass DeiTModelIntegrationTes...  \n",
       "499996  class DetrFeatureExtractionTester(unittest.Tes...  \n",
       "499997  @require_torch\\n@require_vision\\nclass DetrFea...  \n",
       "499998  @require_vision\\nclass CLIPProcessorTest(unitt...  \n",
       "499999  class TFDistilBertModelTester():\\n\\n    def __...  \n",
       "\n",
       "[156212 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76503237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      23\n",
       "15      22\n",
       "12      21\n",
       "14      18\n",
       "21      18\n",
       "        ..\n",
       "1200     1\n",
       "743      1\n",
       "406      1\n",
       "531      1\n",
       "528      1\n",
       "Name: path, Length: 246, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_df.groupby(\"repo_name\").agg(\"count\")[\"path\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e108121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers.models\n",
    "\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/codebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216e2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/kuba/.cache/torch/sentence_transformers/microsoft_codebert-base. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "sentence_transformer_model = sentence_transformers.SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484993c",
   "metadata": {},
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Tell pytorch to run this model on the GPU.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name,truncation = True)\n",
    "model = RobertaModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True,\n",
    ")\n",
    "model = model.to(device).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd543e05",
   "metadata": {},
   "source": [
    "\n",
    "path_tokenizer = RobertaTokenizer.from_pretrained(model_name,truncation = True)\n",
    "path_model = RobertaForMaskedLM.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True,\n",
    ").roberta\n",
    "path_model = model.to(device).half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c7ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592b3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_grouped = functions_df.groupby(function_paths).apply(\n",
    "    lambda df: list(df[\"function_code\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "413e1203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1jsingh/rl_navigation agents/bst.py                                    1\n",
       "1jsingh/rl_navigation agents/dqn_agent.py                              1\n",
       "1jsingh/rl_navigation agents/model.py                                  1\n",
       "5yearsKim/Conditional-Normalizing-Flow data/dataset.py                 1\n",
       "5yearsKim/Conditional-Normalizing-Flow inference.py                    1\n",
       "                                                                      ..\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow data/create_synth90k_tfrecord.py     1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/create_crnn_ctc_tfrecord.py    1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_crnn_ctc.py               1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/inference_crnn_ctc.py          1\n",
       "ztoString/CRNN_CTC_OCR_TensorFlow tools/train_crnn_ctc.py              1\n",
       "Length: 24053, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_grouped.groupby(functions_grouped.index).agg(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4447df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Node(object):\\n\\n    def __init__(self, data):\\n        self.data = data\\n        self.freq = 1\\n        self.leftchild = None\\n        self.rightchild = None\\n\\n    def assign(self, another_node):\\n        self.data = another_node.data\\n        self.freq = another_node.freq\\n        self.leftchild = another_node.leftchild\\n        self.rightchild = another_node.rightchild\\n\\n    def remove_child(self, child_node):\\n        assert ((child_node == self.leftchild) or (child_node == self.rightchild))\\n        if (self.data < child_node.data):\\n            self.rightchild = None\\n        else:\\n            self.leftchild = None',\n",
       " \"class FixedSize_BinarySearchTree(object):\\n\\n    def __init__(self, capacity):\\n        self.capacity = capacity\\n        self.size = 0\\n        self.values = deque(maxlen=capacity)\\n        self.value_sum = 0\\n        self.root = None\\n\\n    def update(self, value, idx, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        assert (idx < self.size)\\n        self.remove(self.values[idx])\\n        self.insert(value)\\n        self.value_sum += (value - self.values[idx])\\n        self.values[idx] = value\\n        self.size += 1\\n\\n    def insert(self, value, node=None):\\n        '\\\\n\\\\t\\\\tupdate tree node value\\\\n\\\\t\\\\t'\\n        if (self.root is None):\\n            self.root = Node(value)\\n            assert (len(self.values) == 0)\\n        elif (node is None):\\n            return self.insert(value, node=self.root)\\n        elif (value == node.data):\\n            node.freq += 1\\n        elif (value > node.data):\\n            if node.rightchild:\\n                return self.insert(value, node=node.rightchild)\\n            else:\\n                node.rightchild = Node(value)\\n        elif node.leftchild:\\n            return self.insert(value, node.leftchild)\\n        else:\\n            node.leftchild = Node(value)\\n\\n    def add(self, value, node=None):\\n        '\\\\n\\\\t\\\\tadd tree node\\\\n\\\\t\\\\t'\\n        if (self.size == self.capacity):\\n            self.remove(self.values[0])\\n            self.value_sum -= self.values[0]\\n        self.insert(value)\\n        self.value_sum += value\\n        self.values.append(value)\\n        self.size += 1\\n\\n    def search(self, value):\\n        '\\\\n\\\\t\\\\tsearch for node with a particular value in the tree\\\\n\\\\t\\\\t'\\n        parent_node = None\\n        node = self.root\\n        while ((node is not None) and (value != node.data)):\\n            parent_node = node\\n            if (value > node.data):\\n                node = node.rightchild\\n            else:\\n                node = node.leftchild\\n        return (parent_node, node)\\n\\n    def RightMinChild(self, node):\\n        '\\\\n\\\\t\\\\tget min value subchild for a node\\\\n\\\\t\\\\t'\\n        assert node.rightchild, 'there is no right child for the given node'\\n        parent_node = node\\n        node = node.rightchild\\n        while node.leftchild:\\n            parent_node = node\\n            node = node.leftchild\\n        return (parent_node, node)\\n\\n    def remove(self, value):\\n        '\\\\n\\\\t\\\\tremove tree node\\\\n\\\\t\\\\t'\\n        (parent_node, node) = self.search(value)\\n        if (node is None):\\n            raise Exception('binary search tree has no node with value: {}'.format(value))\\n        elif (node.freq >= 2):\\n            node.freq -= 1\\n        elif ((node.rightchild is None) and (node.leftchild is None)):\\n            if parent_node:\\n                parent_node.remove_child(node)\\n            else:\\n                self.root = None\\n        elif (node.rightchild is None):\\n            node.assign(node.leftchild)\\n        elif (node.leftchild is None):\\n            node.assign(node.rightchild)\\n        else:\\n            (parent_min_node, min_value_node) = self.RightMinChild(node)\\n            (temp_data, temp_freq) = (min_value_node.data, min_value_node.freq)\\n            if min_value_node.rightchild:\\n                min_value_node.assign(min_value_node.rightchild)\\n            else:\\n                parent_min_node.remove_child(min_value_node)\\n            node.data = temp_data\\n            node.freq = temp_freq\\n        self.size -= 1\\n\\n    def max_value(self):\\n        assert (self.size != 0)\\n        node = self.root\\n        while (node.rightchild is not None):\\n            node = node.rightchild\\n        return node.data\\n\\n    def __len__(self):\\n        return self.size\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        θ_target = τ*θ_local + (1 - τ)*θ_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        θ_target = τ*θ_local + (1 - τ)*θ_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\",\n",
       " 'class Agent():\\n\\n    def __init__(self, state_size, action_size, num_agents, gamma=0.99, lr=0.001, buffer_size=int(1000000.0), batch_size=128, tau=0.001, random_seed=0):\\n        self.qnet_local = Qnetwork(state_size, action_size).to(device)\\n        self.qnet_target = Qnetwork(state_size, action_size).to(device)\\n        self.soft_update(tau=1.0)\\n        self.memory = ReplayBuffer(buffer_size, random_seed)\\n        self.num_agents = num_agents\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        self.buffer_size = buffer_size\\n        self.batch_size = batch_size\\n        self.gamma = gamma\\n        self.lr = lr\\n        self.tau = tau\\n        self.t_step = 0\\n        self.seed = random.seed(random_seed)\\n        self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=self.lr)\\n\\n    def step(self, state, action, reward, next_state, done):\\n        \\' saves the step info in the memory buffer and perform a learning iteration\\\\n        Input : \\\\n            state,action,reward,state,done : non-batched numpy arrays\\\\n        \\\\n        Output : \\\\n            none\\\\n        \\'\\n        max_priority = self.memory._get_max_priority()\\n        for i in range(self.num_agents):\\n            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i], max_priority)\\n        self.t_step = ((self.t_step + 1) % UPDATE_EVERY)\\n        if ((self.t_step == 0) and (len(self.memory) > self.batch_size)):\\n            experiences = self.memory.sample(self.batch_size)\\n            self.learn(experiences)\\n\\n    def learn(self, experiences):\\n        \\' perform a learning iteration by using sampled experience batch\\\\n        Input : \\\\n            experience : tuple from the memory buffer\\\\n            states, actions, rewards, next_states, dones = experiences\\\\n            eg : states.shape = [N,state_size]\\\\n        Output : \\\\n            none\\\\n        \\'\\n        (states, actions, rewards, next_states, dones, idxs, is_weights) = experiences\\n        self.optimizer.zero_grad()\\n        q_pred = self.qnet_local.forward(states).gather(1, actions)\\n        next_action_local = self.qnet_local.forward(next_states).max(1)[1]\\n        q_target = (rewards + ((self.gamma * (1 - dones)) * self.qnet_target.forward(next_states)[(range(self.batch_size), next_action_local)].unsqueeze(1)))\\n        td_error = (q_target - q_pred)\\n        self.memory.update_priorities(idxs, td_error.detach().cpu().numpy().squeeze())\\n        loss = ((is_weights * td_error) ** 2).mean()\\n        loss.backward()\\n        self.optimizer.step()\\n        self.soft_update(self.tau)\\n\\n    def act(self, state, eps=0.0):\\n        \" return the local model\\'s predicted action for the given state\\\\n        Input : \\\\n            state : [state_size]\\\\n        \\\\n        Output : \\\\n            action : scalar action as action space is discrete with dim = 1\\\\n        \"\\n        state = torch.from_numpy(state).float().to(device)\\n        self.qnet_local.eval()\\n        with torch.no_grad():\\n            max_actions = np.argmax(self.qnet_local(state).cpu().data.numpy(), axis=1)\\n        self.qnet_local.train()\\n        rand_num = np.random.rand(self.num_agents)\\n        rand_actions = np.random.randint(low=0, high=self.action_size, size=self.num_agents)\\n        check = (rand_num < eps)\\n        action = ((check * rand_actions) + (np.invert(check) * max_actions))\\n        return action\\n\\n    def soft_update(self, tau):\\n        \\'Soft update model parameters.\\\\n        θ_target = τ*θ_local + (1 - τ)*θ_target\\\\n\\\\n        \\'\\n        for (target_param, local_param) in zip(self.qnet_target.parameters(), self.qnet_local.parameters()):\\n            target_param.data.copy_(((tau * local_param.data) + ((1.0 - tau) * target_param.data)))',\n",
       " \"class ReplayBuffer():\\n\\n    def __init__(self, buffer_size, seed, alpha=0.4, beta=0.4):\\n        self.buffer = deque(maxlen=buffer_size)\\n        self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\\n        self.tree = FixedSize_BinarySearchTree(capacity=buffer_size)\\n        self.epsilon = 1e-05\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.beta_increment_per_sampling = 0.001\\n        self.base_priority = (self.epsilon ** self.alpha)\\n\\n    def add(self, state, action, reward, next_state, done, max_priority):\\n        self.tree.add(max_priority)\\n        e = self.experience(state, action, reward, next_state, done)\\n        self.buffer.append(e)\\n\\n    def _get_max_priority(self):\\n        try:\\n            max_priority = self.tree.max_value()\\n        except:\\n            max_priority = self.base_priority\\n        return max_priority\\n\\n    def update_priorities(self, idxs, td_errors):\\n        new_priorities = (np.abs(td_errors) ** self.alpha)\\n        for (idx, new_priority) in zip(idxs, new_priorities):\\n            self.tree.update(new_priority, idx)\\n\\n    def sample(self, batch_size):\\n        sampling_probabilities = (np.array(self.tree.values) / self.tree.value_sum)\\n        idxs = np.random.choice(range(self.tree.size), batch_size, replace=False, p=sampling_probabilities)\\n        sampling_probabilities = sampling_probabilities[idxs]\\n        experiences = [self.buffer[i] for i in idxs]\\n        is_weights = np.power((self.tree.size * sampling_probabilities), (- self.beta))\\n        is_weights /= is_weights.max()\\n        is_weights = torch.from_numpy(np.vstack(is_weights)).float().to(device)\\n        self.beta = min(1.0, (self.beta + self.beta_increment_per_sampling))\\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if (e is not None)])).float().to(device)\\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if (e is not None)])).long().to(device)\\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if (e is not None)])).float().to(device)\\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if (e is not None)])).float().to(device)\\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if (e is not None)]).astype(np.uint8)).float().to(device)\\n        return (states, actions, rewards, next_states, dones, idxs, is_weights)\\n\\n    def __len__(self):\\n        return len(self.buffer)\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(functions_grouped.iloc[:2], start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b46993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef135b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trangvu/ape-npi translate/import_graph.py    [class ImportGraph():\\n    '  Importing and ru...\n",
       "trangvu/ape-npi translate/import_graph.py    [class ImportGraph():\\n    '  Importing and ru...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "trangvu/ape-npi translate/models.py          [def auto_reuse(fun):\\n    \"\\n    Wrapper that...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions_grouped[function_paths.iloc[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb6879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tasks = paperswithcode_with_imports_df[[\"repo\", \"tasks\"]].groupby(\"repo\").agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d89a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_dataset(repo_tasks, functions_df):\n",
    "    functions_df = functions_df.merge(repo_tasks, left_on=\"repo_name\", right_on=\"repo\")\n",
    "    function_paths = functions_df[\"repo_name\"] + \" \" + functions_df[\"path\"]\n",
    "    functions_groupby = functions_df.groupby(function_paths.values)\n",
    "    functions_grouped = functions_groupby.apply(\n",
    "        lambda df: frozenset(df[\"function_code\"])\n",
    "    )\n",
    "    tasks_grouped = functions_groupby.apply(lambda df: list(df[\"tasks\"]))\n",
    "    data_df = (\n",
    "        pd.DataFrame({\"tasks\": tasks_grouped, \"functions\": functions_grouped})\n",
    "        .explode(\"tasks\")\n",
    "        .explode(\"tasks\")\n",
    "        .reset_index()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    data_df[\"functions\"] = data_df[\"functions\"].apply(list)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3addd639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tasks</th>\n",
       "      <th>functions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1jsingh/rl_navigation agents/bst.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_normal.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_minkowski.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_mdm.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geomstats/geomstats tests/tests_geomstats/test_matrices.py</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_02122019.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_09132018_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_10142018_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbardool/mrcnn3 mrcnn/ArchivedCode/fcn_scoring_layer_02052019_.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lincaiming/py-faster-rcnn-update caffe-fast-rcnn/python/classify.py</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tasks  functions\n",
       "index                                                               \n",
       "1jsingh/rl_navigation agents/bst.py                     1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "geomstats/geomstats tests/tests_geomstats/test_...      1          1\n",
       "...                                                   ...        ...\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_02...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_09...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/model_fcn_10...     13         13\n",
       "kbardool/mrcnn3 mrcnn/ArchivedCode/fcn_scoring_...     13         13\n",
       "lincaiming/py-faster-rcnn-update caffe-fast-rcn...     13         13\n",
       "\n",
       "[18400 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_dataset(repo_tasks, functions_df).groupby(\"index\").agg(\"count\").sort_values(\n",
    "    \"functions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ca3279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices_iterator(data_df, batch_size=8, shuffle=True):\n",
    "    sample_index = data_df.index.values\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    for i in range(0, len(sample_index), batch_size):\n",
    "        yield sample_index[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "262494ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_file_dataset(repo_tasks, functions_df)\n",
    "sample_index = data_df.index.values\n",
    "np.random.shuffle(sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a282230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tasks</th>\n",
       "      <th>functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112781</th>\n",
       "      <td>1jsingh/rl_navigation agents/bst.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class FixedSize_BinarySearchTree(object):\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>1jsingh/rl_navigation agents/dqn_agent.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class ReplayBuffer():\\n\\n    def __init__(sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248440</th>\n",
       "      <td>1jsingh/rl_navigation agents/model.py</td>\n",
       "      <td>Atari Games</td>\n",
       "      <td>[class Qnetwork(nn.Module):\\n\\n    def __init_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65105</th>\n",
       "      <td>5yearsKim/Conditional-Normalizing-Flow data/da...</td>\n",
       "      <td>Colorization</td>\n",
       "      <td>[class ImgDatasets(torch.utils.data.Dataset):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65621</th>\n",
       "      <td>5yearsKim/Conditional-Normalizing-Flow data/da...</td>\n",
       "      <td>Image Generation</td>\n",
       "      <td>[class ImgDatasets(torch.utils.data.Dataset):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218204</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_c...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24874</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...</td>\n",
       "      <td>Scene Text</td>\n",
       "      <td>[def _int_to_string(value, char_map_dict=None)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112880</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _int_to_string(value, char_map_dict=None)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108265</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...</td>\n",
       "      <td>Scene Text</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177723</th>\n",
       "      <td>ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...</td>\n",
       "      <td>Scene Text Recognition</td>\n",
       "      <td>[def _read_tfrecord(tfrecord_path, num_epochs=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43986 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    index  \\\n",
       "112781                1jsingh/rl_navigation agents/bst.py   \n",
       "9923            1jsingh/rl_navigation agents/dqn_agent.py   \n",
       "248440              1jsingh/rl_navigation agents/model.py   \n",
       "65105   5yearsKim/Conditional-Normalizing-Flow data/da...   \n",
       "65621   5yearsKim/Conditional-Normalizing-Flow data/da...   \n",
       "...                                                   ...   \n",
       "218204  ztoString/CRNN_CTC_OCR_TensorFlow tools/eval_c...   \n",
       "24874   ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...   \n",
       "112880  ztoString/CRNN_CTC_OCR_TensorFlow tools/infere...   \n",
       "108265  ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...   \n",
       "177723  ztoString/CRNN_CTC_OCR_TensorFlow tools/train_...   \n",
       "\n",
       "                         tasks  \\\n",
       "112781             Atari Games   \n",
       "9923               Atari Games   \n",
       "248440             Atari Games   \n",
       "65105             Colorization   \n",
       "65621         Image Generation   \n",
       "...                        ...   \n",
       "218204  Scene Text Recognition   \n",
       "24874               Scene Text   \n",
       "112880  Scene Text Recognition   \n",
       "108265              Scene Text   \n",
       "177723  Scene Text Recognition   \n",
       "\n",
       "                                                functions  \n",
       "112781  [class FixedSize_BinarySearchTree(object):\\n\\n...  \n",
       "9923    [class ReplayBuffer():\\n\\n    def __init__(sel...  \n",
       "248440  [class Qnetwork(nn.Module):\\n\\n    def __init_...  \n",
       "65105   [class ImgDatasets(torch.utils.data.Dataset):\\...  \n",
       "65621   [class ImgDatasets(torch.utils.data.Dataset):\\...  \n",
       "...                                                   ...  \n",
       "218204  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "24874   [def _int_to_string(value, char_map_dict=None)...  \n",
       "112880  [def _int_to_string(value, char_map_dict=None)...  \n",
       "108265  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "177723  [def _read_tfrecord(tfrecord_path, num_epochs=...  \n",
       "\n",
       "[43986 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4750096",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_iter = get_sample_indices_iterator(data_df, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a53e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_inputs(batch):\n",
    "    indices = list(np.cumsum(batch[\"functions\"].apply(len).values))\n",
    "    file_function_idxs_start = np.array([0] + indices[:-1])\n",
    "    file_function_idxs_end = indices\n",
    "    reshaped_batch = batch.explode(\"functions\")\n",
    "\n",
    "    return (\n",
    "        batch[\"tasks\"].values,\n",
    "        batch[\"index\"].values,\n",
    "        reshaped_batch[\"functions\"].values,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f6f1bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_sample_indices_iterator at 0x7f48bb10a2e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817407d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3627\u001b[0m         \"\"\"\n\u001b[0;32m-> 3628\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3615\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3616\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7692d2ad11db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_function_idxs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_function_idxs_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "batch = data_df.iloc[next(iter(index_iter))]\n",
    "(\n",
    "    batch_tasks,\n",
    "    batch_paths,\n",
    "    batch_functions,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ") = prepare_model_inputs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17127ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_function_idxs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_function_idxs_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfc8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f95bf11",
   "metadata": {},
   "source": [
    "# Idea: use thinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, tokenizer, texts, max_length=512):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, max_length=max_length)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"][:, :max_length]\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, :max_length]\n",
    "    inputs = inputs.to(model.device)\n",
    "    return model(**inputs).last_hidden_state.mean(axis=1)\n",
    "\n",
    "\n",
    "# path_embeddings = get_embeddings(model, tokenizer, paths_batch)#.last_hidden_state.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_embeddings = get_embeddings(model, tokenizer, functions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeeb2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions_embeddings_reshaped = function_embeddings.repeat((5,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zrób dla jednego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a678f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_file_functions = function_embeddings[file_function_idxs_start[0]:file_function_idxs_end[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23233f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embeddings.shape\n",
    "# first_file_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionSelector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossAttentionSelector, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        path_embeddings,\n",
    "        function_embeddings,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        attn = F.sigmoid(path_embeddings @ function_embeddings.T)\n",
    "        attn_mask = torch.ones_like(attn).cuda().half()\n",
    "        for i in range(len(file_function_idxs_start)):\n",
    "            attn_mask[i, : file_function_idxs_start[i]] = 0\n",
    "            attn_mask[i, file_function_idxs_end[i] :] = 0\n",
    "        return (attn_mask * attn) @ function_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = CrossAttentionSelector().cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0001fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathBasedFileEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_features, hidden_size, dropout_prob=0.5):\n",
    "        super(PathBasedFileEmbedder, self).__init__()\n",
    "        self.cross_attention_pooler = CrossAttentionSelector()\n",
    "        self.path_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.function_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        path_embeddings,\n",
    "        function_embeddings,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        path_nl_embeddings = F.leaky_relu(self.bn1(self.path_dense(path_embeddings)))\n",
    "        function_nl_embeddings = F.leaky_relu(\n",
    "            self.bn2(self.function_dense(function_embeddings))\n",
    "        )\n",
    "        file_embeddings = self.cross_attention_pooler(\n",
    "            path_nl_embeddings,\n",
    "            function_nl_embeddings,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        return (path_nl_embeddings, F.leaky_relu(self.bn3(file_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e71700",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_embedder = PathBasedFileEmbedder(768, 20).cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings = sentence_transformer_model.encode(\n",
    "    batch_paths, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()\n",
    "function_embeddings = sentence_transformer_model.encode(\n",
    "    batch_functions, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()\n",
    "task_embeddings = sentence_transformer_model.encode(\n",
    "    batch_tasks, convert_to_numpy=False, convert_to_tensor=True\n",
    ").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2703b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector(\n",
    "    path_embeddings,\n",
    "    function_embeddings,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1794757",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(path_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81e1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleSimilarityHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(TripleSimilarityHead, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, u, v, reference):\n",
    "        # imilarity_features = torch.column_stack(\n",
    "        #    [u, v, torch.abs(u-reference), torch.abs(v-reference)]\n",
    "        # )\n",
    "        # hidden = F.tanh(self.bn(self.dense(similarity_features)))\n",
    "        return torch.column_stack(\n",
    "            [\n",
    "                torch.cosine_similarity(u, reference),\n",
    "                torch.cosine_similarity(v, reference),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add19ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(5))\n",
    "\n",
    "a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateSimilarityScorer(nn.Module):\n",
    "    def __init__(\n",
    "        self, sentence_transformer, embedding_features, hidden_size, dropout_prob=0.5\n",
    "    ):\n",
    "        super(AggregateSimilarityScorer, self).__init__()\n",
    "        self.sentence_transformer = sentence_transformer\n",
    "        self.file_embedder = PathBasedFileEmbedder(embedding_features, hidden_size)\n",
    "        self.task_dense = nn.Linear(embedding_features, hidden_size)\n",
    "        self.similarity_head = TripleSimilarityHead(hidden_size)\n",
    "        self.bn_task = nn.BatchNorm1d(hidden_size)\n",
    "        self.transformer_batch_size = 512\n",
    "\n",
    "    def get_trainable_params(self):\n",
    "        return itertools.chain(\n",
    "            self.file_embedder.parameters(),\n",
    "            self.task_dense.parameters(),\n",
    "            self.similarity_head.parameters(),\n",
    "            self.bn_task.parameters(),\n",
    "        )\n",
    "\n",
    "    def embed_tasks(self, tasks):\n",
    "        with torch.no_grad():\n",
    "            base_embeddings = self.sentence_transformer.encode(\n",
    "                tasks, convert_to_numpy=False, convert_to_tensor=True\n",
    "            ).half()\n",
    "        task_embeddings = F.leaky_relu(self.bn_task(self.task_dense(base_embeddings)))\n",
    "        return task_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tasks,\n",
    "        tasks_negative,\n",
    "        paths,\n",
    "        functions,\n",
    "        file_function_idxs_start,\n",
    "        file_function_idxs_end,\n",
    "    ):\n",
    "        all_inputs = list(tasks) + list(paths) + list(functions) + list(tasks_negative)\n",
    "        embeddings = self.sentence_transformer.encode(\n",
    "            all_inputs,\n",
    "            convert_to_numpy=False,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=self.transformer_batch_size,\n",
    "        ).half()\n",
    "        base_task_embeddings = embeddings[: len(tasks)]\n",
    "        base_path_embeddings = embeddings[len(tasks) : len(tasks) + len(paths)]\n",
    "        function_embeddings = embeddings[len(tasks) + len(paths) : -len(tasks_negative)]\n",
    "        base_negative_task_embeddings = embeddings[-len(tasks_negative) :]\n",
    "        task_embeddings = F.leaky_relu(\n",
    "            self.bn_task(self.task_dense(base_task_embeddings))\n",
    "        )\n",
    "        negative_task_embeddings = F.leaky_relu(\n",
    "            self.bn_task(self.task_dense(base_negative_task_embeddings))\n",
    "        )\n",
    "        path_embeddings, file_embeddings = self.file_embedder(\n",
    "            base_path_embeddings,\n",
    "            function_embeddings,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        positive_similarity = self.similarity_head(\n",
    "            path_embeddings, file_embeddings, task_embeddings\n",
    "        )\n",
    "        negative_similarity = self.similarity_head(\n",
    "            path_embeddings, file_embeddings, negative_task_embeddings\n",
    "        )\n",
    "        return positive_similarity, negative_similarity\n",
    "\n",
    "    def embed_files(\n",
    "        self, paths, functions, file_function_idxs_start, file_function_idxs_end\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            path_embeddings = self.sentence_transformer.encode(\n",
    "                paths,\n",
    "                convert_to_numpy=False,\n",
    "                convert_to_tensor=True,\n",
    "                batch_size=self.transformer_batch_size,\n",
    "            ).half()\n",
    "            function_embeddings = self.sentence_transformer.encode(\n",
    "                functions,\n",
    "                convert_to_numpy=False,\n",
    "                convert_to_tensor=True,\n",
    "                batch_size=self.transformer_batch_size,\n",
    "            ).half()\n",
    "            path_embeddings, file_embeddings = self.file_embedder(\n",
    "                path_embeddings,\n",
    "                function_embeddings,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            )\n",
    "        return path_embeddings, file_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273dcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_embeddings.shape#* functions_embeddings_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd42749",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = sentence_transformer_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = (\n",
    "    AggregateSimilarityScorer(\n",
    "        sentence_transformer_model,\n",
    "        embedding_features=embedding_features,\n",
    "        hidden_size=512,\n",
    "    )\n",
    "    .cuda()\n",
    "    .half()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_batch = [\"classification\"] + [\"domain adaptation\"] * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9abdd",
   "metadata": {},
   "source": [
    "base_path_embeddings = get_embeddings(model, tokenizer, paths_batch)\n",
    "function_embeddings = get_embeddings(model, tokenizer, functions_batch)\n",
    "base_task_embeddings = get_embeddings(model, tokenizer, tasks_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a38bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be610d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tasks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71af065",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82862ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = similarity_scorer(\n",
    "    batch_tasks,\n",
    "    random_tasks,\n",
    "    batch_paths,\n",
    "    batch_functions,\n",
    "    file_function_idxs_start,\n",
    "    file_function_idxs_end,\n",
    ")\n",
    "# negative_scores = similarity_scorer(random_tasks, batch_paths, batch_functions, file_function_idxs_start, file_function_idxs_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5339d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468c238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483a1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.logsigmoid(x)\n",
    "            minus_logprobs = torch.nn.functional.logsigmoid(-x)\n",
    "\n",
    "            target_smoothed = target * (1 - self.smoothing) + self.smoothing / 2\n",
    "            smoothed_loss = -logprobs * target_smoothed - minus_logprobs * (\n",
    "                1 - target_smoothed\n",
    "            )\n",
    "            return smoothed_loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc288340",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.tensor([-100, 100]).float().reshape(2, 1)  # ('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015aaeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0, 1]).reshape(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32da209",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af187220",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing(log_probs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperswithcode\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_area_grouped_tasks():\n",
    "    client = paperswithcode.PapersWithCodeClient()\n",
    "    areas = client.area_list().results\n",
    "    s = 0\n",
    "\n",
    "    area_grouped_tasks = {}\n",
    "\n",
    "    for a in areas:\n",
    "        area_tasks = [\n",
    "            t.id for t in client.area_task_list(a.id, items_per_page=1000).results\n",
    "        ]\n",
    "        area_grouped_tasks[a.id] = area_tasks\n",
    "        n_tasks_per_area = len(area_tasks)\n",
    "        print(a.id, \":\", n_tasks_per_area)\n",
    "        s += n_tasks_per_area\n",
    "    tasks_df = pd.DataFrame(\n",
    "        {\"area\": area_grouped_tasks.keys(), \"task\": area_grouped_tasks.values()}\n",
    "    ).explode(\"task\")\n",
    "    tasks_df[\"task\"] = tasks_df[\"task\"].str.split(\"-\").apply(\" \".join)\n",
    "    return tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f742aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "area_grouped_tasks = get_area_grouped_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef56e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"tasks\"] = data_df[\"tasks\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tasks = data_df[\"tasks\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = [area_grouped_tasks[area_grouped_tasks[\"task\"] == t] for t in unique_tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef890ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_area(task):\n",
    "    maybe_area = area_grouped_tasks[area_grouped_tasks[\"task\"] == task][\"area\"]\n",
    "    if len(maybe_area) == 0:\n",
    "        return area_grouped_tasks[\"area\"].sample(1).iloc[0]\n",
    "    else:\n",
    "        return maybe_area.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_non_matching_task(task):\n",
    "    task_area = get_task_area(task)\n",
    "    return (\n",
    "        area_grouped_tasks[area_grouped_tasks[\"area\"] != task_area]\n",
    "        .sample(1)\n",
    "        .iloc[0][\"task\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_non_matching_task(\"atari games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_tasks = dict(\n",
    "    [(t, [get_random_non_matching_task(t) for __ in range(10)]) for t in unique_tasks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(non_matching_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(non_matching_tasks[\"atari games\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d08de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = (\n",
    "    AggregateSimilarityScorer(\n",
    "        sentence_transformer_model,\n",
    "        embedding_features=embedding_features,\n",
    "        hidden_size=512,\n",
    "    )\n",
    "    .cuda()\n",
    "    .half()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = similarity_scorer.get_trainable_params()\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.5, nesterov=True)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b00abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = torch.load(\"output/similarity_scorer.pt\")  # .state_dict()\n",
    "similarity_scorer.transformer_batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cfbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "??prepare_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42831f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = prepare_model_inputs(batch)\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff565497",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34418b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings, file_embeddings = similarity_scorer.embed_files(*inputs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beafe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_outputs(model, input, batch_size=256):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(int(np.ceil(len(input) / batch_size))):\n",
    "            batch_outputs = model(input[i * batch_size : (i + 1) * batch_size])\n",
    "            outputs.append(batch_outputs.cpu().numpy())\n",
    "\n",
    "    return np.row_stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_path_file_features(similarity_scorer, data_df, batch_size=128):\n",
    "    index_iterator = get_sample_indices_iterator(\n",
    "        data_df, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    all_path_embeddings = []\n",
    "    all_file_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for indices in tqdm.tqdm(\n",
    "            index_iterator, total=np.ceil(len(data_df) / batch_size)\n",
    "        ):\n",
    "            batch = data_df.iloc[indices]\n",
    "            (\n",
    "                __,\n",
    "                batch_paths,\n",
    "                batch_functions,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            ) = prepare_model_inputs(batch)\n",
    "            path_embeddings, file_embeddings = similarity_scorer.embed_files(\n",
    "                batch_paths,\n",
    "                batch_functions,\n",
    "                file_function_idxs_start,\n",
    "                file_function_idxs_end,\n",
    "            )\n",
    "            all_path_embeddings.append(path_embeddings.cpu().numpy())\n",
    "            all_file_embeddings.append(file_embeddings.cpu().numpy())\n",
    "    return np.row_stack(all_path_embeddings), np.row_stack(all_file_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b36f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df[\"tasks\"] = sample_data_df[\"tasks\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ffe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df = data_df.iloc[:1000].reset_index(drop=True)\n",
    "from sklearn import metrics\n",
    "\n",
    "task_embeddings = get_batched_outputs(\n",
    "    similarity_scorer.embed_tasks, area_grouped_tasks[\"task\"].values\n",
    ")\n",
    "\n",
    "matching_task_embeddings = get_batched_outputs(\n",
    "    similarity_scorer.embed_tasks, sample_data_df[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc59017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_features, file_features = prepare_path_file_features(\n",
    "    similarity_scorer, sample_data_df, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de649ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_path_task_similarity = np.diag(\n",
    "    metrics.pairwise.cosine_similarity(path_features, matching_task_embeddings)\n",
    ")\n",
    "matching_file_task_similarity = np.diag(\n",
    "    metrics.pairwise.cosine_similarity(file_features, matching_task_embeddings)\n",
    ")\n",
    "\n",
    "path_task_similarity = metrics.pairwise.cosine_similarity(\n",
    "    path_features, task_embeddings\n",
    ")\n",
    "file_task_similarity = metrics.pairwise.cosine_similarity(\n",
    "    file_features, task_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(matching_path_task_similarity).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(path_task_similarity.reshape(-1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(matching_file_task_similarity + matching_path_task_similarity).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e529ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((path_task_similarity + file_task_similarity).reshape(-1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_task_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    (path_task_similarity / matching_path_task_similarity[:, np.newaxis]).reshape(-1)\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5662ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    (file_task_similarity / matching_file_task_similarity[:, np.newaxis]).reshape(-1)\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f78285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(matching_path_task_similarity.reshape(-1), label=\"matching\")\n",
    "sns.histplot(path_task_similarity[:30].reshape(-1), label=\"other\", color=\"red\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(matching_file_task_similarity.reshape(-1), label=\"matching\", color=\"red\")\n",
    "plt.show()\n",
    "sns.histplot(file_task_similarity.reshape(-1), label=\"other\", alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5219e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    (matching_path_task_similarity + matching_file_task_similarity),\n",
    "    label=\"matching\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.show()\n",
    "sns.histplot(\n",
    "    (path_task_similarity + file_task_similarity).reshape(-1), label=\"other\", alpha=0.5\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ea502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla pojedynczego negatywnego taska problem jest za łatwy i się nie generalizuje - trzeba zrobić jakiś ranking czy coś"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87089f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlosses = livelossplot.PlotLosses(from_step=1)\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(\"epoch\", epoch)\n",
    "    index_iterator = get_sample_indices_iterator(data_df, batch_size=batch_size)\n",
    "    non_matching_tasks = dict(\n",
    "        [\n",
    "            (t, [get_random_non_matching_task(t) for __ in range(10)])\n",
    "            for t in unique_tasks\n",
    "        ]\n",
    "    )\n",
    "    n_batches = np.ceil(len(data_df) / batch_size)\n",
    "    for i, batch_indices in tqdm.tqdm(enumerate(index_iterator), total=n_batches):\n",
    "        batch = data_df.iloc[batch_indices]\n",
    "        (\n",
    "            batch_tasks,\n",
    "            batch_paths,\n",
    "            batch_functions,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        ) = prepare_model_inputs(batch)\n",
    "        random_tasks = [random.choice(non_matching_tasks[t]) for t in batch_tasks]\n",
    "        scores, negative_scores = similarity_scorer(\n",
    "            batch_tasks,\n",
    "            random_tasks,\n",
    "            batch_paths,\n",
    "            batch_functions,\n",
    "            file_function_idxs_start,\n",
    "            file_function_idxs_end,\n",
    "        )\n",
    "        scores = scores.sum(dim=-1, keepdim=True)\n",
    "        negative_scores = negative_scores.sum(dim=-1, keepdim=True)\n",
    "        all_scores = torch.cat([scores, negative_scores])\n",
    "        label = torch.ones(len(batch) * 2, 1).cuda().half()\n",
    "        label[batch_size:] = 0\n",
    "\n",
    "        loss = label_smoothing(all_scores, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (\n",
    "            ((scores > negative_scores)).float().mean().item()\n",
    "        )  # detach().cpu().numpy().mean()\n",
    "        plotlosses.update({\"loss\": loss.item(), \"accuracy\": acc})\n",
    "        if i % 5 == 0:\n",
    "            plotlosses.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8edf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30515870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(similarity_scorer, \"output/similarity_scorer.pt\")#.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks = (\n",
    "    repo_tasks.sample(10).explode(\"tasks\").sample(len(batch_tasks))[\"tasks\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40988a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed466ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fadb8e",
   "metadata": {},
   "source": [
    "batch size mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36142b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(functions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings = get_embedding(model, tokenizer, functions_batch).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Sigmoid()(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp github_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_python_files(owner, repo_name):\n",
    "    url_template = 'https://api.github.com/repos/{}/{}/git/trees/master?recursive=1'\n",
    "    files = json.loads(requests.get(url_template.format(owner, repo_name)).text)['tree']\n",
    "\n",
    "    for maybe_file in files:\n",
    "        if maybe_file['type'] == 'blob' and maybe_file['path'][-3:] == '.py':\n",
    "            path = maybe_file['path']\n",
    "            raw_file_url_template = 'https://raw.githubusercontent.com/{}/{}/master/{}'\n",
    "            raw_file_url = raw_file_url_template.format(owner, repo_name, path)\n",
    "            yield owner, repo_name, path, requests.get(raw_file_url).text, maybe_file['sha'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_owner = 'lambdaofgod'\n",
    "example_repo = 'mlutil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def get_python_files_df(owner, repo_name):\n",
    "    file_tuples = list(get_python_files(owner, repo_name))\n",
    "    if len(file_tuples) > 0:\n",
    "        df = pd.DataFrame.from_records(file_tuples)\n",
    "        df.columns = ['owner', 'repo_name', 'file_path', 'content', 'sha']\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_files_df = get_python_files_df(example_owner, example_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import gzip\\nfrom typing import Dict, Callable\\nimport numpy as np\\n\\n\\nclass CompressedKeyedVectors(object):\\n\\n    def __init__(self, vocab_path: str, embedding_path: str, to_lowercase: bool=True):\\n        \"\"\"\\n        Class from sdadas polish-nlp-resources\\n        https://github.com/sdadas/polish-nlp-resources\\n        I need to get it somewhere from where I can import it easily for using with custom BentoML model\\n        \"\"\"\\n        self.vocab_path: str = vocab_path\\n        self.embedding_path: str = embedding_path\\n        self.to_lower: bool = to_lowercase\\n        self.vocab: Dict[str, int] = self.__load_vocab(vocab_path)\\n        embedding = np.load(embedding_path)\\n        self.codes: np.ndarray = embedding[embedding.files[0]]\\n        self.codebook: np.ndarray = embedding[embedding.files[1]]\\n        self.m = self.codes.shape[1]\\n        self.k = int(self.codebook.shape[0] / self.m)\\n        self.dim: int = self.codebook.shape[1]\\n\\n    def __load_vocab(self, vocab_path: str) -> Dict[str, int]:\\n        open_func: Callable = gzip.open if vocab_path.endswith(\".gz\") else open\\n        with open_func(vocab_path, \"rt\", encoding=\"utf-8\") as input_file:\\n            return {line.strip():idx for idx, line in enumerate(input_file)}\\n\\n    def vocab_vector(self, word: str):\\n        if word == \"<pad>\": return np.zeros(self.dim)\\n        val: str = word.lower() if self.to_lower else word\\n        index: int = self.vocab.get(val, self.vocab[\"<unk>\"])\\n        codes = self.codes[index]\\n        code_indices = np.array([idx * self.k + offset for idx, offset in enumerate(np.nditer(codes))])\\n        return np.sum(self.codebook[code_indices], axis=0)\\n\\n    def __getitem__(self, key):\\n        return self.vocab_vector(key)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_files_df.head()['content'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

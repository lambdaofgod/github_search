{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44dd107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/github_search\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1582be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training output/sbert/cross_encoder_repo_path_task_10k/ transformer model\n"
     ]
    }
   ],
   "source": [
    "from github_search.sentence_embeddings_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5964fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from github_search import paperswithcode_tasks, sentence_embeddings\n",
    "from github_search.sentence_embeddings import RNN_MODEL_TYPES\n",
    "\n",
    "logging.basicConfig(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5fea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at output/sbert/cross_encoder_repo_path_task_10k/ were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at output/sbert/cross_encoder_repo_path_task_10k/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = sentence_embeddings.make_model(\n",
    "    model_type,\n",
    "    w2v_file,\n",
    "    num_layers=num_layers,\n",
    "    n_hidden=n_hidden,\n",
    "    dropout=dropout,\n",
    "    max_seq_length=max_seq_length,\n",
    "    **pooling_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfced35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading readme-imports examples\n",
      "INFO:root:loading task-readme examples\n",
      "100%|██████████| 501512/501512 [00:09<00:00, 50185.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['latent variable models', '# PyTorch Implementation of Differentiable ODE Solvers This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through all solvers is supported using the adjoint method. For usage of ODE solvers in deep learning applications, see [1]. As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. --- <p align=\"center\"> <img align=\"middle\" src=\"./assets/resnet_0_viz.png\"'], ['multivariate time series forecasting', '# PyTorch Implementation of Differentiable ODE Solvers This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through all solvers is supported using the adjoint method. For usage of ODE solvers in deep learning applications, see [1]. As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. --- <p align=\"center\"> <img align=\"middle\" src=\"./assets/resnet_0_viz.png\"'], ['multivariate time series imputation', '# PyTorch Implementation of Differentiable ODE Solvers This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through all solvers is supported using the adjoint method. For usage of ODE solvers in deep learning applications, see [1]. As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. --- <p align=\"center\"> <img align=\"middle\" src=\"./assets/resnet_0_viz.png\"'], ['latent variable models', '# PyTorch Implementation of Differentiable ODE Solvers This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through all solvers is supported using the adjoint method. For usage of ODE solvers in deep learning applications, see [1]. As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. --- <p align=\"center\"> <img align=\"middle\" src=\"./assets/resnet_0_viz.png\"'], ['multivariate time series forecasting', '# PyTorch Implementation of Differentiable ODE Solvers This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through all solvers is supported using the adjoint method. For usage of ODE solvers in deep learning applications, see [1]. As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. --- <p align=\"center\"> <img align=\"middle\" src=\"./assets/resnet_0_viz.png\"']]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"loading readme-imports examples\")\n",
    "# imports_train_input_examples = get_sbert_inputs(\n",
    "#    , train_target_col, train_source_cols, model\n",
    "# )\n",
    "train_feature_col = \"readme\"\n",
    "\n",
    "logging.info(\"loading task-readme examples\")\n",
    "df = pd.read_feather(df_path)\n",
    "input_df = df.merge(paperswithcode_df, on=\"repo\")[[\"tasks\", train_feature_col]].explode(\"tasks\")\n",
    "tasks_train_input_examples = get_sbert_inputs(\n",
    "    input_df,\n",
    "    \"tasks\",\n",
    "    [train_feature_col],\n",
    "    model,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_input_examples = tasks_train_input_examples\n",
    "print([i.texts for i in train_input_examples[:5]])\n",
    "paperswithcode_df = pd.read_csv(paperswithcode_filepath)\n",
    "use_imports_as_doc = False\n",
    "if use_imports_as_doc:\n",
    "    test_df = get_merged_import_df(paperswithcode_df, pd.read_csv(df_path))\n",
    "    ir_evaluator = sentence_embeddings.get_ir_evaluator(test_df, \"tasks\", \"imports\")\n",
    "else:\n",
    "    ir_evaluator = sentence_embeddings.get_ir_evaluator(\n",
    "        input_df, doc_col=train_feature_col\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955a1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
